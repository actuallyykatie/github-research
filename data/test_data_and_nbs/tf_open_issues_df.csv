,full_name,repo_name,issue_number,user_login,issue_title,issue_body,num_comments,issue_assignee,issue_assignees,issue_created_at,issue_state,issue_closed_at,issue_closed_by,issue_labels,issue_updated_at
0,tensorflow/models,models,6550,austinmw,Object detection exported SavedModel input has no shape,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: V100, 16GB
- **Exact command to reproduce**:  `saved_model_cli show --dir <saved_model_path> --all`

### Describe the problem
The object detection API automatically exported a SavedModel with `variables` directory and `saved_model.pb` upon my training completion. When I run `saved_model_cli` on this directory, the SavedModel SignatureDef input has no shape:

```
signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['serialized_example'] tensor_info:
        dtype: DT_STRING
        shape: ()
        name: tf_example:0
```
Is this a bug or the intended result? The training size in my model .config file was set to `width: 640`, `height: 480`. Are there kwargs for `model_main.py` related to this?",0,,[],2019-04-09 21:12:48,open,,,[],2019-04-09 21:56:27
1,tensorflow/models,models,6549,JaosonMa,use the mobilenet_v2_140_224 on windows + opencv4.01 error!,"- **What is the top-level directory of the model you are using**:mobilenet_v2_1.4_224
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.10.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0 / 712
- **GPU model and memory**:12GB
- **Exact command to reproduce**:

I want use the mobilenet_v2_1.4_224 train my data ,because i am going to use the model on the 
widows CPU,  so i use the tensorflow's slim to train my model, train steps with sh code like this 

funtu last layers.
`CUDA_VISIBLE_DEVICES=1 python3 train_image_classifier.py \
   --train_dir=./data/flowers_2/models \
  --dataset_name=flowers \
  --dataset_split_name=train \
  --dataset_dir=./data/flowers_2 \
  --model_name=mobilenet_v2_140 \
  --checkpoint_path=./data/flowers_2/m_v2_1.4_224/mobilenet_v2_1.4_224.ckpt \
  --checkpoint_exclude_scopes=MobilenetV2/Logits,MobilenetV2/AuxLogits \
  --trainable_scopes=MobilenetV2/Logits,MobilenetV2/AuxLogits \
  --max_number_of_steps=10000 \
  --batch_size=32 \
  --learning_rate=0.01 \
  --learning_rate_decay_type=fixed \
  --save_interval_secs=60 \
  --save_summaries_secs=60 \
  --log_every_n_steps=100 \
  --optimizer=rmsprop \
  --weight_decay=0.00004
  `
all layers
`CUDA_VISIBLE_DEVICES=1 python3 train_image_classifier.py \
  --train_dir=./data/flowers_2/models/all \
  --dataset_name=flowers \
  --dataset_split_name=train \
  --dataset_dir=./data/flowers_2 \
  --model_name=mobilenet_v2_140 \
  --checkpoint_path=./data/flowers_2/models \
  --max_number_of_steps=100000 \
  --batch_size=16 \
  --learning_rate=0.0001 \
  --learning_rate_decay_type=fixed \
  --save_interval_secs=60 \
  --save_summaries_secs=60 \
  --log_every_n_steps=10 \
  --optimizer=rmsprop \
  --weight_decay=0.00004
`
### Describe the problem
after train over, i got the mode.ckpt , then i export_graph and freeze_graph with code like this:
`PRETRAINED_CHECKPOINT_DIR=./data/flowers_2/models/all
TRAIN_DIR=/tmp/flowers-models/mobilenet_v1_1.0_224
DATASET_DIR=./data/flowers_2
DATASET_NAME=flowers
INFER_DIR=./data/flowers_2/models/all/infer
MODEL_NAME=mobilenet_v2_140

echo ""create model.pb start""
CUDA_VISIBLE_DEVICES=0 python3 -u export_inference_graph.py \
    --model_name=${MODEL_NAME} \
    --output_file=${INFER_DIR}/flowers.pb \
    --dataset_name=${DATASET_NAME} \
    --dataset_dir=${DATASET_DIR}

echo ""start create frzee pb""
CUDA_VISIBLE_DEVICES=0 python3 -u /usr/local/lib/python3.5/dist-packages/tensorflow/python/tools/freeze_graph.py \
    --input_graph=${INFER_DIR}/flowers.pb \
    --input_checkpoint=${PRETRAINED_CHECKPOINT_DIR}/model.ckpt-44885 \
    --output_graph=${INFER_DIR}/my_freeze.pb \
    --input_binary=True \
    --output_node_name=MobilenetV2/Predictions/Reshape_1`
then i got the [flowers.pb](https://github.com/JaosonMa/for-models-download/blob/master/flowers.pb)
and the [my_freeze.pb](https://github.com/JaosonMa/for-models-download/blob/master/my_freeze.pb)

the use the  same code ,the got error :
![image](https://user-images.githubusercontent.com/23551774/55776601-e1879500-5acf-11e9-9f65-34c1bda09410.png)
it seems like that some layers was not found in opencv4.01,so i check the two pbfile(my_freeze.pb and mobilenet_v2_1.0_224_frozen.pb) with code like this:

`mobile_net = '../data/mobilenet_v2_1.4_224/mobilenet_v2_1.4_224_frozen.pb'
my_mobile_net = '../data/flowers_2/models/all/infer/my_freeze.pb'
def create_graph():
    with tf.gfile.FastGFile(my_mobile_net, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        tf.import_graph_def(graph_def, name='')
create_graph()
tensor_name_list_my = [tensor.name for tensor in tf.get_default_graph().as_graph_def().node]

print(len(tensor_name_list))
print(len(tensor_name_list_my))
for idx,tensor_name in enumerate(tensor_name_list_my):
    if(idx<len(tensor_name_list)):
        print(idx+1,""-->"",tensor_name,idx+1,""-->"",tensor_name_list[idx],'\n')
    else:
        print(idx+1,""-->"",tensor_name,'\n')
`
the length is not same ,
![image](https://user-images.githubusercontent.com/23551774/55777285-14328d00-5ad2-11e9-8fd2-cc32c41eb3eb.png)
...
![image](https://user-images.githubusercontent.com/23551774/55777307-2280a900-5ad2-11e9-96a5-938c4a5abee2.png)
...
![image](https://user-images.githubusercontent.com/23551774/55777323-2b717a80-5ad2-11e9-8c9b-ae41b0fa2276.png)

so i think some thing was wrong with my export_inference_graph.py, can you tell me how can i got the pb just as same as the tf's mobilenet_v1_1.0_224_.pb,

@tensorflowbutler 
",0,,[],2019-04-09 06:56:44,open,,,[],2019-04-09 07:08:23
2,tensorflow/models,models,6548,lindong28,Move metrics info from extras to metrics field in test_log.proto,,1,,[],2019-04-09 06:32:28,open,,,['cla: yes'],2019-04-09 07:05:37
3,tensorflow/models,models,6547,austinmw,[Feature Request] PR curves in object detection,"### System information
- **What is the top-level directory of the model you are using**: object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

The ""PR CURVES"" tab is not utilized in the object detection API. I found this [pr curves demo from tensorflow](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/pr_curve_demo.py). Any chance we could get this integrated into `eval_util.py` and `object_detection_evaluation.py`?",0,,[],2019-04-08 19:45:41,open,,,[],2019-04-08 19:46:18
4,tensorflow/models,models,6544,PetreanuAndi,ssdlite_mobilenet_v2_coco Corrupted Archive,"Hello.

After successfully downloading the "".tar.gz"" archive for the ssdlite_mobilenet_v2_coco, i can not extract it because the model file is corrupted. The actual error is : 

""gzip decompression failed""

Tried multiple archive-tools, same outcome. 
I think you should upload a new model. 

Thank you!",1,,[],2019-04-08 08:17:26,open,,,['stat:awaiting response'],2019-04-09 00:22:39
5,tensorflow/models,models,6543,AI-LastWish,What is the meaning of “second_stage_batch_size” setting when training with faster_rcnn_nas_coco model?,"I used Object detection API : to train faster_rcnn_nas model, but I got ""out-of-memory"" error, but when I add

    second_stage_batch_size: 4

The above line of code solve my problem and I'm training now. But I wonder what is the meaning of the above line?

Here is the definition of the above line: https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py, 

but I don't really understand. Any one can explain for me?

System information
What is the top-level directory of the model you are using:(I don't really understand the meaning of ""top-level directory of the model""): NA

I used faster_rcnn_nas_lowproposals_coco downloaded from
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md

OS Platform and Distribution:
Ubuntu 16.04
TensorFlow installed from:
GPU version of Tensorflow
TensorFlow version:
v1.12
Bazel version:
NA: don't know what is Bazel
CUDA/cuDNN version:
NA: don't know, but it's not the related to the problem
GPU model and memory:
Titan V
Exact command to reproduce:
NA",1,,[],2019-04-08 07:46:57,open,,,['stat:awaiting response'],2019-04-09 00:22:34
6,tensorflow/models,models,6542,2018daofeng,train,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-04-08 03:07:32,open,,,[],2019-04-08 03:07:32
7,tensorflow/models,models,6541,2018daofeng,train,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-04-08 03:07:21,open,,,[],2019-04-08 03:07:21
8,tensorflow/models,models,6540,2018daofeng,When run train.py  no error and no result,"INFO:tensorflow:Graph was finalized.
2019-04-08 10:49:14.407681: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-04-08 10:49:14.545614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.61GiB
2019-04-08 10:49:14.551962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-04-08 10:49:15.019684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-08 10:49:15.024102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-04-08 10:49:15.027101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-04-08 10:49:15.030463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6362 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
 
When the program executes here, the program ends, I can't find the problem!!!!!",1,,[],2019-04-08 03:06:54,open,,,['stat:awaiting response'],2019-04-09 00:22:29
9,tensorflow/models,models,6539,rabiaali95,Issue in Results--Object Detection API eval.py,"I have 203 test images and 3 classes. I have modified my config file for ssd_mobilenet_v1_coco accordingly. Now when i run eval.py, all test images are evaluated twice and i see results twice. Also i can not see all metrics like average recall and precision at different IoUs.
![img](https://user-images.githubusercontent.com/31028173/55693574-b7a87280-59ea-11e9-842b-de2e89b3be6e.png)


Does anyone know how to solve this issue?

Here is the modifies config file


model {
  ssd {
    num_classes: 3
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.03
              mean: 0.0
            }
          }
          batch_norm {
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v1'
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.9997,
          epsilon: 0.001,
        }
      }
    }
    loss {
      classification_loss {
        weighted_sigmoid {
        }
      }
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 0
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  batch_size: 10
  optimizer {
    rms_prop_optimizer: {
      learning_rate: {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.004
          decay_steps: 800720
          decay_factor: 0.95
        }
      }
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""ssd_mobilenet_v1_coco_11_06_2017/model.ckpt""
  from_detection_checkpoint: true
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""data/train.record""
  }
  label_map_path: ""data/object-detection.pbtxt""
}

eval_config: {
  num_examples: 203
  num_visualizations: 203
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""data/test.record""
  }
  label_map_path: ""data/object-detection.pbtxt""
  shuffle: false
  num_readers: 1
}
What is the top-level directory of the model you are using: models-master/research/object_detection
Have I written custom code: No (The changes in config file are as above)
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: GPU version of Tensorflow
TensorFlow version: 1.12.0
Bazel version: NA: don't know what is Bazel
CUDA/cuDNN version: NA (not the related to the problem)
GPU model and memory: Titan V
Exact command to reproduce: 
    python3  eval.py \
    --logtostderr \
    --checkpoint_dir=training \
    --eval_dir=eval \
    --pipeline_config_path=training/ssd_mobilenet_v1_coco.config

",1,,[],2019-04-08 01:37:34,open,,,['stat:awaiting response'],2019-04-09 01:13:05
10,tensorflow/models,models,6538,ndvbd,Syntaxnet Binary Installation (Ubuntu 16) - Running demo.sh,"I am trying to follow the binary installation guidelines of the SyntaxNet. After upgrading libc to 3.4 and successfully running the 5 commands shown in the readme, I am trying to see if Syntaxnet works. 

When I am doing: `echo 'test' | syntaxnet/demo.sh`
I get:

```
syntaxnet/demo.sh: line 31: bazel-bin/syntaxnet/parser_eval: No such file or directory
syntaxnet/demo.sh: line 43: bazel-bin/syntaxnet/parser_eval: No such file or directory
syntaxnet/demo.sh: line 55: bazel-bin/syntaxnet/conll2tree: No such file or directory

```
Please advise how to test/use the syntaxnet after I did the manual installation.
In addition, if my computer has GPU, can I assume the binary installation instructions will make the syntaxnet to use the GPU?

------------------------
### System information
- **What is the top-level directory of the model you are using**: - models/research/syntaxnet
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: - Ubuntu  16
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Bazel version (if compiling from source)**: 0.24.1
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Tesla K80


",1,,[],2019-04-07 20:18:47,open,,,['stat:awaiting response'],2019-04-08 12:19:57
11,tensorflow/models,models,6537,austinmw,[Feature Request] Object Detection API Mixed Precision,"What is the top-level directory of the model you are using: N/A
Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: N/A
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A


### Describe the problem
Training times are very slow if you're not able to put your data on a TPU. No multi-GPU or mixed precision training is available. It sounds like multi-GPU support is going to stay broken for a while. Is there any chance that mixed precision is easier to implement?

",1,,[],2019-04-07 16:18:49,open,,,['stat:awaiting response'],2019-04-08 18:32:14
12,tensorflow/models,models,6536,bubbles1990,"object_detection: Using IoU localization loss raises ""NaN loss during training""","### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.13.0-dev20181228
- **Bazel version (if compiling from source)**: --
- **CUDA/cuDNN version**: 10.0/7.3.0
- **GPU model and memory**: GTX 1080 Ti 11GB
- **Exact command to reproduce**: python object_detection/model_main.py --model_dir=$MODEL_DIR --pipeline_config_path=$PIPELINE_CONFIG_PATH

### Describe the problem
When trying to use IoU localization loss, i.e. use these lines in the config file:
```
localization_loss {
    weighted_iou {
   }
}
```
After several training steps (usually 0 - 100) I always get the error:  `tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.`

If I add a small constant (e.g. 1e-7) to the denominator in `matched_iou` (in [`object_detection/core/box_list_ops.py`](https://github.com/tensorflow/models/blob/master/research/object_detection/core/box_list_ops.py#L277)) like so:
```
def matched_iou(boxlist1, boxlist2, scope=None):
  """"""Compute intersection-over-union between corresponding boxes in boxlists.

  Args:
    boxlist1: BoxList holding N boxes
    boxlist2: BoxList holding N boxes
    scope: name scope.

  Returns:
    a tensor with shape [N] representing pairwise iou scores.
  """"""
  with tf.name_scope(scope, 'MatchedIOU'):
    intersections = matched_intersection(boxlist1, boxlist2)
    areas1 = area(boxlist1)
    areas2 = area(boxlist2)
    unions = areas1 + areas2 - intersections
    eps = 1e-7  # <- this is the small constant
    return tf.where(
        tf.equal(intersections, 0.0),
        tf.zeros_like(intersections), tf.truediv(intersections, unions + eps))
```
The `NaN` error is gone but loss values are really high and the training doesn't converge.

Did anyone experienced this issue or has any experience with IoU localization loss?
Thank you.
",0,,[],2019-04-07 09:09:16,open,,,[],2019-04-07 09:09:16
13,tensorflow/models,models,6535,seemuch,Added unit tests keras cifar and imagenet,,2,,[],2019-04-06 20:29:47,open,,,['cla: yes'],2019-04-09 19:05:01
14,tensorflow/models,models,6534,dannygoodmangs,"Fix double logging, unicode/python 3 compatibility, and evaluate_precision_recall","This fixes #3137 #5369 and allows the use of ObjectDetectionEvaluator instances with evaluate_precision_recall=True
",3,,[],2019-04-05 23:27:27,open,,,['cla: no'],2019-04-09 22:39:07
15,tensorflow/models,models,6531,ndvbd,tensorflow_model_server,Can the SyntaxNet be served (as a web service) with tensorflow_model_server?,2,,[],2019-04-05 15:11:40,open,,,[],2019-04-08 12:19:37
16,tensorflow/models,models,6526,srinivas491-oneconvergence,AttributeError 'module' object has no attribute 'experimental': during mnist training: ,"##problem Description
I'm trying to train mnist model by running,

`python mnist.py`

Error message I'm getting,
`Traceback (most recent call last):
  File ""mnist.py"", line 27, in <module>
    from official.utils.misc import distribution_utils
  File ""/root/srinivas/TensorFlow_Models/models/official/utils/misc/distribution_utils.py"", line 28, in <module>
    None: tf.distribute.experimental.CollectiveCommunication.AUTO,
AttributeError: 'module' object has no attribute 'experimental'
`


##System Information:
o Python version: 2.7.15
o TensorFlow version: ('v1.13.1-0-g6612da8951', '1.13.1') (Installed through binary)
o OS: NAME=""Ubuntu""
      VERSION=""16.04.6 LTS (Xenial Xerus)""
o Command to reproduce: `python mnist.py`
o GPU : GeForce GTX 1060 6GB
o Cuda version: Cuda-9.0

",1,,[],2019-04-04 19:48:37,open,,,['stat:awaiting response'],2019-04-05 12:18:36
17,tensorflow/models,models,6525,krishpop,update mujoco and tensorflow 1.14/2.0 api calls in efficient-hrl,"Fixes issue addressed in #5321, updating mujoco calls to `self.data` and TensorFlow calls to `tf.CriticalSection` to reflect 1.14/2.0 standard.",0,,[],2019-04-04 19:18:10,open,,,['cla: yes'],2019-04-04 19:20:39
18,tensorflow/models,models,6523,tilaba,Tensorrt INT8 calibration for caffe-yolov3 failed ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,[],2019-04-04 08:20:54,open,,,[],2019-04-04 08:21:02
19,tensorflow/models,models,6522,xunnv,TypeError: 'dict_keys' object does not support indexing,"When I try to train the `ssd_mobilenet_v1_coco` downloaded from object_detection model zoo,
with `samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config`.
There occured an TypeError as mentioned above, so I use list() function to wrap the dict_keys object of the following line in `models/feature_map_generators.py`:
```python
image_features = image_features[image_features.keys()[0]]
```

problem solved

Then I think, maybe there are some other similar problem which not have been fixed, so I use grep recursively search them and fix them. Hopefully, I have fixed most of that.",3,,[],2019-04-04 04:45:53,open,,,['cla: yes'],2019-04-04 05:10:18
20,tensorflow/models,models,6521,FrankYu0502,Tensorflow r1.13 Bazel Build Failure,"OS: Linux version 3.18.6-2.el7.centos.x86_64
Bazel : Build label: 0.20.0
gcc (GCC) 7.2.1 20170829 (Red Hat 7.2.1-1)
Python 2.7.16 :: Anaconda, Inc.

ERROR: /root/.cache/bazel/_bazel_root/9fd418c66caf2752d640a3e143a42ab2/external/org_tensorflow/tensorflow/core/kernels/BUILD:3206:1: C++ compilation of rule '@org_tensorflow//tensorflow/core/kernels:reduction_ops' failed (Exit 1)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:124:0,
                 from external/org_tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from external/org_tensorflow/tensorflow/core/kernels/reduction_ops_common.h:27,
                 from external/org_tensorflow/tensorflow/core/kernels/reduction_ops_sum.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In static member function 'static void std::_Function_handler<void(_ArgTypes ...), _Functor>::_M_invoke(const std::_Any_data&, _ArgTypes&& ...) [with _Functor = Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable, Tileable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >; bool Vectorizable = true; bool Tileable = false]::<lambda(Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex, Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex)>; _ArgTypes = {long int, long int}]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801:9: internal compiler error: in emit_move_insn, at expr.c:3698
         values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,
         ^~~~~~
Please submit a full bug report,
with preprocessed source if appropriate.
See <http://bugzilla.redhat.com/bugzilla> for instructions.
Preprocessed source stored into /tmp/cc383lvq.out file, please attach this to your bugreport.
INFO: Elapsed time: 548.113s, Critical Path: 209.08s
INFO: 4386 processes: 4386 local.
FAILED: Build did NOT complete successfully",1,,[],2019-04-04 03:21:32,open,,,['stat:awaiting response'],2019-04-05 00:21:45
21,tensorflow/models,models,6519,fboylu,Mask RCNN predict_instance_masks: false KeyError: 'mask_predictions',"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: v1.12.0-0-ga6d8ffae09 1.12.0
- **CUDA/cuDNN version**: CUDA 9.0/cuDNN 7.2.1
- **GPU model and memory**: K80 x2
- **Bazel version**: N/A
- **Exact command to reproduce**: python object_detection/model_main.py 

### Describe the problem
I am trying to train mask_rcnn_inception_v2_coco_2018_01_28 with my own dataset that doesn't have masks and I have set predict_instance_masks: false as well as load_instance_masks: false in the config file however I am getting the error below. I appreciate if you can help me understand why I am getting the error. Thanks.


### Source code / logs
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/data/anaconda/envs/tfod/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/data/anaconda/envs/tfod/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/data/anaconda/envs/tfod/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/data/anaconda/envs/tfod/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/data/anaconda/envs/tfod/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/anaconda/envs/tfod/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/data/anaconda/envs/tfod/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1237, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/data/anaconda/envs/tfod/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/datadrive/models/research/object_detection/model_lib.py"", line 288, in model_fn
    features[fields.InputDataFields.true_image_shape])
  File ""/datadrive/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 692, in predict
    prediction_dict, true_image_shapes)
  File ""/datadrive/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 880, in _predict_third_stage
    box_predictor.MASK_PREDICTIONS], axis=1)
KeyError: 'mask_predictions'
",2,,[],2019-04-03 21:06:27,open,,,[],2019-04-05 12:18:25
22,tensorflow/models,models,6516,tilaba,INT8 calibartion for tensorrt-yolov3 failed,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,[],2019-04-03 15:25:14,open,,,[],2019-04-03 15:35:47
23,tensorflow/models,models,6515,lxs802lxs8858,InvalidArgumentError (see above for traceback): padded_shape[0]=257 is not divisible by block_shape[0]=2,"INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting evaluation at 2019-04-03-13:08:05
Traceback (most recent call last):
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[0]=257 is not divisible by block_shape[0]=2
	 [[Node: xception_65/exit_flow/block2/unit_1/xception_module/separable_conv1_depthwise/depthwise/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](xception_65/exit_flow/block1/unit_1/xception_module/add-0-0-TransposeNCHWToNHWC-LayoutOptimizer, xception_65/exit_flow/block2/unit_1/xception_module/separable_conv1_depthwise/depthwise/SpaceToBatchND/block_shape, xception_65/exit_flow/block2/unit_1/xception_module/separable_conv1_depthwise/depthwise/SpaceToBatchND/paddings)]]
	 [[Node: mean_iou/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch/_4465 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2128_...ert/Switch"", tensor_type=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""deeplab/eval.py"", line 179, in <module>
    tf.app.run()
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""deeplab/eval.py"", line 172, in main
    eval_interval_secs=FLAGS.eval_interval_secs)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/training/python/training/evaluation.py"", line 449, in evaluate_repeatedly
    session.run(eval_ops, feed_dict)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 577, in run
    run_metadata=run_metadata)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1053, in run
    run_metadata=run_metadata)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1144, in run
    raise six.reraise(*original_exc_info)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1129, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1201, in run
    run_metadata=run_metadata)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 981, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[0]=257 is not divisible by block_shape[0]=2
	 [[Node: xception_65/exit_flow/block2/unit_1/xception_module/separable_conv1_depthwise/depthwise/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](xception_65/exit_flow/block1/unit_1/xception_module/add-0-0-TransposeNCHWToNHWC-LayoutOptimizer, xception_65/exit_flow/block2/unit_1/xception_module/separable_conv1_depthwise/depthwise/SpaceToBatchND/block_shape, xception_65/exit_flow/block2/unit_1/xception_module/separable_conv1_depthwise/depthwise/SpaceToBatchND/paddings)]]
	 [[Node: mean_iou/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch/_4465 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2128_...ert/Switch"", tensor_type=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'xception_65/exit_flow/block2/unit_1/xception_module/separable_conv1_depthwise/depthwise/SpaceToBatchND', defined at:
  File ""deeplab/eval.py"", line 179, in <module>
    tf.app.run()
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""deeplab/eval.py"", line 118, in main
    image_pyramid=FLAGS.image_pyramid)
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/model.py"", line 183, in predict_labels
    fine_tune_batch_norm=True)
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/model.py"", line 313, in multi_scale_logits
    nas_training_hyper_parameters=nas_training_hyper_parameters)
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/model.py"", line 553, in _get_logits
    nas_training_hyper_parameters=nas_training_hyper_parameters)
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/model.py"", line 395, in extract_features
    use_bounded_activation=model_options.use_bounded_activation)
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/core/feature_extractor.py"", line 341, in extract_features
    scope=name_scope[model_variant])
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/core/feature_extractor.py"", line 408, in network_fn
    *args, **kwargs)
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/core/xception.py"", line 655, in xception_65
    scope=scope)
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/core/xception.py"", line 464, in xception
    net = stack_blocks_dense(net, blocks, output_stride)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/core/xception.py"", line 379, in stack_blocks_dense
    net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/core/xception.py"", line 293, in xception_module
    scope='separable_conv' + str(i+1))
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/core/xception.py"", line 284, in _separable_conv
    scope=scope)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/core/xception.py"", line 185, in separable_conv2d_same
    outputs = _split_separable_conv2d(padding='SAME')
  File ""/home/kt210/Person/amy/deeplab/models-master/research/deeplab/core/xception.py"", line 175, in _split_separable_conv2d
    **kwargs)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 2810, in separable_convolution2d
    data_format=data_format)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py"", line 461, in depthwise_conv2d
    op=op)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 364, in with_space_to_batch
    return new_op(input, None)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 520, in __call__
    return self.call(inp, filter)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 503, in _with_space_to_batch_call
    input=inp, block_shape=dilation_rate, paddings=paddings)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 7570, in space_to_batch_nd
    paddings=paddings, name=name)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/home/kt210/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): padded_shape[0]=257 is not divisible by block_shape[0]=2
	 [[Node: xception_65/exit_flow/block2/unit_1/xception_module/separable_conv1_depthwise/depthwise/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](xception_65/exit_flow/block1/unit_1/xception_module/add-0-0-TransposeNCHWToNHWC-LayoutOptimizer, xception_65/exit_flow/block2/unit_1/xception_module/separable_conv1_depthwise/depthwise/SpaceToBatchND/block_shape, xception_65/exit_flow/block2/unit_1/xception_module/separable_conv1_depthwise/depthwise/SpaceToBatchND/paddings)]]
	 [[Node: mean_iou/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch/_4465 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2128_...ert/Switch"", tensor_type=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
",2,,[],2019-04-03 13:16:39,open,,,[],2019-04-08 00:14:52
24,tensorflow/models,models,6514,Prilyf,layer speed,How to obtain the speed of each layer of the model in the inference process?,1,,[],2019-04-03 10:02:18,open,,,['stat:awaiting response'],2019-04-04 00:21:24
25,tensorflow/models,models,6513,ZisIsNotZis,"Line 182 in `caption_generator.py` of `im2txt` should be `[-self.beam_size:]` instead of `[:-self.beam_size]` since the comment above explicitly says the ""last `beam_size` elements""","### System information
- **What is the top-level directory of the model you are using**: research/im2txt
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary, anaconda
- **TensorFlow version (use command below)**: b'unknown' 1.12.0
- **Bazel version (if compiling from source)**:  0.22.0- (@non-git)
- **CUDA/cuDNN version**: 10.1
- **GPU model and memory**:  GeForce GTX 1080 / 8116MiB
- **Exact command to reproduce**: N/A

### Describe the problem
Line 182 in `caption_generator.py` of `im2txt` should be `[-self.beam_size:]` instead of `[:-self.beam_size]`, since the comment above explicitly says the ""last `beam_size` elements"".

        # For this partial caption, get the beam_size most probable next words.
        # Sort the indexes with numpy, select the last self.beam_size
        # (3 by default) (ie, the most likely) and then reverse the sorted
        # indexes with [::-1] to sort them from higher to lower.
        most_likely_words = np.argsort(word_probabilities)[:-self.beam_size][::-1]

`run_inference` will output length 1 description without this, like

    Captions for image COCO_val2014_000000224477.jpg:
      0)  (p=0.000000)
      1) surfer (p=0.000000)
      2) man (p=0.000000)

while it will output much more ""normal"" output with this:

    Captions for image COCO_val2014_000000224477.jpg:
      0) a person riding a surf board on a wave <S> <S> <S> <S> <S> <S> <S> <S> . (p=0.000896)
      1) a person riding a surf board on a wave <S> <S> <S> <S> <S> <S> . <S> <S> (p=0.000732)
      2) a person riding a surf board on a wave <S> <S> <S> <S> <S> . <S> <S> <S> (p=0.000456)

### Source code / logs
N/A",0,,[],2019-04-03 09:08:47,open,,,[],2019-04-03 09:10:39
26,tensorflow/models,models,6512,tonychen257,GCP Pet training failed,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2,,[],2019-04-03 08:17:58,open,,,[],2019-04-03 08:22:31
27,tensorflow/models,models,6511,caoxu915683474,some questions about dropout in VAT,"hello, I have some questions about dropout in VAT.

If I use dropout in VAT, the output distribution will change even without perturbation.

thanks!",1,,[],2019-04-03 03:40:26,open,,,['stat:awaiting response'],2019-04-04 00:21:19
28,tensorflow/models,models,6509,ToddMorrill,steps_per_epoch not honored in tf.keras.fit ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

## Please refer to this issue: https://github.com/tensorflow/tensorflow/issues/24075


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-04-02 22:11:33,open,,,[],2019-04-02 22:11:33
29,tensorflow/models,models,6504,Argantonio65,[Deeplab] SemSeg Error at eval/vis when using a pre-trained model for inference in a tailored dataset,"### System information
- **What is the top-level directory of the model you are using**:
C:\tensorflow
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 8.1
- **TensorFlow installed from (source or binary)**:
pip install tensorflow (only CPU)
- **TensorFlow version (use command below)**:
version = 1.13.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
Only CPU
- **GPU model and memory**:
- **Exact command to reproduce**:
sh local_test.sh

### Describe the problem
I used a pre-trained model (deeplab3_pascal_train_aug) to perform semantic segmentation in my own dataset (one label + background, num_classes = 2) by retraining only the last layer. Training seems to go through without error. However, visualization or evaluation gives: Invalid argument: padded_shape[X] = Y is not divisible by block_shape[X] = Z. No matter what I do, I keep getting this error.

My database is composed by images of size [640, 480]. I followed the instructions to build the ground truth images with labels [background = 0, label_1 = 1] and create the tfrecords etc. I saw in previous issue reports that the selection of crop-size during evaluation has to cover the full image. Thus I increased the value eval_crop_size (641 = k * 16 +1 > imagesize) to ensure that it is larger than any dimension of the image.

Running the original local_test.sh works fine.

### Source code / logs
Error for the visualization call: 
```
...
Caused by op 'xception_65/exit_flow/block2/unit_1/xception_module/separable_conv
1_depthwise/depthwise/SpaceToBatchND', defined at:
  File ""C:/tensorflow/models/research/deeplab/vis.py"", line 312, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\python\
platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:/tensorflow/models/research/deeplab/vis.py"", line 230, in main
    image_pyramid=FLAGS.image_pyramid)
  File ""C:\tensorflow\models\research\deeplab\model.py"", line 183, in predict_la
bels
    fine_tune_batch_norm=False)
  File ""C:\tensorflow\models\research\deeplab\model.py"", line 313, in multi_scal
e_logits
    nas_training_hyper_parameters=nas_training_hyper_parameters)
  File ""C:\tensorflow\models\research\deeplab\model.py"", line 553, in _get_logit
s
    nas_training_hyper_parameters=nas_training_hyper_parameters)
  File ""C:\tensorflow\models\research\deeplab\model.py"", line 395, in extract_fe
atures
    use_bounded_activation=model_options.use_bounded_activation)
  File ""C:\tensorflow\models\research\deeplab\core\feature_extractor.py"", line 3
41, in extract_features
    scope=name_scope[model_variant])
  File ""C:\tensorflow\models\research\deeplab\core\feature_extractor.py"", line 4
08, in network_fn
    *args, **kwargs)
  File ""C:\tensorflow\models\research\deeplab\core\xception.py"", line 655, in xc
eption_65
    scope=scope)
  File ""C:\tensorflow\models\research\deeplab\core\xception.py"", line 464, in xc
eption
    net = stack_blocks_dense(net, blocks, output_stride)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\contrib
\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""C:\tensorflow\models\research\deeplab\core\xception.py"", line 379, in st
ack_blocks_dense
    net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\contrib
\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""C:\tensorflow\models\research\deeplab\core\xception.py"", line 293, in xc
eption_module
    scope='separable_conv' + str(i+1))
  File ""C:\tensorflow\models\research\deeplab\core\xception.py"", line 284, in _s
eparable_conv
    scope=scope)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\contrib
\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""C:\tensorflow\models\research\deeplab\core\xception.py"", line 185, in se
parable_conv2d_same
    outputs = _split_separable_conv2d(padding='SAME')
  File ""C:\tensorflow\models\research\deeplab\core\xception.py"", line 175, in _s
plit_separable_conv2d
    **kwargs)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\contrib
\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\contrib
\layers\python\layers\layers.py"", line 2822, in separable_convolution2d
    data_format=data_format)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\python\
ops\nn_impl.py"", line 522, in depthwise_conv2d
    op=op)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\python\
ops\nn_ops.py"", line 435, in with_space_to_batch
    return new_op(input, None)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\python\
ops\nn_ops.py"", line 591, in __call__
    return self.call(inp, filter)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\python\
ops\nn_ops.py"", line 574, in _with_space_to_batch_call
    input=inp, block_shape=dilation_rate, paddings=paddings)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\python\
ops\gen_array_ops.py"", line 8648, in space_to_batch_nd
    paddings=paddings, name=name)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\python\
framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\python\
util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\python\
framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\envs\delta\lib\site-packages\tensorflow\python\
framework\ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): padded_shape[0]=127 is not divis
ible by block_shape[0]=2
         [[node xception_65/exit_flow/block2/unit_1/xception_module/separable_co
nv1_depthwise/depthwise/SpaceToBatchND (defined at C:\tensorflow\models\research
\deeplab\core\xception.py:175) ]]
```

My code:

changes in data_generator.py
```
_MyDataset = DatasetDescriptor(
    splits_to_sizes={
        'train': 125,  # num of samples in images/training
        'val': 125,  # num of samples in images/validation
        'trainval': 250,  # num of samples in images/validation
    },
    num_classes=2,
    ignore_label=255,
)
_DATASETS_INFORMATION = {
    'cityscapes': _CITYSCAPES_INFORMATION,
    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
    'ade20k': _ADE20K_INFORMATION,
    'MyDataset': _MyDataset
}
```
My modified local_test.sh script:
```
cd ..
# Set up the working environment.
CURRENT_DIR=$(pwd)
WORK_DIR=""${CURRENT_DIR}/deeplab""
DATASET_DIR=""datasets""

# Set up the working directories.
PASCAL_FOLDER=""MyDataset""
EXP_FOLDER=""exp/train_on_trainval_set""
PASCAL_DATASET=""C:\tensorflow\models\research\deeplab\datasets\MyDataset\tfrecord""

INIT_FOLDER=""C:/tensorflow\models\research\deeplab\datasets\pascal_voc_seg\init_models""
EVAL_LOGDIR=""${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/${EXP_FOLDER}/eval""
TRAIN_LOGDIR=""${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/${EXP_FOLDER}/train""
VIS_LOGDIR=""${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/${EXP_FOLDER}/vis""
EXPORT_LOGDIR=""${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/${EXP_FOLDER}/export""


mkdir -p ""${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/exp""
mkdir -p ""${TRAIN_LOGDIR}""

NUM_ITERATIONS=10
python ""${WORK_DIR}""/train.py \
  --logtostderr \
  --train_split=""train"" \
  --model_variant=""xception_65"" \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=16 \
  --decoder_output_stride=4 \
  --train_crop_size=321 \
  --train_crop_size=321 \
  --train_batch_size=1 \
  --training_number_of_steps=""${NUM_ITERATIONS}"" \
  --fine_tune_batch_norm=false \
  --initialize_last_layer=false \
  --last_layers_contain_logits_only=true \
  --dataset=""${PASCAL_FOLDER}"" \
  --tf_initial_checkpoint=""${INIT_FOLDER}/deeplabv3_pascal_train_aug/model.ckpt"" \
  --train_logdir=""${TRAIN_LOGDIR}"" \
  --dataset_dir=""${PASCAL_DATASET}""


# Run evaluation. 
python ""${WORK_DIR}""/eval.py \
  --logtostderr \
  --eval_split=""val"" \
  --model_variant=""xception_65"" \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=16 \
  --decoder_output_stride=4 \
  --eval_crop_size=641 \ 
  --eval_crop_size=641 \
  --checkpoint_dir=""${TRAIN_LOGDIR}"" \
  --eval_logdir=""${EVAL_LOGDIR}"" \
  --dataset_dir=""${PASCAL_DATASET}"" \
  --dataset=""${PASCAL_FOLDER}"" \
  --max_number_of_evaluations=1 

  # Visualize the results.
python ""${WORK_DIR}""/vis.py \
  --logtostderr \
  --vis_split=""val"" \
  --model_variant=""xception_65"" \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=16 \
  --decoder_output_stride=4 \
  --vis_crop_size=641\
  --vis_crop_size=641\
  --checkpoint_dir=""${TRAIN_LOGDIR}"" \
  --vis_logdir=""${VIS_LOGDIR}"" \
  --dataset_dir=""${PASCAL_DATASET}"" \
  --dataset=""${PASCAL_FOLDER}"" \
  --max_number_of_iterations=1
```
",0,,[],2019-04-01 15:00:05,open,,,[],2019-04-01 19:13:15
30,tensorflow/models,models,6503,Atom-101,Upgraded wide_deep model for Tf 2.0,"The model uses canned estimators. The tensorflow_estimators package obtainable from pip didn't work with Tf 2.0. Building the latest version of the estimators repository from source is required.

Had to remove train hook from model training code. Keeping it raises a KeyError .

Example error for deep model:
`""The name 'dnn/head/truediv:0' refers to a Tensor which does not exist. The operation, 'dnn/head/truediv', does not exist in the graph."" `

The final accuracy for census model is in line with what is mentioned in the README.
The final evaluation for a census wide model:
`I0401 18:03:14.976323 139656691826816 logger.py:147] Benchmark metric: {'name': 'accuracy', 'value': 0.8340396881103516, 'unit': None, 'global_step': 32580, 'timestamp': '2019-04-01T12:33:14.976269Z', 'extras': []}`

`I0401 18:03:14.976451 139656691826816 logger.py:147] Benchmark metric: {'name': 'accuracy_baseline', 'value': 0.7637737393379211, 'unit': None, 'global_step': 32580, 'timestamp': '2019-04-01T12:33:14.976425Z', 'extras': []}`

`I0401 18:03:14.976567 139656691826816 logger.py:147] Benchmark metric: {'name': 'auc', 'value': 0.8789075613021851, 'unit': None, 'global_step': 32580, 'timestamp': '2019-04-01T12:33:14.976542Z', 'extras': []}`

`I0401 18:03:14.976674 139656691826816 logger.py:147] Benchmark metric: {'name': 'auc_precision_recall', 'value': 0.6867934465408325, 'unit': None, 'global_step': 32580, 'timestamp': '2019-04-01T12:33:14.976653Z', 'extras': []}`

`I0401 18:03:14.976778 139656691826816 logger.py:147] Benchmark metric: {'name': 'average_loss', 'value': 0.359136164188385, 'unit': None, 'global_step': 32580, 'timestamp': '2019-04-01T12:33:14.976758Z', 'extras': []}`

`I0401 18:03:14.976883 139656691826816 logger.py:147] Benchmark metric: {'name': 'label/mean', 'value': 0.23622627556324005, 'unit': None, 'global_step': 32580, 'timestamp': '2019-04-01T12:33:14.976863Z', 'extras': []}`

`I0401 18:03:14.976987 139656691826816 logger.py:147] Benchmark metric: {'name': 'loss', 'value': 14.331116676330566, 'unit': None, 'global_step': 32580, 'timestamp': '2019-04-01T12:33:14.976967Z', 'extras': []}`

`I0401 18:03:14.977092 139656691826816 logger.py:147] Benchmark metric: {'name': 'precision', 'value': 0.6924629807472229, 'unit': None, 'global_step': 32580, 'timestamp': '2019-04-01T12:33:14.977071Z', 'extras': []}`

`I0401 18:03:14.977226 139656691826816 logger.py:147] Benchmark metric: {'name': 'prediction/mean', 'value': 0.22958678007125854, 'unit': None, 'global_step': 32580, 'timestamp': '2019-04-01T12:33:14.977175Z', 'extras': []}`

`I0401 18:03:14.977340 139656691826816 logger.py:147] Benchmark metric: {'name': 'recall', 'value': 0.5351014137268066, 'unit': None, 'global_step': 32580, 'timestamp': '2019-04-01T12:33:14.977319Z', 'extras': []}`

  ",2,,[],2019-04-01 14:15:58,open,,,['cla: yes'],2019-04-03 20:10:54
31,tensorflow/models,models,6502,jiangjiajun,download_and_preprocess_imagenet.sh not found,"in `ReadMe.md` of models/research/slim
```
# location of where to place the ImageNet data
DATA_DIR=$HOME/imagenet-data

# build the preprocessing script.
bazel build slim/download_and_preprocess_imagenet

# run it
bazel-bin/slim/download_and_preprocess_imagenet ""${DATA_DIR}""
```
but there is no script named `download_and_preprocess_imagenet.sh` now",1,,[],2019-04-01 12:56:56,open,,,[],2019-04-02 02:57:16
32,tensorflow/models,models,6501,Sergei-Lebedev,Resnet model creates new nccl communicator for each allreduce,"### System information
- **What is the top-level directory of the model you are using**:
models/official/resnet/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 2.0 alpha
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 10.0/
- **GPU model and memory**: V100 16GB
- **Exact command to reproduce**: NCCL_DEBUG=info NCCL_DEBUG_SUBSYS=coll python imagenet_main.py --num_gpus=2 --use_synthetic_data --train_epochs=1 --max_train_steps=1000 --use_train_and_evaluate=True  --distribution_strategy=multi_worker_mirrored --all_reduce_alg=nccl --worker_hosts=""SOME_HOSTS"" --task_index=SOME_INDEX

### Describe the problem
It looks like that resnet model (or TensorFlow itself?) creates new nccl communicator for each collective operation which is too expensive for distributed computations. Is it a bug in TensorFlow?

### Source code / logs
I0401 05:03:06.554658 140477256177408 session_manager.py:500] Running local_init_op.
I0401 05:03:06.692303 140477256177408 session_manager.py:502] Done running local_init_op.
I0401 05:03:11.522593 140477256177408 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /tmp/model.ckpt.
[0] NCCL INFO NET/Socket : Using [0]ib0:21.21.21.61<0> [1]ib2:22.22.22.61<0> [2]ib4:23.23.23.61<0> [3]ib6:24.24.24.61<0>
[0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
[0] NCCL INFO NET/IB : Using [0]mlx5_6:1/IB [1]mlx5_4:1/IB [2]mlx5_2:1/IB [3]mlx5_0:1/IB ; OOB ib0:21.21.21.61<0>
[0] NCCL INFO Using network IB
NCCL version 2.4.3+cuda10.0
[0] NCCL INFO Setting affinity for GPU 0 to 3ffff0,0003ffff
[0] NCCL INFO comm 0x7fb840002a10 rank 0 nranks 2 cudaDev 0 nvmlDev 0
[0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  SOC SOC SOC PIX
[0] NCCL INFO Channel 00 :    0   1
[0] NCCL INFO Channel 01 :    0   1
[0] NCCL INFO Ring 00 : 1 -> 0 [receive] via NET/IB/3
[0] NCCL INFO Ring 00 : 0 -> 1 [send] via NET/IB/3
[0] NCCL INFO Ring 01 : 1 -> 0 [receive] via NET/IB/3
[0] NCCL INFO Ring 01 : 0 -> 1 [send] via NET/IB/3
[0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
[0] NCCL INFO comm 0x7fb840002a10 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE
**[0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fba1e1f6300 recvbuff 0x7fba2b49f200 count 64 datatype 7 op 0 root 0 comm 0x7fb840002a10 [nranks=2] stream 0x7fb4400014a0**
[0] NCCL INFO Launch mode Parallel
[0] NCCL INFO Setting affinity for GPU 0 to 3ffff0,0003ffff
[0] NCCL INFO comm 0x7fb84002da90 rank 0 nranks 2 cudaDev 0 nvmlDev 0
[0] NCCL INFO Channel 00 :    0   1
[0] NCCL INFO Channel 01 :    0   1
[0] NCCL INFO Ring 00 : 1 -> 0 [receive] via NET/IB/3
[0] NCCL INFO Ring 00 : 0 -> 1 [send] via NET/IB/3
[0] NCCL INFO Ring 01 : 1 -> 0 [receive] via NET/IB/3
[0] NCCL INFO Ring 01 : 0 -> 1 [send] via NET/IB/3
[0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
[0] NCCL INFO comm 0x7fb84002da90 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE
**[0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fba1e1f6400 recvbuff 0x7fba2b49f300 count 64 datatype 7 op 0 root 0 comm 0x7fb84002da90 [nranks=2] stream 0x7fb4400014a0**",0,,[],2019-04-01 12:04:24,open,,,[],2019-04-01 12:04:24
33,tensorflow/models,models,6500,wfzliuchen,im2txt:result is not correct,"after rename_ckpt.py ,newmodel.ckpt-2000000.data-00000-of-00001 is generated。
But,by running  run_inference.py ,the result below seems like uncorrect!

Captions for image 4.jpg:
  0)  (p=0.000030)
  1) an (p=0.000001)
  2) the (p=0.000000)

tensorflow version:1.12  CPU
window8 
pycharm
without bazel",1,,[],2019-04-01 08:20:02,open,,,['stat:awaiting response'],2019-04-02 09:53:00
34,tensorflow/models,models,6499,nijatmursali,Why tensorflow lags with our own model?,"Hello,

I have trained my own model and met problem with camera lag while detecting the objects on webcam. I didn't have any problem with Tensorflow's own models, but lags too much with my own model. 

Could you please describe why it is like that and how can I solve it?

I included OpenCV for the webcam. 
Windows 10 Home Edition
TensorFlow installed from the Github repository
TensorFlow version is 1.13",2,,[],2019-04-01 06:09:35,open,,,[],2019-04-03 12:21:43
35,tensorflow/models,models,6498,snehakandpal,[attention_ocr] Different results from eval.py and demo_inference.py,"### System information
- **What is the top-level directory of the model you are using**: models/research/attention_ocr/python/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac os 10.14
- **TensorFlow installed from (source or binary)**: https://www.tensorflow.org
- **TensorFlow version (use command below)**: 1.10
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA 
- **GPU model and memory**: NA
- **Exact command to reproduce**:


### Describe the problem
I tried training model with just a few images and the loss after a while stays around 5.5. However, eval.py script run on the same images give accuracy of 1.0 but when I run demo_inference.py on the same images, the predictions are incorrect.",0,,[],2019-04-01 06:02:36,open,,,[],2019-04-01 06:02:36
36,tensorflow/models,models,6497,batravarun125,Distributed transformer execution fails,"I have a 4 machine cluster. I am trying to do transformer distributed training. 

I have set the TF_CONFIG. 
TF_CONFIG on PS -  {""cluster"": {""ps"": [""10.10.1.2:2219""], ""worker"": [""10.10.1.3"", ""10.10.1.4:2219"", ""10.10.1.1:2219""]}, ""task"": {""index"": 0, ""type"": ""ps""}}

TF_CONFIG on workers with increasing index = {""cluster"": {""ps"": [""10.10.1.2:2219""], ""worker"": [""10.10.1.3"", ""10.10.1.4:2219"", ""10.10.1.1:2219""]}, ""task"": {""index"": 0, ""type"": ""worker""}}

I run ""python transformer_main.py --data_dir=/users/kshiteej/varunimagenet/tensor2tensor/t2t_data/ --vocab_file=/users/kshiteej/varunimagenet/tensor2tensor/t2t_data/vocab.translate_ende_wmt32k.32768.subwords --param_set=tiny"" command on my 4 machines. 

But I get the error - 2019-04-01 00:17:48.765151: W tensorflow/core/framework/op_kernel.cc:1426] OP_REQUIRES failed at save_restore_v2_ops.cc:109 : Not found: /tmp/transformer_model/model.ckpt-0_temp_ea5f60f06af142498fa270460cdd5f2e; No such file or directory


- **What is the top-level directory of the model you are using**: model-master/official/transformer
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I am using train_and_evaluate instead of estimator.train and .eval
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 18.04
- **TensorFlow version (use command below)**:1.13


",1,,[],2019-04-01 05:20:54,open,,,['stat:awaiting response'],2019-04-02 00:19:33
37,tensorflow/models,models,6494,shedon85,Converting model checkpoint into frozen graph ,"### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Mojave 10.14.3
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: 
- **Exact command to reproduce**:
```
$python object_detection/export_tflite_ssd_graph.py \
--pipeline_config_path=$CONFIG_FILE \
--trained_checkpoint_prefix=$CHECKPOINT_PATH \
--output_directory=$OUTPUT_DIR \
--add_postprocessing_op=true

*config file- path to my bucket/data/pipeline.config.
*checkout file - a path to my bucket and to the cpkt-4200 file.
*output dir- a tmp folder in my local machine to store the files.
```
### Describe the problem
After successfully running a training job on the cloud (with tpu) I've tried to convert the cpkt-4200 file into frozen graph. expected to get these files:
 tflite_graph.pb and tflite_graph.pbtxt , but got the error below.
I've used for transfer learning this model:
ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03

When I've followed the same steps with different model I had success creating the frozen graph files.

## Links:
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
https://github.com/tensorflow/models/blob/r1.12.0/research/object_detection/export_tflite_ssd_graph.py




### Source code / logs

  File ""/Users/shedon/.local/share/virtualenvs/tensorflow-object-detection-2eAebk_d/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""export_tflite_ssd_graph.py"", line 139, in main
    FLAGS.max_classes_per_detection, FLAGS.use_regular_nms)
  File ""/Users/shedon/tensorflow-object-detection/models/research/object_detection/export_tflite_ssd_graph_lib.py"", line 235, in export_tflite_graph
    predicted_tensors = detection_model.predict(image, true_image_shapes=None)
  File ""/Users/shedon/tensorflow-object-detection/models/research/object_detection/meta_architectures/ssd_meta_arch.py"", line 568, in predict
    preprocessed_inputs)
  File ""/Users/shedon/tensorflow-object-detection/models/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py"", line 82, in extract_features
    'image_features': image_features['Conv2d_11_pointwise']
  File ""/Users/shedon/tensorflow-object-detection/models/research/object_detection/models/feature_map_generators.py"", line 546, in pooling_pyramid_feature_maps
    image_features = image_features[image_features.keys()[0]]
TypeError: 'dict_keys' object does not support indexing

",0,,[],2019-03-31 13:32:03,open,,,[],2019-03-31 13:57:27
38,tensorflow/models,models,6493,Nimishkhurana,Migrated boosted trees model to TensorFlow 2.0 in boosted_trees_upgraded folder.,"Preliminary work for GSoC
",2,,[],2019-03-31 11:05:55,open,,,['cla: yes'],2019-04-05 19:03:33
39,tensorflow/models,models,6491,Aaron19980207,No result when running object_detection_turtorial.ipynb,"There is no error message in jupyter notebook.
But the dog didn't come up on the page.

But there's some message in terminal:
`[I 16:08:46.781 NotebookApp] Replaying 10 buffered messages
2019-03-31 16:09:07.125713: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-31 16:09:07.174181: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1512000000 Hz
2019-03-31 16:09:07.177170: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2a90c50 executing computations on platform Host. Devices:
2019-03-31 16:09:07.177264: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-31 16:09:15.914296: W tensorflow/core/framework/allocator.cc:124] Allocation of 14601600 exceeds 10% of system memory.
2019-03-31 16:09:15.919579: W tensorflow/core/framework/allocator.cc:124] Allocation of 14601600 exceeds 10% of system memory.
[I 16:09:40.738 NotebookApp] Saving file at /object_detection_tutorial.ipynb
`
Is it the reason for the results?
",1,,[],2019-03-31 08:16:54,open,,,['stat:awaiting response'],2019-04-01 00:15:52
40,tensorflow/models,models,6490,FBEMPSS,opensource trained model,"Is there an open source model that has been trained well. 
If there is,could you show me the address of the checkpoints.


",1,,[],2019-03-31 07:32:37,open,,,['stat:awaiting response'],2019-04-01 00:15:48
41,tensorflow/models,models,6489,harsathAI,tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory,I am Getting this Error When I Try To Run My Tensorflow Record File. Please Help Me,1,,[],2019-03-30 14:44:58,open,,,['stat:awaiting response'],2019-03-31 12:17:22
42,tensorflow/models,models,6488,Nimishkhurana,Migrated mnist model to Tensorflow 2.0 in mnist_upgraded folder,Preliminiary work for GSoC project - Core Model Migration to TensorFlow 2.0,0,,[],2019-03-30 12:44:07,open,,"NamedUser(login=""Nimishkhurana"")",['cla: yes'],2019-03-30 14:20:05
43,tensorflow/models,models,6487,shiba17,How do you display the time taken for SSD MobileNet to detect objects?,"I've managed to retrained SSD MobileNet for my own custom object detector. But I can't find ways to display the time taken for the SSD MobileNet to detect objects, in the command prompt. Any changes I need to apply to my current code?

````````input code``````````````````
import os
import cv2
import numpy as np
import tensorflow as tf
import sys
sys.path.append("".."")
from utils import label_map_util
from utils import visualization_utils as vis_util
from utils.label_map_util import load_labelmap as ll

MODEL_NAME ='xxxx_graph'

CWD_PATH = os.getcwd()
PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')
PATH_TO_LABELS = os.path.join(CWD_PATH,'training','object-detection.pbtxt')

NUM_CLASSES = 2
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)
detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_CKPT,'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')
  sess = tf.Session(graph=detection_graph)
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
num_detections = detection_graph.get_tensor_by_name('num_detections:0')

#REAL-TIME INPUT
video = cv2.VideoCapture(0)
ret = video.set(3,1000)
ret = video.set(4,800)
while(True):
  ret, frame = video.read()
  frame_expanded = np.expand_dims(frame, axis=0)
  (boxes, scores, classes, num) = sess.run(
    [detection_boxes, detection_scores, detection_classes, num_detections],
    feed_dict={image_tensor:frame_expanded})
  vis_util.visualize_boxes_and_labels_on_image_array(
   frame,
   np.squeeze(boxes),
   np.squeeze(classes).astype(np.int32),
   np.squeeze(scores),
   category_index,
   use_normalized_coordinates=True,
   line_thickness=8,
   min_score_thresh=0.8)

  cv2.imshow('object detection', frame)
  if cv2.waitKey(25) & 0xFF == ord('q'):
   cv2.destroyAllWindows()
   break
```````````````````````````````````````````````````````````",1,,[],2019-03-30 06:53:28,open,,,['stat:awaiting response'],2019-03-31 00:17:17
44,tensorflow/models,models,6486,storm-rain,one question about filtering anchors by area (<0) in faster rcnn net,"https://github.com/tensorflow/models/blob/8e7051a8c1c4069f004a072362b173e70a8e260a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py#L664

here is my question: 
If we can filter anchors by area (<0) in clip_to_window function , how we can match dimensions between anchors and rpn_box_encodings (or rpn_objectness_predictions_with_background)。I think that dimensions of anchors is [boxNum,4] ,rpn_box_encodings is [?，boxNum,4] , but if we filter anchors by area < 0 , anchors.shape[0] may not equal to boxNum.  ",1,,[],2019-03-30 06:01:46,open,,,[],2019-04-02 12:50:48
45,tensorflow/models,models,6484,zhkmxx9302013,Update train.py,"The variable name at line 597 `train_`  is wrong, it should be `train_op`.",3,,[],2019-03-30 03:28:00,open,,,['cla: yes'],2019-03-30 15:20:12
46,tensorflow/models,models,6482,momo1986,No model saved for object_detection API using,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


****
**Hello, I use the example of ssd_mobilenet_v1_focal_loss_pets.config. I use on detection my own object, e.g., hands, I set the num_steps as 1000000 which is very big. Some guys told me that the check-points can be saved every ten minutes, however, I have not found the saved check-point currently on my training directory.  
How can I resolve the issue?
Thanks & Regards!**

****",2,,[],2019-03-30 01:48:02,open,,,[],2019-04-01 10:20:56
47,tensorflow/models,models,6480,chiyandetaotie,How to solve the problem [Testing the Installation],"**System information**
What is the top-level directory of the model you are using:
deeplab
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ubuntu16.04
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
1.8.0
Use cpu to try
****************************************************************************************************
**Sir,**
I want to test the install .When Quick running the whole code on the PASCAL VOC 2012 dataset , I run the code from tensorflow/models/research/deeplab , as follows:

`sh local_test.sh`


It is wrong as this:
 

**
INFO:tensorflow:Training on trainval set
INFO:tensorflow:Initializing model from path: /home/yl/PycharmProjects/deeplabv3+/models-master/research/deeplab/datasets/pascal_voc_seg/init_models/deeplabv3_pascal_train_aug/model.ckpt
Traceback (most recent call last):
  File ""/home/yl/PycharmProjects/deeplabv3+/models-master/research/deeplab/train.py"", line 500, in <module>
    tf.app.run()
  File ""/home/yl/anaconda3/envs/deeplab3+/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/yl/PycharmProjects/deeplabv3+/models-master/research/deeplab/train.py"", line 492, in main
    hooks=[stop_hook]) as sess:
TypeError: MonitoredTrainingSession() got an unexpected keyword argument 'summary_dir'  **

How can I solve the problem?

",1,,[],2019-03-29 13:18:05,open,,,['stat:awaiting response'],2019-03-31 00:17:13
48,tensorflow/models,models,6479,AI-LastWish,How to do transfer learning in only last layer in Tensorflow?,"I followed this tutorial https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10

I know that by default, TF API initialize from a checkpoint but allow retraining for all layers.

But I just want to retrain only the last layer, because I want to use pre-trained model in COCO to train in Open Images dataset.

Are there anyone can tell me how to do that?
------------------------

### System information
What is the top-level directory of the model you are using:(I don't really understand the meaning of ""top-level directory of the model"")
I just followed https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10

I used faster_rcnn_nas_lowproposals_coco downloaded from
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md

OS Platform and Distribution:
Ubuntu 16.04
TensorFlow installed from:
GPU version of Tensorflow
TensorFlow version:
v1.12
Bazel version:
NA: don't know what is Bazel
CUDA/cuDNN version:
NA: don't know, but it's not the related to the problem
GPU model and memory:
Titan V
Exact command to reproduce:
NA",5,,[],2019-03-29 11:22:36,open,,,[],2019-03-30 13:06:37
49,tensorflow/models,models,6478,Jothisethu,TypeError: predict() got an unexpected keyword argument 'true_image_shapes',"sir,
I have successfully trained the object detection model in my data set. Now I want to convert into tflite model using export_tflite_ssd_graph.py.

I run the following command in command line,

C:\models\research\object_detection>python export_tflite_ssd_graph.py \ --pipeline_con
fig_path training/ssd_mobilenet_v1_pets.config \ --trained_checkpoint_prefix training/model.ckpt-2000 \ --output_directory exported_model_directory/

but I got the following error:

fixed_shape_resizer {
  height: 300
  width: 300
}

Traceback (most recent call last):
  File ""export_tflite_ssd_graph.py"", line 144, in <module>
    tf.app.run(main)
  File ""C:\Users\DST-IT\AppData\Local\Programs\Python\Python36\lib\site-packages
\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""export_tflite_ssd_graph.py"", line 140, in main
    FLAGS.max_classes_per_detection, FLAGS.use_regular_nms)
  File ""C:\Users\DST-IT\AppData\Local\Programs\Python\Python36\Scripts\tensor_an
droid12\models\object_detection\export_tflite_ssd_graph_lib.py"", line 232, in ex
port_tflite_graph
    predicted_tensors = detection_model.predict(image, true_image_shapes=None)
TypeError: predict() got an unexpected keyword argument 'true_image_shapes'

How to fix this error?",2,,[],2019-03-29 09:54:42,open,,,[],2019-04-02 00:19:27
50,tensorflow/models,models,6477,patrick-ucr,TFLite low detection accuracy,"### System information
- **What is the top-level directory of the model you are using**: tensorflow/tensorflow/models
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.6
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: 0.19.1
- **CUDA/cuDNN version**: CUDA 10.0 
- **GPU model and memory**: Tesla P100 12GB Memory
- **Exact command to reproduce**:
`bazel run -c opt tensorflow/lite/toco:toco -- --input_file=tflite_graph.pb --output_file=ssdlite_mobilenet_v2_float.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --allow_custom_ops`

### Describe the problem
I trained a custom model from pretrained ssdlite_mobilenet_v2_coco from the model zoo. When I export it using export_inference_graph.py and test it on Android demo (tensorflow/tensorflow/examples/android/) detection accuracy was good (I use test video datasets).

However when I export it with export_tflite_ssd_graph.py and the above toco script to have tflite FLOAT model file without a serious error and use it in TFLite Android demo (tensorflow/tensorflow/lite/examples/android), following this instruction
[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md](url)

The problem is that now detection accuracy is very bad (recall is fine but precision is not) because of many false positive detections. There is another way of conversion by using tflite_converter but not sure how to do it for a detector model (see some classifier examples)

I plan to convert this to a quantized model but get stuck at this point and after looking at tensorflow issues, I can't see a similar problem experienced by others.

Here is a sample screenshot.
[https://imgur.com/a/pEHVxnm](url)

",0,,[],2019-03-29 04:42:35,open,,,[],2019-03-29 04:42:35
51,tensorflow/models,models,6473,OmorFarukRakib,"While creating frozen inference graph it says: ""I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 4. Tune using inter_op_ parallelism_threads for best performance.""","I trained my model for 5 hours with good total loss and all but when i am about to create frozen inference graph it takes too short time to create it and it gives me horribly wrong detection result.
When i create frozen inference graph, it shows following message----
""2019-03-28 03:03:16.172000: I tensorflow/core/common_runtime/process_util.cc:69]
Creating new thread pool with default inter op setting: 4. Tune using inter_op_
parallelism_threads for best performance.""

 It takes too little time to create all the file in the inference_graph folder.
Please help. Stuck at the very end :(",1,,[],2019-03-28 22:50:06,open,,,['stat:awaiting response'],2019-03-29 12:20:45
52,tensorflow/models,models,6470,gavincmartin,Object Detection: ArithmeticOptimizer and Speed Issues with Instance Segmentation,"### System information
- **What is the top-level directory of the model you are using**: tensorflow/models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS High Sierra
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.13.1
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See script below

### Describe the problem
I am encountering two odd (and possibly related) behaviors when attempting to run the ""mask_rcnn_inception_v2_coco_2018_01_28"" model.

1. When I run a session with a different (non instance mask) model like ""ssd_inception_v2_coco_2018_01_28"", the inference time for the first image is very slow (~9 sec) while the inference time for each subsequent image is much faster (~0.14 sec). This phenomenon is reproducible across models. When I run a session with an instance segmentation model, the inference time is consistently slow (~10 sec) and never improves even though I am not re-starting a session.
2. I get an ArithmeticOptimizer error when running an instance segmentation model, but the model still runs afterwards. This is the same error seen in #6215 

### Source code / logs
I have modified the object_detection_tutorial.ipynb code slightly to run multiple models sequentially for comparison in script form. I also added a few additional images to the test_images directory within object_detection. This script compares the performance of SSD_Inception_V2 vs Mask_RCNN_Inception_V2 and produces the error described above.

```
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO

from matplotlib import pyplot as plt
from PIL import Image

import time

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("".."")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.12.0'):
    raise ImportError(
        'Please upgrade your TensorFlow installation to v1.12.*.')

from utils import label_map_util

from utils import visualization_utils as vis_util


def load_image_into_numpy_array(image):
    (im_width, im_height) = image.size
    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(
        np.uint8)


def run_inference_for_single_image(image, session):
    # Get handles to input and output tensors
    ops = tf.get_default_graph().get_operations()
    all_tensor_names = {output.name for op in ops for output in op.outputs}
    tensor_dict = {}
    for key in [
            'num_detections', 'detection_boxes', 'detection_scores',
            'detection_classes', 'detection_masks'
    ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
            tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
                tensor_name)
    if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0],
                                     tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0],
                                   [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0],
                                   [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
    image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

    # Run inference
    output_dict = sess.run(
        tensor_dict, feed_dict={image_tensor: np.expand_dims(image, 0)})

    # all outputs are float32 numpy arrays, so convert types as appropriate
    output_dict['num_detections'] = int(output_dict['num_detections'][0])
    output_dict['detection_classes'] = output_dict['detection_classes'][
        0].astype(np.uint8)
    output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
    output_dict['detection_scores'] = output_dict['detection_scores'][0]
    if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
    return output_dict


MODEL_NAMES = [
    'ssd_inception_v2_coco_2018_01_28',
    'mask_rcnn_inception_v2_coco_2018_01_28'
]

for MODEL_NAME in MODEL_NAMES:
    print(""Model Name: {}"".format(MODEL_NAME))

    MODEL_FILE = MODEL_NAME + '.tar.gz'
    DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'

    # Path to frozen detection graph. This is the actual model that is used for the object detection.
    PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'

    # List of the strings that is used to add correct label for each box.
    PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')

    if not os.path.exists(PATH_TO_FROZEN_GRAPH):
        opener = urllib.request.URLopener()
        opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)
        tar_file = tarfile.open(MODEL_FILE)
        for file in tar_file.getmembers():
            file_name = os.path.basename(file.name)
            if 'frozen_inference_graph.pb' in file_name:
                tar_file.extract(file, os.getcwd())

    detection_graph = tf.Graph()
    with detection_graph.as_default():
        od_graph_def = tf.GraphDef()
        with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:
            serialized_graph = fid.read()
            od_graph_def.ParseFromString(serialized_graph)
            tf.import_graph_def(od_graph_def, name='')

    category_index = label_map_util.create_category_index_from_labelmap(
        PATH_TO_LABELS, use_display_name=True)

    # For the sake of simplicity we will use only 2 images:
    # If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
    PATH_TO_TEST_IMAGES_DIR = 'test_images'
    TEST_IMAGE_PATHS = []
    for file in os.scandir(PATH_TO_TEST_IMAGES_DIR):
        if file.name.endswith("".jpg""):
            TEST_IMAGE_PATHS.append(
                os.path.join(PATH_TO_TEST_IMAGES_DIR, file.name))

    with detection_graph.as_default():
        with tf.Session() as sess:
            for i, image_path in enumerate(TEST_IMAGE_PATHS):
                image = Image.open(image_path)
                image.resize((640, 480))
                # the array based representation of the image will be used later in order to prepare the
                # result image with boxes and labels on it.
                image_np = load_image_into_numpy_array(image)

                # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
                image_np_expanded = np.expand_dims(image_np, axis=0)
                # Actual detection.
                start = time.time()
                output_dict = run_inference_for_single_image(image_np, sess)
                end = time.time()
                print(""Inference time {}: {:.4f}"".format(i, end - start))
                # Visualization of the results of a detection.
                vis_util.visualize_boxes_and_labels_on_image_array(
                    image_np,
                    output_dict['detection_boxes'],
                    output_dict['detection_classes'],
                    output_dict['detection_scores'],
                    category_index,
                    instance_masks=output_dict.get('detection_masks'),
                    use_normalized_coordinates=True,
                    line_thickness=8)
```

This produces the following console output:
```
Model Name: ssd_inception_v2_coco_2018_01_28
2019-03-28 14:46:24.770281: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Inference time 0: 8.1120
Inference time 1: 0.1437
Inference time 2: 0.1325
Inference time 3: 0.1400
Inference time 4: 0.1409
Model Name: mask_rcnn_inception_v2_coco_2018_01_28
WARNING:tensorflow:From /Users/gavinmartin/tensorflow_repo/models/research/object_detection/tf-od-venv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-28 14:46:44.738988: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0
2019-03-28 14:46:47.741016: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0
Inference time 0: 10.0803
2019-03-28 14:46:55.723338: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0
2019-03-28 14:46:59.112565: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0
Inference time 1: 10.7539
2019-03-28 14:47:08.015176: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0
2019-03-28 14:47:11.565041: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0
Inference time 2: 11.2618
2019-03-28 14:47:19.476457: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0
2019-03-28 14:47:22.729805: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0
Inference time 3: 10.4220
2019-03-28 14:47:31.414572: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0
2019-03-28 14:47:35.067059: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0
Inference time 4: 11.3128
```

I am running the script from within a venv that has the following dependencies installed:
```
Package              Version
-------------------- --------
absl-py              0.7.1
appnope              0.1.0
astor                0.7.1
attrs                19.1.0
backcall             0.1.0
bleach               3.1.0
contextlib2          0.5.5
cycler               0.10.0
Cython               0.29.6
decorator            4.4.0
defusedxml           0.5.0
entrypoints          0.3
gast                 0.2.2
grpcio               1.19.0
h5py                 2.9.0
ipykernel            5.1.0
ipython              7.4.0
ipython-genutils     0.2.0
ipywidgets           7.4.2
jedi                 0.13.3
Jinja2               2.10
jsonschema           3.0.1
jupyter              1.0.0
jupyter-client       5.2.4
jupyter-console      6.0.0
jupyter-core         4.4.0
Keras-Applications   1.0.7
Keras-Preprocessing  1.0.9
kiwisolver           1.0.1
lxml                 4.3.3
Markdown             3.1
MarkupSafe           1.1.1
matplotlib           3.0.3
mistune              0.8.4
mock                 2.0.0
nbconvert            5.4.1
nbformat             4.4.0
notebook             5.7.7
numpy                1.16.2
object-detection     0.1
opencv-python        4.0.0.21
pandocfilters        1.4.2
parso                0.3.4
pbr                  5.1.3
pexpect              4.6.0
pickleshare          0.7.5
Pillow               5.4.1
pip                  10.0.1
prometheus-client    0.6.0
prompt-toolkit       2.0.9
protobuf             3.7.1
ptyprocess           0.6.0
Pygments             2.3.1
pyparsing            2.3.1
pyrsistent           0.14.11
python-dateutil      2.8.0
pyzmq                18.0.1
qtconsole            4.4.3
Send2Trash           1.5.0
setuptools           39.0.1
six                  1.12.0
tensorboard          1.13.1
tensorflow           1.13.1
tensorflow-estimator 1.13.0
termcolor            1.1.0
terminado            0.8.1
testpath             0.4.2
tornado              6.0.2
traitlets            4.3.2
wcwidth              0.1.7
webencodings         0.5.1
Werkzeug             0.15.1
wheel                0.33.1
widgetsnbextension   3.4.2
```

Interestingly enough, if I downgrade to TensorFlow 1.12 and run this script again, the speeds are still problematic, but the ArithmeticOptimizer error goes away:

```
Model Name: ssd_inception_v2_coco_2018_01_28
2019-03-28 14:51:04.669860: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Inference time 0: 9.2073
Inference time 1: 0.1434
Inference time 2: 0.1429
Inference time 3: 0.1598
Inference time 4: 0.2158
Model Name: mask_rcnn_inception_v2_coco_2018_01_28
Inference time 0: 12.2215
Inference time 1: 11.5347
Inference time 2: 11.7466
Inference time 3: 14.0183
Inference time 4: 15.0711
```",0,,[],2019-03-28 19:55:37,open,,,[],2019-03-28 19:55:37
53,tensorflow/models,models,6469,Eavis,Multi-GPU Error with Deeplabv3 updated version,"### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**:docker-tensorflow-gpu
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:10.0
- **GPU model and memory**: GTX1080 * 2, memory 11G each
- **Exact command to reproduce**: 
research_home_dir='/app/'
export PYTHONPATH=$PYTHONPATH:${research_home_dir}:${research_home_dir}slim

python /app/deeplab/ori_train_e.py \
    --logtostderr \
    --initialize_last_layer=False \
    --last_layers_contain_logits_only=False \
    --num_clones=2 \
    --training_number_of_steps=150000 \
    --train_split=""train"" \
    --model_variant=""xception_65"" \
    --atrous_rates=6 \
    --atrous_rates=12 \
    --atrous_rates=18 \
    --output_stride=16 \
    --decoder_output_stride=4 \
    --train_crop_size=513 \
    --train_crop_size=513 \
    --train_batch_size=4 \
    --fine_tune_batch_norm=False \
    --min_resize_value=513 \
    --max_resize_value=513 \
    --resize_factor=16 \
    --dataset=""ade20k17"" \
    --tf_initial_checkpoint=""/app/tf_initial_checkpoint/model.ckpt"" \
    --train_logdir=""/app/trainlog"" \
    --dataset_dir=""/app/tfrecord"" \
    --checkpoint_dir=""/app/checkpoint_dir"" \
    --eval_logdir=""/app/evallog"" \
    --log_steps=10 \
    --save_summaries_images=True \
    --save_interval_secs=1200 \
    --save_summaries_secs=600 \
    --learning_rate_decay_step=4000
~                                     
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
**Every time, I set the gpu(num_clones) greater or equals to 2, the program will just freeze and become a zombie that I could not kill.

My question is, does anyone have the same issue with me that the code is not compatible with multi-gpus?**

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


The program just stuck there and there is no information come out. 

Any help is welcome!
Thanks!",0,,[],2019-03-28 18:46:23,open,,,[],2019-03-28 18:48:32
54,tensorflow/models,models,6468,rohith513,Making a Keras layer dynamic for Tensorflow operations(Tensorflow object detection API),"### System information
- **What is the top-level directory of the model you are using**:N/A
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Arch Linux
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.13.1
- **Bazel version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:10.0.130
- **GPU model and memory**:Nvidia GTX 1070
- **Exact command to reproduce**:

### Describe the problem
Hello,
     I am replacing a Convolution layer with a Locally connected layer in ResNet (with Faster RCNN). Tensorflow imports this layer from keras and and it says that the dimensions of the input to this layer should be fully defined. When I run it it throws the below error.

So how to make this keras layer dynamic?

### Source code / logs
**Bottleneck block from Resnet (resnet_v1.py):**

residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1, scope='conv1')
residual = Local_connection.LocallyConnected2D(filters=depth_bottleneck, kernel_size=3, strides = (2,2), data_format='channels_last')(residual)
residual = slim.conv2d(residual, depth, [1, 1], stride=1,
                           activation_fn=None, scope='conv3')

**Error**:
ValueError: The spatial dimensions of the inputs to a LocallyConnected2D layer should be fully-defined, but layer received the inputs shape (1, None, None, 256)

**Link to the keras layer (check line 309):**
[https://github.com/keras-team/keras/blob/master/keras/layers/local.py](url)

",1,,[],2019-03-28 10:55:26,open,,,[],2019-04-03 11:24:00
55,tensorflow/models,models,6467,joytianya,im2txt: model is model.ckpt-1000000; but why is test caption not true,"INFO: Analysed target //im2txt:run_inference (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
Target //im2txt:run_inference up-to-date:
  bazel-bin/im2txt/run_inference
INFO: Elapsed time: 0.085s, Critical Path: 0.00s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
INFO:tensorflow:Building model.
INFO:tensorflow:Initializing vocabulary from file: /data/im2txt/data/mscoco/word_counts.txt
INFO:tensorflow:Created vocabulary with 11520 words
INFO:tensorflow:Running caption generation on 1 files matching /data/im2txt/data/mscoco/raw-data/val2014/COCO_val2014_000000224477.jpg
2019-03-28 15:39:03.360760: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-28 15:39:03.371340: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUDA_ERROR_NO_DEVICE
2019-03-28 15:39:03.371391: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: 5d1471fb-a71b-4e4d-8288-55075aee64f4
2019-03-28 15:39:03.371409: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: 5d1471fb-a71b-4e4d-8288-55075aee64f4
2019-03-28 15:39:03.371540: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 390.46.0
2019-03-28 15:39:03.371592: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 390.46.0
2019-03-28 15:39:03.371610: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 390.46.0
INFO:tensorflow:Loading model from checkpoint: /data/im2txt/model/train/model.ckpt-1000000
INFO:tensorflow:Restoring parameters from /data/im2txt/model/train/model.ckpt-1000000
INFO:tensorflow:Successfully loaded checkpoint: model.ckpt-1000000
Captions for image COCO_val2014_000000224477.jpg:
  0)  (p=0.000003)
  1) man (p=0.000000)
  2) surfer surfing (p=0.000000)",3,,[],2019-03-28 07:39:28,open,,,[],2019-04-07 10:49:04
56,tensorflow/models,models,6464,ilous12,Deeplab on mobile performace,"Hi @aquariusjay I tried to improve inference speed on android and used mobilenetv2.

I set output_stride = 32 to remove the SpaceToBatchNd,BatchToSpaceNd in deeplab and I got a fastest inference speed.
But The accuracy was more lower.

I have 2 questions

(1) Is it possible ""output_stride == 16 without SpaceToBatchNd,BatchToSpaceNd""?
(2) What do I set dilation values to 1?",2,,[],2019-03-28 06:00:57,open,,,[],2019-04-08 07:55:51
57,tensorflow/models,models,6463,intvo,Can we train the model on coco dataset?,"Is it possible to train the model on coco dataset? 
Thanks ",1,,[],2019-03-28 02:18:16,open,,,['stat:awaiting response'],2019-03-29 00:22:55
58,tensorflow/models,models,6461,YunzeLi725,invalid version number '1.13.0-dev20190227',"### System information
- **What is the top-level directory of the model you are using**:/models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:b'v1.13.0-rc2-0-gc865ec5621' 1.13.0-rc2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:CUDA 10/ cuDNN 7.4.2
- **GPU model and memory**:RTX2080
- **Exact command to reproduce**:jupyter notebook

### Describe the problem
Try to run object_detection_tutorial in jupyter, got the ValueError: invalid version number '1.13.0-dev20190227'

### Source code / logs
ValueError                                Traceback (most recent call last)
<ipython-input-2-db5b0bb000e2> in <module>
     17 from object_detection.utils import ops as utils_ops
     18 
---> 19 if StrictVersion(tf.__version__) < StrictVersion('1.13.0'):
     20   raise ImportError('Please upgrade your TensorFlow installation to v1.12.*.')

/usr/lib/python3.5/distutils/version.py in __init__(self, vstring)
     38     def __init__ (self, vstring=None):
     39         if vstring:
---> 40             self.parse(vstring)
     41 
     42     def __repr__ (self):

/usr/lib/python3.5/distutils/version.py in parse(self, vstring)
    135         match = self.version_re.match(vstring)
    136         if not match:
--> 137             raise ValueError(""invalid version number '%s'"" % vstring)
    138 
    139         (major, minor, patch, prerelease, prerelease_num) = \

ValueError: invalid version number '1.13.0-dev20190227'
",0,,[],2019-03-28 00:46:09,open,,,[],2019-03-28 00:46:09
59,tensorflow/models,models,6458,shamik111691,Adding background images to training data,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: tcsh
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.4
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I have tried to add some background images to my training data. I have added the following to the CSV file, 
pattern-141.jpg,546,373,background,,,,

the fields in my CSV file are: filename,width,height,class,xmin,ymin,xmax,ymax
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
I have tried: python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config and got the error  tensorflow.python.framework.errors_impl.InvalidArgumentError: All bounding box coordinates must be in [0.0, 1.0]: nan

I have tried /remote/t3dev4/itsmgr/Python-3.5_tf_ssl/bin/python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_resnet_v2_atrous_coco.config

and got InvalidArgumentError (see above for traceback): LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](clone_loss)]]
after step 
INFO:tensorflow:Recording summary at step 33.
INFO:tensorflow:global step 34: loss = 0.7557 (24.100 sec/step)
INFO:tensorflow:global step 35: loss = 0.7095 (15.436 sec/step)
INFO:tensorflow:global step 36: loss = 1.4553 (16.423 sec/step)
INFO:tensorflow:global step 37: loss = 1.1022 (15.649 sec/step)
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](clone_loss)]]

What is the right way to add background images to the training data?
",0,,[],2019-03-27 16:30:50,open,,,[],2019-03-27 16:31:46
60,tensorflow/models,models,6457,littlehome-eugene,ncf_keras_main doesn't run ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: models/official/recommendation
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux ubuntu 18.04
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: b'v1.13.1-0-g6612da8951' 1.13.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.0 , 7.5
- **GPU model and memory**: 
- **Exact command to reproduce**:./run_keras.sh

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm trying to run_keras.sh and it outputs an error `ValueError: When using iterators as input to a model, you should specify the `steps_per_epoch` argument.` 

so I tried adding `steps_per_epoch=producer.train_batches_per_epoch` 
but it gives me warning message

`W0327 23:25:59.123543 140526636000832 training_arrays.py:273] Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can g
enerate at least `steps_per_epoch * epochs` batches (in this case, 311 batches)`

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2,"NamedUser(login=""seemuch"")","[NamedUser(login=""seemuch"")]",2019-03-27 14:54:02,open,,"NamedUser(login=""seemuch"")",['models: official'],2019-04-04 12:19:29
61,tensorflow/models,models,6456,Atom-101,[WIP] Migration of official/resnet and Cifar-10 to TF 2.0,"Migrated `resnet_model.py` to TF 2.0 and the training code for Cifar-10 to TF 2.0

Wrote two variants of the Model class, using keras functional API. One directly creates tf.keras.Model object while the other returns an object that can perform forward pass.

Refactored the Cifar-10 code to use keras API calls instead of estimators. Tried to retain as much functionality of the original scripts as possible. I have not implemented fp16 support. I removed tf flags and used defined constants instead.",5,,[],2019-03-27 14:53:34,open,,,['cla: no'],2019-04-01 12:39:17
62,tensorflow/models,models,6452,Sri-vatsa,Running SSD_ResNet_101_FPN with custom dataset fails on GCP ML Engine ,"### System information
- **What is the top-level directory of the model you are using**: object_detection.model_main
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04, Python 3.5
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: 1.13.1
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: -

### Describe the problem

Tried training the SSD_ResNet_101_FPN (i.e. RetinaNet) using GCP ML Engine with 1 Master, 5 workers (each with 4 K80 GPUs) & 3 parameter servers.  Used the standard pipeline config file from `object_detection.samples.config` and downloaded pretrained model from `tensorflow model zoo`. 
I zipped `pycocotools`, `slim` and `object_detection` by cloning the `tensorflow/models` repository as of 27 Mar 5.30 am (EST).  

The job runs fine for training but when `object_detection_evaluation.py` is run, there is an error: `NameError: name 'unicode' is not defined`

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

`The replica master 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): [...] File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 511, in _actual_eval return _evaluate() File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 493, in _evaluate self._evaluate_build_graph(input_fn, hooks, checkpoint_path)) File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1424, in _evaluate_build_graph self._call_model_fn_eval(input_fn, self.config)) File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1460, in _call_model_fn_eval features, labels, model_fn_lib.ModeKeys.EVAL, config) File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1112, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File ""/root/.local/lib/python3.5/site-packages/object_detection/model_lib.py"", line 454, in model_fn eval_config, list(category_index.values()), eval_dict) File ""/root/.local/lib/python3.5/site-packages/object_detection/eval_util.py"", line 913, in get_eval_metric_ops_for_evaluators evaluators_list = get_evaluators(eval_config, categories, evaluator_options) File ""/root/.local/lib/python3.5/site-packages/object_detection/eval_util.py"", line 890, in get_evaluators **kwargs_dict)) File ""/root/.local/lib/python3.5/site-packages/object_detection/utils/object_detection_evaluation.py"", line 569, in __init__ group_of_weight=group_of_weight) File ""/root/.local/lib/python3.5/site-packages/object_detection/utils/object_detection_evaluation.py"", line 194, in __init__ self._build_metric_names() File ""/root/.local/lib/python3.5/site-packages/object_detection/utils/object_detection_evaluation.py"", line 213, in _build_metric_names category_name = unicode(category_name, 'utf-8') NameError: name 'unicode' is not defined`
",1,,[],2019-03-27 09:45:34,open,,,[],2019-03-28 16:10:44
63,tensorflow/models,models,6451,feifaxiaoming,Invalid argument: No OpKernel was registered to support Op 'Slice' with these attrs.,"
GPU：TensorFlow-GPU 0.9.0
cuda：9.0
os：linux ubuntu
python3.5

when i run model it Have  this  error:
Invalid argument: No OpKernel was registered to support Op 'Slice' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  <no registered kernels>

         [[{{node Slice}} = Slice[Index=DT_INT32, T=DT_INT32](Shape_1, Slice/begin, Slice/size)]]
",2,,[],2019-03-27 09:01:30,open,,,[],2019-03-29 00:22:40
64,tensorflow/models,models,6450,syedmustafa54,"OOM when allocating tensor with shape[24,1,2304,1152,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu","I have 1080 ti  Graphics card I7 Pc I am running Mobilenet v1 my config is set to 24 got Error.
And I have 1060 graphics laptop all files I copied to laptop and started the training with same config file and shocking it worked.
Than i have researcher about it and found to decrease batch size so i did it to 16 on my pc  its working but how is this possible my pc is having 1080 11gb graphic and laptop 1060 6 gb.
Here is the full error 
If you think something is wrong with my config just ask me i will update it,
```
INFO:tensorflow:global step 152: loss = 2.8710 (0.523 sec/step)
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.ResourceExhaustedError'>, OOM when allocating tensor with shape[24,1,2304,1152,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node batch}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.ResourceExhaustedError'>, OOM when allocating tensor with shape[24,1,2304,1152,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node batch}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

INFO:tensorflow:global step 153: loss = 2.9659 (0.381 sec/step)
INFO:tensorflow:global step 153: loss = 2.9659 (0.381 sec/step)
INFO:tensorflow:Finished training! Saving model to disk.
INFO:tensorflow:Finished training! Saving model to disk.
/media/syed/ubuntu/syed/anaconda3/envs/tod/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.
  warnings.warn(""Attempting to use a closed FileWriter. ""
Traceback (most recent call last):
  File ""research/object_detection/legacy/train.py"", line 184, in <module>
    tf.app.run()
  File ""/media/syed/ubuntu/syed/anaconda3/envs/tod/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/media/syed/ubuntu/syed/anaconda3/envs/tod/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""research/object_detection/legacy/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/home/syed/models/research/object_detection/legacy/trainer.py"", line 416, in train
    saver=saver)
  File ""/media/syed/ubuntu/syed/anaconda3/envs/tod/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 785, in train
    ignore_live_threads=ignore_live_threads)
  File ""/media/syed/ubuntu/syed/anaconda3/envs/tod/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 832, in stop
    ignore_live_threads=ignore_live_threads)
  File ""/media/syed/ubuntu/syed/anaconda3/envs/tod/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/media/syed/ubuntu/syed/anaconda3/envs/tod/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/media/syed/ubuntu/syed/anaconda3/envs/tod/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257, in _run
    enqueue_callable()
  File ""/media/syed/ubuntu/syed/anaconda3/envs/tod/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1257, in _single_operation_run
    self._call_tf_sessionrun(None, {}, [], target_list, None)
  File ""/media/syed/ubuntu/syed/anaconda3/envs/tod/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[24,1,2304,1152,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node batch}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

```",2,,[],2019-03-27 02:16:26,open,,,[],2019-03-29 12:20:34
65,tensorflow/models,models,6449,rok,WIP: 2.0 Reference Models: MobileNetv2 (TPU with dist strat and Keras),"See [Issue #26997](https://github.com/tensorflow/tensorflow/issues/26997).

This is to make `tensorflow.keras.applications.mobilenet_v2` compatible with TF2.0 and TPUs.",3,,[],2019-03-27 02:03:45,open,,,['cla: yes'],2019-03-29 00:33:19
66,tensorflow/models,models,6448,ThGkasios,Problem with missing module: _pywrap_tensorflow_internal,"### System information
- **Top-level directory**: C:\Users\user\Documents\GitHub\pysc2-examples
- **Code used**: https://github.com/chris-chris/pysc2-examples
- **OS Platform and Distribution**: Windows 10
- **TensorFlow installed from**: pip
- **TensorFlow version installed** (as of publising the issue): tensorflow-cpu 1.12.0
- **GPU model and memory**: Intel HD Graphics 4000
- **Exact command to reproduce**: C:\Users\user\Documents\GitHub\pysc2-examples [master ≡]> python train_mineral_shards.py
- **Python version**: 3.6.3
- **Have I written custom code**: No
- **Bazel version**: Not used (it doesn't seem to be required?)
- **CUDA/cuDNN version**: Not used (it doesn't seem to be required?)

After successfully installing tensorflow and the aforementioned project, after running the aforementioned command, this stacked trace is put out:

```
Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_mineral_shards.py"", line 5, in <module>
    from baselines import deepq
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\baselines\deepq\__init__.py"", line 1, in <module>
    from baselines.deepq import models  # noqa
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\baselines\deepq\models.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

A similar stack trace appears when Tensorflow is imported on python, even when *the current directory is different from where I installed the project*:

```
C:\> python
Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 17:26:49) [MSC v.1900 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

Similar issues have been published, though there is no simple solution to this problem, and all workarounds seem to be outdated, one way or another.

### Edits:

- Edit 1: One of the most notable differences from other is that I don't have a problem with the DLL file, as I have both `Microsoft Visual C++ 2017 Redistributable x64 (14.16.27029)` and `Microsoft Visual C++ 2017 Redistributable x86 (14.16.27029)`. I have tried to follow the stack trace to where it triggers, and it's this `Path Error` that is being triggered inside `pywrap_tensorflow_internal.py`. I have tried to inspect the **apparently not missing** `_pywrap_tensorflow_internal.so` to no avail.

- Edit 2: Added Bot-Required Specifications

- Edit 3: Since making this post, I have installed Microsoft Visual C++ 2015 and 17, and both x64 and x86 at that. The problem still persists.",3,,[],2019-03-27 02:03:17,open,,,[],2019-03-30 00:23:07
67,tensorflow/models,models,6446,ManyaWadhwa,inference using the inaturalist model gives only 5 bounding boxes,"While running inference on the inaturalist pretrained model, it gives five bounding boxes for each of the keys. 

Any help on this? Has anyone used that model?

Edit: I realized that it's a part of it's config file. However, if I just want to run inference, how do I change this parameter to give me more detections? ",1,,[],2019-03-26 22:09:19,open,,,['stat:awaiting response'],2019-03-27 12:20:27
68,tensorflow/models,models,6442,rohith513,Padding='same' not supported in LocallyConnected2D,"
### System information
- **What is the top-level directory of the model you are using**: N/A
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Arch Linux
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.13.1
- **Bazel version (if compiling from source)**:N/A
- **CUDA/cuDNN version**: 10.0.130
- **GPU model and memory**:Nvidia GTX 1070
- **Exact command to reproduce**:

Describe the problem
Hello,
 I am trying to replace few convolution layers in Resnet50 with Locally Connected layers. As you can see below, I want to replace the middle layer of the bottleneck block with locally connected. This layer uses padding='same'. But locally connected right now supports only padding='valid'. Is there a way to implement this?

or Any other ideas to replace few convolution layers with locally connected layers?

**Source code / logs:**

From this:
runs only when depth==256 or 512
'''residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,scope='conv1')                     
residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,rate=rate, 
                                           scope='conv2')
residual = slim.conv2d(residual, depth, [1, 1], stride=1,activation_fn=None, scope='conv3')'''

To this:
runs only when depth==1024 or 2048
'''residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1, scope='conv1')
residual = LocallyConnected2D(filters=depth_bottleneck, kernel_size=3, strides = (2,2), data_format='channels_last',padding='same')(residual)                     
residual = slim.conv2d(residual, depth, [1, 1], stride=1, activation_fn=None, scope='conv3')'''


And also when I replace the layer and run it I get the following error:

Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\rohit\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\Users\rohit\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\object_detection\legacy\trainer.py"", line 291, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\object_detection\legacy\trainer.py"", line 204, in _create_losses
    prediction_dict = detection_model.predict(images, true_image_shapes)
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 647, in predict
    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 978, in _extract_rpn_feature_maps
    scope=self.first_stage_feature_extractor_scope))
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 163, in extract_proposal_features
    return self._extract_proposal_features(preprocessed_inputs, scope)
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\object_detection\models\faster_rcnn_resnet_v1_feature_extractor.py"", line 138, in _extract_proposal_features
    scope=var_scope)
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\slim\nets\resnet_v1.py"", line 311, in resnet_v1_50
    reuse=reuse, scope=scope)
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\slim\nets\resnet_v1.py"", line 246, in resnet_v1
    store_non_strided_activations)
  File ""C:\Users\rohit\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\slim\nets\resnet_utils.py"", line 195, in stack_blocks_dense
    net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))
  File ""C:\Users\rohit\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""C:\Users\rohit\Desktop\pretest\models-master\research\slim\nets\resnet_v1.py"", line 134, in bottleneck
    residual = LocallyConnected2D(filters=depth_bottleneck, kernel_size=3, strides = (2,2), data_format='channels_last')(residual)
  File ""C:\Users\rohit\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 538, in __call__
    self._maybe_build(inputs)
  File ""C:\Users\rohit\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1603, in _maybe_build
    self.build(input_shapes)
  File ""C:\Users\rohit\Anaconda3\lib\site-packages\tensorflow\python\keras\utils\tf_utils.py"", line 151, in wrapper
    output_shape = fn(instance, input_shape)
  File ""C:\Users\rohit\Anaconda3\lib\site-packages\tensorflow\python\keras\layers\local.py"", line 445, in build
    'the inputs shape ' + str(input_shape))
ValueError: The spatial dimensions of the inputs to  a LocallyConnected2D layer should be fully-defined, but layer received the inputs shape (1, None, None, 256)


",0,,[],2019-03-26 13:24:07,open,,,[],2019-03-26 13:24:07
69,tensorflow/models,models,6441,littlehome-eugene,ncf bisection negative sampling doesn't look right,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: recommendation
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux ubuntu 18.04
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: b'v1.13.1-0-g6612da8951' 1.13.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.0, 7.5
- **GPU model and memory**: geforce 2060, 6gig
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.


I'm trying to use ncf_keras from recommendation model.

in `lookup_negative_items` , 
there's a comment that says: 

`# Expected state after bisection pass:                                                                                                                       #   The right index is the smallest index whose tally is greater than the                                                                                    #   negative item choice index.
`

I couldn't make sense of it, so I tried with a small sample , and printed out variables.  

I think the above comment means, 
`right_index` 's value should be smallest index of `self._total_negatives` which is greather than `neg_item_choice`

Then  I think `self._total_negatives[right_index] - neg_item_choice:  [-1 -2  0  3  0 -2]`  shouldn't have negative values.

Besides 
```
negative_users:  [1 0 2 2 2 1]
negative_items:  [4 3 6 0 6 5]  

```
says user 2 has not rated 6, 0 but user 2 did rate 6 given 
```
_train_pos_users: [0, 0, 1, 1, 1, 2, 2, 2, 2]
_sorted_train_pos_items: [0 1 0 1 3 3 4 5 6]

```

Am I misinterpretating the comments or the code is buggy because data size is too small?


```
     output[not_use_shortcut] = (
         self._sorted_train_pos_items[right_index] -
         (self._total_negatives[right_index] - neg_item_choice)
     )
```
The above code, is interpreted:
`self._sorted_train_pos_items[right_index]` get the item at right_index (which is smallest index whose tally is bigger than neg_item_choice)
`self._total_negatives[right_index]` total negative tally at right_index
`neg_item_choice` a random choice 
,

so in a sense,  it picks an item at right_index and substracts a random amount, I don't see how it could be guaranteed to pick a negative(unrated) item..  



I forcefully bypassed `use_shortcut` 

```
user_map: {0: 0, 1: 1, 3: 2}
item_map: {33: 2, 2: 5, 3: 3, 22: 1, 7: 6, 8: 4, 9: 7, 11: 0}
batch_ind_mod:  [3 1 8 6 5 3]
users:  [1 0 2 2 2 1]
negative_indices:  [ True  True  True  True  True  True]                                                                                                     negative_users:  [1 0 2 2 2 1]
index_bounds: [0 2 5 9]
num_positives:  [3 2 4 4 4 3]
num_negatives:  [5 6 4 4 4 5]
neg_item_choice: [2 2 3 0 3 3]
left_index: [2 0 5 5 5 2]
right_index: [4 1 8 8 8 4]                                                                                                                                   num_positives:  [3 2 4 4 4 3]                                                                                                                                num_negatives:  [5 6 4 4 4 5]                                                                                                                                neg_item_choice: [2 2 3 0 3 3]                                                                                                                               total_negatives:  [0 0 0 0 1 3 3 3 3]                                                                                                                        use_shortcut:  [False False False False False False]                                                                                                                                                                                                                                        mid_index:  [3 0 7 5 7 3]
right_criteria:  [False False False  True False False]                                                                                                       right_index:  [4 1 8 5 8 4]
left_index:  [3 0 7 5 7 3]
_sorted_train_pos_items: [0 1 0 1 3 3 4 5 6]
_total_negatives:  [0 0 0 0 1 3 3 3 3]
self._sorted_train_pos_items[right_index]:  [3 1 6 3 6 3]
self._total_negatives[right_index]:  [1 0 3 3 3 1]
self._total_negatives[right_index] - neg_item_choice:  [-1 -2  0  3  0 -2]
negative_items:  [4 3 6 0 6 5] 
```


I tested this with following data
```
user_id,item_id,rating,timestamp
0,11,1,2
0,22,1,2
0,33,1,2                                                                                                                                                     1,3,1,2
1,22,1,2
1,11,1,2                                                                                                                                                     1,8,1,2
3,2,1,2
3,3,1,2
3,7,1,2
3,8,1,2
3,9,1,2

```
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-03-26 13:00:26,open,,,[],2019-03-26 15:02:18
70,tensorflow/models,models,6439,JoseGnRLuis,keep_checkpoint_max in model_main.py not working?,"I'm using modified model_main.py to train a faster_rcnn_inception_v2_coco model.
I modified this line:

config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir)

To add keep_checkpoint_max , save_summary_steps, log_step_count_steps

 config = tf.estimator.RunConfig(  model_dir            = FLAGS.model_dir, 
                                      keep_checkpoint_max  = 0,
                                      save_summary_steps   = 1000, 
                                      log_step_count_steps = 10)

(also tried keep_checkpoint_max = 9999

but the param keep_checkpoint_max  doesn't seem to be working.
when training it only keeps the last 5 checkpoints.

### System information
- **What is the top-level directory of the model you are using**: tensorflow/models/blob/master/research/object_detection/model_main.py
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: only added the 3 params mentioned.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
- **TensorFlow installed from (source or binary)**: 1.13.1
- **TensorFlow version (use command below)**: 1.13.1
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 10
- **GPU model and memory**: GTX 1070
- **Exact command to reproduce**: 

python.exe model_main.py --pipeline_config_path=../../Data/Config/faster_rcnn_inception_v2_coco.config --num_train_steps=100000

![image](https://user-images.githubusercontent.com/32518279/54991927-c596d600-4f9c-11e9-88c5-8d0c60cd9652.png)",2,,[],2019-03-26 10:57:32,open,,,[],2019-03-30 12:21:31
71,tensorflow/models,models,6438,Zantares,Fuse embedding table cause performance regression on NCF,"### System information
- **What is the top-level directory of the model you are using**: 
models/official/recommendation/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.4.1708 (Core)
- **TensorFlow installed from (source or binary)**:
pip3 install --user tensorflow-gpu==1.12.0
- **TensorFlow version (use command below)**:
v1.12.0-0-ga6d8ffae09 1.12.0
- **Bazel version (if compiling from source)**:
0.19.2
- **CUDA/cuDNN version**:
cuda-9.0
- **GPU model and memory**:
Tesla P100-PCIE 16280MiB
- **Exact command to reproduce**:
```
python3 ncf_main.py -synth --clean -hk examplespersecondhook --dataset ml-20m -te 1 -bs 98304 --layers 256,256,128,64 --num_factors 64 -ng 1
```

### Describe the problem
NCF fused 4 embedding tables to 2 tables with this commit https://github.com/tensorflow/models/commit/c5ff4ec7ad1b287cd6d7907569081a2022aa225b#diff-d826ebc7bd84ae254fa5001253caf608.
It says
```
# It turns out to be significantly more effecient to store the MF and MLP
# embedding portions in the same table, and then slice as needed.
```
but I found opposite result - throughput on GPU has reduced from ~4400K to ~4000K, decreased about 9%.
Profiling breakdown shows that StrideSlice costed much time, which uses for slicing the fusion table:
Profile:
```
node name | requested bytes | total execution time | accelerator execution time | cpu execution time | op occurrence (run|defined)
MatMul                       500.03MB (100.00%, 29.69%),      11.88ms (100.00%, 44.18%),      11.09ms (100.00%, 47.18%),        792us (100.00%, 23.52%),      12|12
ResourceApplyAdam                     0B (0.00%, 0.00%),        2.79ms (55.82%, 10.39%),        2.46ms (52.82%, 10.47%),          329us (76.48%, 9.77%),      10|10
UnsortedSegmentSum             132.34MB (70.31%, 7.86%),         1.59ms (45.43%, 5.92%),         1.54ms (42.34%, 6.57%),           48us (66.71%, 1.43%),        2|2
ResourceGather                188.06MB (62.45%, 11.17%),         1.32ms (39.51%, 4.90%),         1.16ms (35.78%, 4.95%),          153us (65.28%, 4.54%),        2|2
StridedSliceGrad              312.44MB (51.28%, 18.55%),         1.25ms (34.61%, 4.64%),         1.16ms (30.83%, 4.92%),           91us (60.74%, 2.70%),        4|4
BiasAdd                               0B (0.00%, 0.00%),         1.10ms (29.97%, 4.09%),          777us (25.91%, 3.31%),          323us (58.03%, 9.59%),        4|4
ReluGrad                              0B (0.00%, 0.00%),         1.01ms (25.88%, 3.77%),          979us (22.60%, 4.17%),           34us (48.44%, 1.01%),        3|3
AddN                                  0B (0.00%, 0.00%),          858us (22.11%, 3.19%),          834us (18.43%, 3.55%),           23us (47.43%, 0.68%),        2|2
StridedSlice                  177.16MB (32.73%, 10.52%),          804us (18.92%, 2.99%),          607us (14.89%, 2.58%),          194us (46.75%, 5.76%),       6|14
Relu                                  0B (0.00%, 0.00%),          736us (15.93%, 2.74%),          683us (12.30%, 2.91%),           51us (40.99%, 1.51%),        3|3
ConcatV2                      179.46MB (22.21%, 10.66%),          685us (13.19%, 2.55%),           599us (9.40%, 2.55%),           84us (39.47%, 2.49%),        3|8
Slice                          165.03MB (11.55%, 9.80%),          673us (10.65%, 2.50%),           609us (6.85%, 2.59%),           62us (36.98%, 1.84%),        5|6
BiasAddGrad                     133.40KB (1.75%, 0.01%),           586us (8.14%, 2.18%),           462us (4.26%, 1.97%),          121us (35.14%, 3.59%),        4|4
Mul                              25.17MB (1.74%, 1.49%),           546us (5.96%, 2.03%),           437us (2.29%, 1.86%),          107us (31.54%, 3.18%),       8|16
Cast                            786.43KB (0.25%, 0.05%),           220us (3.93%, 0.82%),             6us (0.43%, 0.03%),          213us (28.36%, 6.33%),        2|4
```
It's easily to see the increased cost of  Slice, StridedSlice and StridedSliceGrad.
When runs on CPU, it has similar regression.


### Source code / logs
Here the performance log:

before commit:
```
I0326 13:45:57.172207 140520725944128 tf_logging.py:115] global_step/sec: 45.1547
I0326 13:45:57.172728 140520725944128 tf_logging.py:115] loss = 3.037029e-06, step = 500 (2.215 sec)
I0326 13:45:57.305662 140520725944128 tf_logging.py:115] Benchmark metric: {'timestamp': '2019-03-26T05:45:57.305630Z', 'global_step': 506, 'unit': None, 'value': 4434164.686207868, 'extras': [], 'name': 'average_examples_per_sec'}
I0326 13:45:57.305775 140520725944128 tf_logging.py:115] Benchmark metric: {'timestamp': '2019-03-26T05:45:57.305757Z', 'global_step': 506, 'unit': None, 'value': 4440229.400848767, 'extras': [], 'name': 'current_examples_per_sec'}
```

after commit:
```
I0326 18:12:32.288321 139745620174656 tf_logging.py:115] global_step/sec: 40.9326
I0326 18:12:32.288844 139745620174656 tf_logging.py:115] loss = 3.161498e-06, step = 500 (2.442 sec)
I0326 18:12:32.435392 139745620174656 tf_logging.py:115] Benchmark metric: {'value': 4022308.8085546126, 'unit': None, 'name': 'average_examples_per_sec', 'timestamp': '2019-03-26T10:12:32.435358Z', 'extras': [], 'global_step': 506}
I0326 18:12:32.435533 139745620174656 tf_logging.py:115] Benchmark metric: {'value': 4024523.1156207095, 'unit': None, 'name': 'current_examples_per_sec', 'timestamp': '2019-03-26T10:12:32.435496Z', 'extras': [], 'global_step': 506}
```


",4,,[],2019-03-26 10:37:02,open,,,[],2019-04-03 17:48:24
72,tensorflow/models,models,6437,AakashKumarNain,Mismatch in SSD anchor boxes ,"### System information
- **What is the top-level directory of the model you are using**: Object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
I was digging into the code for anchor box generation for SSD. [This](https://github.com/tensorflow/models/blob/master/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py) is the file for `multiple grid anchor generator`. In the same file, if you navigate to this function(line 252):
```python
def create_ssd_anchors(num_layers=6,
                       min_scale=0.2,
                       max_scale=0.95,
                       scales=None,
                       aspect_ratios=(1.0, 2.0, 3.0, 1.0 / 2, 1.0 / 3),
                       interpolated_scale_aspect_ratio=1.0,
                       base_anchor_size=None,
                       anchor_strides=None,
                       anchor_offsets=None,
                       reduce_boxes_in_lowest_layer=True):
  """"""Creates MultipleGridAnchorGenerator for SSD anchors.
  This function instantiates a MultipleGridAnchorGenerator that reproduces
  ``default box`` construction proposed by Liu et al in the SSD paper.
  See Section 2.2 for details. Grid sizes are assumed to be passed in
  at generation time from finest resolution to coarsest resolution --- this is
  used to (linearly) interpolate scales of anchor boxes corresponding to the
  intermediate grid sizes.
  Anchors that are returned by calling the `generate` method on the returned
  MultipleGridAnchorGenerator object are always in normalized coordinates
  and clipped to the unit square: (i.e. all coordinates lie in [0, 1]x[0, 1]).
  Args:
    num_layers: integer number of grid layers to create anchors for (actual
      grid sizes passed in at generation time)
    min_scale: scale of anchors corresponding to finest resolution (float)
    max_scale: scale of anchors corresponding to coarsest resolution (float)
    scales: As list of anchor scales to use. When not None and not empty,
      min_scale and max_scale are not used.
    aspect_ratios: list or tuple of (float) aspect ratios to place on each
      grid point.
    interpolated_scale_aspect_ratio: An additional anchor is added with this
      aspect ratio and a scale interpolated between the scale for a layer
      and the scale for the next layer (1.0 for the last layer).
      This anchor is not included if this value is 0.
    base_anchor_size: base anchor size as [height, width].
      The height and width values are normalized to the minimum dimension of the
      input height and width, so that when the base anchor height equals the
      base anchor width, the resulting anchor is square even if the input image
      is not square.
    anchor_strides: list of pairs of strides in pixels (in y and x directions
      respectively). For example, setting anchor_strides=[(25, 25), (50, 50)]
      means that we want the anchors corresponding to the first layer to be
      strided by 25 pixels and those in the second layer to be strided by 50
      pixels in both y and x directions. If anchor_strides=None, they are set to
      be the reciprocal of the corresponding feature map shapes.
    anchor_offsets: list of pairs of offsets in pixels (in y and x directions
      respectively). The offset specifies where we want the center of the
      (0, 0)-th anchor to lie for each layer. For example, setting
      anchor_offsets=[(10, 10), (20, 20)]) means that we want the
      (0, 0)-th anchor of the first layer to lie at (10, 10) in pixel space
      and likewise that we want the (0, 0)-th anchor of the second layer to lie
      at (25, 25) in pixel space. If anchor_offsets=None, then they are set to
      be half of the corresponding anchor stride.
    reduce_boxes_in_lowest_layer: a boolean to indicate whether the fixed 3
      boxes per location is used in the lowest layer.
  Returns:
    a MultipleGridAnchorGenerator
  """"""
  if base_anchor_size is None:
    base_anchor_size = [1.0, 1.0]
  box_specs_list = []
  if scales is None or not scales:
    scales = [min_scale + (max_scale - min_scale) * i / (num_layers - 1)
              for i in range(num_layers)] + [1.0]
  else:
    # Add 1.0 to the end, which will only be used in scale_next below and used
    # for computing an interpolated scale for the largest scale in the list.
    scales += [1.0]

  for layer, scale, scale_next in zip(
      range(num_layers), scales[:-1], scales[1:]):
    layer_box_specs = []
    if layer == 0 and reduce_boxes_in_lowest_layer:
      layer_box_specs = [(0.1, 1.0), (scale, 2.0), (scale, 0.5)]
    else:
      for aspect_ratio in aspect_ratios:
        layer_box_specs.append((scale, aspect_ratio))
      # Add one more anchor, with a scale between the current scale, and the
      # scale for the next layer, with a specified aspect ratio (1.0 by
      # default).
      if interpolated_scale_aspect_ratio > 0.0:
        layer_box_specs.append((np.sqrt(scale*scale_next),
                                interpolated_scale_aspect_ratio))
    box_specs_list.append(layer_box_specs)

  return MultipleGridAnchorGenerator(box_specs_list, base_anchor_size,
                                     anchor_strides, anchor_offsets)

```

If I print out the `box_specs_list`, then there are 10 pairs of `scale` and `aspect_ratio` for each layer. Some of the elements are being repeated. For example, this is the first item in that list:
```
[(0.35, 1.0), (0.4183300132670378, 1.0), (0.35, 2.0), (0.4183300132670378, 1.0), (0.35, 3.0), (0.4183300132670378, 1.0), (0.35, 0.5), (0.4183300132670378, 1.0), (0.35, 0.3333333333333333), (0.4183300132670378, 1.0)]
```

You can see that the pair `(0.4183300132670378, 1.0)` is repeated multiple times. Now:

1) As per the SSD paper, there should be `5` default boxes when there are no two default boxes for `ar==1.0` while there should be `6` default boxes when two boxes for `ar==1.0` is considered. Hence for each cell in the grid, there should be at `max 6` default boxes. But if we pass the above list, then there will be too many boxes and that too redundant ones. Are the duplicate default boxes being removed? If yes, can you please point me to that line of code?

2) When creating a `meshgrid` from `scales` and `aspect_ratios` list, how is the case handled when there two scales are present for one layer(case when two boxes for an aspect ratio of 1 is considered)? If I am correct, then there would be more default boxes again if this case isn't handled
",0,,[],2019-03-26 10:16:41,open,,,[],2019-03-26 10:16:41
73,tensorflow/models,models,6436,itraaj,Tensorflow Train.py hits an error how to reslove it?,"(tensorflow2) c:\tensorflow1\models\research\object_detection>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\platform\app.py:125: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
WARNING:tensorflow:From C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\legacy\trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:From C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:num_readers has been reduced to 0 to match input file shards.
WARNING:tensorflow:From C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\builders\dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\legacy\trainer.py"", line 280, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\legacy\trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""train.py"", line 121, in get_next
    dataset_builder.build(config)).get_next()
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\builders\dataset_builder.py"", line 135, in build
    config.input_path[:], input_reader_config)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\builders\dataset_builder.py"", line 80, in read_dataset
    sloppy=config.shuffle))
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1605, in apply
    return DatasetV1Adapter(super(DatasetV1, self).apply(transformation_func))
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1127, in apply
    dataset = transformation_func(self)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\data\experimental\ops\interleave_ops.py"", line 88, in _apply_fn
    buffer_output_elements, prefetch_input_elements)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\data\ops\readers.py"", line 133, in __init__
    cycle_length, block_length)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2827, in __init__
    super(InterleaveDataset, self).__init__(input_dataset, map_func)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2798, in __init__
    map_func, self._transformation_name(), dataset=input_dataset)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2124, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\framework\function.py"", line 490, in add_to_graph
    self._create_definition_if_needed()
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\framework\function.py"", line 341, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\framework\function.py"", line 355, in _create_definition_if_needed_impl
    whitelisted_stateful_ops=self._whitelisted_stateful_ops)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\framework\function.py"", line 883, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2099, in tf_data_structured_function_wrapper
    ret = func(*nested_args)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\data\ops\readers.py"", line 247, in __init__
    filenames, compression_type, buffer_size, num_parallel_reads)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\data\ops\readers.py"", line 199, in __init__
    filenames = ops.convert_to_tensor(filenames, dtype=dtypes.string)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\framework\ops.py"", line 1039, in convert_to_tensor
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\framework\ops.py"", line 1097, in convert_to_tensor_v2
    as_ref=False)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\framework\ops.py"", line 1175, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\irijdm\AppData\Local\Continuum\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\framework\ops.py"", line 977, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(""arg0:0"", shape=(), dtype=float32, device=/device:CPU:0)'
",1,,[],2019-03-26 04:47:32,open,,,['stat:awaiting response'],2019-03-27 00:18:35
74,tensorflow/models,models,6432,MitPatel971997,unable to convert ssd_mobilenet_v2_oid frozen_inference_graph.pb to .tflite ,"

### System information
- **What is the top-level directory of the model you are using**:
ssd_mobilenet_v2_oid_v4_2018_12_12
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
tflite_convert --graph_def_file=ssd_mobilenet_v2_oid_v4_2018_12_12/frozen_inference_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_file=demo.tflite --input_shape=1,300,300,3 --input_array=image_tensor --output_array=detection_boxes,detection_scores,num_detections,detection_classes




### Source code / logs
Traceback (most recent call last):
  File ""/home/uadmin/Projects/Model_Conversion/venv/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/uadmin/Projects/Model_Conversion/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/uadmin/Projects/Model_Conversion/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/uadmin/Projects/Model_Conversion/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/home/uadmin/Projects/Model_Conversion/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 162, in _convert_model
    output_data = converter.convert()
  File ""/home/uadmin/Projects/Model_Conversion/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py"", line 453, in convert
    **converter_kwargs)
  File ""/home/uadmin/Projects/Model_Conversion/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py"", line 342, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/uadmin/Projects/Model_Conversion/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py"", line 135, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
",0,,[],2019-03-25 12:19:33,open,,,[],2019-03-25 12:33:52
75,tensorflow/models,models,6427,inakaaay,toco failed error converting .pb file to .tflite,"

### System information
- **What is the top-level directory of the model you are using**:
object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu using windows subsystem
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: CPU use only
- **Exact command to reproduce**:

```
toco -- \
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops
```

### Describe the problem
i was following this [instruction](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) because i want to convert my custom object detection api data set to tflite then an error occured during the process.

### Source code / logs
```
inayano@INA-YANO:/mnt/c/Users/LENOVO-PC/models/research/object_detection$ toco --output_file SSDV2frozentflite/aaaa.tflite --graph_def_file SSDV2frozentflite/tflite_graph.pb --input_shapes 1,300,300,3 --input_arrays normalized_input_image_tensor --output_arrays 'TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type QUANTIZED_UINT8 --mean_values 128 --std_dev_values 128 --change_concat_input_ranges FALSE --allow_custom_ops
2019-03-23 21:07:15.937809: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/home/inayano/.local/bin/toco"", line 10, in <module>
    sys.exit(main())
  File ""/home/inayano/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/inayano/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/inayano/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/home/inayano/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 162, in _convert_model
    output_data = converter.convert()
  File ""/home/inayano/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 464, in convert
    **converter_kwargs)
  File ""/home/inayano/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 311, in toco_convert_graph_def
    input_data.SerializeToString())
  File ""/home/inayano/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 135, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
2019-03-23 21:07:18.633417: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TFLite_Detection_PostProcess
2019-03-23 21:07:18.653654: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 878 operators, 1282 arrays (0 quantized)
2019-03-23 21:07:18.683696: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 878 operators, 1282 arrays (0 quantized)
2019-03-23 21:07:18.731459: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 100 operators, 262 arrays (1 quantized)
2019-03-23 21:07:18.733397: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 100 operators, 262 arrays (1 quantized)
2019-03-23 21:07:18.734528: F tensorflow/contrib/lite/toco/tooling_util.cc:1634] Array FeatureExtractor/MobilenetV2/Conv/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV2/expanded_conv/depthwise/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Aborted (core dumped)

None
```
",1,,[],2019-03-23 13:24:52,open,,,[],2019-03-29 03:14:50
76,tensorflow/models,models,6425,Buted,can't run image_classification.py after installing tensorflow-gpu,"### System information
- **What is the top-level directory of the model you are using**:
> /home/nvidia/mobilenet
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux tegra-ubuntu 4.4.38-tegra
- **TensorFlow installed from (source or binary)**:
I install tensorflow-gpu using pip3, doing the same way with the web:
https://docs.nvidia.com/deeplearning/dgx/install-tf-jetsontx2/index.html
- **TensorFlow version (use command below)**:
run:
~~~
 python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
~~~
the result is 
> b'18.08-stage-0-ge269373' 1.9.0
- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.3.1
- **GPU model and memory**:
(TX2) 256 core NVIDIA Pascal GPU   8G
- **Exact command to reproduce**:
~~~ 
python3 image_classification.py --model resnet_v1_50 \
    --data_dir /data/imagenet/train-val-tfrecord \
    --use_trt \
    --precision fp16
~~~~
### Describe the problem
I'm using models to classify images on Jetson TX2, seeing https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html#image-classification
When I tried to run image_classification.py, I got this error:
> Traceback (most recent call last):
  File ""../tensorrt/tftrt/examples/image-classification/image_classification.py"", line 30, in <module>
    import official.resnet.imagenet_main
  File ""/home/nvidia/mobilenet/models/official/resnet/imagenet_main.py"", line 27, in <module>
    from official.utils.flags import core as flags_core
  File ""/home/nvidia/mobilenet/models/official/utils/flags/core.py"", line 30, in <module>
    from official.utils.flags import _base
  File ""/home/nvidia/mobilenet/models/official/utils/flags/_base.py"", line 25, in <module>
    from official.utils.logs import hooks_helper
  File ""/home/nvidia/mobilenet/models/official/utils/logs/hooks_helper.py"", line 29, in <module>
    from official.utils.logs import hooks
  File ""/home/nvidia/mobilenet/models/official/utils/logs/hooks.py"", line 28, in <module>
    class ExamplesPerSecondHook(tf.estimator.SessionRunHook):
AttributeError: module 'tensorflow.estimator' has no attribute 'SessionRunHook'

I thought it an error of the model, so I changed hooks.py : 
class ExamplesPerSecondHook(tf.train.SessionRunHook)
Then I got:
> Traceback (most recent call last):
  File ""../tensorrt/tftrt/examples/image-classification/image_classification.py"", line 30, in <module>
    import official.resnet.imagenet_main
  File ""/home/nvidia/mobilenet/models/official/resnet/imagenet_main.py"", line 27, in <module>
    from official.utils.flags import core as flags_core
  File ""/home/nvidia/mobilenet/models/official/utils/flags/core.py"", line 30, in <module>
    from official.utils.flags import _base
  File ""/home/nvidia/mobilenet/models/official/utils/flags/_base.py"", line 25, in <module>
    from official.utils.logs import hooks_helper
  File ""/home/nvidia/mobilenet/models/official/utils/logs/hooks_helper.py"", line 31, in <module>
    from official.utils.logs import metric_hook
  File ""/home/nvidia/mobilenet/models/official/utils/logs/metric_hook.py"", line 24, in <module>
    class LoggingMetricHook(tf.estimator.LoggingTensorHook):
AttributeError: module 'tensorflow.estimator' has no attribute 'LoggingTensorHook'


<b>What can I do to fix this?</b>

Thanks a lot.
",5,,[],2019-03-23 06:05:45,open,,,[],2019-03-26 13:35:59
77,tensorflow/models,models,6423,theangels,ObjectDetection API not suitable for tf 2.0.0-alpha0,"### System information
- **What is the top-level directory of the model you are using**: /Appendix/tensorflow_models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Nope
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.0.0-alpha0, and the models lib clone on Mar 11, 2019
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.0
- **GPU model and memory**: 1050Ti
- **Exact command to reproduce**:
PIPELINE_CONFIG_PATH='/home/jovyan/Codelab/model/pipeline.config'
MODEL_DIR='/home/jovyan/Codelab/data'
NUM_TRAIN_STEPS=50000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python3 object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr

tf_upgrade_v2 --intree . --outtree . --copyotherfiles False

### Describe the problem
I try to use ObjectDetection API in TensorFlow 2.0.0-alpha0, but the program told me that `AttributeError: module 'tensorflow' has no attribute 'contrib'`, clearly Google has delete the contrlib library. So next step I try to use the `tf_upgrade_v2 utility` to help me converting existing TensorFlow 1.x Python scripts to TensorFlow 2.0. But finally I failed. Here are the message from terminal.

### Source code / logs
![snipaste20190322_205738](https://user-images.githubusercontent.com/11308780/54824185-25fbee00-4ce5-11e9-8646-aa9fcc850387.png)

@wangtz ",5,,[],2019-03-22 12:59:14,open,,,[],2019-04-04 19:19:35
78,tensorflow/models,models,6421,xhzhao,coco object detection accuracy bugs,"i'm trying to repeat the pretrained object detection model accuracy on coco, and we could not repeat this mAP numbers after some suffering hacks. here is my result:
![image](https://user-images.githubusercontent.com/17486215/54798058-c294a080-4c91-11e9-9253-d9620d210c7b.png)

model | coco image   count | image resize | mAP
-- | -- | -- | --
ssd_resnet_50_fpn_coco | standard: 40504 | None | 37.97
ssd_resnet_50_fpn_coco | standard: 40504 | 640 | 36.97
ssd_resnet_50_fpn_coco | subset: 8059 | 640 | 31.67

Am i missing something?
This is the first time that the model zoo accuracy could not be repeated, and i thought tensorflow, as a leading framework, should not have this kind of navie bugs.

Please comment.
",4,,[],2019-03-22 03:08:09,open,,,[],2019-03-24 07:26:30
79,tensorflow/models,models,6420,smalgan,What is the problem? Tensorflow is not install ( Raspbian),"I did not want to install it after I installed it with ""wget"". I tried pip3 again. Where is the error

![hata](https://user-images.githubusercontent.com/48032201/54782555-3dc56a80-4c30-11e9-9ee1-90bfc203352c.png)
",1,,[],2019-03-21 20:22:52,open,,,['stat:awaiting response'],2019-03-22 12:19:07
80,tensorflow/models,models,6417,momo1986,"InvalidArgumentError (see above for traceback): indices[0] = 0 is not in [0, 0)","Hello, I use tensorflow 1.12's object-detection 's API.

I tried to use the solution of Mask-RCNN by FAIR. The pretrained model is TF's ResNet101-v1:
Here is my configuration file:
```
# Mask R-CNN with Resnet-101 (v1) configured for the Oxford-IIIT Pet Dataset.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for ""PATH_TO_BE_CONFIGURED"" to find the fields that
# should be configured.

model {
  faster_rcnn {
    num_classes: 2
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
      }
    }
    number_of_stages: 3
    feature_extractor {
      type: 'faster_rcnn_resnet101'
      first_stage_features_stride: 16
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 16
        width_stride: 16
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        predict_instance_masks: true
        conv_hyperparams {
          op: CONV
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.01
            }
          }
        }
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 300
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 1
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.0007
          schedule {
            step: 15000
            learning_rate: 0.00007
          }
          schedule {
            step: 30000
            learning_rate: 0.000007
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""/fast/junyan/HandDetection/hands-detection-gcloud/models/research/resnet_v1_101/resnet_v1_101.ckpt""
  from_detection_checkpoint: true
  load_all_detection_checkpoint_vars: true
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""/fast/junyan/HandDetection/hands-detection-gcloud/hands_train.record""
  }
  label_map_path: ""/fast/junyan/HandDetection/hands-detection-gcloud/hands_label_map.pbtxt""
  load_instance_masks: true
}

eval_config: {
  metrics_set: ""coco_mask_metrics""
  num_examples: 738
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""/fast/junyan/HandDetection/hands-detection-gcloud/hands_val.record""
  }
  label_map_path: ""/fast/junyan/HandDetection/hands-detection-gcloud/hands_label_map.pbtxt""
  load_instance_masks: true
  shuffle: false
  num_readers: 1
}

```
But it tells me the error:

> Caused by op 'GatherV2_4', defined at:
>   File ""object_detection/model_main.py"", line 117, in <module>
>     tf.app.run()
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""object_detection/model_main.py"", line 113, in main
>     tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
>     return executor.run()
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
>     return self.run_local()
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
>     saving_listeners=saving_listeners)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
>     loss = self._train_model(input_fn, hooks, saving_listeners)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
>     return self._train_model_default(input_fn, hooks, saving_listeners)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1237, in _train_model_default
>     features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
>     model_fn_results = self._model_fn(features=features, **kwargs)
>   File ""/fast/junyan/HandDetection/hands-detection-gcloud/models/research/object_detection/model_lib.py"", line 288, in model_fn
>     features[fields.InputDataFields.true_image_shape])
>   File ""/fast/junyan/HandDetection/hands-detection-gcloud/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 688, in predict
>     self._anchors.get(), image_shape, true_image_shapes))
>   File ""/fast/junyan/HandDetection/hands-detection-gcloud/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 775, in _predict_second_stage
>     anchors, image_shape_2d, true_image_shapes)
>   File ""/fast/junyan/HandDetection/hands-detection-gcloud/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1306, in _postprocess_rpn
>     groundtruth_weights_list)
>   File ""/fast/junyan/HandDetection/hands-detection-gcloud/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1389, in _sample_box_classifier_batch
>     single_image_groundtruth_weights)
>   File ""/fast/junyan/HandDetection/hands-detection-gcloud/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1506, in _sample_box_classifier_minibatch_single_image
>     groundtruth_weights=groundtruth_weights)
>   File ""/fast/junyan/HandDetection/hands-detection-gcloud/models/research/object_detection/core/target_assigner.py"", line 185, in assign
>     reg_weights = self._create_regression_weights(match, groundtruth_weights)
>   File ""/fast/junyan/HandDetection/hands-detection-gcloud/models/research/object_detection/core/target_assigner.py"", line 322, in _create_regression_weights
>     groundtruth_weights, ignored_value=0., unmatched_value=0.)
>   File ""/fast/junyan/HandDetection/hands-detection-gcloud/models/research/object_detection/core/matcher.py"", line 205, in gather_based_on_match
>     gathered_tensor = self._gather_op(input_tensor, gather_indices)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 2675, in gather
>     return gen_array_ops.gather_v2(params, indices, axis, name=name)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3332, in gather_v2
>     ""GatherV2"", params=params, indices=indices, axis=axis, name=name)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
>     return func(*args, **kwargs)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
>     op_def=op_def)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
>     self._traceback = tf_stack.extract_stack()
> 
> InvalidArgumentError (see above for traceback): indices[0] = 0 is not in [0, 0)
>          [[node GatherV2_4 (defined at /fast/junyan/HandDetection/hands-detection-gcloud/models/research/object_detection/core/matcher.py:205)  = GatherV2[Taxis=DT_INT32, Tindices=DT_INT64, Tparams=DT_FLOAT, _device=""/device:CPU:0""](Reshape_8, Reshape_9, GatherV2_3/axis)]]
>          [[node IteratorGetNext (defined at object_detection/model_main.py:113)  = IteratorGetNext[output_shapes=[[1], [1,?,?,3], [1,2], [1,3], [1,100], [1,100,4], [1,100,2], [1,100,2], [1,100], [1,100,?,?], [1,100], [1,100], [1]], output_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_FLOAT, DT_BOOL, DT_FLOAT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](IteratorV2)]]
>          [[{{node GroupCrossDeviceControlEdges_0/Loss/BoxClassifierLoss/concat_6/axis/_7512}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1708_GroupCrossDeviceControlEdges_0/Loss/BoxClassifierLoss/concat_6/axis"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

I am not sure what is the root cause.

How can I resolve it?

Thanks & Regards!
",1,,[],2019-03-21 05:01:33,open,,,[],2019-03-21 05:30:26
81,tensorflow/models,models,6416,zp1018,deeplab:issue running train.py,"
INFO:tensorflow:global_step/sec: 2.21484
Total loss is :[5.13572121]
INFO:tensorflow:global_step/sec: 2.19443
Total loss is :[5.00604486]
INFO:tensorflow:global_step/sec: 2.2007
Total loss is :[4.87157393]
INFO:tensorflow:global_step/sec: 2.18627
Traceback (most recent call last):
  File ""D:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.
py"", line 1322, in _do_call
    return fn(*args)
  File ""D:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.
py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""D:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.
py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Nan in summary histogram for: clone_0/
image_pooling/BatchNorm/moving_variance
         [[Node: clone_0/image_pooling/BatchNorm/moving_variance = HistogramSummary[T=DT_FLOAT, _dev
ice=""/job:localhost/replica:0/task:0/device:CPU:0""](clone_0/image_pooling/BatchNorm/moving_variance/
tag, image_pooling/BatchNorm/moving_variance/read/_5715)]]
         [[Node: clone_0/xception_65/entry_flow/block1/unit_1/xception_module/separable_conv1_pointw
ise/kernel/Regularizer/l2_regularizer/_5411 = _Recv[client_terminated=false, recv_device=""/job:local
host/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", sen
d_device_incarnation=1, tensor_name=""edge_5074_...egularizer"", tensor_type=DT_FLOAT, _device=""/job:l
ocalhost/replica:0/task:0/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:
",1,,[],2019-03-21 03:35:06,open,,,['stat:awaiting response'],2019-03-22 00:19:33
82,tensorflow/models,models,6415,iggy12345,Update installation.md,"It might seem redundant, but I followed this whole tutorial not knowing what <path-to-tensorflow> meant, and tried to install into the actual ""tensorflow"" package inside of my python installation.",0,,[],2019-03-20 18:48:02,open,,,['cla: yes'],2019-03-20 18:48:06
83,tensorflow/models,models,6413,rapsealk,Added python script to compile *.proto files.,"- Recommended command on installation guide is not working.
- Tested with Python 3.6 and protoc installed.",4,,[],2019-03-20 12:05:14,open,,,['cla: yes'],2019-03-24 17:10:20
84,tensorflow/models,models,6412,Sri-vatsa,Running model_main to train faster-rcnn-resnet-101 exits with error 1,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Used default `model_main.py` in `tensorflow/models/research/object_detection` 

- **OS Platform and Distribution (Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, Python 3.5.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Training Faster-rcnn-resnet-101 (run using model_main.py) with custom dataset on ML Engine Standard CPU exits with the following error:
```
TypeError: resize_images() got an unexpected keyword argument 'preserve_aspect_ratio'
```

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
Traceback (most recent call last):
  [...]
  File ""/root/.local/lib/python3.5/site-packages/object_detection/inputs.py"", line 515, in transform_and_pad_input_data_fn
    tensor_dict=transform_data_fn(tensor_dict),
  File ""/root/.local/lib/python3.5/site-packages/object_detection/inputs.py"", line 129, in transform_input_data
    tf.expand_dims(tf.to_float(image), axis=0))
  File ""/root/.local/lib/python3.5/site-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 543, in preprocess
    parallel_iterations=self._parallel_iterations)
  File ""/root/.local/lib/python3.5/site-packages/object_detection/utils/shape_utils.py"", line 237, in static_or_dynamic_map_fn
    outputs = [fn(arg) for arg in tf.unstack(elems)]
  File ""/root/.local/lib/python3.5/site-packages/object_detection/utils/shape_utils.py"", line 237, in <listcomp>
    outputs = [fn(arg) for arg in tf.unstack(elems)]
  File ""/root/.local/lib/python3.5/site-packages/object_detection/core/preprocessor.py"", line 2264, in resize_to_range
    lambda: _resize_portrait_image(image))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2040, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1890, in BuildCondBranch
    original_result = fn()
  File ""/root/.local/lib/python3.5/site-packages/object_detection/core/preprocessor.py"", line 2263, in <lambda>
    lambda: _resize_landscape_image(image),
  File ""/root/.local/lib/python3.5/site-packages/object_detection/core/preprocessor.py"", line 2245, in _resize_landscape_image
    align_corners=align_corners, preserve_aspect_ratio=True)
TypeError: resize_images() got an unexpected keyword argument 'preserve_aspect_ratio'
```",4,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2019-03-20 09:30:52,open,,,"['models: research', 'stat:awaiting tensorflower']",2019-03-21 02:10:43
85,tensorflow/models,models,6411,pritammahadik94,error while converting to tflite model,"Please go to Stack Overflow for help and support:

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.13.1
- **Bazel version (if compiling from source)**: 18.0
- **CUDA/cuDNN version** :none
- **GPU model and memory**: none 
- **Exact command to reproduce**:
bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=temp/box.pb --output_file=temp/detect.tflite -input_shapes=1,300,300,3--input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops

### Describe the problem
We trained the custom object detection model for the box(inventory box) detection using tensorflow object detection API. We used weights of pretrained  ssd_mobilenet_v1_coco models for training. 
Then we convert frozen graph for the model from checkpoints using export_tflite_ssd_graph.py file.
Now I wanted to convert an optimized model by using TOCO so I can run our model on android devices. 
We followed all the mentioned steps, also we tried with various version of bazel (0.22.0, 0.20.0, 0.19.1) but not able to transform the model successfully.
While running above command with bazel 0.18.0 I got the following error. Please help me with the same.
Also, is there any update to convert a frozen inference graph of Faster RCNN model to Tflite, as iI read from Tensorflow API documentation, currently SSD mobilenet model only support for the tflite conversion.


### Source code / logs
ERROR: D:/tensorflow/tensorflow/contrib/lite/BUILD:34:1: C++ compilation of rule '//tensorflow/contrib/lite:arena_planner' failed (Exit 2): cl.exe failed: error executing command
  cd C:/users/pritamm1/_bazel_pritamm1/26orbg4z/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;;C:\windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/pritamm1/AppData/Local/conda/conda/envs/tensorflow111/python.exe
    SET PYTHON_LIB_PATH=C:/Users/pritamm1/AppData/Local/conda/conda/envs/tensorflow111/lib/site-packages
    SET TEMP=C:\Users\pritamm1\AppData\Local\Temp
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TMP=C:\Users\pritamm1\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw -w /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/contrib/lite/_objs/arena_planner/arena_planner.obj /c tensorflow/contrib/lite/arena_planner.cc
.\tensorflow/contrib/lite/context.h(183): error C2144: syntax error: 'float' should be preceded by ';'
.\tensorflow/contrib/lite/context.h(183): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int
Target //tensorflow/contrib/lite/toco:toco failed to build
INFO: Elapsed time: 33.305s, Critical Path: 1.15s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
",0,,[],2019-03-20 08:05:45,open,,,[],2019-03-20 08:05:45
86,tensorflow/models,models,6410,ruksin,can't see mAP in tensorboard. Also want to see pr curve,#3422 This issue is closed. I want to know how this problem is solved??,2,,[],2019-03-20 05:20:26,open,,,[],2019-03-22 00:19:23
87,tensorflow/models,models,6408,bingoty,ok,,1,,[],2019-03-20 03:14:28,open,,,['cla: no'],2019-03-20 03:14:31
88,tensorflow/models,models,6405,smalgan,"""protoc --python_out =. .\object_detection\protos\calibration.proto "" Error","
I want to compile the protobuf using this code. But he gives me this error "".: Permission denied"". What should I do?
Tensorflow 1.13
CUDA 9
cudnn 7.5
win10 64 bit

protoc --python_out =. .\object_detection\protos\calibration.proto",3,,[],2019-03-19 19:23:41,open,,,[],2019-03-22 11:39:12
89,tensorflow/models,models,6404,rosaliabq,Unable to train and export model with convert_to_grayscale:true,"### System information
- **What is the top-level directory of the model you are using**: 
models/research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Microsoft Windows 10 Pro 10.0.17134
- **TensorFlow installed from (source or binary)**:
Conda tensorflow-gpu (https://anaconda.org/anaconda/tensorflow-gpu)
- **TensorFlow version (use command below)**:
1.13.1
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX 1060 TI
- **Exact command to reproduce**:

### Describe the problem
Graph and model freeze is not working well after training SSD lite - Mobilenetv2 after setting convert_to_grayscale:true in the image_resizer options.

### Source code / logs

Config file

model {
  ssd {
    num_classes: 8
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
	**convert_to_grayscale: true**
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 3
        use_depthwise: true
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.03
              mean: 0.0
            }
          }
          batch_norm {
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v2'
      min_depth: 16
      depth_multiplier: 1.0
      use_depthwise: true
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.9997,
          epsilon: 0.001,
        }
      }
    }
    loss {
      classification_loss {
        weighted_sigmoid {
        }
      }
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 3
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  batch_size: 24
  optimizer {
    rms_prop_optimizer: {
      learning_rate: {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.004
          decay_steps: 800720
          decay_factor: 0.95
        }
      }
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""PATH_TO_BE_CONFIGURED/model.ckpt""
  fine_tune_checkpoint_type:  ""detection""
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100""
  }
  label_map_path: ""PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt""
}

eval_config: {
  num_examples: 8000
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010""
  }
  label_map_path: ""PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt""
  shuffle: false
  num_readers: 1
}


-----------

**Log**

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3,3,1,32] rhs shape= [3,3,3,32]
         [[node save/Assign_88 (defined at D:/proj/RDProjects/NLVehicleDetector/Python/tfmodels/research\object_detection\exporter.py:289) ]]


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/export_inference_graph.py"", line 154, in <module>
    tf.app.run()
  File ""D:\conda\envs\tfobjdet\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/export_inference_graph.py"", line 150, in main
    write_inference_graph=FLAGS.write_inference_graph)
  File ""D:/proj/RDProjects/NLVehicleDetector/Python/tfmodels/research\object_detection\exporter.py"", line 454, in export_inference_graph
    write_inference_graph=write_inference_graph)
  File ""D:/proj/RDProjects/NLVehicleDetector/Python/tfmodels/research\object_detection\exporter.py"", line 382, in _export_inference_graph
    trained_checkpoint_prefix=checkpoint_to_use)
  File ""D:/proj/RDProjects/NLVehicleDetector/Python/tfmodels/research\object_detection\exporter.py"", line 293, in write_graph_and_checkpoint
    saver.restore(sess, trained_checkpoint_prefix)
  File ""D:\conda\envs\tfobjdet\lib\site-packages\tensorflow\python\training\saver.py"", line 1312, in restore
    err, ""a mismatch between the current graph and the graph"")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Assign requires shapes of both tensors to match. lhs shape= [3,3,1,32] rhs shape= [3,3,3,32]
         [[node save/Assign_88 (defined at D:/proj/RDProjects/NLVehicleDetector/Python/tfmodels/research\object_detection\exporter.py:289) ]]

Caused by op 'save/Assign_88', defined at:
  File ""object_detection/export_inference_graph.py"", line 154, in <module>
    tf.app.run()
  File ""D:\conda\envs\tfobjdet\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/export_inference_graph.py"", line 150, in main
    write_inference_graph=FLAGS.write_inference_graph)
  File ""D:/proj/RDProjects/NLVehicleDetector/Python/tfmodels/research\object_detection\exporter.py"", line 454, in export_inference_graph
    write_inference_graph=write_inference_graph)
  File ""D:/proj/RDProjects/NLVehicleDetector/Python/tfmodels/research\object_detection\exporter.py"", line 382, in _export_inference_graph
    trained_checkpoint_prefix=checkpoint_to_use)
  File ""D:/proj/RDProjects/NLVehicleDetector/Python/tfmodels/research\object_detection\exporter.py"", line 289, in write_graph_and_checkpoint
    tf.import_graph_def(inference_graph_def, name='')
  File ""D:\conda\envs\tfobjdet\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""D:\conda\envs\tfobjdet\lib\site-packages\tensorflow\python\framework\importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""D:\conda\envs\tfobjdet\lib\site-packages\tensorflow\python\framework\importer.py"", line 235, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""D:\conda\envs\tfobjdet\lib\site-packages\tensorflow\python\framework\ops.py"", line 3433, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""D:\conda\envs\tfobjdet\lib\site-packages\tensorflow\python\framework\ops.py"", line 3433, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""D:\conda\envs\tfobjdet\lib\site-packages\tensorflow\python\framework\ops.py"", line 3325, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""D:\conda\envs\tfobjdet\lib\site-packages\tensorflow\python\framework\ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()",1,,[],2019-03-19 19:22:51,open,,,[],2019-03-20 13:08:13
90,tensorflow/models,models,6403,jianlong-yuan,[deeplabv3+] How to test on ADE20K with deeplabv3_mnv2_ade20k_train_2018_12_03.tar,"how to test, and  gives error



InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [1,256,33,33] vs. shape[1] = [1,256,33,43]
         [[Node: concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](concat-0-TransposeNHWCToNCHW-LayoutOptimizer, aspp0/Relu, concat-2-LayoutOptimizer)]]
         [[Node: ArgMax/_1793 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1175_ArgMax"", tensor_type=DT_INT64, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

",3,,[],2019-03-19 14:31:11,open,,,[],2019-03-23 20:55:27
91,tensorflow/models,models,6402,Cavan09,model_main.py Stuck Periodically,"### System information
- **What is the top-level directory of the model you are using**: Object Detection 
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I've only customized some of the scripts to generate my own data sets
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro x64
- **TensorFlow installed from (source or binary)**: Installed from Binary
- **TensorFlow version (use command below)**: Version 1.12
- **Bazel version (if compiling from source)**: 0.17.2
- **CUDA/cuDNN version**: cudnn version 7.xx (I believe 7.31)
- **GPU model and memory**: RTX 2080 ti 11GBs
- **Exact command to reproduce**: 

model_main.py --logtostderr --model_dir=..\..\..\runs\HyperParameterTuning\AnchorPoints\training --pipeline_config_path=..\..\..\runs\HyperParameterTuning\AnchorPoints\training\pipeline.config --num_train_steps=50000

### Describe the problem
I have been using the legacy train.py for a while and decided to switch over to model_main.py as train.py was deprecated.

While running model_main.py the training will pause indefinitely which seems to occur when a checkpoint is being saved. The only way through is to press ctrl+c and it will resume. This doesn't seem to cause any problems with training or evaluating, but it does make it hard to run in the background. 

I have added tf.logging.set_verbosity(tf.logging.INFO) to see the output while training.

I am currently using the model faster_rcnn_inception_resnet_v2_atrous_coco for training.
My batch size is set to 1, and I'm using a value of 1 for sample_1_of_n_eval_examples.
My evaluation batch size is also 1.

**Example of the logs during the freeze**
![image](https://user-images.githubusercontent.com/11929685/54614775-587ec000-4a33-11e9-9f6e-be6d4d35999c.png)

**After pressing ctrl + c**
![image](https://user-images.githubusercontent.com/11929685/54614908-924fc680-4a33-11e9-9bf9-c9f4615ead8b.png)

",2,,[],2019-03-19 14:18:16,open,,,[],2019-03-21 14:46:02
92,tensorflow/models,models,6400,smalgan,I can't get images when I'm testing the object_deteciton.,"Tensorflow 1.13.1
CUDA 9.0
cudnn 7.5
Windows 10 
protobuf 3.4
python 3.7


I could not install Protobuf 3.7. so I installed protobuf 3.4. The code is running when I run jupyter but I can't get the output. 
![Adsız](https://user-images.githubusercontent.com/48032201/54588641-a3e59e00-4a34-11e9-84ee-f41faf195450.png)
![dd](https://user-images.githubusercontent.com/48032201/54588643-a47e3480-4a34-11e9-8210-96119df79604.png)
",2,,[],2019-03-19 07:49:22,open,,,[],2019-03-21 00:21:09
93,tensorflow/models,models,6398,lighTQ, how to use multi gpu for inferencing ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

What is the top-level directory of the model you are using: research/object_detection
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux cent os 7.5): Linux Cent OS 7.5
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.12.0
Bazel version (if compiling from source): NA
CUDA/cuDNN version: 10.0
GPU model and memory: 2x Tesla P100 16Gb
Exact command to reproduce: NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
how to using multi gpu card for inferencing？

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-03-19 03:24:37,open,,,[],2019-03-19 03:24:37
94,tensorflow/models,models,6395,ruthvikb14,time stamps on images,"I'm having problem in creating the timestamps, can anyone explain ",1,,[],2019-03-18 23:47:47,open,,,['stat:awaiting response'],2019-03-19 12:20:43
95,tensorflow/models,models,6394,dubey,Update ResNet README for multi worker.,,0,,[],2019-03-18 20:08:05,open,,,['cla: yes'],2019-03-18 23:47:40
96,tensorflow/models,models,6391,lyltencent,How to implement Scale-Transferrable Detection Network from CVPR 2018 using the API,"I would like to use the API to implement the Scale-Transferrable Detection Network, any ideas?",2,,[],2019-03-18 14:39:04,open,,,[],2019-03-20 12:21:44
97,tensorflow/models,models,6388,RADHI18,object_detection:,"I'm using Windows 10, Python 3.7.2 and Tensorflow 1.13.0-rc2 , I am getting this error while trying to run train.py

WARNING:tensorflow:From C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\platform\app.py:125: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
Traceback (most recent call last):
  File ""train.py"", line 183, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 92, in main
    configs = config_util.get_configs_from_pipeline_file(FLAGS.pipeline_config_path)
  File ""C:\tensorflow1\models\research\object_detection\utils\config_util.py"", line 97, in get_configs_from_pipeline_file
    proto_str = f.read()
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 125, in read
    self._preread_check()
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 85, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open: training/faster_rcnn_inception_v2_pets.config﻿ : Le fichier sp\udce9cifi\udce9 est introuvable.
; No such file or directory
please help me ! how can solve this problem？",2,,[],2019-03-18 11:05:56,open,,,[],2019-03-18 13:18:35
98,tensorflow/models,models,6387,sed0724963,How to close down the other Features?(for tensorflow object detection api),"I'm use the new api and it's use the model_main.py to training ,but it is so slow,
I watched the tensorboard and discover the api have many features,like ""DetectionBoxes_Precision"",""DetectionBoxes_Recall"",""learning_rate""....
I assume that features is the reason of why api is so slow,
SO, my question is : If I want to boost the training speeds,I assume I should clear the other features,only use loss ,and loss_1,loss_2,other features about evalution I want to delete.
How can I do?modify what?
------------------------
![123](https://user-images.githubusercontent.com/38509051/54524073-f0d16200-49ab-11e9-8fd9-1176fb7733fc.png)


System information
What is the top-level directory of the model you are using:
model/research
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
win10
TensorFlow installed from (source or binary):
pip
TensorFlow version (use command below):
1.9.0
Bazel version (if compiling from source):
don't know
CUDA/cuDNN version:
cuda:9 , cuDNN:7
GPU model and memory:
ASUS ROG Strix GeForce® GTX 1080
Exact command to reproduce:
don't",10,,[],2019-03-18 10:30:48,open,,,[],2019-03-22 12:08:51
99,tensorflow/models,models,6386,python292,Print image tensors while running inference for image,"I have trained my model for a single class of object. Now while running inference for single image I also want to print layer wise tensors of that image. I want to know how to do that?
",1,,[],2019-03-18 08:52:08,open,,,['stat:awaiting response'],2019-03-19 00:22:45
100,tensorflow/models,models,6384,adursun,Add missing line breaks,,3,,[],2019-03-18 07:55:14,open,,,['cla: yes'],2019-03-18 07:56:28
101,tensorflow/models,models,6383,smalgan,How to fix this problem ? ( object_detection.builders import dataset_builder ) Tensorflow,"Can you help with the solution?
Tensorflow 1.5 Windows 10

**CODE :**  C:\tensorflow1\models\research>python object_detection/train.py --logtostderr --train_dir=object_detection/training/ --pipeline_config_path=object_detection/training/ssd_inception_v2_coco.config


### **Problem**
Traceback (most recent call last):
  File ""object_detection/train.py"", line 49, in <module>
    from object_detection.builders import dataset_builder
  File ""C:\tensorflow1\models\research\object_detection\builders\dataset_builder.py"", line 27, in <module>
    from object_detection.data_decoders import tf_example_decoder
  File ""C:\tensorflow1\models\research\object_detection\data_decoders\tf_example_decoder.py"", line 24, in <module>
    from object_detection.protos import input_reader_pb2
ImportError: cannot import name 'input_reader_pb2'",4,,[],2019-03-17 16:49:18,open,,,[],2019-03-19 07:41:07
102,tensorflow/models,models,6378,joytianya,why transformer decoder is no mask in  attention_layer.py?,"transformer encoder and decoder are both use attention_layer ,but decoder need mask ,why transformer decoder is no mask in code attention_layer.py",3,,[],2019-03-16 13:15:37,open,,,[],2019-03-19 00:22:34
103,tensorflow/models,models,6377,kaixinxiao,deeplab pascal finetune on trainval set parameters,"System information

    What is the top-level directory of the model you are using: deeplab
    Have I written custom code (as opposed to using a stock example script provided in    TensorFlow): no
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
    TensorFlow installed from (source or binary): binary
    TensorFlow version (use command below): 1.10
    Bazel version (if compiling from source): no
    CUDA/cuDNN version: 9.0
    GPU model and memory: TeslaV100(32G) 
    Exact command to reproduce: no

Describe the problem
I don't pretrain on COCO. I use bn=16,OS=16 and atrous rate[6,12,18] on train_aug dataset.
When I finetune on trainval set, I use bn=16 ,freeze bn, atrous rate[12,24,36] ，OS=8 and learning rate=0.0001.
But result is not good I think. Are these parameters true，especially in finetune? I also find the paper says learning rate is 0.001 in finetune but the code uses 0.0001 .",0,,[],2019-03-16 09:07:38,open,,,[],2019-03-16 09:07:38
104,tensorflow/models,models,6376,HuiyanWen,The accuracy rate didn't rise when I adapt vggnet on my own dataset.,"### The problem
I didn't change any word of vggnet.py. The only deference that my picture is single channel. Does it cause the problem?

### Source code / logs

![1](https://user-images.githubusercontent.com/35653098/54472722-25a4b400-4808-11e9-90d3-016fc7838fb3.png)

",2,,[],2019-03-16 08:25:50,open,,,[],2019-03-16 08:32:48
105,tensorflow/models,models,6375,Ramay7,Questions about Autoaugment,"@BarretZoph Hi,

I have some questions about AutoAugment. I want to know how the reward (accuracy on validation set) will guide the training in detail. Because I think the reward signal is too weak and may change only in small scale. Can you share the source code of reinforcement learning part ?

And how much time did it take to train a policy as it has been reported in the paper? How about machine configuration?

Thanks!",1,,[],2019-03-16 01:45:33,open,,,['stat:awaiting response'],2019-03-17 00:19:05
106,tensorflow/models,models,6374,d-jax,Feature request: A better WER threshold for Deep Speech V2,"### System information
- **What is the top-level directory of the model you are using**:
models/research/deep_speech/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
tensorflow-gpu==1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.0
- **GPU model and memory**:
Nvidia Tesla V100
- **Exact command to reproduce**:
run_deep_speech.sh

### Describe the problem
The default WER threshold for training Deep Speech V2 is 23.0, something the author copied from [mlperf](https://github.com/mlperf/training/tree/master/speech_recognition). This is significantly higher than the WER reported by the paper. It is also unclear if the model can reproduce the SotA WER if ""unlimited"" hardware resource / time can be spent to train it. 

Please consider providing a training configuration that has been confirmed to reproduce the SotA.",0,,[],2019-03-15 21:42:53,open,,,[],2019-03-15 21:42:53
107,tensorflow/models,models,6373,roburst2,How Transfer learning is working for tensorflow object detection api,"I have used Faster RCNN Resnet101 as pretrained weight with default pipeline config settings.
I want to know whether it will retrain the all layers with pretrained weights or only last layer with pretrained weights.",1,,[],2019-03-15 13:32:13,open,,,[],2019-03-15 22:55:33
108,tensorflow/models,models,6372,ravikantSTAR,absl.flags._exceptions.IllegalFlagValueError: flag --pipeline_config_path=None: Flag --pipeline_config_path must have a value other than None.,"I'm trying to export the trained model in colab. Getting the following error.
Please help!

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py"", line 528, in _assert_validators
    validator.verify(self)
  File ""/usr/local/lib/python3.6/dist-packages/absl/flags/_validators.py"", line 81, in verify
    raise _exceptions.ValidationError(self.message)
absl.flags._exceptions.ValidationError: Flag --pipeline_config_path must have a value other than None.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/export_inference_graph.py"", line 156, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 119, in run
    argv = flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/flags.py"", line 112, in __call__
    return self.__dict__['__wrapped'].__call__(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py"", line 636, in __call__
    self._assert_all_validators()
  File ""/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py"", line 510, in _assert_all_validators
    self._assert_validators(all_validators)
  File ""/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py"", line 531, in _assert_validators
    raise _exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))
absl.flags._exceptions.IllegalFlagValueError: flag --pipeline_config_path=None: Flag --pipeline_config_path must have a value other than None.",1,,[],2019-03-15 12:50:53,open,,,['stat:awaiting response'],2019-03-16 12:17:08
109,tensorflow/models,models,6371,AI-LastWish,Object detection oid_v4_label_map.pbtxt doesn't match oidv4 model(faster_rcnn_inception_resnet_v2_atrous_oidv4 and ssd_mobilenetv2_oidv4),"I've tested oidv4 model(faster_rcnn_inception_resnet_v2_atrous_oidv4 and ssd_mobilenetv2_oidv4) with oid_v4_label_map.pbtxt and there are some mismatch between bounding box and labels.

The specific result can be seen at here: https://github.com/AI-LastWish/Deep-Learning-CV/blob/master/Bug_Report/faster_rcnn_inception_resnet_v2_atrous_oidv4.ipynb
and here 
https://github.com/AI-LastWish/Deep-Learning-CV/blob/master/Bug_Report/ssd_mobilenetv2_oidv4.ipynb

You can see in the first picture of faster_rcnn_inception_resnet_v2_atrous_oidv4.ipynb file, there is a ""Computer monitor 43%"", which is very very weird and a lot of ""screwdriver"" while I think must be a ""person"". In the second picture of the same file, the model predicts ""sofa bed"", while indeed it is a face, another unacceptable mismatch. The same for the second file.

One more question: 
From 
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
Researchers tested by using COCO 14 minival set(about 8000 images). So they used approximately 8000 over 40000 images in  COCO 14 Test Dataset, but how could they evaluate? Did they upload only 8000/40000 images to Codalab and got corrected result? So to calculate mAP of test set, we don't need to test over all 40000 images, just a subset and upload to Codalab also OK, right? We don't have ground truth annotation of Test set?",4,,[],2019-03-15 06:03:41,open,,,[],2019-03-18 02:06:39
110,tensorflow/models,models,6367,xunkai55,Test mobilenet v2,,1,,[],2019-03-14 22:25:07,open,,,['cla: yes'],2019-04-03 15:46:26
111,tensorflow/models,models,6366,shamik111691,TypeError: __init__() missing 2 required positional arguments: 'inputs' and 'outputs',"Hi, 

I want to try the object-detection API of tensorflow. I have tried the following to test if the installation is correct.
python3 object_detection/builders/model_builder_test.py

However, I get the following error:
.......E........
======================================================================
ERROR: test_create_ssd_models_from_config (__main__.ModelBuilderTest)
test_create_ssd_models_from_config (__main__.ModelBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 203, in test_create_ssd_models_from_config
    model = model_builder.build(model_proto, is_training=True)
  File ""<my_path>/models/research/object_detection/builders/model_builder.py"", line 121, in build
    return _build_ssd_model(model_config.ssd, is_training, add_summaries)
  File ""<my_path>/models/research/object_detection/builders/model_builder.py"", line 244, in _build_ssd_model
    is_training=is_training)
  File ""<my_path>/models/research/object_detection/builders/model_builder.py"", line 220, in _build_ssd_feature_extractor
    return feature_extractor_class(**kwargs)
  File ""
<my_path>/models/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py"", line 86, in __init__
    name=name)
  File ""<my_path>/models/research/object_detection/meta_architectures/ssd_meta_arch.py"", line 180, in __init__
    super(SSDKerasFeatureExtractor, self).__init__(name=name)
TypeError: __init__() missing 2 required positional arguments: 'inputs' and 'outputs'

----------------------------------------------------------------------
Ran 16 tests in 0.070s

FAILED (errors=1)

Could you please point me to how can I fix this?

Thanks.
",2,,[],2019-03-14 17:44:28,open,,,[],2019-03-16 12:17:02
112,tensorflow/models,models,6363,momo1986,ValueError: Duplicate node name in graph,"I tried to import mobile-net in my object-detection project:
Here is my code:
```
model {
  ssd {
    num_classes: 2
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.03
              mean: 0.0
            }
          }
          batch_norm {
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v1'
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.9997,
          epsilon: 0.001,
        }
      }
    }
    loss {
      classification_loss {
        weighted_sigmoid {
          anchorwise_output: true
        }
      }
      localization_loss {
        weighted_smooth_l1 {
          anchorwise_output: true
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 0
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  batch_size: 16
  optimizer {
    rms_prop_optimizer: {
      learning_rate: {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.004
          decay_steps: 800720
          decay_factor: 0.95
        }
      }
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""/fast/junyan/HandDetection/hands-detection-gcloud/models/mobilenet_v1/mobilenet_v1_1.0_224.ckpt""
  from_detection_checkpoint: true
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""../hands_train.record""
  }
  label_map_path: ""../hands_label_map.pbtxt""
}

eval_config: {
  num_examples: 738
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""../hands_val.record""
  }
  label_map_path: ""../hands_label_map.pbtxt""
  shuffle: false
  num_readers: 1
}

```

Here is my error:
> Traceback (most recent call last):
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1628, in _create_c_op
>     c_op = c_api.TF_FinishOperation(op_desc)
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Duplicate node name in graph: 'MobilenetV1/Conv2d_0/weights/RMSProp'
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""object_detection/train.py"", line 198, in <module>
>     tf.app.run()
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""object_detection/train.py"", line 194, in main
>     worker_job_name, is_chief, FLAGS.train_dir)
>   File ""/fast/junyan/HandDetection/hands-detection-gcloud/models/object_detection/trainer.py"", line 244, in train
>     global_step=global_step)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/opt/python/training/moving_average_optimizer.py"", line 94, in apply_gradients
>     grads_and_vars, global_step=global_step, name=name)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 593, in apply_gradients
>     self._create_slots(var_list)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py"", line 124, in _create_slots
>     self._name)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 1118, in _get_or_make_slot_with_initializer
>     var, initializer, shape, dtype, op_name)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py"", line 157, in create_slot_with_initializer
>     dtype)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py"", line 65, in _create_slot_var
>     validate_shape=validate_shape)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1487, in get_variable
>     aggregation=aggregation)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1237, in get_variable
>     aggregation=aggregation)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 540, in get_variable
>     aggregation=aggregation)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 492, in _true_getter
>     aggregation=aggregation)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 922, in _get_single_variable
>     aggregation=aggregation)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 183, in __call__
>     return cls._variable_v1_call(*args, **kwargs)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 146, in _variable_v1_call
>     aggregation=aggregation)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 125, in <lambda>
>     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 2444, in default_variable_creator
>     expected_shape=expected_shape, import_scope=import_scope)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 187, in __call__
>     return super(VariableMetaclass, cls).__call__(*args, **kwargs)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 1329, in __init__
>     constraint=constraint)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 1443, in _init_from_args
>     name=name)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py"", line 77, in variable_op_v2
>     shared_name=shared_name)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 1357, in variable_v2
>     shared_name=shared_name, name=name)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
>     return func(*args, **kwargs)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
>     op_def=op_def)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1792, in __init__
>     control_input_ops)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1631, in _create_c_op
>     raise ValueError(str(e))
> ValueError: Duplicate node name in graph: 'MobilenetV1/Conv2d_0/weights/RMSProp'

",2,,[],2019-03-14 12:47:36,open,,,[],2019-03-19 00:22:30
113,tensorflow/models,models,6362,juliaschell,Multiple model.ckpt.data files out of 0003 rather than 0001,"### Describe the problem
I trained my model using Google Cloud ML engine. The resulting checkpoint files were model.ckpt.index and model.ckpt.meta as expected, but it output 3 .data files as
model.ckpt.data-00000-of-00003
model.ckpt.data-00001-of-00003
model.ckpt.data-00002-of-00003
rather than a single model.ckpt.data-00000-of-00001 as expected. 

I attempted to use the export_inference_graph.py script in the models/research/object_detection directory, which specifies an input files 
model.ckpt.data-00000-of-00001
model.ckpt.index
model.ckpt.meta

The script completed with no errors, but when I tried to use the resulting frozen_inference_graph.pb with rest of my system (that works with a pretrained model downloaded from the model zoo) I got the error: 
```
libc++abi.dylib: terminating with uncaught exception of type cv::Exception:OpenCV(4.0.1) 
/tmp/opencv-20190105-31032-o160to/opencv-4.0.1/modules/dnn/src/tensorflow/tf_importer.cpp:530: 
error: (-2:Unspecified error) Const input blob for weights not found in function 'getConstBlob'
```
I'm hoping there's a way to freeze and export my model with this file structure!
",1,,[],2019-03-14 07:13:48,open,,,['stat:awaiting response'],2019-03-15 00:25:38
114,tensorflow/models,models,6361,NightFury10497,"1. Changing the learning may help, because the one exists now in the pipeline.config is probably not what you need and it the one that was used for the training that was done from scratch.","1. Changing the learning may help, because the one exists now in the pipeline.config is probably not what you need and it the one that was used for the training that was done from scratch.
2. Regarding crops... - I guess it depends on the image resolution you have... I would try first to continue as you did - meaning work with 512x512 Fixed resizer and compare it to results you get on 300x300.
3.More thoughts: take in mind the sized of the objects to intend to detect.
The model is trained to have 6 output branches with 6 anchors per pixel (except for the first branch which has 3 anchors) - 
`
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
`
  - do you really need these 6 output branches? 
  - i guess you can even remove the two last aspect ratios (3:1, 1:3) - because face tends to be more ""boxy""  - 
  -         min_scale: 0.2
            max_scale: 0.95  
      after you will see that the model starts to learn something - this it another thing you may want to tune . min scale defines the anchors scale relative to the image an the first output branch, max scale --> anchors scale relative to the image at the last layer (it is interpolated in all the output layers inbetween)
 reducing the scales may help to find smaller objects...

_Originally posted by @tmyapple in https://github.com/tensorflow/models/issues/3196#issuecomment-472709544_
Do you mean i should alter:
anchor_generator {
ssd_anchor_generator {
num_layers: 6
min_scale: 0.20000000298023224
max_scale: 0.949999988079071
aspect_ratios: 1.0
aspect_ratios: 2.0
aspect_ratios: 0.5
aspect_ratios: 3.0
aspect_ratios: 0.33329999446868896
}
}
to
anchor_generator {
ssd_anchor_generator {
num_layers: 6
min_scale: 0.2
max_scale: 0.95
}
}",1,,[],2019-03-14 06:41:38,open,,,['stat:awaiting response'],2019-03-15 00:25:33
115,tensorflow/models,models,6360,xhzhao,object detection - How to repeate the pretrained model accuracy on coco?,"Our team is working on tensorflow performance optimization for object detections, and we tried to repeat the pretrained model accuracy on coco dataset. AFAIK, there is no guide to show details to repeat the model accuracy.
We tried to use the guide for the Open Image Dataset on this [link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/oid_inference_and_evaluation.md), and it is really suffering and we have to make many hacks, and bad news is that the finally mAP accuracy is not the same as the released data (27 vs 24).

a similar issue found [here](https://github.com/tensorflow/models/issues/6341)
Am i missing something?


![image](https://user-images.githubusercontent.com/17486215/54327811-82587100-4646-11e9-93d8-d9a664de0f80.png)
",2,,[],2019-03-14 02:53:26,open,,,[],2019-03-14 18:24:52
116,tensorflow/models,models,6357,chenjunweii,how to train ssd with larger input resolution,"I would like to train ssd mobilenet 512, I have modified the config like the following code, but the performance seems not much different compared to default  ssd mobilenet 300, is there any other places that i have to modify ? thanks

```
image_resizer {
       fixed_shape_resizer {
        height: 512
        width: 512
      }
}

```",3,,[],2019-03-13 09:55:18,open,,,[],2019-03-16 15:19:10
117,tensorflow/models,models,6354,KaaiLee,object detection model file,I downloaded the pre-training model for object detection. There is no graph proto（graph.pbtxt) file in the file. Where can I download the .pbtxt file?,2,,[],2019-03-13 02:11:43,open,,,[],2019-03-14 11:45:21
118,tensorflow/models,models,6348,oleksandr-c,[Object detection] Postprocessing added by export_tflite_ssd_graph.py returns indices that are larger then the amount of classes or are negative on inference.,"### System information
- **What is the top-level directory of the model you are using**: /models/research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Only for the example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.2
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.12.0-9988-g74b961829e 1.14.1-dev20190312
- **Bazel version (if compiling from source)**: 0.22.0
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**: GTX 1080ti 11GB
- **Exact command to reproduce**: 

### Describe the problem
Postprocessing added by export_tflite_ssd_graph.py returns indices that are larger then the amount of classes or are negative.

### Source code / logs
I trained a ssdlite_mobilenet_v2_coco model in quantization aware mode by adding the following to the pipeline.config file:
```bash
graph_rewriter {
  quantization {
    delay: 1800
    activation_bits: 8
    weight_bits: 8
  }
}
```
Checkpoint converted to `.pb` using:
```bash
python object_detection/export_tflite_ssd_graph.py \
    --pipeline_config_path=$CONFIG_FILE \
    --trained_checkpoint_prefix=$CHECKPOINT_PATH \
    --output_directory=$OUTPUT_DIR \
    --add_postprocessing_op=true
```
`.pb`converted to `.tflite` using:
```bash
bazel run --config=opt tensorflow/lite/toco:toco -- \
    --input_file=$OUTPUT_DIR/tflite_graph.pb \
    --output_file=$OUTPUT_DIR/detect.tflite \
    --input_shapes=1,300,300,3 \
    --input_arrays=normalized_input_image_tensor \
    --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
    --inference_type=QUANTIZED_UINT8 \ 
    --mean_values=128 \
    --std_values=128 \
    --change_concat_input_ranges=false \
    --allow_custom_ops
```
[Here](https://drive.google.com/open?id=1kz7AD8aUv0DR5arqZ0X7LJf6dahlGawa) is the resulting model file. [Here](https://drive.google.com/open?id=1oiwsCb7sAt61gpePiV1-1tz3JpHJLPXP) is an example image.
Minimal example:
```python
import numpy as np
import tensorflow as tf
import cv2
tf.enable_eager_execution()

mnetv2_path = ""detect.tflite""
mnetv2 = tf.lite.Interpreter(model_path=mnetv2_path)
mnetv2.allocate_tensors()

input_details_mnetv2 = mnetv2.get_input_details()
output_details_mnetv2 = mnetv2.get_output_details()

img = cv2.imread('broken.png')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = cv2.resize(img, (300,300))
img = np.expand_dims(img, axis=0).astype('uint8')
input_tensor = tf.convert_to_tensor(img, dtype=np.uint8)
mnetv2.set_tensor(input_details_mnetv2[0]['index'], input_tensor)
mnetv2.invoke()

classes = mnetv2.get_tensor(output_details_mnetv2[1]['index'])
print(classes.astype('int'))
```
This will return `[[0  0  0  576460752303423488 0  0  0  0  0  0]]`
",0,,[],2019-03-12 16:06:31,open,,,[],2019-03-12 16:06:31
119,tensorflow/models,models,6346,xiehuc,add original thompson_sampling for deep_contextual_bandits,"@rikel i add a original MAB thompson sampling, helpful for compare with Contextual Bandit algorighm effective.

please merge this pull request",3,,[],2019-03-12 06:25:49,open,,,['cla: yes'],2019-03-12 06:28:38
120,tensorflow/models,models,6344,Prilyf,TensorRT speed up ssd_mobilenet_v1_fpn_coco,"When I try to use tensorRT speed up ssd_mobilenet_v1_fpn_coco,I encountered the following error.How can I solve this?
![image](https://user-images.githubusercontent.com/8620727/54173894-bbfe7000-44be-11e9-980e-58bcf25c5976.png)
![image](https://user-images.githubusercontent.com/8620727/54173902-c3be1480-44be-11e9-853d-a3189f32edb0.png)
",3,,[],2019-03-12 04:03:38,open,,,[],2019-04-04 00:21:05
121,tensorflow/models,models,6343,theangels,ObjectDetection API Get wrong inference result when I update the tensorflow version and models version,"### System information
- **What is the top-level directory of the model you are using**: /Appendix/tensorflow_models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Nope
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.13.1, and the models lib clone on Mar 11, 2019
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 10.0
- **GPU model and memory**: 1050Ti
- **Exact command to reproduce**:
PIPELINE_CONFIG_PATH='/home/jovyan/Codelab/model/pipeline.config'
MODEL_DIR='/home/jovyan/Codelab/data'
NUM_TRAIN_STEPS=50000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python3 object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr

### Describe the problem

We use the old version of tf (1.11) and the old version of the models library (clone on December 4, 2018) to train the model and got the checkpoint file (we called C file) and frozen_inference_graph.pb file(we called F file)

In the environment of tf 1.13.1 and the latest models library(clone on Mar 11, 2019), it is right to use the original version of the F file directly, but if the old version of the C file is converted to the new version of the F file, the conversion proceeds smoothly, but the prediction result is wrong.

Here are the screenshots.

tf-1.11-models-old
TensorFlow version: 1.11-gpu installed by pip
models version: clone on December 4, 2018
![snipaste20190312_104819](https://user-images.githubusercontent.com/11308780/54172292-91111d80-44b8-11e9-80f4-f709d663e299.png)
![snipaste20190312_104916](https://user-images.githubusercontent.com/11308780/54172293-91111d80-44b8-11e9-829d-f2f1d52076d8.png)

TensorFlow version: 1.13-gpu installed by pip
models version: clone on Mar 11, 2019
tf-1.13-models-new-directlyinf
![snipaste20190312_110744](https://user-images.githubusercontent.com/11308780/54172300-9a9a8580-44b8-11e9-96f6-98f7eb4f23ec.png)

TensorFlow version: 1.13-gpu installed by pip
models version: clone on Mar 11, 2019
tf-1.13-models-new-convertedCfile
![snipaste20190312_105849](https://user-images.githubusercontent.com/11308780/54172345-b7cf5400-44b8-11e9-8e32-a344fd1d81d3.png)
![snipaste20190312_105932](https://user-images.githubusercontent.com/11308780/54172346-b7cf5400-44b8-11e9-8f70-11bd1ecde1a8.png)

@wangtz ",1,,[],2019-03-12 03:24:21,open,,,['stat:awaiting response'],2019-03-22 13:09:00
122,tensorflow/models,models,6341,lbingbing,Are the object detection evaluation scripts still working for coco dataset?,"Are the evaluation scripts still working for coco dataset, say on model ssd_inception_v2?
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/oid_inference_and_evaluation.md

I tried the flow above with tfrecord generated by create_coco_tf_record. But it looks like it's quite bumpy, that I have to fix several typo-looking issues in the source code to make it through.

```
object_detection/inference/detection_inference.py:65
-  with tf.gfile.Open(inference_graph_path, 'r') as graph_def_file:
+  with tf.gfile.Open(inference_graph_path, 'rb') as graph_def_file:

object_detection/metrics/offline_eval_map_corloc.py:159
-  input_config = configs['eval_input_config']
+  input_config = configs['eval_input_configs'][0]
```

Finally all image got skipped when computing evaluation measures from tfrecord, because some 'groundtruth_classes' can't be found in annotations.

So how can I evaluate ssd_inception_v2 in model zoo on coco dataset exactly? Do I need to try on Python2? Do I need to try object_detection/legacy/eval.py?",2,,[],2019-03-11 13:18:57,open,,,[],2019-03-19 13:36:46
123,tensorflow/models,models,6340,Prilyf,ssd_mobilenet_v1_fpn_coco,"When it's config set ""use_depthwise:true"",training after detection,the result is nothing.What can I do about it?",3,,[],2019-03-11 09:03:23,open,,,[],2019-03-11 14:51:22
124,tensorflow/models,models,6338,Linranran,which version tensorflow mnist demo need,"when i run the offical mnist demo, there are a lot of errors occur 
`Traceback (most recent call last):
  File ""C:/Users/ranran/Desktop/project/models/official/mnist/mnist.py"", line 25, in <module>
    from official.utils.flags import core as flags_core
  File ""C:\Users\ranran\Desktop\project\models\official\utils\flags\core.py"", line 30, in <module>
    from official.utils.flags import _base
  File ""C:\Users\ranran\Desktop\project\models\official\utils\flags\_base.py"", line 25, in <module>
    from official.utils.logs import hooks_helper
  File ""C:\Users\ranran\Desktop\project\models\official\utils\logs\hooks_helper.py"", line 29, in <module>
    from official.utils.logs import hooks
  File ""C:\Users\ranran\Desktop\project\models\official\utils\logs\hooks.py"", line 28, in <module>
    class ExamplesPerSecondHook(tf.estimator.SessionRunHook):
AttributeError: module 'tensorflow.python.estimator.estimator_lib' has no attribute 'SessionRunHook'`


",1,,[],2019-03-11 02:08:20,open,,,['stat:awaiting response'],2019-03-12 00:18:23
125,tensorflow/models,models,6337,zychen2016,how to use Android Studio do UI development with object detection app?,"how to use Android Studio do UI development and camera zooming with object detection app?

I have use bazel do a object detection app with my own data, But how could I use bazel do UI development and camera zooming? Using bazel on linux  command line, I can get app that run smoothly.
But When I use Android Studio build app，It will crash and report error ""Object tracking support not found"".
So I was puzzled.


### System information
- **What is the top-level directory of the model you are using**:  

Using Object detection api train my own data use SSD_mobilenetv2_model.
import tensorflow as tf---->version 1.12
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: using pip
- **TensorFlow version (use command below)**:1.10
- **Bazel version (if compiling from source)**:

download tensorflow 1.10 source code .
Just Using bazel build tensorflow/python/tools:freeze_graph and tensorflow/contrib/lite/toco:toco.
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:1080Ti
- **Exact command to reproduce**:
",0,"NamedUser(login=""miaout17"")","[NamedUser(login=""miaout17"")]",2019-03-11 00:46:04,open,,,[],2019-03-12 21:26:40
126,tensorflow/models,models,6335,bugLoser,[deeplab] reproduce details,"### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.13.1
- **Bazel version (if compiling from source)**: no
- **CUDA/cuDNN version**: 10.0 7.4
- **GPU model and memory**: V100(16G) X 4
- **Exact command to reproduce**: no

### Describe the problem
I spent lots of weeks in reproducing the deeplabv3+(xception) result on test dataset of voc2012, but I failed. The highest score i got was 86.66, smaller than 87.8. So could you please share much more details on training? I have some quentions：
1）coco pretrain details are showed in deeplabv3 papers:

> From the MS-COCO trainval_minus_minival set, we only select the images that have annotation regions larger than 1000 pixels and contain the classes defined in PASCAL VOC 2012, resulting in about
60K images for training.

I used the same split, and selected the images that have annotation regions(the classes defined in PASCAL VOC 2012) larger than 1000 pixels, but got 90K+ images。Could you please share the script?

2）With coco pretrained, which training mode is right? or another?
name(dataset_split, training steps, init learning rate, output stride)
①pretrain(coco, 500k, 0.07, 16) + train(train_aug, 3k, 0.07, 8) + finetune(trainval, 3k, 0.0001, 8)
②pretrain(coco, 500k, 0.07, 16) + train(train_aug, 3k, 0.07, 16) + finetune(trainval, 3k, 0.0001, 8)
③pretrain(coco+train_aug, 500k, 0.07, 16) + finetune(trainval, 3k, 0.0001, 8)

And without coco?

NOTE: Finetune details in deeplabv3 paper。
> In particular, we duplicate the images that contain hard classes (namely bicycle, chair, table, pottedplant, and sofa) in the training set.

I duplicate the images in the trainval set and name it hardtrainval(3609 images)。
Follow ① with coco( more than 10K pixels, 70k+ images) and finetune hardtrainval, i got best result(86.66)。

3）There are three trained model provided.
①xception_65_imagenet_coco
②xception65_coco_voc_trainaug
③xception65_coco_voc_trainval

① + train(train_aug, 3k, 0.07, 16) = ② 
① + train(train_aug, 3k, 0.07, 16) + finetune(train, 3k, 0.0001, 16) = ②
① + train(train_aug, 3k, 0.07, 8) = ② 
① + train(train_aug, 3k, 0.07, 8) + finetune(train, 3k, 0.0001, 8) = ②
② + finetune(trainval, 3k, 0.0001, 8) = ③
which modes are right? or others?

4） If i want to reproduce the cityscapes result,  how to deal without/with coco and coarse set

Thanks!
### Source code / logs
no
",1,,[],2019-03-10 08:52:35,open,,,[],2019-03-28 09:33:16
127,tensorflow/models,models,6334,rhlr,Loss of Object detection model using ssd_mobilenet_v2_quantized_300x300_coco increases after every 10k-12k steps,"------------------------

### System information
- **What is the top-level directory of the model you are using**:
  ~/models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
  No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
  Linux instance-3 4.15.0-1028-gcp #29~16.04.1-Ubuntu SMP Tue Feb 12 16:31:10 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
  VERSION=""16.04.5 LTS (Xenial Xerus)""
  VERSION_ID=""16.04""
  VERSION_CODENAME=xenial

- **TensorFlow installed from (source or binary)**:
  binary
- **TensorFlow version (use command below)**:
  tf.VERSION = 1.12.0
  tf.GIT_VERSION = v1.12.0-0-ga6d8ffae09
  tf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
  9.0/7.4
- **GPU model and memory**:
  NVIDIA TESLA K80: 11439MiB
- **Exact command to reproduce**:
`python object_detection/model_main.py --pipeline_config_path=ssd_mobilenet_v2_quantized_300x300_coco.config  --model_dir=train  --num_train_steps=500000 --alsologtostder`

### Describe the problem
I'm re-training `ssd_mobilenet_v2_quantized_300x300_coco` `object detection` model on a custom dataset. The dataset consists of approx 2.6k images and 19 classes. After the training step reaches 10k-12k the loss graph starts increasing. This happens even if I change my model to `ssd_mobilenet_v2_coco` and at the same step range. I couldn't find anything that is related to this behaviour in the config file. Also this disappers when using `faster_rcnn` models. When the issue arises the `mAP` becomes almost constant. Also tha accuracy doesn't go beyond 50%. Can anyone explain this behaviour ?


### Source code / logs
![Screenshot from 2019-03-10 10-15-53-02](https://user-images.githubusercontent.com/1620866/54081184-67c08800-4326-11e9-9e1e-3624cdc13e8f.png)
a) ssd_mobilenet_v2_quantized_300x300_coco
![Screenshot from 2019-03-10 10-17-26-01](https://user-images.githubusercontent.com/1620866/54081185-6db66900-4326-11e9-9009-31e3c088b6ae.png)
b) ssd_mobilenet_v2_coco

### Sample dataset:

![individualImage](https://user-images.githubusercontent.com/1620866/54081211-e3bad000-4326-11e9-8a0b-3c993252ed3e.png)

### Config

[ssd_mobilenet_v2_quantized_300x300_coco.txt](https://github.com/tensorflow/models/files/2949100/ssd_mobilenet_v2_quantized_300x300_coco.txt)
a) ssd_mobilenet_v2_quantized_300x300_coco.config

[ssd_mobilenet_v2_coco.txt](https://github.com/tensorflow/models/files/2949102/ssd_mobilenet_v2_coco.txt)
b) ssd_mobilenet_v2_coco.config
",5,,[],2019-03-10 05:59:10,open,,,[],2019-03-12 02:23:13
128,tensorflow/models,models,6333,swirlingsand,Clarify docs on encoded_image_string_tensor expects bytes with magic jpeg start bytes,"In the context of exporting a model using either `encoded_image_string_tensor` or `tf_example` and getting predictions.  For example in ml engine using
`row = {'inputs': {'b64': base64.b64encode(image).decode()}}`

I had made an assumption that converting a numpy array ie
```
image = blob.download_as_string()
image = scipy.misc.imread(BytesIO(image))
```
would work. All of the data that's need is there! But this results in:

 `[Unable to decode bytes as JPEG, PNG, GIF, or BMP]` 

Which then lead me to assume there was some issue with the encoding, ie the ickiness of base64 + the magic 'b64' for ml engine + the decode() for utf in python 3. Like come on the issue should be somewhere in that mess right? But no!

The issue appears to be that `encoded_image_string_tensor` doesn't really want an encoded image, at least in terms of raw data, it wants a JPEG specific image.

There's a few issues already filed under [Unable to decode bytes as JPEG, PNG, GIF, or BMP], and one pointed to something about jpeg, but I think the docs need updating. Going to stack overflow / viewing as support isn't really right for such a critical element, that's so poorly define in other places. Some of the existing help articles point towards issues with python 2/3 that are more misleading than helpful. 

Suggestion: Declare that encoded_image_string_tensor expects the ""image"" to be loaded as a jpeg format, and just providing the raw image data itself will yield  `[Unable to decode bytes as JPEG, PNG, GIF, or BMP]`. This may be especially relevant for people used to working with earlier versions of tensorflow and providing numpy arrays directly.

Edit, to clarify what does work: for cloud ml engine downloading it directly works
```
image = blob.download_as_string()
row = {'inputs': {'b64': base64.b64encode(image).decode()}}
```
And for local use cases there is some help here: https://github.com/tensorflow/tensorflow/issues/13044
",1,,[],2019-03-10 02:09:28,open,,,['stat:awaiting response'],2019-03-11 00:16:36
129,tensorflow/models,models,6329,swirlingsand,TypeError: resnet_v1_50() got an unexpected keyword argument 'min_depth',"### System information
- **What is the top-level directory of the model you are using**:  models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Cloud ml engine
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**:  Cloud ml engine
- **GPU model and memory**: standard
- **Exact command to reproduce**: Run a model with resnet_v1_50

### Describe the problem & Source code / logs
```
line 568, in predict preprocessed_inputs) File ""/root/.local/lib/python3.5/site-packages/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py"", line 169, in extract_features scope=scope) TypeError: resnet_v1_50() got an unexpected keyword argument 'min_depth'

```

Also same error for `depth_multiplier`
",7,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2019-03-09 00:01:20,open,,,['models: research'],2019-03-27 11:11:00
130,tensorflow/models,models,6325,juanluisrosaramos,Change in one comment from coordinates,I think there is an error in the coordinates (if I am not wrong),0,,[],2019-03-08 11:58:09,open,,,['cla: yes'],2019-03-08 11:58:11
131,tensorflow/models,models,6324,HanGuo97,Quick Draw Tutorial: Added Dropout to `DropoutWrapper`,"Currently, the dropout seems to be ignored in regular RNN given that it's not actually used in the `DropoutWrapper`:

```python
if params.dropout > 0.0:
      cells_fw = [tf.contrib.rnn.DropoutWrapper(cell) for cell in cells_fw]
      cells_bw = [tf.contrib.rnn.DropoutWrapper(cell) for cell in cells_bw]
```
",0,,[],2019-03-08 05:44:28,open,,,['cla: yes'],2019-03-08 05:48:42
132,tensorflow/models,models,6323,grandpahao,Failure to set GPU options in models/official/mnist/mnist.py,"I am encountered with a failure in setting GPU options when running an official program, models/official/mnist/mnist.py.
I have added the following line ""session_config.gpu_options.allow_growth=True"" right after the construction of ""session_config"".
But it fails to work, and the program still takes up all of the space of my GPU.
I wonder if the problem can be addressed within the scope of ""tf.estimator"".",2,,[],2019-03-08 02:13:42,open,,,[],2019-03-08 03:51:42
133,tensorflow/models,models,6321,HYOJINPARK,modified xception  train for imageNet ,"I try to train modified xception from scratch in imageNet for segmentation.
I am confused about using atrous rate for making imageNet pretrained model
Is it right that inserting atrous rate in modified xception model ?",1,,[],2019-03-07 08:46:55,open,,,['stat:awaiting response'],2019-03-08 00:22:18
134,tensorflow/models,models,6319,veesamkrao999,"error @ ssd_inception_v2_coco while running cmd on ubuntu 18.04 "" python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco.config  "" ","WARNING:tensorflow:From /home/krao/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py:125: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()

  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 574, in Merge
    descriptor_pool=descriptor_pool)
  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 631, in MergeLines
    return parser.MergeLines(lines, message)
  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 654, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 676, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 801, in _MergeField
    merger(tokenizer, message, field)
  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 875, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 801, in _MergeField
    merger(tokenizer, message, field)
  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 875, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 801, in _MergeField
    merger(tokenizer, message, field)
  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 875, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/home/krao/.local/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 768, in _MergeField
    (message_descriptor.full_name, name))
google.protobuf.text_format.ParseError: 135:1 : Message type ""object_detection.protos.PostProcessing"" has no field named ""model"".",2,,[],2019-03-07 06:05:25,open,,,[],2019-03-09 00:19:11
135,tensorflow/models,models,6316,vighneshbirodkar,Refactored ResNet code and added additional architectures.,,10,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2019-03-07 02:24:15,open,,,['cla: yes'],2019-04-09 17:51:52
136,tensorflow/models,models,6312,danyfang,fix unitest failure in faster_rcnn_meta_arch_test_lib.py,"The pull request contains two fixes.
The first one is the missing method `_get_box_classifier_features_shape` in class `FasterRCNNMetaArchTestBase`, this will cause the following error 'AttributeError: 'FasterRCNNMetaArchTestBase' object has no attribute '_get_box_classifier_features_shape'.

The second one is caused by python version. In `faster_rcnn_meta_arch_test_lib.py` line 1677 the `uninitialized_vars_list` is returned as a bytes array in python3 so the assertion in the following line will fail in  python3.",4,,[],2019-03-06 16:44:30,open,,,['cla: yes'],2019-03-06 17:53:12
137,tensorflow/models,models,6308,kulkarnivishal,Feature request: Cloud ML engine training support for tensorflow version 1.13 ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
models/research/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.13
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
10
- **GPU model and memory**:
Nvidia Tesla V100
- **Exact command to reproduce**:
       python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Training a model locally is equivalent to shooting in the dark at present. The logtostderr flag doesn't work as it doesn't log loss and model evaluation results. It would be really helpful if you include support for latest tensorflow version (tf1.13) on Google's Cloud ML engine. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

N/A
",0,,[],2019-03-05 18:55:20,open,,,[],2019-03-06 00:07:59
138,tensorflow/models,models,6306,wlongxiang,Add option to export model with best mAP@0.5 during training,"Hi community,
this is my first PR, so please bear with me if the changes are too naive. Thanks.",4,,[],2019-03-05 14:42:23,open,,,['cla: yes'],2019-03-16 13:53:26
139,tensorflow/models,models,6305,nguyenkhaithinh,Mask RCNN issue with CUDA: Check failed: work_element_count > 0 (-2064695296 vs. 0),"### System information
- **What is the top-level directory of the model you are using**: models/research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip3
- **TensorFlow version (use command below)**:tensorflow-gpu 1.13.1
- **CUDA/cuDNN version**: CUDA 10/cuDNN v7.4.2
- **GPU model and memory**: GTX 1080Ti, 11GB
- **Exact command to reproduce**:
python3 object_detection/model_main.py \
--pipeline_config_path=""/path/to/config"" \
--model_dir=""/path/to/model"" \
--alsologtostderr \
--checkpoint_dir=""path/to/checkpoints""


### Describe the problem
1. I followed the sample code from ""create_pet_tf_record.py"" to create dataset for Mask-RCNN (I'm using model ""mask_rcnn_resnet101_atrous_coco_2018_01_28""). I made my own modification due to the sample code is for one object per category, not for multiple objects and categories.


2. The training process is fine without any problems. However, when I evaluate the model, it returns this error:

```
2019-03-05 13:33:44.814437: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2019-03-05 13:33:46.705803: F ./tensorflow/core/util/cuda_launch_config.h:126] Check failed: work_element_count > 0 (-2064695296 vs. 0)
Aborted (core dumped)
```

I'm not sure whether this is a bug in tensorflow or an error within my model. The sizes of images are 200x200.

3. How to limit the amount of GPU memory during training process (for instance, 60%) so that the rest of memory will be used or use CPU only for evaluation?

Thank you very much.",2,,[],2019-03-05 07:35:44,open,,,['models: research'],2019-03-13 20:57:16
140,tensorflow/models,models,6303,olgn,Update object detection tutorial for use in Colab,Google Colab will only display image results if the `%matplotlib inline` _precedes_ `from matplotlib import pyplot as plt`. This pr moves the ENV code cell above the IMPORTS cell.,0,,[],2019-03-04 19:50:33,open,,,['cla: yes'],2019-03-04 19:50:36
141,tensorflow/models,models,6301,tejank10,Migration of official/mnist to TF 2.0,"Migrated `dataset.py`, `mnist_eager.py` and `mnist_eager_test.py` to TF 2.0

I am not sure about `defun` tests in `mnist_eager_test.py`, have skipped them for now. Can someone tell me about how to go about them?",3,,[],2019-03-04 16:06:31,open,,,['cla: yes'],2019-03-09 06:50:42
142,tensorflow/models,models,6299,EnricoBeltramo,vid2depth run on windows,"Is it possible to use vid2depth in Wondows 10?

I'm using 
OS: Windows 10
Tensorflow version: 1.12.0
CUDA 9.0
cudnn: 7.1

The error that appears running the inference command is:
E0303 23:47:52.008982 14784 icp_op.py:29] Could not load object file for ICP op.
That I suppose i related to load of custom ops (not available on Windows)",3,,[],2019-03-03 23:00:04,open,,,['models: research'],2019-03-29 21:55:46
143,tensorflow/models,models,6298,godblezzme29,struct2depth Convolution not supported for input with rank,"### System information
- **What is the top-level directory of the model you are using**: models/research/struct2depth/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: WIndows 10
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.5
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9 cuDNN 7
- **GPU model and memory**: GTX 1050 2GB
- **Exact command to reproduce**: 
python train.py \ --logtostderr \ --checkpoint_dir D:\Kuliah\S2\Depth_Estimation\models-master\research\struct2depth\train_checkpoint \ --data_dir D:\Kuliah\S2\Depth_Estimation\models-master\research\struct2depth\KITTI_procesed \ --architecture resnet \ --imagenet_ckpt resnet_pretrained/model.ckpt \ --imagenet_norm true

### Describe the problem
1. Hello I tried to run train.py, but I found problem with Convolution not supported for input with rank, Is there any wrong with my training procedure, I generate train.txt manually with format (image-processed_file_path) (image-processed_file_name), Is there anything wrong with this??

2. And in the data_dir path, which folder I need to use, KITTI_procesed folder from the gen_data_kitti.py or kitti-raw-uncompressed ??

3. Last, where I can find model.ckpt file for resnet model?? because I only found pre-trained model from the https://sites.google.com/view/struct2depth

Thank You very much

Error :
I0304 12:27:19.917803 14332 reader.py:291] data_dir: D:\Kuliah\S2\Depth_Estimation\models-master\research\struct2depth\KITTI_procesed
I0304 12:27:20.101785 14332 reader.py:158] image_stack: Tensor(""data_loading/batching/shuffle_batch:0"", shape=(4, 128, 416, 9), dtype=float32)
Traceback (most recent call last):
  File ""train.py"", line 259, in <module>
    app.run(main)
  File ""F:\Anaconda3\envs\depth\lib\site-packages\absl\app.py"", line 300, in run
    _run_main(main, args)
  File ""F:\Anaconda3\envs\depth\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""train.py"", line 184, in main
    size_constraint_weight=FLAGS.size_constraint_weight)
  File ""D:\Kuliah\S2\Depth_Estimation\models-master\research\struct2depth\model.py"", line 158, in __init__
    self.build_train_graph()
  File ""D:\Kuliah\S2\Depth_Estimation\models-master\research\struct2depth\model.py"", line 169, in build_train_graph
    self.build_inference_for_training()
  File ""D:\Kuliah\S2\Depth_Estimation\models-master\research\struct2depth\model.py"", line 393, in build_inference_for_training
    weight_reg=self.weight_reg)
  File ""D:\Kuliah\S2\Depth_Estimation\models-master\research\struct2depth\nets.py"", line 129, in objectmotion_net
    cnv1 = slim.conv2d(image_stack, 16, [7, 7], stride=2, scope='cnv1')
  File ""F:\Anaconda3\envs\depth\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""F:\Anaconda3\envs\depth\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 1035, in convolution
    input_rank)
ValueError: ('Convolution not supported for input with rank', None)",9,,[],2019-03-03 15:22:38,open,,,[],2019-03-21 08:02:19
144,tensorflow/models,models,6296,neelakanth,deep_speech2 NanLossDuringTrainingError,"### System information
- **What is the top-level directory of the model you are using**:
https://github.com/tensorflow/models/tree/master/research/deep_speech
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No, using stock example from deep_speech2
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: V10.0.130
- **GPU model and memory**: GeForce GTX TITAN X, 12GB, Dual GPU
- **Exact command to reproduce**: Execute run_deep_speech.sh, configure the it to use lstm, bi -directional with rnn_hidden_size=1600 
Using Anaconda envirnoment

### Describe the problem
Hitting NanLossDuringTrainingError. Nor sure it's deep_speech2 bug or Tensorflow bug.

### Source code / logs
Full log attached

Below is the traceback
Traceback (most recent call last):
  File ""/data/tf_git_1.12.0/models/research/deep_speech/deep_speech.py"", line 455, in <module>
    absl_app.run(main)
  File ""/home/exx/.local/lib/python3.5/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/exx/.local/lib/python3.5/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/data/tf_git_1.12.0/models/research/deep_speech/deep_speech.py"", line 448, in main
    run_deep_speech(flags_obj)
  File ""/data/tf_git_1.12.0/models/research/deep_speech/deep_speech.py"", line 324, in run_deep_speech
    estimator.train(input_fn=input_fn_train, hooks=train_hooks)
  File ""/data/anaconda3/envs/deepspeech_gpu/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/anaconda3/envs/deepspeech_gpu/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 1143, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/data/anaconda3/envs/deepspeech_gpu/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 1368, in _train_model_distributed
    saving_listeners)
  File ""/data/anaconda3/envs/deepspeech_gpu/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 1451, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/data/anaconda3/envs/deepspeech_gpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 583, in run
    run_metadata=run_metadata)
  File ""/data/anaconda3/envs/deepspeech_gpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1059, in run
    run_metadata=run_metadata)
  File ""/data/anaconda3/envs/deepspeech_gpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1150, in run
    raise six.reraise(*original_exc_info)
  File ""/home/exx/.local/lib/python3.5/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/data/anaconda3/envs/deepspeech_gpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1135, in run
    return self._sess.run(*args, **kwargs)
  File ""/data/anaconda3/envs/deepspeech_gpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1215, in run
    run_metadata=run_metadata))
  File ""/data/anaconda3/envs/deepspeech_gpu/lib/python3.5/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 635, in after_run
    raise NanLossDuringTrainingError
tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.

[deepspeech2_nan.log](https://github.com/tensorflow/models/files/2922858/deepspeech2_nan.log)
",0,,[],2019-03-03 07:28:08,open,,,[],2019-03-03 07:28:08
145,tensorflow/models,models,6294,GhislainAdon,Please i need good detailled help for mask rcnn training,I want a script to create tf_record.py for rcnn mask training. Please need help,10,,[],2019-03-02 14:30:18,open,,,[],2019-03-06 20:46:31
146,tensorflow/models,models,6289,zychen2016,What different between ssd_mobilenet_v1_0.75_depth_coco and ssd_mobilenet_v1_0.75_depth_quantized_coco?,"In https://github.com/pkulzc/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md, There are many models but what is the difference between them?

Like ssd_mobilenet_v2_quantized_coco and ssdlite_mobilenet_v2_coco, ssd_mobilenet_v1_0.75_depth_coco and ssd_mobilenet_v1_0.75_depth_quantized_coco?",5,,[],2019-03-01 01:49:39,open,,,[],2019-03-08 15:36:44
147,tensorflow/models,models,6288,kdatta,Adding option to sort input sentences by token count for inference,"This commit adds an option to order the input sentences on token-count instead of wordcount to translate.py. Order of input sentences sorted on wordcount is quite different than sentences sorted on token-count. This is the reason possibly the padding overhead in wordcount is higher than token-count. We found that default sorting on wordcount results in slower inference throughput. Hence, the commit. We ensured that the ordering has no effect on BLEU score.

To run with this option, pass --sort_by_tokens=True from the command line to translate.py

We also add options to pass inter_op and intra_op from command line.",1,,[],2019-03-01 01:42:58,open,,,['cla: no'],2019-03-01 01:43:02
148,tensorflow/models,models,6284,aaronhan92,"Feature request: A single graph with 3 lines (training, testing and validating accuracy)  to determine the model is either overfitting or underfitting.","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-02-28 15:38:55,open,,,[],2019-02-28 15:38:55
149,tensorflow/models,models,6279,nguyen-alexa,Fix typos in util.py,"weighed -> weighted
weigth -> weight",0,,[],2019-02-27 12:51:20,open,,,['cla: yes'],2019-02-27 12:51:24
150,tensorflow/models,models,6277,sydney0zq,FEELVOS cannot run,"When I run `bash train.sh`, it just prompts too many obvious errors. After I fix some of them, it still fails to run. Please update your code to make it usable.",13,,[],2019-02-27 11:34:25,open,,,['type:support'],2019-04-09 11:39:40
151,tensorflow/models,models,6270,757227488,TypeError: TF_SessionRun_wrapper: expected all values in input dict to be ndarray,"* **### **OS Platform and Distribution** ### 👍 
  **win10**
* **TensorFlow installed from (source or binary)**:
  Anaconda3
* **TensorFlow version (use command below)**:
  1.13.0-rcl
* **Python version (use command below)**:
  3.7.1

### Source code:

import tensorflow as tf;
a= tf.add(2,4)
b=tf.multiply(a,5)
sess=tf.Session()
dict={a:100}
sess.run(b,feed_dict=dict)
sess.close()

### log:
Traceback (most recent call last):
  File ""D:/python/test/sessionTest.py"", line 6, in <module>
    sess.run(b,feed_dict=dict)
  File ""D:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""D:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""D:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""D:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""D:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""D:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
TypeError: TF_SessionRun_wrapper: expected all values in input dict to be ndarray
",4,,[],2019-02-26 10:02:38,open,,,"['stat:awaiting response', 'type:support']",2019-03-01 00:22:29
152,tensorflow/models,models,6268,jhpenger,Object Detection share GPU,"------------------------

### System information
- **What is the top-level directory of the model you are using**:
object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I only copied and pasted the example code to process video feed
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
16.04
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA: 10.0, cuDNN: 7.5
- **GPU model and memory**:
GeForce GTX TITAN 12GB
- **Exact command to reproduce**:


### Describe the problem
I'm tryin to use `object_detection` to process a video feed output, but part of my code requries older tensorflow-gpu version (or at least that's what I've gathered from online forums regarding similar error) but `object_detection` requries the newer versions.

I've included the error log in the next section.

**Edit**
Both processes (`object_detection` and my other code) uses the GPU. I originally thought the error was caused by tf versions, but its more likely due to `object_detection` using up the GPU resources (I only have 1) and causing the error when my other process requests the GPU. Does `object_detection` ueses the entire GPU? If so, is there a way to force it to use a ""fraction"" (like only as much as it needs and share to GPU)
**Edit Ends**

In order to get my code working, I had to `pip uninstall tensorflow-gpu` but that leaves me with 27-35s of processing time per image (~0.03fps) using the CPUs (32 core)

My question is: is it possible to use multiple versions `tensorflow-gpu` within the same conda environment? or is there any solution to a problem like mine?

Thanks,


### Source code / logs
I get the following error for my other code when I use the newest version of `tensorflow-gpu`:
```
UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to in
itialize, so try looking to see if a warning log message was printed above.
         [[node FirstStageFeatureExtractor/conv0/Conv2D (defined at /root/mount/gibson/ray-gibson/models/research/object
_detection/my_drone_env.py:50)  = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""VALID"", stride
s=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FirstStageFeatureExtracto
r/conv0/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, FirstStageFeatureExtractor/conv0/weights/read/_112__cf__118)]]
         [[{{node SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipTo
Window_69/Gather/Gather_2/_625}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CP
U:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_11273...r
/Gather_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopSecondStagePostprocesso
r/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_31)]]
```",3,"NamedUser(login=""dreamdragon"")","[NamedUser(login=""dreamdragon"")]",2019-02-26 07:51:15,open,,,"['models: research', 'stat:awaiting tensorflower']",2019-03-12 17:05:35
153,tensorflow/models,models,6267,harewei,Object detection API random resize method doesn't correctly interpret target_size,"### System information
- **What is the top-level directory of the model you are using**: models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: GTX 1080Ti
- **Exact command to reproduce**:

### Describe the problem
I was trying to use faster_rcnn_inception_resnet_v2_atrous from the object detection api.  I wanted to apply augmentation to the training data, so I added quite a few to the config file.  Everything I've tried works except the random_resize_method, which is supposed to take in target_height and target_width arguments, but doesn't seem to process it correctly.

I can solve the issue by going to [Link](https://github.com/tensorflow/models/blob/master/research/object_detection/core/preprocessor.py#L2093)
and manually adding
```
target_size = [100, 100]
```

### Source code / logs
Terminal command
```
PIPELINE_CONFIG_PATH=/home/user/xxx.config
MODEL_DIR=/home/user/xxx
NUM_TRAIN_STEPS=50000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python3 object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr
```

xxx.config (I'm only showing the relevant part with the augmentation option)
```
  data_augmentation_options {
    random_distort_color {
    }
  }
  data_augmentation_options {
    random_jitter_boxes {
    }
  }
  data_augmentation_options {
    random_resize_method {
      target_height: 100
      target_width: 100
    }
  }
```

Error message:
```
    lambda x, method: tf.image.resize_images(x, target_size, method),
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/image_ops_impl.py"", line 1012, in resize_images
    raise ValueError('\'size\' must be a 1-D int32 Tensor')
ValueError: 'size' must be a 1-D int32 Tensor
```",1,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2019-02-26 06:09:37,open,,,['models: research'],2019-03-03 02:12:54
154,tensorflow/models,models,6266,justrish,Not able to MeanIOU or confusion matrix for deepLabv3+ inference,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2019-02-26 01:28:14,open,,,"['models: research', 'stat:awaiting tensorflower']",2019-02-27 22:10:56
155,tensorflow/models,models,6265,balakrishnanv,Improvements to CIFAR10 tutorial,"Hi Jon, Sherry
I updated the cifar10.py's input pipeline to use the new DataSet API. Also the cifar10_train.py is updated to do both train and eval (after every epoch) instead of doing them in two different py files.
Please take a look",1,,[],2019-02-25 19:50:00,open,,,['cla: no'],2019-02-26 00:17:39
156,tensorflow/models,models,6264,Eavis,Ask for instructions on setting weight for unbalance dataset,"### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu18
- **TensorFlow installed from (source or binary)**: nvidia-docker-tf-gpu
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.0
- **GPU model and memory**: GTX TITAN 12G
- **Exact command to reproduce**: 
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I have modified the ADE20K scene parsing benchmark produced by MIT. The original dataset contains 150 categoried(151 include background) and set background=0 to be ignored.
I have chosen 17 categories and set the previous background(label as 0) to 255, and set the other categories as 0(background). Thus ignore_class = 255

I count the pixels in the 20210 images after the modification. <category idx, number of pixel counts>
0,1637454744
1,737138625
2,503599787
3,414602893
4,290185267
5,211497624
6,93447406
7,84733791
8,78265150
9,74475972
10,70227430
11,55492152
12,28967419
13,28361240
14,24627334
15,9994821
16,7816482
17,3719357
255,411637498

The background is 0.6 of all the other categories in total(1-17)
However, the background is 440 times the 17th category. (1637454744 / 3719357= 440.3)
(16th: 1637454744 / 7816482 = 209.5)
Does anyone have any suggestion on how to set the weight for these categories?
Should I adjust the weight for only a few categories or all of the 17 categories?

```
weights = tf.to_float(tf.equal(scaled_labels, 0)) * label0_weight + tf.to_float(tf.equal(scaled_labels, 1)) * label1_weight + tf.to_float(tf.equal(scaled_labels, ignore_label)) * 0.0
    where you need to tune the label0_weight and label1_weight (e.g., set label0_weight=1 and increase label1_weight).
```
",0,"NamedUser(login=""YknZhu"")","[NamedUser(login=""YknZhu"")]",2019-02-25 18:39:37,open,,,"['models: research', 'stat:awaiting tensorflower']",2019-02-27 22:11:27
157,tensorflow/models,models,6262,Samjith888,"cannot import name 'string_int_label_map_pb2' ,even after run 'protoc object_detection/protos/*.proto --python_out=.'","#3012  @tensorflow-jenkins @andydavis1  Getting the following error even after running the ` ''protoc object_detection/protos/*.proto --python_out=. '' `command from `''tensorflow/models/research/''`. Hence the .py files are created in the protos folder. 

`Traceback (most recent call last):
  File ""C:/Users/Samjith.CP/PycharmProjects/Tensorflow/tensor_demo.py"", line 20, in <module>
    from object_detection.utils import label_map_util
  File ""C:\Users\Samjith.CP\PycharmProjects\Tensorflow\object_detection\utils\label_map_util.py"", line 21, in <module>
    from object_detection.protos import string_int_label_map_pb2
ImportError: cannot import name 'string_int_label_map_pb2'`


",2,,[],2019-02-25 10:11:33,open,,,"['models: research', 'stat:awaiting tensorflower']",2019-02-27 22:13:03
158,tensorflow/models,models,6260,gulingfengze,Model freeze size limit,"## System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: * N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: * Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:  * source
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: * N/A
- **CUDA/cuDNN version**: * 9.0 / 7.0
- **GPU model and memory**: * GeForce GTX1080Ti   11G
- **Exact command to reproduce**: * N/A


### Describe the problem
At present, after the network training, the image size set in config will be obtained when the model is frozen. In the inference prediction, if the image size is much larger than or smaller than the image size in the frozen model, the detection effect is very poor. I do not know how to get rid of the size limit and better predict the image of any size?
",0,,[],2019-02-25 08:24:37,open,,,['type:support'],2019-02-27 21:43:22
159,tensorflow/models,models,6258,Priyanshu-pg,Replace deprecated native git_repository rule with Skylark version,"The native git_repository rule has been deprecated. Use the Skylark version available via load(""@bazel_tools//tools/build_defs/repo:git.bzl"", ""git_repository"") instead.
Using git_repository method gives following error:
ERROR: Skipping 'single_task:run.par': error loading package 'single_task': Encountered error while reading extension file 'subpar.bzl': no such package '@subpar//': The native git_repository rule is deprecated. load(""@bazel_tools//tools/build_defs/repo:git.bzl"", ""git_repository"") for a replacement.
Use --incompatible_remove_native_git_repository=false to temporarily continue using the native rule.
[Source](https://blog.bazel.build/2018/04/11/bazel-0.12.html)",3,,[],2019-02-24 20:59:08,open,,,['cla: yes'],2019-02-24 21:02:50
160,tensorflow/models,models,6254,ronykalfarisi,multiple image inference for mask-rcnn is ~10x slower than faster-rcnn for the same image size,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18.3 Sylvia
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: V9.2.148/7.1.4
- **GPU model and memory**: Quadro GV100 32 GiB
- **Exact command to reproduce**:


### Describe the problem
I successfully retrained mask-rcnn and faster-rcnn models with my own custom dataset and I want to run inference for multiple images. I modified the single image inference function from the demo with the code below. I got the following result if I used retrained faster-rcnn resnet101
![frcnn-resnet101](https://user-images.githubusercontent.com/33510059/53273394-e7f9b300-36c1-11e9-9ab4-ebe4c3556be4.png)
and the following result if I used retrained mask-rcnn resnet101
![mrcnn-resnet101](https://user-images.githubusercontent.com/33510059/53273878-46736100-36c3-11e9-967d-eead74b6ee4d.png)
The following if I run with faster-rcnn inception-resnet
![frcnn-inception-resnet](https://user-images.githubusercontent.com/33510059/53274084-f5b03800-36c3-11e9-89a3-532d5862e8c9.png)
and the following with mask-rcnn inception-resnet
![mrcnn-inception-resnet](https://user-images.githubusercontent.com/33510059/53274104-0365bd80-36c4-11e9-8c1b-4bf2cfa34bea.png)
All images have resolution of 1024x768. Please help whether this is the right behavior or not. Thanks

### Source code / logs
```
def run_inference_for_multiple_images(images, graph):
  with graph.as_default():
        with tf.Session() as sess:
            output_dict_array = []
            dict_time = []
            for image in images:
                # Get handles to input and output tensors
                ops = tf.get_default_graph().get_operations()
                all_tensor_names = {output.name for op in ops for output in op.outputs}
                tensor_dict = {}
                for key in [
                    'num_detections', 'detection_boxes', 'detection_scores',
                    'detection_classes', 'detection_masks'
                ]:
                    tensor_name = key + ':0'
                    if tensor_name in all_tensor_names:
                        tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
                            tensor_name)
                if 'detection_masks' in tensor_dict:
                    detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
                    detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
                    # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
                    real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
                    detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
                    detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
                    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
                        detection_masks, detection_boxes, image.shape[0], image.shape[1])
                    detection_masks_reframed = tf.cast(
                        tf.greater(detection_masks_reframed, 0.5), tf.uint8)
                    # Follow the convention by adding back the batch dimension
                    tensor_dict['detection_masks'] = tf.expand_dims(
                        detection_masks_reframed, 0)
                image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

                # Run inference
                start = time.time()
                output_dict = sess.run(tensor_dict,
                                       feed_dict={image_tensor: np.expand_dims(image, 0)})
                end = time.time()
                print('inference time : {}'.format(end-start))

                # all outputs are float32 numpy arrays, so convert types as appropriate
                output_dict['num_detections'] = int(output_dict['num_detections'][0])
                output_dict['detection_classes'] = output_dict[
                    'detection_classes'][0].astype(np.uint8)
                output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
                output_dict['detection_scores'] = output_dict['detection_scores'][0]
                if 'detection_masks' in output_dict:
                    output_dict['detection_masks'] = output_dict['detection_masks'][0]

                output_dict_array.append(output_dict)
                dict_time.append(end-start)
  return output_dict_array, dict_time
```
the following is the code to run the function
```
subdir_path = os.path.join(PATH_TO_TEST_IMAGES_DIR, path)
(_, _, filenames) = next(os.walk(subdir_path))
# Listing the result images
result_img_path = os.path.join(PATH_TO_RESULT_IMAGES_DIR, path + mod)
if not os.path.exists(result_img_path):
	os.makedirs(result_img_path)
(_, _, filenames_result) = next(os.walk(result_img_path))

batch_size = 10
chunks = len(diff_files) // batch_size + 1
ave_time = []
for i in range(chunks):
	batch = diff_files[i*batch_size:(i+1)*batch_size]
	images = []
	files = []
	proc_time = []
	for file in batch:
		image_path = os.path.join(subdir_path, file)
		print('Reading file {}'.format(image_path))
		image = cv2.imread(image_path)
		image_np = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
		images.append(image_np)
		files.append(file)

	output_dicts, out_time = run_inference_for_multiple_images(images, detection_graph)
	print('length of output_dicts is : {}'.format(len(output_dicts)))
	if len(output_dicts) == 0:
		break

	for idx in range(len(output_dicts)):
		output_dict = output_dicts[idx]
		image_np = images[idx]
		file = files[idx]
		# Visualization of the results of a detection.
		start = time.time()
		vis_util.visualize_boxes_and_labels_on_image_array(
			  image_np,
			  output_dict['detection_boxes'],
			  output_dict['detection_classes'],
			  output_dict['detection_scores'],
			  category_index,
			  instance_masks=output_dict.get('detection_masks'),
			  use_normalized_coordinates=True, min_score_thresh=.5,
			  line_thickness=4, skip_scores=False,
			  skip_labels=False,
			  skip_boxes=False)
			height, width, chan = image_np.shape

		# Saving the processed image
		image_np = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
		cv2.imwrite(os.path.join(result_img_path, file), image_np)
		print('Saving {}, time : {}'.format(file, time.time()-start))
		proc_time.append(time.time()-start + out_time[idx])
		# count += 1

		#  SAVING BOUNDING BOX AND CONFIDENCE RATE
		boxes = output_dict['detection_boxes']
		classes = output_dict['detection_classes']
		scores = output_dict['detection_scores']		
		
	if len(proc_time) != 0:
		mean_batch_time = statistics.mean(proc_time)
		print('mean processing time: {}'.format(mean_batch_time))
		ave_time.append(mean_batch_time)
	proc_time.clear()
	output_dicts.clear()
```",3,,[],2019-02-22 22:18:16,open,,,['models: research'],2019-04-09 08:41:14
161,tensorflow/models,models,6253,wrkgm,LSTM Object Detection Model Does Not Run,"### System information
* **What is the top-level directory of the model you are using**: lstm_object_detection
* **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Trying to
* **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
* **TensorFlow installed from (source or binary)**: source
* **TensorFlow version (use command below)**: 1.12.0
* **Bazel version (if compiling from source)**:
* **CUDA/cuDNN version**: 9.0
* **GPU model and memory**: GTX 1070 ti
* **Exact command to reproduce**: python train.py --train_dir=training --pipeline_config_path=configs/lstm_ssd_mobilenet_v1_imagenet.config

### Describe the problem
Training the LSTM object detection model does not work. After making a tfrecord, modifying the config as necessary, creating a training dir, and running the command, I get this error:

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Tried to explicitly squeeze dimension 0 but dimension was not 1: 0
         [[Node: Squeeze_1 = Squeeze[T=DT_INT64, squeeze_dims=[0], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](split_2)]]
```

More documentation, including a simple example file of how to make a tfrecord and train, would be very helpful. I have tried two ways to create a tfrecord, both of which are shown below. I thought maybe the record structure is wrong, but if I put a typo in the record keys, I get a different error complaining about that, so perhaps I structured the records correctly. I tried looking at the model in tensorboard and modifying the training code in slim/learning.py to fetch values from individual nodes near Squeeze_1. I print the node, the output, and the shape of the output. Here are results from these attempts:

```
try run:  split_1:0
Tensor(""split_1:0"", shape=(?, ?, 4), dtype=float32, device=/device:CPU:0)
value: []
size: (0, 0, 4)

try run:  ParseSingleSequenceExample/ParseSingleSequenceExample:0
Tensor(""ParseSingleSequenceExample/ParseSingleSequenceExample:0"", shape=(), dtype=string, device=/device:CPU:0)
value: b''
size: ()

try run:  ResizeImage/resize_images/ResizeBilinear:0
Tensor(""ResizeImage/resize_images/ResizeBilinear:0"", shape=(4, 256, 256, 3), dtype=float32, device=/device:CPU:0)
value: (big numpy array)
size: (4, 256, 256, 3)
```

It seems that split_1 and ParseSingleSequenceExample are not actually receiving any data, and thus cause this squeeze error since there is nothing to squeeze. But resize image still gets data.
Additionally, if I ONLY fetch ResizeImage/resize_images/ResizeBilinear:0, I can fetch it a couple of times (repeatedly fetching in a loop), and then it fails. Perhaps the model fails after one batch?

I'm not sure if this counts a duplicate, but here are some related threads:
https://github.com/tensorflow/models/issues/6027 
https://github.com/tensorflow/models/issues/5869
https://stackoverflow.com/questions/54093931/lstm-object-detection-tensorflow

I've also emailed the authors and heard nothing back.

### EDIT:
I should mention, I removed ssd_random_crop from data augmentation options in the config because it was giving me an error ""the function ssd_random_crop requires argument groundtruth_weights""
Not sure if this would matter at all

### Source code / logs
I tried two ways of creating tfrecords. The first was taken from tf_sequence_example_decoder_test.py, in this repo. The only change was swapping to sequences of length 4 to match the config file.

```
writer = tf.python_io.TFRecordWriter(path)
with tf.Session() as sess:
    for _ in range(2000):
        image_tensor = np.random.randint(255, size=(16, 16, 3)).astype(np.uint8)
        print(image_tensor)

        encoded_jpeg = tf.image.encode_jpeg(tf.constant(image_tensor)).eval()

        sequence_example = example_pb2.SequenceExample(
            context=feature_pb2.Features(
                feature={
                    'image/format':
                        feature_pb2.Feature(
                            bytes_list=feature_pb2.BytesList(
                                value=['jpeg'.encode('utf-8')])),
                    'image/height':
                        feature_pb2.Feature(
                            int64_list=feature_pb2.Int64List(value=[16])),
                    'image/width':
                        feature_pb2.Feature(
                            int64_list=feature_pb2.Int64List(value=[16])),
                }),
            feature_lists=feature_pb2.FeatureLists(
                feature_list={
                    'image/encoded':
                        feature_pb2.FeatureList(feature=[
                            feature_pb2.Feature(
                                bytes_list=feature_pb2.BytesList(
                                    value=[encoded_jpeg])), feature_pb2.Feature(
                                bytes_list=feature_pb2.BytesList(
                                    value=[encoded_jpeg])), feature_pb2.Feature(
                                bytes_list=feature_pb2.BytesList(
                                    value=[encoded_jpeg])), feature_pb2.Feature(
                                bytes_list=feature_pb2.BytesList(
                                    value=[encoded_jpeg]))
                        ]),
                    'image/object/bbox/xmin':
                        feature_pb2.FeatureList(feature=[
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[0.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[0.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[0.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[0.0]))
                        ]),
                    'image/object/bbox/xmax':
                        feature_pb2.FeatureList(feature=[
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[1.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[1.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[1.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[1.0]))
                        ]),
                    'image/object/bbox/ymin':
                        feature_pb2.FeatureList(feature=[
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[0.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[0.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[0.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[0.0]))
                        ]),
                    'image/object/bbox/ymax':
                        feature_pb2.FeatureList(feature=[
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[1.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[1.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[1.0])),
                            feature_pb2.Feature(
                                float_list=feature_pb2.FloatList(value=[1.0]))
                        ]),
                    'image/object/class/label':
                        feature_pb2.FeatureList(feature=[
                            feature_pb2.Feature(
                                int64_list=feature_pb2.Int64List(value=[1])),
                            feature_pb2.Feature(
                                int64_list=feature_pb2.Int64List(value=[1])),
                            feature_pb2.Feature(
                                int64_list=feature_pb2.Int64List(value=[1])),
                            feature_pb2.Feature(
                                int64_list=feature_pb2.Int64List(value=[1]))
                        ]),
                }))

        writer.write(sequence_example.SerializeToString())
writer.close()
```

I also tried adapting a method I found here: https://github.com/wakanda-ai/tf-detectors
For this I used a couple sample xml files in PASCAL VOC format from a training set I have for one of the normal object_detection models.
```
    # Iterate frames
    for data, img_path in zip(dicts, imgs_path):
        ## open single frame
        with tf.gfile.FastGFile(img_path, 'rb') as fid:
            encoded_jpg = fid.read()
        encoded_jpg_io = io.BytesIO(encoded_jpg)
        image = Image.open(encoded_jpg_io)
        if image.format != 'JPEG':
            raise ValueError('Image format not JPEG')
        key = hashlib.sha256(encoded_jpg).hexdigest()

        ## validation
        assert int(data['size']['height']) == height
        assert int(data['size']['width']) == width

        ## iterate objects
        xmin, ymin = [], []
        xmax, ymax = [], []
        name = []
        classval =  []
        occluded = []
        generated = []
        if 'object' in data:
            for obj in data['object']:
                xmin.append(float(obj['bndbox']['xmin']) / width)
                ymin.append(float(obj['bndbox']['ymin']) / height)
                xmax.append(float(obj['bndbox']['xmax']) / width)
                ymax.append(float(obj['bndbox']['ymax']) / height)
                name.append(obj['name'].encode('utf8'))
                classval.append(1)
                occluded.append(0)
                generated.append(0)
        else:
            xmin.append(float(-1))
            ymin.append(float(-1))
            xmax.append(float(-1))
            ymax.append(float(-1))
            name.append('NoObject'.encode('utf8'))
            classval.append(1)
            occluded.append(0)
            generated.append(0)

        ## append tf_feature to list
        filenames.append(dataset_util.bytes_feature(data['filename'].encode('utf8')))
        encodeds.append(dataset_util.bytes_feature(encoded_jpg))
        sources.append(dataset_util.bytes_feature(data['source']['database'].encode('utf8')))
        keys.append(dataset_util.bytes_feature(key.encode('utf8')))
        formats.append(dataset_util.bytes_feature('jpeg'.encode('utf8')))
        xmins.append(dataset_util.float_list_feature(xmin))
        ymins.append(dataset_util.float_list_feature(ymin))
        xmaxs.append(dataset_util.float_list_feature(xmax))
        ymaxs.append(dataset_util.float_list_feature(ymax))
        names.append(dataset_util.bytes_list_feature(name))
        occludeds.append(dataset_util.int64_list_feature(occluded))
        generateds.append(dataset_util.int64_list_feature(generated))
        class_labels.append(dataset_util.int64_list_feature(classval))

    # Non sequential features
    context = tf.train.Features(feature={
        'video/folder': dataset_util.bytes_feature(folder.encode('utf8')),
        'video/frame_number': dataset_util.int64_feature(len(imgs_path)),
        'video/height': dataset_util.int64_feature(height),
        'video/width': dataset_util.int64_feature(width),
        })
    # Sequential features
    tf_feature_lists = {
        'image/filename': tf.train.FeatureList(feature=filenames),
        'image/encoded': tf.train.FeatureList(feature=encodeds),
        'image/sources': tf.train.FeatureList(feature=sources),
        'image/key/sha256': tf.train.FeatureList(feature=keys),
        'image/format': tf.train.FeatureList(feature=formats),
        'image/object/bbox/xmin': tf.train.FeatureList(feature=xmins),
        'image/object/bbox/xmax': tf.train.FeatureList(feature=xmaxs),
        'image/object/bbox/ymin': tf.train.FeatureList(feature=ymins),
        'image/object/bbox/ymax': tf.train.FeatureList(feature=ymaxs),
        'image/object/class/text': tf.train.FeatureList(feature=names),
        'image/object/class/label': tf.train.FeatureList(feature=class_labels),
        'image/object/occluded': tf.train.FeatureList(feature=occludeds),
        'image/object/generated': tf.train.FeatureList(feature=generateds),
        }
    feature_lists = tf.train.FeatureLists(feature_list=tf_feature_lists)
    # Make single sequence example
    tf_example = tf.train.SequenceExample(context=context, feature_lists=feature_lists)
    return tf_example
```

Tfrecords created with both of these approaches yielded identical errors.",9,"NamedUser(login=""dreamdragon"")","[NamedUser(login=""dreamdragon"")]",2019-02-22 19:17:47,open,,,"['models: research', 'stat:awaiting tensorflower']",2019-03-01 18:23:11
162,tensorflow/models,models,6249,bladeDD,This may cause a hash collision.,In my use I met some hash collisions. https://github.com/tensorflow/models/blob/5f4d34fc982f599c105e152819fc3b7c82c92960/research/object_detection/inputs.py#L374,3,,[],2019-02-22 08:03:48,open,,,['models: research'],2019-03-16 00:17:35
163,tensorflow/models,models,6248,TheFlashover,Update create_pet_tf_record.py,"```
nonzero_x_indices = np.where(nonbackground_indices_x)
nonzero_y_indices = np.where(nonbackground_indices_y)
```

returns tuple of 2 ndarrays like


```
(array([*x_position1*, *x_position2*, ..., *x_position_max*]), 
array([*Red_channel_position*, *Green_channel_position*, *Blue_channel_position*]))
```

i.e.


```
(array([10, 10, 10, 11, 11, 11, ..., 720, 720, 720]), array([0, 1, 2, 0, 1, 2, ..., 0, 1, 2]))
if our mask starts at x pixel value == 10
```

so the following code

```
else :
    xmin = float(np.min(nonzero_x_indices))
    xmax = float(np.max(nonzero_x_indices))
    ymin = float(np.min(nonzero_y_indices))
    ymax = float(np.max(nonzero_y_indices))
```

sometimes returns wrong values because it looks for minimal value in the second array",3,,[],2019-02-22 07:58:27,open,,,['cla: yes'],2019-02-22 23:26:24
164,tensorflow/models,models,6237,danial880,Feature Request : Yolov3 model in TensorFlow model zoo,"Feature Request : Yolov3 model in TensorFlow model zoo
",0,"NamedUser(login=""dreamdragon"")","[NamedUser(login=""dreamdragon"")]",2019-02-20 16:35:54,open,,,"['models: research', 'stat:awaiting tensorflower']",2019-02-27 22:06:42
165,tensorflow/models,models,6235,hasibul442,to find Confusion matrix,"I want to find  confusion matrix for tensorflow API. 
I am using faster_rcnn_inception_v2_coco_2018_01_28. How can i find confusion matrix for this model.

i am using tensorflow cpu version. and windows 10",0,,[],2019-02-20 16:01:03,open,,,"['models: research', 'type:support']",2019-02-27 22:20:43
166,tensorflow/models,models,6234,xunkai55,Benchmark Keras Application Models in TF2.0,"(WIP) This PR adds codes for training Keras App Models in TF2.0 and benchmarking them.

Not finished yet. Let's firstly discuss code structure and design principles. I tried to follow the principles below, but they're just my opinions and we should definitely discuss.

1. Try to use 2.0 codes and avoid ""compat.v1"".
2. Try to not introduce cross-directory dependency in the training code. I think it would be easier for external users adopting those examples, as they don't need to understand hierarchical code structures or non-core utils.
3. Try to adopt best practice for simple training -- dataset, keras and dist_strat.
4. Try to use APIs only in tf.data, tf.keras and tf.distribute.

As a result, the code looks less ""engineered"". I'm not very sure if the principles are correct. Opening the Draft PR for discussing before I go too far.",3,,[],2019-02-20 15:42:46,open,,,['cla: yes'],2019-02-22 15:50:52
167,tensorflow/models,models,6232,xiaohedu,can you share pretrained resnet model on imagenet dataset for struct2depth? ,"@VincentCa
@AneliaAngelova 

hi , where can i find pretrained resnet-18 model for struct2depth project ?",4,,[],2019-02-20 08:39:49,open,,,['models: research'],2019-03-05 09:03:21
168,tensorflow/models,models,6229,hodaatef,"int() argument must be a string, a bytes-like object or a number, not 'NoneType' >>> ","
i received this error when i try to run object_detection_image.py
please someone help me

Traceback (most recent call last):
  File ""C:\tensorflow1\models\research\object_detection\Object_detection_image.py"", line 101, in <module>
    feed_dict={image_tensor: image_expanded})
  File ""C:\Users\DoDo\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""C:\Users\DoDo\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\client\session.py"", line 1121, in _run
    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)
  File ""C:\Users\DoDo\Anaconda3\envs\tensorflow1\lib\site-packages\numpy\core\numeric.py"", line 501, in asarray
    return array(a, dtype, copy=False, order=order)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'

",2,,[],2019-02-19 21:46:14,open,,,['models: research'],2019-02-22 12:20:36
169,tensorflow/models,models,6228,dudeperfect95,Inference super slow on GPU,"* **What is the top-level directory of the model you are using**: object_detection
* **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, just for inference
* **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10
* **TensorFlow installed from (source or binary)**: N/A
* **TensorFlow version (use command below)**: 1.9.0
* **Bazel version (if compiling from source)**:
* **CUDA/cuDNN version**:
* **GPU model and memory**: Nvidia Geforce 740m 2GB
* **Exact command to reproduce**:

Im trying to run a little modified code found on tensorflow models object_detection:

```
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile
import datetime
from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("".."")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')

# This is needed to display the images.
#%matplotlib inline

from utils import label_map_util

from utils import visualization_utils as vis_util

# What model to download.
MODEL_NAME = 'faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28'
MODEL_FILE = MODEL_NAME + '.tar.gz'
DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'

# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = os.path.join('data', 'mscoco_complete_label_map.pbtxt')

'''
opener = urllib.request.URLopener()
opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)
tar_file = tarfile.open(MODEL_FILE)
for file in tar_file.getmembers():
  file_name = os.path.basename(file.name)
  if 'frozen_inference_graph.pb' in file_name:
    tar_file.extract(file, os.getcwd())
'''
	
detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')

category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)

def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)

# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 1) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (8, 8)

def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      print('Get handles to input and output tensors')
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      print('Run inference')
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
  
print('Loaded')
while True:
	line = sys.stdin.readline()
	a = datetime.datetime.now()
	words = line.split("","")
	i = 0
	print('for image_path in TEST_IMAGE_PATHS')
	image = Image.open(words[0])
	# the array based representation of the image will be used later in order to prepare the
	print('result image with boxes and labels on it.')
	image_np = load_image_into_numpy_array(image)
	print('Expand dimensions since the model expects images to have shape: [1, None, None, 3]')
	image_np_expanded = np.expand_dims(image_np, axis=0)
	print('Actual detection.')
	output_dict = run_inference_for_single_image(image_np, detection_graph)
	b = datetime.datetime.now()
	print(b-a)
	#print(output_dict)
	# Visualization of the results of a detection.
	vis_util.visualize_boxes_and_labels_on_image_array(
		image_np,
		output_dict['detection_boxes'],
		output_dict['detection_classes'],
		output_dict['detection_scores'],
		category_index,
		instance_masks=output_dict.get('detection_masks'),
		use_normalized_coordinates=True,
		line_thickness=8,
		min_score_thresh=.0)

	plt.figure(figsize=IMAGE_SIZE)
	plt.imshow(image_np)
	plt.savefig('imageee'+str(i)+'.png')
	print(""FINISHED"")
```

But noticed that inference process is super slow, > 2minutes. Even on CPU I found that it takes about minute and a half, and on GPU this is even slower. This is the output message:

> Loaded
> C:\Users\PC\Downloads\image1.png,
> for image_path in TEST_IMAGE_PATHS
> result image with boxes and labels on it.
> Expand dimensions since the model expects images to have shape: [1, None, None, 3]
> Actual detection.
> 2019-02-19 16:46:05.074220: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1405] Found device 0 with properties:
> name: GeForce GT 740M major: 3 minor: 5 memoryClockRate(GHz): 1.0325
> pciBusID: 0000:01:00.0
> totalMemory: 2.00GiB freeMemory: 1.66GiB
> 2019-02-19 16:46:05.091408: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1484] Adding visible gpu devices: 0
> 2019-02-19 16:46:11.601992: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2019-02-19 16:46:11.606829: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971]      0
> 2019-02-19 16:46:11.609773: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:984] 0:   N
> 2019-02-19 16:46:11.613614: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1439 MB memory) -> physical GPU (device: 0, name: GeForce GT 740M, pci bus id: 0000:01:00.0, compute capability: 3.5)
> Get handles to input and output tensors
> Run inference
> 2019-02-19 16:47:28.958381: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 2019-02-19 16:48:20.383043: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 2019-02-19 16:48:21.113145: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.31GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 2019-02-19 16:48:21.136468: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 2019-02-19 16:48:21.159923: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.55GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 2019-02-19 16:48:21.732722: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.15GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 2019-02-19 16:48:21.881223: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.31GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 2019-02-19 16:48:29.276762: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.31GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 2019-02-19 16:48:29.733310: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.59GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 2019-02-19 16:48:29.762131: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.11GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 0:02:41.000948
> FINISHED

I noticed that it tries to allocate memory and fails. Is that why inference is super slow on GPU? If yes, is there anything I can do?",6,,[],2019-02-19 14:58:48,open,,,"['models: research', 'stat:awaiting response']",2019-02-27 22:24:53
170,tensorflow/models,models,6227,sed0724963,why run the model_main.py it's so slow?(use faster-rcnn resnet101.coco),"I'm using the object detecton api and use the model_main.py to training a detection model,
I use the faster-rcnn resnet101.coco and batch size =1,but it's so slow? 

what can I do to let it to be faster??

![main](https://user-images.githubusercontent.com/38509051/52998524-766dfa80-345e-11e9-8105-4fb4671a74fd.png)



-----------------------

### System information
- **What is the top-level directory of the model you are using**:
model/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
win10
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
1.9.0
- **Bazel version (if compiling from source)**:
don't know
- **CUDA/cuDNN version**:
cuda:9 , cuDNN:7
- **GPU model and memory**:
ASUS ROG Strix GeForce® GTX 1080
- **Exact command to reproduce**:
don't

",5,,[],2019-02-19 07:54:27,open,,,['models: research'],2019-03-25 14:25:26
171,tensorflow/models,models,6226,lunasdejavu,how to preprocess the image just before the real training function?,"### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
windows 7x64
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA9.0
- **GPU model and memory**:
GTX1060 6G
- **Exact command to reproduce**:
python 3.6

I use the CLAHE algorithm to enhance the feature of the objects, but it has a drawback that it caused something similar to a grid on the image. I  always do the data augmentation first then do put the preprocessed image into the real training function so that the grid will become noise to the image instead of the feature. It is easier to implement by the customized project was written by Keras, but can someone tell me how to write it by tensorflow object detection API?

I just want to do something like :

```
image1 = data_augmentation (image0,rotate,reflect,.....)
image2 = CLAHE(image1)
train_per_step(image2....)
```
I found  the class_TrainingExecutor tensorflow\python\estimaotor\training.py
So I should modify the run()  or make another class just like lass_TrainingExecutor? 
",1,,[],2019-02-19 05:49:55,open,,,['models: research'],2019-02-23 07:53:26
172,tensorflow/models,models,6223,dudeperfect95,Convert model to .h5 model,"**What is the top-level directory of the model you are using:** faster_rcnn_inception_resnet_v2_atrous_coco
**Have I written custom code**: Not yet
**OS Platform and Distribution** Windows 10 64bit
**TensorFlow installed from**: https://pypi.org/
**TensorFlow version:** 1.9.0
**Bazel version** : N/A
**CUDA/cuDNN version:** : CUDA 9.0/cuDNN 7.0.4 (not sure about cuDNN)
**GPU model and memory**: Nvidia Geforce 740m 2GB
**Exact command to reproduce**: N/A

Is it possible to convert object detection model such as faster_rcnn_inception_resnet_v2_atrous_coco to .h5 model? If yes, how to accomplish this? I searched for this on the internet but found only information about the opposite process(from .h5 to .pb), and I need to get .h5 model of needed tensorflow model.",3,,[],2019-02-18 23:07:22,open,,,['type:support'],2019-03-02 06:30:42
173,tensorflow/models,models,6222,jessiffmm,Training problems,"Hi!!

I try to train my own dataset. I used the train.py file but I get some warnings:
docker@86a8431717e2:~/Jessi/models_new/models/research/object_detection/legacy$ python train.py --logtostderr --train_dir training/ --pipeline_config_path=./data/ssd_mobilenet_v2_coco.config
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py:125: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
WARNING:tensorflow:From /home/docker/Jessi/models_new/models/research/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /home/docker/Jessi/models_new/models/research/object_detection/builders/dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From /home/docker/Jessi/models_new/models/research/object_detection/core/preprocessor.py:1218: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From /home/docker/Jessi/models_new/models/research/object_detection/core/batcher.py:96: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:751: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:751: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0

It trains but when I try to generate the .pb network graph file I get a .pb file which doesn't work.
python ../export_inference_graph.py --input_type image_tensor --pipeline_config_path data/ssd_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-XXX --output_directory mymodel/

Regards",4,,[],2019-02-18 17:15:07,open,,,[],2019-03-09 12:55:16
174,tensorflow/models,models,6220,moussas1,"""Unavailable OSError"" when training tensorflow object detection model on google cloud.","### Describe the problem
I have been trying to use a pretrained model on the tensorflow object detection API and retrain it on Google cloud, however after almost ~40 trials, I am stuck with this error. Sometimes the job tears down from the beginning and sometimes it runs for a few steps then fails. 

### System information
- **What is the top-level directory of the model you are using**: models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Only config modification
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10(base). Training on Google Cloud ML Engine Runtime 1.9
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.9 gpu
- **Bazel version (if compiling from source)**:


- **CUDA/cuDNN version**: v 9.0
- **GPU model and memory**: standard_gpu on google cloud. Local: Gtx 1050 4GB
- **Exact command to reproduce**: 
```
gcloud ml-engine jobs submit training hardhat40 
--job-dir=gs://ppe_detection/data/
--packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,dist/absl-py-0.7.0.tar.gz
--module-name object_detection.train
--region europe-west1 
--config object_detection/samples/cloud/cloud.yml 
--runtime-version=1.9 -- 
--train_dir=gs://ppe_detection/data/
--pipeline_config_path=gs://ppe_detection/data/faster_rcnn_inception_resnet_v2_atrous_oid.config
```
- **Exact config to reproduce**: 
```
model {
  faster_rcnn {
    num_classes: 2
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
      }
    }
    feature_extractor {
      type: 'faster_rcnn_inception_resnet_v2'
      first_stage_features_stride: 8
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 8
        width_stride: 8
      }
    }
    first_stage_atrous_rate: 2
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 17
    maxpool_kernel_size: 1
    maxpool_stride: 1
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 1
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.00006
          schedule {
            step: 6000000
            learning_rate: .000006
          }
          schedule {
            step: 7000000
            learning_rate: .0000006
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""gs://ppe_detection/data/model.ckpt""
  from_detection_checkpoint: true
  load_all_detection_checkpoint_vars: true
 
  num_steps: 400000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""gs://ppe_detection/data/train.record""
  }
  label_map_path: ""gs://ppe_detection/data/labelmap.pbtxt""
}

eval_config: {
  metrics_set: ""open_images_metrics""
  num_examples: 104
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""gs://ppe_detection/data/test.record""
  }
  label_map_path: ""gs://ppe_detection/data/labelmap.pbtxt""
  shuffle: false
  num_readers: 1
}

```

- **Cloud yaml file**:
```
trainingInput:
  --runtimeVersion: ""1.9""
 -- scaleTier: CUSTOM
 -- masterType: standard_gpu
 --workerCount: 3
 --workerType: standard_gpu
 --parameterServerCount: 3
 --parameterServerType: standard
```

- **Model Type**:
faster_rcnn_inception_resnet_v2_atrous_oid

### Source code / logs
**master-replica-0**
`Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 184, in <module> tf.app.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run _sys.exit(main(argv)) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 250, in new_func return func(*args, **kwargs) File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 180, in main graph_hook_fn=graph_rewriter_fn) File ""/root/.local/lib/python2.7/site-packages/object_detection/legacy/trainer.py"", line 415, in train saver=saver) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 785, in train ignore_live_threads=ignore_live_threads) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 833, in stop ignore_live_threads=ignore_live_threads) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join six.reraise(*self._exc_info_to_raise) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception yield File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 495, in run self.run_loop() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 1035, in run_loop self._sv.global_step]) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call raise type(e)(node_def, op, message) UnavailableError: OS Error [[Node: SecondStageFeatureExtractor/InceptionResnetV2/Repeat/block8_2/Branch_0/Conv2d_1x1/weights/read_S10901 = _Recv[client_terminated=false, recv_device=""/job:master/replica:0/task:0/device:GPU:0"", send_device=""/job:ps/replica:0/task:1/device:CPU:0"", send_device_incarnation=7265986053344373253, tensor_name=""edge_10431...ights/read"", tensor_type=DT_FLOAT, _device=""/job:master/replica:0/task:0/device:GPU:0""]()]] [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Size_G289 = _Recv[client_terminated=false, recv_device=""/job:master/replica:0/task:0/device:CPU:0"", send_device=""/job:master/replica:0/task:0/device:GPU:0"", send_device_incarnation=-2716543638016603643, tensor_name=""edge_11837...Field/Size"", tensor_type=DT_INT32, _device=""/job:master/replica:0/task:0/device:CPU:0""](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Assert/Assert/data_0_S4645)]]`",7,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2019-02-18 09:02:59,open,,,['models: research'],2019-03-09 03:13:40
175,tensorflow/models,models,6218,ShantanuShinde,Update crate_dataset.py,"In the loop for initializing writers, Flags.output_shards is being used even though output_shards is passed as a parameter to the function ""convert_data()"".",2,,[],2019-02-17 23:25:48,open,,,['cla: no'],2019-02-17 23:28:02
176,tensorflow/models,models,6217,ShantanuShinde,Does the no. of output shards mean no. of classes?,The output_shards is passed as parameter and has default value 10. does that mean the no. of classes to be recognized by the model is 10?,1,,[],2019-02-17 15:44:05,open,,,"['stat:awaiting response', 'type:support']",2019-02-20 19:46:42
177,tensorflow/models,models,6214,YusukeO,Change train image size to 64*64,"### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:anaconda
- **TensorFlow version (use command below)**:v1.6.0-0-gd2e24b6039 1.6.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:GTX1080ti
- **Exact command to reproduce**:
```
$ DATASET_DIR=/home/yusuke/PycharmProjects/nasnet-tensorflow/data/car/real/traintfrecord 
$ TRAIN_DIR=./train

# For Nasnet-a-mobile
# --dataset_name=customized
$ CHECKPOINT_PATH=./pre-trained/nasnet-a_mobile_04_10_2017/model.ckpt
python train_image_classifier.py \
    --train_dir=${TRAIN_DIR} \
    --dataset_dir=${DATASET_DIR} \
    --dataset_name=customized \
    --dataset_split_name=train \
    --model_name=nasnet_mobile \
    --checkpoint_path=${CHECKPOINT_PATH} \
    --checkpoint_exclude_scopes=final_layer,aux_7,aux_6,aux_5,aux_4,aux3,aux2,aux1 \
    --trainable_scopes=final_layer,aux_7,aux_6,aux_5,aux_4,aux3,aux2,aux1 \
    --max_number_of_steps=5000 \
    --train_image_size=64
```

### Describe the problem
I tried to train nasnet model with my own data whose shape is 64, 64, 3. 
When I ran below command without `--train_image_size=64` argument, that worked fine (but with low accuracy). 
But after I add `--train_image_size=64` argument like below, the code said following error. 
Should I add the argument when I train 64,64,3 images? If so, how can I avoid the error? 
Thank you!

```
(dagan) yusuke@yusuke-lab-pc:~/PycharmProjects/nasnet-tensorflow$ bash train_mobile.sh 
WARNING:tensorflow:From train_image_classifier.py:400: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
2019-02-17 13:59:02.647080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-17 13:59:02.647479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:02:00.0
totalMemory: 10.91GiB freeMemory: 9.97GiB
2019-02-17 13:59:02.647502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2019-02-17 13:59:02.864017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/device:GPU:0 with 9646 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
INFO:tensorflow:A GPU is available on the machine, consider using NCHW data format for increased speed on GPU.
Traceback (most recent call last):
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 686, in _call_cpp_shape_fn_impl
    input_tensors_as_shapes, status)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 5 from 4 for 'aux_7/aux_logits/AvgPool2D/AvgPool' (op: 'AvgPool') with input shapes: [32,4,4,528].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_image_classifier.py"", line 574, in <module>
    tf.app.run()
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train_image_classifier.py"", line 474, in main
    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])
  File ""/home/yusuke/PycharmProjects/nasnet-tensorflow/deployment/model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""train_image_classifier.py"", line 457, in clone_fn
    logits, end_points = network_fn(images)
  File ""/home/yusuke/PycharmProjects/nasnet-tensorflow/nets/nets_factory.py"", line 135, in network_fn
    return func(images, num_classes, is_training=is_training, **kwargs)
  File ""/home/yusuke/PycharmProjects/nasnet-tensorflow/nets/nasnet/nasnet.py"", line 370, in build_nasnet_mobile
    final_endpoint=final_endpoint)
  File ""/home/yusuke/PycharmProjects/nasnet-tensorflow/nets/nasnet/nasnet.py"", line 495, in _build_nasnet_base
    scope='aux_{}'.format(cell_num))
  File ""/home/yusuke/PycharmProjects/nasnet-tensorflow/nets/nasnet/nasnet.py"", line 225, in _build_aux_head
    aux_logits, [5, 5], stride=3, padding='VALID')
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 122, in avg_pool2d
    outputs = layer.apply(inputs)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 809, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 696, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/layers/pooling.py"", line 277, in call
    data_format=utils.convert_data_format(self.data_format, 4))
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 2095, in avg_pool
    name=name)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 68, in _avg_pool
    data_format=data_format, name=name)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3273, in create_op
    compute_device=compute_device)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3313, in _create_op_helper
    set_shapes_for_outputs(op)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2501, in set_shapes_for_outputs
    return _set_shapes_for_outputs(op)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2474, in _set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2404, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 627, in call_cpp_shape_fn
    require_shape_fn)
  File ""/home/yusuke/.pyenv/versions/anaconda3-5.1.0/envs/dagan/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 691, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Negative dimension size caused by subtracting 5 from 4 for 'aux_7/aux_logits/AvgPool2D/AvgPool' (op: 'AvgPool') with input shapes: [32,4,4,528].
```
",0,,[],2019-02-17 05:01:22,open,,,[],2019-02-17 05:03:09
178,tensorflow/models,models,6208,Bahramudin,Points to take care off while preparing a data-set for training,"### System information
- **What is the top-level directory of the model you are using**: Object Detection 
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 64 Bit
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0 / 7.2.5
- **GPU model and memory**:GTX 1080 8GB 
- **Exact command to reproduce**: N/A

### Describe the problem
Please don't mind why I am asking this question. Because I want to possibly improve the accuracy of the Object Detecting. Because training is done on a data-set, so part of the accuracy depends on how the quality of our data-set is. I think a good quality of dataset will help to improve the detecting accuracy. 

So as an expert in AI and object detection, what is the important point to take care of, when collecting the images, labeling and so on.  What kind of images is better for training. And while labeling, how to label the images. Labeling is easy, but the points to take care off is very important, such as the below image:
![image](https://user-images.githubusercontent.com/13574895/52844789-5da0d480-3140-11e9-9b31-653a68bc3ced.png)
 And so on points.

**The second** part of the question is for what kind of object we need at least how much images to collect and label. For example, training **small objects** vs training **big objects**? And how much image is very enough for both kind. And also how long we need to train the dataset for each kind of data.

These points I think are very important to know, maybe there is no problem with Framework, but the problem with data, or no problem with data, but the problem with parameters to set and use during each kind data... and so on.

So please share the ideas and the experience if you have or know to help others may have the same problem or question about training as well.

Thanks, everyone!",4,,[],2019-02-15 08:52:35,open,,,['models: research'],2019-03-12 01:26:53
179,tensorflow/models,models,6207,holyhao,[Object detection TFlite] PPN tflite show total wrong result on mobile,"**System information**
* Have I written custom code (as opposed to using a stock example script provided in TensorFlow):N
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: HUAWEI MATE20
* TensorFlow installed from (source or binary):binary
* TensorFlow version (use command below):1.10
* Python version:3.6
* Bazel version (if compiling from source):1.15
* GCC/Compiler version (if compiling from source):1.9
* CUDA/cuDNN version:8.0
* GPU model and memory:16GB

I used object detection API, the ssd_mobilenetV2 is working well including the train, eval ,inference on pc ,and .tflite on phone.
Recently, i try ppn_mobilenetv1, the train and eval is well. The frozen_inference_graph.pb also work well on pc and the results are right. When i convert it to tflite_graph.pb, then convert it to detect.tflite. No errors occur. But when it runs on the mobile, it shows totally wrong results. I Look through the export_tflite_ssd_graph.py and export_inference_graph.py . I wonder how it works well for ssd_mobilenetV2 and wrong for ppn_mobilenetv1 on mobile, and how it works well for ppn_mobilenetv1 on pc and wrong on the mobile. Looking forward to your help.

To reproduce the bug as following:
1.Download the ppn_mobilnetV1 http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03.tar.gz
2.Conver it to tflite, no errors occur
```
CONFIG_PATH=../pipeline.config 
CHECKPOINT_PATH=../model.ckpt 
OUTPUT_DIR=../results/	
```
```
python object_detection/export_tflite_ssd_graph.py \ 
--pipeline_config_path $CONFIG_PATH \ 
--trained_checkpoint_prefix $CHECKPOINT_PATH \ 
--output_directory $OUTPUT_DIR \ 
--add_postprocessing_op=true
```
```
bazel-bin/tensorflow/contrib/lite/toco/toco \ 
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \ 
--input_shapes=1,300,300,3 \ 
--input_arrays=normalized_input_image_tensor \ 
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \ 
--inference_type=FLOAT \
--allow_custom_ops
```
3. Run it on mobile, i use the tensorflow-lite from tensoflow 1.10",3,,[],2019-02-15 06:51:56,open,,,['models: research'],2019-02-20 19:47:23
180,tensorflow/models,models,6201,mshtmfv,Inference not working after a training with convert_to_grayscale: true ,"### System information
- **What is the top-level directory of the model you are using**:
models/research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux e4e220f99f6f 4.4.0-112-generic #135-Ubuntu SMP Fri Jan 19 11:48:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
tf.VERSION = 1.12.0
tf.GIT_VERSION = v1.12.0-0-ga6d8ffae09
tf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09
- **CUDA/cuDNN version**:
CUDA 9.0
cudnn 7.4
- **GPU model and memory**:
NVIDIA Tesla V100 16GB

### Describe the problem
Have problem with `convert_to_grayscale` parameter in pipeline.config when trying to inference on frozen graph.

### Source code / logs
```
InvalidArgumentError: PartialTensorShape: Incompatible shapes during merge: [1,1,1] vs. [640,640,1] [[node Preprocessor/map/TensorArrayStack/TensorArrayGatherV3 (defined at <ipython-input-363-81ec5e7b9a34>:7) = TensorArrayGatherV3[dtype=DT_FLOAT, element_shape=[640,640,1], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Preprocessor/map/TensorArray_1, Preprocessor/map/TensorArrayStack/range, Preprocessor/map/while/Exit_2)]]
```

My config file:
```
# SSD with Resnet 50 v1 FPN feature extractor, shared box predictor and focal
# loss (a.k.a Retinanet).
# See Lin et al, https://arxiv.org/abs/1708.02002
# Trained on COCO, initialized from Imagenet classification checkpoint

# Achieves 35.2 mAP on COCO14 minival dataset. Doubling the number of training
# steps to 50k gets 36.9 mAP

# This config is TPU compatible

model {
  ssd {
    inplace_batchnorm_update: true
    freeze_batchnorm: false
    num_classes: 14
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    encode_background_as_zeros: true
    anchor_generator {
      multiscale_anchor_generator {
        min_level: 3
        max_level: 7
        anchor_scale: 4.0
        aspect_ratios: [1.0, 2.0, 0.5]
        scales_per_octave: 2
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 640
        width: 640
        convert_to_grayscale: true
      }
    }
    box_predictor {
      weight_shared_convolutional_box_predictor {
        depth: 256
        class_prediction_bias_init: -4.6
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.000004
            }
          }
          initializer {
            random_normal_initializer {
              stddev: 0.01
              mean: 0.0
            }
          }
          batch_norm {
            scale: true,
            decay: 0.997,
            epsilon: 0.001,
          }
        }
        num_layers_before_predictor: 4
        kernel_size: 3
      }
    }
    feature_extractor {
      type: 'ssd_resnet50_v1_fpn'
      fpn {
        min_level: 3
        max_level: 7
      }
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.000004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          scale: true,
          decay: 0.997,
          epsilon: 0.001,
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    loss {
      classification_loss {
        weighted_sigmoid_focal {
          alpha: 0.25
          gamma: 2.0
        }
      }
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_weight: 2.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    normalize_loc_loss_by_codesize: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 0.1
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  fine_tune_checkpoint: ""/workspace/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/model.ckpt""
  from_detection_checkpoint: true
  batch_size: 8
  sync_replicas: true
  startup_delay_steps: 0
  replicas_to_aggregate: 8
  num_steps: 2000000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    random_crop_image {
      min_object_covered: 0.0
      min_aspect_ratio: 0.75
      max_aspect_ratio: 3.0
      min_area: 0.75
      max_area: 1.0
      overlap_thresh: 0.0
    }
  }
  optimizer {
   momentum_optimizer {
      learning_rate {
        manual_step_learning_rate {
          initial_learning_rate: 0.005
          schedule {
            step: 50000
            learning_rate: 0.0005
          }
          schedule {
            step: 100000
            learning_rate: 0.00005
          }
        }
      }
      momentum_optimizer_value: 0.899999976158
    }
    use_moving_average: false
  }
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}
train_input_reader {
  label_map_path: ""/workspace/label_map.pbtxt""
  tf_record_input_reader {
    input_path: ""/workspace/data_train_shuffle/train.record-?????-of-00100""
  }
}
eval_config {
  num_examples: 3000
  max_evals: 10
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""/workspace/label_map.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""/workspace/eval-dataset.tfrecords""
  }
}
```",0,,[],2019-02-14 14:23:34,open,,,['models: research'],2019-02-14 23:40:11
181,tensorflow/models,models,6198,peterpaniff,"How to add layer to the pre-trained model for retrain? I know it is very easy to add a layer in CAFFE(just change prototxt formate file), how to do it in tensorflow?","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-02-14 01:54:34,open,,,['type:support'],2019-02-14 23:40:45
182,tensorflow/models,models,6191,adeebakausar,if multiple object of same class in the given image than how to make tf record for mask RCNN,"multiple object of same class in the given image.
![image1](https://user-images.githubusercontent.com/36010288/52641785-f992bd80-2e6c-11e9-8723-cc13d0776aad.JPG)

 **i have the data set folder .**
    Annotations ( have png mask)
   JPEGImages . (jpg images)
    Create_mask_rcnn_tf_record.py 
   label.pbtxt 


But this create _mask rcnn_tf_record.py only take one mask of same class in given image .how i create the tf record for multiple object of same class in the given image because the  create _mask rcnn_tf_record.py file cannot used for multiple objects of same class than how i create the tf record from mask rcnn in this scenario.




**System information**

What is the top-level directory of the model you are using**:
  ../models/research

Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
* 
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10

TensorFlow installed from (source or binary):pip

TensorFlow version (use command below):tensorflow 1.2-gpu

Bazel version (if compiling from source)**:I don't know
* 
*CUDA/cuDNN version:  cuda 9.0, cnDNN7.2.4

GPU model and memory:NVIDIA TITAN Xp wth 12G

",1,,[],2019-02-12 14:23:56,open,,,['stat:awaiting response'],2019-02-13 12:21:18
183,tensorflow/models,models,6190,sed0724963,How to use tensorboard to show training and validations line ?(how to know how many training steps I must to set?),"I use the"" tensorflow object detection api ""to training a detection model.
and I use the VOC format data,model is faster rcnn resnet 101 coco.

the training config and train.py is from the samples.
URL:https://github.com/tensorflow/models/tree/master/research/object_detection

SO,I use the recommended directory structure to run tensorboard,command line is tensorboard --logdir=${MODEL_DIR}.

it's output is this image:it have many loss function.:have total loss,clone loss...about six,but it's only have one lines on every image.
<img width=""1025"" alt=""2019-02-12 6 44 02"" src=""https://user-images.githubusercontent.com/38509051/52630230-ab2a0100-2ef6-11e9-9c74-912ccc9f60e5.png"">

 I want to show train and validation line on same image,like this:it's have two lines(train and validation) on same image.
<img width=""777"" alt=""2019-02-12 6 47 15"" src=""https://user-images.githubusercontent.com/38509051/52630321-e6c4cb00-2ef6-11e9-9d00-dde29b79f23e.png"">

So, how can I sucessful to do this??
the most teaching from Internet are all use mnist,and I don't understand mnist.
the tensorboard code I assumed it's on trainer.py,but I don't how to modify it to become my want.





------------------------

### System information
- **What is the top-level directory of the model you are using**: 
../models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
windows 10
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
tensorflow 1.8-gpu
- **Bazel version (if compiling from source)**:
I don't know
- **CUDA/cuDNN version**:
cuda 9.0, cnDNN7.1.4
- **GPU model and memory**:
ASUS GeForce GTX1080 8G
- **Exact command to reproduce**:
I don't know

",0,,[],2019-02-12 11:07:20,open,,,['models: research'],2019-02-14 23:41:02
184,tensorflow/models,models,6178,Lanbig,[Object Detection] Found Inf or NaN global norm. : Tensor had Inf values,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (pip install tensorflow-gpu)
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: NVIDIA P100
- **Exact command to reproduce**: 

### Describe the problem
Tensor had Inf values while training faster_rcnn_inception_resnet_v2_atrous_coco in my own dataset.


### Source code / logs
**Error log** [tmux-history-crash.txt](https://github.com/tensorflow/models/files/2848176/tmux-history-crash.txt)


**config**

model {
  faster_rcnn {
    num_classes: 5
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 1200
        max_dimension: 1200
      }
    }
    feature_extractor {
      type: 'faster_rcnn_inception_resnet_v2'
      first_stage_features_stride: 8
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 8
        width_stride: 8
      }
    }
    first_stage_atrous_rate: 2
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 17
    maxpool_kernel_size: 1
    maxpool_stride: 1
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 1
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.0003
          schedule {
            step: 900000
            learning_rate: .00003
          }
          schedule {
            step: 1200000
            learning_rate: .000003
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""zoo/faster_rcnn_inception_resnet_v2_atrous_coco/model.ckpt""
  from_detection_checkpoint: true
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""train.record""
  }
  label_map_path: ""label_map.pbtxt""
}

eval_config: {
  num_examples: 1200
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""eval.record""
  }
  label_map_path: ""label_map.pbtxt""
  shuffle: false
  num_readers: 1
}

----------------------------------------------------------------------------------

**Error log**

`
INFO:tensorflow:loss = 0.0033215743, step = 63717 (1.122 sec)
INFO:tensorflow:global_step/sec: 0.985096
INFO:tensorflow:loss = 0.3018198, step = 63718 (1.015 sec)
INFO:tensorflow:global_step/sec: 0.891359
INFO:tensorflow:loss = 0.07413207, step = 63719 (1.122 sec)
INFO:tensorflow:global_step/sec: 0.901555
INFO:tensorflow:loss = 0.010225122, step = 63720 (1.109 sec)
INFO:tensorflow:global_step/sec: 0.895415
INFO:tensorflow:loss = 0.05728304, step = 63721 (1.117 sec)
INFO:tensorflow:global_step/sec: 1.08617
INFO:tensorflow:loss = 0.006452726, step = 63722 (0.921 sec)
INFO:tensorflow:global_step/sec: 0.889418
INFO:tensorflow:loss = 0.014497861, step = 63723 (1.124 sec)
INFO:tensorflow:global_step/sec: 0.896101
INFO:tensorflow:loss = 0.02584396, step = 63724 (1.116 sec)
INFO:tensorflow:global_step/sec: 0.892287
INFO:tensorflow:loss = 0.015612617, step = 63725 (1.121 sec)
INFO:tensorflow:global_step/sec: 0.897164
INFO:tensorflow:loss = 0.011534013, step = 63726 (1.115 sec)
INFO:tensorflow:global_step/sec: 0.899451
INFO:tensorflow:loss = 0.024849497, step = 63727 (1.112 sec)
INFO:tensorflow:global_step/sec: 0.917954
INFO:tensorflow:loss = 0.23693836, step = 63728 (1.089 sec)
INFO:tensorflow:global_step/sec: 0.911387
INFO:tensorflow:loss = 0.09265502, step = 63729 (1.097 sec)
2019-02-09 18:43:08.167044: E tensorflow/core/kernels/check_numerics_op.cc:185] abnormal_detected_host @0x7f703ba43100 = {0, 1} Found Inf or NaN global norm.
Traceback (most recent call last):
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Found Inf or NaN global norm. : Tensor had Inf values
         [[{{node VerifyFinite/CheckNumerics}} = CheckNumerics[T=DT_FLOAT, message=""Found Inf or NaN global norm."", _device=""/job:localhost/replica:0/task:0/dev
ice:GPU:0""](global_norm_1/global_norm)]]
         [[{{node control_dependency/_12753}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:loc
alhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_26797_control_dependency"", tensor_type=DT_FLOAT, _device=""/job:localhost/rep
lica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 111, in <module>
    tf.app.run()
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 107, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
    saving_listeners)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1471, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1156, in run
    run_metadata=run_metadata)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    raise six.reraise(*original_exc_info)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1240, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1312, in run
    run_metadata=run_metadata)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1076, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Found Inf or NaN global norm. : Tensor had Inf values
         [[node VerifyFinite/CheckNumerics (defined at /home/safetyml/venv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py:306
)  = CheckNumerics[T=DT_FLOAT, message=""Found Inf or NaN global norm."", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](global_norm_1/global_norm)]]
         [[{{node control_dependency/_12753}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:loc
alhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_26797_control_dependency"", tensor_type=DT_FLOAT, _device=""/job:localhost/rep
lica:0/task:0/device:CPU:0""]()]]

Caused by op 'VerifyFinite/CheckNumerics', defined at:
  File ""object_detection/model_main.py"", line 111, in <module>
    tf.app.run()
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 107, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1237, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/safetyml/models/research/object_detection/model_lib.py"", line 382, in model_fn
    name='')  # Preventing scope prefix on all variables.
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 260, in optimize_loss
    gradients = _clip_gradients_by_norm(gradients, clip_gradients)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 306, in _clip_gradients_by_norm
    clipped_gradients, _ = clip_ops.clip_by_global_norm(gradients, clip_gradients)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py"", line 265, in clip_by_global_norm
    ""Found Inf or NaN global norm."")
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/ops/numerics.py"", line 47, in verify_tensor_all_finite
    verify_input = array_ops.check_numerics(t, message=msg)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 817, in check_numerics
    ""CheckNumerics"", tensor=tensor, message=message, name=name)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/home/safetyml/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Found Inf or NaN global norm. : Tensor had Inf values
         [[node VerifyFinite/CheckNumerics (defined at /home/safetyml/venv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py:306
)  = CheckNumerics[T=DT_FLOAT, message=""Found Inf or NaN global norm."", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](global_norm_1/global_norm)]]
         [[{{node control_dependency/_12753}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:loc
alhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_26797_control_dependency"", tensor_type=DT_FLOAT, _device=""/job:localhost/rep
lica:0/task:0/device:CPU:0""]()]]

`",3,,[],2019-02-09 19:25:38,open,,,['models: research'],2019-03-19 14:08:43
185,tensorflow/models,models,6176,adeebakausar, if  multiple object of same class in the given image than how to make tf record for mask RCNN," multiple object of same class in the given image.
![image1](https://user-images.githubusercontent.com/36010288/52522687-0371b200-2c1c-11e9-9498-a048b493a555.JPG)
",2,,[],2019-02-09 15:37:37,open,,,['models: research'],2019-02-14 23:53:10
186,tensorflow/models,models,6173,saeed68gm,struct2depth training data prep request for documentation,"### Describe the problem
I did not find enough documentation on preparing the data for the training. I am trying to replicate the results for struct2depth on kitti or cityscapes dataset.
However, I do not exactly know how to generate the data in the correct format.
Mainly, I would like to know how to do the following:

1. generate the train.txt and valid.txt files for the dataset (currently I am using the script in vid2depth subdir to do that)
2. How to generate the segmentation results and what is the naming convention used? ( is it using Mask RCNN model? should it be <name>-fseg.png?
3. It is mentioned that ""It is assumed that motion masks are already generated and stored as images"". Can you explain how to do this? what is the naming convention and how to generate this?

Thank you for sharing results of your work.
This is a really impressive paper and your response is appreciated.
@aneliaangelova @VincentCa

### System information
- models/research/struct2depth:
- Stock code:
- Centos 7:
- Tensorflow from binary (pip):
- v1.11.0-0-gc19e29306c 1.11.0:
- CUDA 9/CUDNN 7
- V100 32GB:
python gen_data_kitti.py

",12,,[],2019-02-08 15:46:47,open,,,['models: research'],2019-03-12 22:03:15
187,tensorflow/models,models,6171,bibererik,Multiple dropout layers in class_head and box_head for fast MCDropout,"What is the top-level directory of the model you are using: https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
Have I written custom code: Yes
OS Platform and Distribution: ubuntu 16.04 LTS
TensorFlow installed from: pip3
TensorFlow version: 1.10 
Bazel version: N/A
CUDA/cuDNN version: CUDA 9.0/cuDNN 7.1
GPU model and memory: TITAN X (Pascal)/PCIe/SSE2, 32 GiB
Exact command to reproduce: N/A

*********************************************************************************************
I am using Faster R-CNN Resnet 101. 

I added multiple dropout layers to class_head and box_head prediction methods for a faster MC Dropout sampling during evaluation. In model's configuration file I set use_dropout to true, and keep rate to 0.5. For instance, I want to receive 300*50 detections if I add 50 dropout layers. Then I will compute variance of that output.

An example from box_head.py after my change is below. I added a new method called multi_dropout in box_head.py, and I call this new method in the predict method of the same script.

When I start evaluation with ./legacy/eval.py, I receive an error. I think the model can't understand that I am sending a batch of 50 outputs, but considers it as one output since I am using the same image. The problem is with decoding the boxes using anchors in the second stage before NMS. 

I wonder if there is a way to overcome this problem with dimensions. I really need to speed up the inference with a trick like this one.

box_head.py
 ```
# New method 
   def multi_dropout(self, features, keep_prob=0.5, is_training=True, n_samples=10):
    
    output = []

    for dropout_index in range(n_samples):
      output.append(slim.dropout(
          features,
          keep_prob=keep_prob,
          is_training=is_training))
    stacked = tf.stack(output)
    return stacked

  def predict(self, features, num_predictions_per_location=1):
    """"""Predicts boxes.

    Args:
      features: A float tensor of shape [batch_size, height, width,
        channels] containing features for a batch of images.
      num_predictions_per_location: Int containing number of predictions per
        location.

    Returns:
      box_encodings: A float tensor of shape
        [batch_size, 1, num_classes, code_size] representing the location of the
        objects.

    Raises:
      ValueError: If num_predictions_per_location is not 1.
    """"""
    if num_predictions_per_location != 1:
      raise ValueError('Only num_predictions_per_location=1 is supported')
    spatial_averaged_roi_pooled_features = tf.reduce_mean(
        features, [1, 2], keep_dims=True, name='AvgPool')
    flattened_roi_pooled_features = slim.flatten(
        spatial_averaged_roi_pooled_features)
    if self._use_dropout:
      # New controls 
      if self._is_training:
          flattened_roi_pooled_features = slim.dropout(
              flattened_roi_pooled_features,
              keep_prob=self._dropout_keep_prob,
              is_training=self._is_training)
        
       else:
          flattened_roi_pooled_features = self.multi_dropout(features=flattened_roi_pooled_features, keep_prob=self._dropout_keep_prob, is_training=self._is_training, n_samples=50)
   
      
    number_of_boxes = 1
    if not self._share_box_across_classes:
      number_of_boxes = self._num_classes

    with slim.arg_scope(self._fc_hyperparams_fn()):
      box_encodings = slim.fully_connected(
          flattened_roi_pooled_features,
          number_of_boxes * self._box_code_size,
          activation_fn=None,
          scope='BoxEncodingPredictor')
    box_encodings = tf.reshape(box_encodings,
                               [-1, 1, number_of_boxes, self._box_code_size])
    return box_encodings
```

**ERROR:**

```
Traceback (most recent call last):
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1576, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 45000 and 900 for 'SecondStagePostprocessor/Decode/mul' (op: 'Mul') with input shapes: [45000], [900].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./legacy/eval.py"", line 141, in <module>
    tf.app.run()
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 272, in new_func
    return func(*args, **kwargs)
  File ""./legacy/eval.py"", line 138, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/models/research/object_detection/legacy/evaluator.py"", line 187, in evaluate
    ignore_groundtruth=eval_config.ignore_groundtruth)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/models/research/object_detection/legacy/evaluator.py"", line 79, in _extract_predictions_and_losses
    detections = model.postprocess(prediction_dict, true_image_shapes)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1175, in postprocess
    mask_predictions=mask_predictions)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1595, in _postprocess_box_classifier
    refined_box_encodings_batch, proposal_boxes)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1665, in _batch_decode_boxes
    tiled_anchors_boxlist)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/models/research/object_detection/core/box_coder.py"", line 86, in decode
    return self._decode(rel_codes, anchors)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/models/research/object_detection/box_coders/faster_rcnn_box_coder.py"", line 111, in _decode
    w = tf.exp(tw) * wa
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 850, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 1094, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4936, in mul
    ""Mul"", x=x, y=y, name=name)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1731, in __init__
    control_input_ops)
  File ""/home/a21/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1579, in _create_c_op
    raise ValueError(str(e))
ValueError: Dimensions must be equal, but are 45000 and 900 for 'SecondStagePostprocessor/Decode/mul' (op: 'Mul') with input shapes: [45000], [900].
```
NOTE: I also tried sending the output to box decoder one by one, but again received an error 
`
ValueError: Shape must be rank 3 but is rank 4 for 'BatchMultiClassNonMaxSuppression/map/while/Slice' (op: 'Slice') with input shapes: [1,1,1,4], [3], [3].`",2,,[],2019-02-08 15:01:04,open,,,['models: research'],2019-02-14 23:56:55
188,tensorflow/models,models,6170,aashish-0393,Mask Output for Object detection in TF-Serving is of  [100 * 33 * 33 ],"Hi.

I am deploying ObjectDetection Mask (From their Model zoo) Pretrained model on tensorflow Serving.
When hitting the API with the image array, I am getting Mask in strange manner. Rest all the outputs are OK.
For Mask I am getting below output :
an array of [100 * 33 * 33 ] instead of [N*width*height]. - this resultant array dimensions are same for different image sizes.
The array contains float values but not 0 and 1

Is this expected or something has to be changed from my side??",5,,[],2019-02-08 12:08:49,open,,,['models: research'],2019-03-25 04:16:43
189,tensorflow/models,models,6169,ajithkrishnan,vid2depth - Cannot reproduce depth estimation results with default Hyperparameter Values,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: * models/research/vid2depth*
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: *Yes* - I have included code to estimate the validation loss
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: *Linux Ubuntu 16.04*
- **TensorFlow installed from (source or binary)**: *via pip*
- **TensorFlow version (use command below)**: *1.8.0*
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**:  *cuda 9.0*
- **GPU model and memory**: *Tesla V100-SXM2*
- **Exact command to reproduce**:     * python inference.py   --kitti_dir kitti-raw-uncompressed   --output_dir inference   --kitti_video kitti-raw-uncompressed/2011_09_26/2011_09_26_drive_0001_sync   --model_ckpt trained-model/model-119496*

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I followed the instructions given in README.md to analyse the depth estimation performance of the pretrained model (model-119496). I would like to reproduce the results by training the network from scratch. 
I used the entire KITTI dataset (175 GB - mentioned in the README) and i trained the networks using the default Hyperparameter values provided in *train.py*. However my results are subpar. I altered the train_steps from 200,000 (default in train.py) to 119,496 (referring to the filename of the pretrained-model). This too did not produce the expected results. I referred to the paper, the github issues section and stackoverflow, but could not find any extra information regarding this topic. 


I have attached screenshots of the results from inference. I have used the same frame from the kitti-raw-eigen dataset for comparison. Left - The model i trained for 119496 Iterations; Centre - Pretrained model from Tensorflow; Right - Model i trained for 212645 iterations.
As can be seen, in case of both the models i have trained, there are pixels which are estimated to have infinite depth.  

Could you please recommend some directions to optimize the network? Should i further reduce the learning rate form 0.0002 or change the batch size from 4.


### Source code / logs
Here is the train.py i updated to included to estimate the validation loss along with the training loss.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math
import os
import random
import time
from absl import app
from absl import flags
from absl import logging
import model
import reader
import numpy as np
import tensorflow as tf
import util

gfile = tf.gfile

HOME_DIR = os.path.expanduser('~')
DEFAULT_DATA_DIR = os.path.join(HOME_DIR, 'vid2depth/data/kitti_raw_eigen')
DEFAULT_CHECKPOINT_DIR = os.path.join(HOME_DIR, 'vid2depth/checkpoints')

flags.DEFINE_string('data_dir', DEFAULT_DATA_DIR, 'Preprocessed data.')
flags.DEFINE_float('learning_rate', 0.0002, 'Adam learning rate.')
flags.DEFINE_float('beta1', 0.9, 'Adam momentum.')
flags.DEFINE_float('reconstr_weight', 0.85, 'Frame reconstruction loss weight.')
flags.DEFINE_float('smooth_weight', 0.05, 'Smoothness loss weight.')
flags.DEFINE_float('ssim_weight', 0.15, 'SSIM loss weight.')
flags.DEFINE_float('icp_weight', 0.0, 'ICP loss weight.')
flags.DEFINE_integer('batch_size', 4, 'The size of a sample batch')
flags.DEFINE_integer('img_height', 128, 'Input frame height.')
flags.DEFINE_integer('img_width', 416, 'Input frame width.')
### Note: Training time grows linearly with sequence length.  Use 2 or 3.
flags.DEFINE_integer('seq_length', 3, 'Number of frames in sequence.')
flags.DEFINE_string('pretrained_ckpt', None, 'Path to checkpoint with '
                    'pretrained weights.  Do not include .data* extension.')
flags.DEFINE_string('checkpoint_dir', DEFAULT_CHECKPOINT_DIR,
                    'Directory to save model checkpoints.')
flags.DEFINE_integer('train_steps', 500000, 'Number of training steps.')
flags.DEFINE_integer('summary_freq', 100, 'Save summaries every N steps.')
flags.DEFINE_bool('legacy_mode', False, 'Whether to limit losses to using only '
                  'the middle frame in sequence as the target frame.')
FLAGS = flags.FLAGS


MAX_TO_KEEP = 500

NUM_SCALES = 4


def main(_):

  seed = 8964
  tf.set_random_seed(seed)
  np.random.seed(seed)
  random.seed(seed)

  if FLAGS.legacy_mode and FLAGS.seq_length < 3:
    raise ValueError('Legacy mode supports sequence length > 2 only.')

  if not gfile.Exists(FLAGS.checkpoint_dir):
    gfile.MakeDirs(FLAGS.checkpoint_dir)

  train_model = model.Model(data_dir=FLAGS.data_dir,
                            is_training=True,
                            learning_rate=FLAGS.learning_rate,
                            beta1=FLAGS.beta1,
                            reconstr_weight=FLAGS.reconstr_weight,
                            smooth_weight=FLAGS.smooth_weight,
                            ssim_weight=FLAGS.ssim_weight,
                            icp_weight=FLAGS.icp_weight,
                            batch_size=FLAGS.batch_size,
                            img_height=FLAGS.img_height,
                            img_width=FLAGS.img_width,
                            seq_length=FLAGS.seq_length,
                            legacy_mode=FLAGS.legacy_mode)

  ### creating an instance of DataReader to obtain the necessary image_stacks
  ### which can then be passed to the session in the feed_dict
  train_model.reader = reader.DataReader(train_model.data_dir, train_model.batch_size,
                                      train_model.img_height, train_model.img_width,
                                      train_model.seq_length, NUM_SCALES)

  ### returns the training image stack
  (image_stack_training, _, _) = (train_model.reader.read_data('train')) 
  ###print(image_stack_train)

  ### Returns the validation image stack
  (image_stack_validation, _, _) = (train_model.reader.read_data('val'))

  train(train_model, FLAGS.pretrained_ckpt, FLAGS.checkpoint_dir,
        FLAGS.train_steps, FLAGS.summary_freq, image_stack_training, image_stack_validation)


def train(train_model, pretrained_ckpt, checkpoint_dir, train_steps,
          summary_freq, image_stack_train, image_stack_val):
  """"""Train model.""""""
  best_val_loss = 10
  if pretrained_ckpt is not None:
    vars_to_restore = util.get_vars_to_restore(pretrained_ckpt)
    pretrain_restorer = tf.train.Saver(vars_to_restore)
  vars_to_save = util.get_vars_to_restore()
  saver = tf.train.Saver(vars_to_save + [train_model.global_step],
                         max_to_keep=MAX_TO_KEEP)
  sv = tf.train.Supervisor(logdir=checkpoint_dir, save_summaries_secs=0,
                           saver=None)
  config = tf.ConfigProto()
  config.gpu_options.allow_growth = True
  with sv.managed_session(config=config) as sess:
    if pretrained_ckpt is not None:
      logging.info('Restoring pretrained weights from %s', pretrained_ckpt)
      pretrain_restorer.restore(sess, pretrained_ckpt)
    logging.info('Attempting to resume training from %s...', checkpoint_dir)
    checkpoint = tf.train.latest_checkpoint(checkpoint_dir)
    logging.info('Last checkpoint found: %s', checkpoint)
    if checkpoint:
      saver.restore(sess, checkpoint)
      
    logging.info('Training...')
    start_time = time.time()
    last_summary_time = time.time()
    steps_per_epoch = train_model.reader.steps_per_epoch
    step = 1
    
    while step <= train_steps:
      train_model.image_stack = image_stack_train

      fetches = {
          'train': train_model.train_op,
          'global_step': train_model.global_step,
          'incr_global_step': train_model.incr_global_step
      }     

      if step % summary_freq == 0:
        fetches['loss'] = train_model.total_loss        
        fetches['summary'] = sv.summary_op

      #results = sess.run(fetches, feed_dict={train_model.image_stack: image_stack_train})      
      results = sess.run(fetches)
      
      global_step = results['global_step']

      #print(""step = {}"".format(step))
      #print('global_step = {}'.format(global_step))
      #print('train_steps = {}'.format(train_steps))

      if step % summary_freq == 0:
        #print('step / summary_freq == 0')
        train_model.image_stack = image_stack_val
        fetches_val = {
          'validation_loss': train_model.total_loss,
        }
        results_val, validation_summary = sess.run([fetches_val, train_model.validation_loss])
        

      if step % summary_freq == 0:        
        sv.summary_writer.add_summary(results['summary'], global_step)
        sv.summary_writer.add_summary(validation_summary, global_step)
        train_epoch = math.ceil(global_step / steps_per_epoch)
        train_step = global_step - (train_epoch - 1) * steps_per_epoch
        this_cycle = time.time() - last_summary_time
        last_summary_time += this_cycle
        logging.info(
            'Epoch: [%2d] [%5d/%5d] iteration no.: %.3f time: %4.2fs (%ds total) loss: %.3f validation_loss: %.3f',
            train_epoch, train_step, steps_per_epoch, global_step, this_cycle,
            time.time() - start_time, results['loss'], results_val['validation_loss'])
        #print('Validation Summary: {}'.format(validation_summary))

      if step % steps_per_epoch == 0:
        ###print('step steps per epoch')
        if results_val['validation_loss'] < best_val_loss:
          print('[*] Validation Loss decreased from {} to {}. Saving checkpoint to {}...'.format(best_val_loss, results_val['validation_loss'], checkpoint_dir))
          best_val_loss = results_val['validation_loss']
          saver.save(sess, os.path.join(checkpoint_dir, 'model'),
                        global_step=global_step)
        else:
            print('[*] Validation loss did not decrease from {}. Saving checkpoint to {}...'.format(best_val_loss, checkpoint_dir))
            saver.save(sess, os.path.join(checkpoint_dir, 'last_model'))

      ### Setting step to global_step allows for training for a total of
      ### train_steps even if the program is restarted during training.
      step = global_step + 1


if __name__ == '__main__':
  app.run(main)
![vid2dpeth_compare](https://user-images.githubusercontent.com/30627046/52472219-0c6a7100-2b93-11e9-8c54-1515e9c73bc9.png)

",0,,[],2019-02-08 10:17:43,open,,,['models: research'],2019-02-14 23:57:27
190,tensorflow/models,models,6166,karanchahal,DeepLab training on own dataset Issue Assign Shape Error," I am also training on my custom dataset. It has just 2 classes foreground and background and I am using the mobile net pretrained weights. However I am receiving this error:
```
Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Assign requires shapes of both tensors to match. lhs shape= [2] rhs shape= [19]
```
I am using the following command:
```
%cd /content/models/research

# From tensorflow/models/research/
!python deeplab/train.py \
    --logtostderr \
    --training_number_of_steps=30000 \
    --train_split=""train"" \
    --model_variant=""mobilenet_v2"" \
    --output_stride=16 \
    --decoder_output_stride=4 \
    --train_crop_size=513 \
    --train_crop_size=513 \
    --train_batch_size=12 \
    --dataset=""bdd"" \
    --initialize_last_layer = True \
    --fine_tune_batch_norm = False \
    --tf_initial_checkpoint=deeplab/deeplabv3_mnv2_cityscapes_train/model.ckpt\
    --train_logdir=deeplab/graphs/ \
    --dataset_dir=deeplab/bdd_seg/tfrecord
```

My dataset description looks like this:
```


_DATASETS_INFORMATION = {
    'cityscapes': _CITYSCAPES_INFORMATION,
    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
    'ade20k': _ADE20K_INFORMATION,
    'bdd': _BDD_INFORMATION,
}

```

I am quite confused. Why is this error happening ?",2,,[],2019-02-07 16:53:46,open,,,['models: research'],2019-02-18 11:52:13
191,tensorflow/models,models,6165,amartyobanerjee,2 ROOTS in English sentences while running script tensorflow/demo.sh in docker image tensorflow/syntaxnet,"### System information
- **What is the top-level directory of the model you are using**:
 models/research/syntaxnet/

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Modified syntaxnet/demo.sh from the docker image listed below to output CONLL format, and *not* call conll2tree.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

docker image tensorflow/syntaxnet from https://hub.docker.com/r/tensorflow/syntaxnet running on Ubuntu 18.04

- **Exact command to reproduce**:

See below.

### Describe the problem

While running containers based on the docker image tensorflow/syntaxnet, I am very often getting multiple roots on English sentences. So far it is always 2 roots, where the second root is always the punctuation ending the sentence.

The container was started in the following way:

$ docker run -it -v /home/amartyo/English/:/models tensorflow/syntaxnet /bin/bash


Here is the list of containers running on my laptop with their IDs and the images they are derived from:
$ docker container ls
CONTAINER ID        IMAGE                  COMMAND             CREATED             STATUS              PORTS               NAMES
db6821aa7def        tensorflow/syntaxnet   ""/bin/bash""         10 days ago         Up 21 hours         8888/tcp            admiring_cori
635c31cf0b80        brianlow/syntaxnet     ""/bin/bash""         10 days ago         Up 21 hours                             zealous_ramanujan

Here are the list of docker images on my laptop and their respective IDS:
$ docker image ls
REPOSITORY             TAG                 IMAGE ID            CREATED             SIZE
<none>                 <none>              21a090b765dd        2 weeks ago         4.1GB
tensorflow/syntaxnet   latest              2f7af814b2c4        22 months ago       4.04GB
brianlow/syntaxnet     latest              4fe2174cc7f5        2 years ago         2.35GB

I am running syntaxnet/demo.sh, which I have modified to output CONLL format without calling conll2tree. Here is my modified demo.sh:
$ docker container exec -i db6821aa7def cat syntaxnet/demo.sh
\#!/bin/bash
\# Copyright 2016 Google Inc. All Rights Reserved.
\#
\# Licensed under the Apache License, Version 2.0 (the ""License"");
\# you may not use this file except in compliance with the License.
\# You may obtain a copy of the License at
\#
\#     http://www.apache.org/licenses/LICENSE-2.0
\#
\# Unless required by applicable law or agreed to in writing, software
\# distributed under the License is distributed on an ""AS IS"" BASIS,
\# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
\# See the License for the specific language governing permissions and
\# limitations under the License.
\# ==============================================================================

\# A script that runs a tokenizer, a part-of-speech tagger and a dependency
\# parser on an English text file, with one sentence per line.
\#
\# Example usage:
\#  echo ""Parsey McParseface is my favorite parser!"" | syntaxnet/demo.sh

\# To run on a conll formatted file, add the --conll command line argument.
\#

PARSER_EVAL=bazel-bin/syntaxnet/parser_eval
MODEL_DIR=syntaxnet/models/parsey_mcparseface
[[ ""$1"" == ""--conll"" ]] && INPUT_FORMAT=stdin-conll || INPUT_FORMAT=stdin

$PARSER_EVAL \\
  --input=$INPUT_FORMAT \\
  --output=stdout-conll \\
  --hidden_layer_sizes=64 \\
  --arg_prefix=brain_tagger \\
  --graph_builder=structured \\
  --task_context=$MODEL_DIR/context.pbtxt \\
  --model_path=$MODEL_DIR/tagger-params \\
  --slim_model \\
  --batch_size=1024 \\
  --alsologtostderr \\
   | \\
  $PARSER_EVAL \\
  --input=stdin-conll \\
  --output=stdout-conll \\
  --hidden_layer_sizes=512,512 \\
  --arg_prefix=brain_parser \\
  --graph_builder=structured \\
  --task_context=$MODEL_DIR/context.pbtxt \\
  --model_path=$MODEL_DIR/parser-params \\
  --slim_model \\
  --batch_size=1024 \\
  --alsologtostderr 
\#  | \\
\#  bazel-bin/syntaxnet/conll2tree \\
\#  --task_context=$MODEL_DIR/context.pbtxt \\
\#  --alsologtostderr


The first sentence is 'Parsey McParseface is my favorite parser!', taken from the source code of syntaxnet/demo.sh. Here is the CONLL output of this sentence:

$ echo 'Parsey McParseface is my favorite parser!'|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	Parsey	_	NOUN	NNP	_	2	nn	_	_
2	McParseface	_	NOUN	NNP	_	6	nsubj	_	_
3	is	_	VERB	VBZ	_	6	cop	_	_
4	my	_	PRON	PRP$	_	6	poss	_	_
5	favorite	_	ADJ	JJ	_	6	amod	_	_
6	parser	_	NOUN	NN	_	0	ROOT	_	_
7	!	_	.	.	_	0	ROOT	_	_

Now consider the following 3 sentences, where the the last 2 sentences are referrering to the first sentence:
The quick brown fox jumps over the lazy dog.
The above sentence uses all the letters in the English alphabet.
It is used as a test of proficiency in typing with 10 fingers.

Here is the CONLL output of these 3 sentences:

$ echo 'The quick brown fox jumps over the lazy dog.'|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	The	_	DET	DT	_	4	det	_	_
2	quick	_	ADJ	JJ	_	4	amod	_	_
3	brown	_	ADJ	JJ	_	4	amod	_	_
4	fox	_	NOUN	NN	_	5	nsubj	_	_
5	jumps	_	VERB	VBZ	_	0	ROOT	_	_
6	over	_	ADP	IN	_	5	prep	_	_
7	the	_	DET	DT	_	9	det	_	_
8	lazy	_	ADJ	JJ	_	9	amod	_	_
9	dog	_	NOUN	NN	_	6	pobj	_	_
10	.	_	.	.	_	5	punct	_	_

$ echo 'The above sentence uses all the letters in the English alphabet.'|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	The	_	DET	DT	_	3	det	_	_
2	above	_	ADJ	JJ	_	3	amod	_	_
3	sentence	_	NOUN	NN	_	4	nsubj	_	_
4	uses	_	VERB	VBZ	_	0	ROOT	_	_
5	all	_	DET	PDT	_	7	predet	_	_
6	the	_	DET	DT	_	7	det	_	_
7	letters	_	NOUN	NNS	_	4	dobj	_	_
8	in	_	ADP	IN	_	7	prep	_	_
9	the	_	DET	DT	_	11	det	_	_
10	English	_	NOUN	NNP	_	11	nn	_	_
11	alphabet	_	NOUN	NN	_	8	pobj	_	_
12	.	_	.	.	_	0	ROOT	_	_

$ echo 'It is used as a test of proficiency in typing with 10 fingers.'|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	It	_	PRON	PRP	_	3	nsubjpass	_	_
2	is	_	VERB	VBZ	_	3	auxpass	_	_
3	used	_	VERB	VBN	_	0	ROOT	_	_
4	as	_	ADP	IN	_	3	prep	_	_
5	a	_	DET	DT	_	6	det	_	_
6	test	_	NOUN	NN	_	4	pobj	_	_
7	of	_	ADP	IN	_	6	prep	_	_
8	proficiency	_	NOUN	NN	_	7	pobj	_	_
9	in	_	ADP	IN	_	6	prep	_	_
10	typing	_	VERB	VBG	_	9	pobj	_	_
11	with	_	ADP	IN	_	10	prep	_	_
12	10	_	NUM	CD	_	13	num	_	_
13	fingers	_	NOUN	NNS	_	11	pobj	_	_
14	.	_	.	.	_	0	ROOT	_	_

As you can see, in 2 out of the 3 sentences 2 ROOTs are being output for one sentence, and the second ROOT is the terminal punctuation mark.

Here are 2 sentences that often turn up on the internet as examples of the importance of punctuation:
Let's eat Grandma!
Let's eat, Grandma!

Here is the CONLL output of these sentences:

$ echo ""Let's eat Grandma!""|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	Let	_	VERB	VB	_	0	ROOT	_	_
2	's	_	PRON	PRP	_	3	nsubj	_	_
3	eat	_	VERB	VB	_	1	ccomp	_	_
4	Grandma	_	NOUN	NNP	_	3	dobj	_	_
5	!	_	.	.	_	0	ROOT	_	_

amartyo@TNQ-LTP-185:~$ echo ""Let's eat, Grandma!""|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	Let	_	VERB	VB	_	0	ROOT	_	_
2	's	_	PRON	PRP	_	3	nsubj	_	_
3	eat	_	VERB	VB	_	1	ccomp	_	_
4	,	_	.	,	_	3	punct	_	_
5	Grandma	_	NOUN	NNP	_	3	dobj	_	_
6	!	_	.	.	_	0	ROOT	_	_

In this case both sentences have multiple ROOTS.

Here are 2 sentences often found on the internet as examples of the importance of the placement of the comma, and how it can change the meaning of a sentence completely:
Pardon impossible, to be sent to Siberia
Pardon, impossible to be sent to Siberia.

$ echo 'Pardon impossible, to be sent to Siberia.'|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	Pardon	_	NOUN	NNP	_	0	ROOT	_	_
2	impossible	_	ADJ	JJ	_	1	acomp	_	_
3	,	_	.	,	_	1	punct	_	_
4	to	_	PRT	TO	_	6	aux	_	_
5	be	_	VERB	VB	_	6	auxpass	_	_
6	sent	_	VERB	VBN	_	1	xcomp	_	_
7	to	_	ADP	IN	_	6	prep	_	_
8	Siberia	_	NOUN	NNP	_	7	pobj	_	_
9	.	_	.	.	_	1	punct	_	_

$ echo 'Pardon, impossible to be sent to Siberia.'|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	Pardon	_	NOUN	NNP	_	0	ROOT	_	_
2	,	_	.	,	_	1	punct	_	_
3	impossible	_	ADJ	JJ	_	1	dep	_	_
4	to	_	PRT	TO	_	6	aux	_	_
5	be	_	VERB	VB	_	6	auxpass	_	_
6	sent	_	VERB	VBN	_	3	xcomp	_	_
7	to	_	ADP	IN	_	6	prep	_	_
8	Siberia	_	NOUN	NNP	_	7	pobj	_	_
9	.	_	.	.	_	0	ROOT	_	_

Here, changing the position of the comma has resulted in the second sentence having 2 ROOTs, unlike the first one. Here too, the second ROOT is the terminal punctuation mark, in this case a full stop.

Finally, here are 3 sentences often used to illustrate how the choice of punctuation can completely change the meaning of a sentence:
Woman without her man is nothing.
Woman, without her man, is nothing.
Woman: Without her, man is nothing.

$ echo 'Woman without her man is nothing.'|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	Woman	_	NOUN	NN	_	6	nsubj	_	_
2	without	_	ADP	IN	_	1	prep	_	_
3	her	_	PRON	PRP$	_	4	poss	_	_
4	man	_	NOUN	NN	_	2	pobj	_	_
5	is	_	VERB	VBZ	_	6	cop	_	_
6	nothing	_	NOUN	NN	_	0	ROOT	_	_
7	.	_	.	.	_	0	ROOT	_	_

$ echo 'Woman, without her man, is nothing.'|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	Woman	_	NOUN	NNP	_	8	discourse	_	_
2	,	_	.	,	_	8	punct	_	_
3	without	_	ADP	IN	_	8	prep	_	_
4	her	_	PRON	PRP$	_	5	poss	_	_
5	man	_	NOUN	NN	_	3	pobj	_	_
6	,	_	.	,	_	8	punct	_	_
7	is	_	VERB	VBZ	_	8	cop	_	_
8	nothing	_	NOUN	NN	_	0	ROOT	_	_
9	.	_	.	.	_	0	ROOT	_	_

$ echo 'Woman: Without her, man is nothing.'|docker container exec -i db6821aa7def /bin/bash syntaxnet/demo.sh 2> /dev/null
1	Woman	_	NOUN	NN	_	0	ROOT	_	_
2	:	_	.	:	_	1	punct	_	_
3	Without	_	ADP	IN	_	8	prep	_	_
4	her	_	PRON	PRP	_	3	pobj	_	_
5	,	_	.	,	_	8	punct	_	_
6	man	_	NOUN	NN	_	8	nsubj	_	_
7	is	_	VERB	VBZ	_	8	cop	_	_
8	nothing	_	NOUN	NN	_	1	dep	_	_
9	.	_	.	.	_	0	ROOT	_	_

In this case, all 3 sentences have 2 ROOTs with the 2nd ROOT being the terminal sentence punctuation.

By contrast, when I use a container based on a different docker image, i.e. brianlow/syntaxnet, which also has a syntaxnet/demo.sh script, I don't get multiple roots in a single sentence. The container was started with the following command:
$ docker run -it brianlow/syntaxnet /bin/bash

Here too I have modified demo.sh to only output CONLL. Here is my modified version of demo.sh:

$ docker container exec -i 635c31cf0b80 cat syntaxnet/demo.sh
\#!/bin/bash
\# Copyright 2016 Google Inc. All Rights Reserved.
\#
\# Licensed under the Apache License, Version 2.0 (the ""License"");
\# you may not use this file except in compliance with the License.
\# You may obtain a copy of the License at
\#
\#     http://www.apache.org/licenses/LICENSE-2.0
\#
\# Unless required by applicable law or agreed to in writing, software
\# distributed under the License is distributed on an ""AS IS"" BASIS,
\# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
\# See the License for the specific language governing permissions and
\# limitations under the License.
\# ==============================================================================

\# A script that runs a tokenizer, a part-of-speech tagger and a dependency
\# parser on an English text file, with one sentence per line.
\#
\# Example usage:
\#  echo ""Parsey McParseface is my favorite parser!"" | syntaxnet/demo.sh

\# To run on a conll formatted file, add the --conll command line argument.
\#

PARSER_EVAL=bazel-bin/syntaxnet/parser_eval
MODEL_DIR=syntaxnet/models/parsey_mcparseface
[[ ""$1"" == ""--conll"" ]] && INPUT_FORMAT=stdin-conll || INPUT_FORMAT=stdin

$PARSER_EVAL \\
  --input=$INPUT_FORMAT \\
  --output=stdout-conll \\
  --hidden_layer_sizes=64 \\
  --arg_prefix=brain_tagger \\
  --graph_builder=structured \\
  --task_context=$MODEL_DIR/context.pbtxt \\
  --model_path=$MODEL_DIR/tagger-params \\
  --slim_model \\
  --batch_size=1024 \\
  --alsologtostderr \\
   | \\
  $PARSER_EVAL \\
  --input=stdin-conll \\
  --output=stdout-conll \\
  --hidden_layer_sizes=512,512 \\
  --arg_prefix=brain_parser \\
  --graph_builder=structured \\
  --task_context=$MODEL_DIR/context.pbtxt \\
  --model_path=$MODEL_DIR/parser-params \\
  --slim_model \\
  --batch_size=1024 \\
  --alsologtostderr

Here is the output of running the same set of sentences against the modified syntaxnet/demo.sh:

$ cat sentences.txt|docker container exec -i 635c31cf0b80 /bin/bash syntaxnet/demo.sh 2> /dev/null
1	Parsey	_	NOUN	NNP	_	2	nn	_	_
2	McParseface	_	NOUN	NNP	_	6	nsubj	_	_
3	is	_	VERB	VBZ	_	6	cop	_	_
4	my	_	PRON	PRP$	_	6	poss	_	_
5	favorite	_	ADJ	JJ	_	6	amod	_	_
6	parser	_	NOUN	NN	_	0	ROOT	_	_
7	!	_	.	.	_	6	punct	_	_

1	The	_	DET	DT	_	4	det	_	_
2	quick	_	ADJ	JJ	_	4	amod	_	_
3	brown	_	ADJ	JJ	_	4	amod	_	_
4	fox	_	NOUN	NN	_	5	nsubj	_	_
5	jumps	_	VERB	VBZ	_	0	ROOT	_	_
6	over	_	ADP	IN	_	5	prep	_	_
7	the	_	DET	DT	_	9	det	_	_
8	lazy	_	ADJ	JJ	_	9	amod	_	_
9	dog	_	NOUN	NN	_	6	pobj	_	_
10	.	_	.	.	_	5	punct	_	_

1	The	_	DET	DT	_	3	det	_	_
2	above	_	ADJ	JJ	_	3	amod	_	_
3	sentence	_	NOUN	NN	_	4	nsubj	_	_
4	uses	_	VERB	VBZ	_	0	ROOT	_	_
5	all	_	DET	PDT	_	7	predet	_	_
6	the	_	DET	DT	_	7	det	_	_
7	letters	_	NOUN	NNS	_	4	dobj	_	_
8	in	_	ADP	IN	_	7	prep	_	_
9	the	_	DET	DT	_	11	det	_	_
10	English	_	NOUN	NNP	_	11	nn	_	_
11	alphabet	_	NOUN	NN	_	8	pobj	_	_
12	.	_	.	.	_	4	punct	_	_

1	It	_	PRON	PRP	_	3	nsubjpass	_	_
2	is	_	VERB	VBZ	_	3	auxpass	_	_
3	used	_	VERB	VBN	_	0	ROOT	_	_
4	as	_	ADP	IN	_	3	prep	_	_
5	a	_	DET	DT	_	6	det	_	_
6	test	_	NOUN	NN	_	4	pobj	_	_
7	of	_	ADP	IN	_	6	prep	_	_
8	proficiency	_	NOUN	NN	_	7	pobj	_	_
9	in	_	ADP	IN	_	6	prep	_	_
10	typing	_	VERB	VBG	_	9	pobj	_	_
11	with	_	ADP	IN	_	10	prep	_	_
12	10	_	NUM	CD	_	13	num	_	_
13	fingers	_	NOUN	NNS	_	11	pobj	_	_
14	.	_	.	.	_	3	punct	_	_

1	Let	_	VERB	VB	_	0	ROOT	_	_
2	's	_	PRON	PRP	_	3	nsubj	_	_
3	eat	_	VERB	VB	_	1	ccomp	_	_
4	Grandma	_	NOUN	NNP	_	3	dobj	_	_
5	!	_	.	.	_	1	punct	_	_

1	Let	_	VERB	VB	_	0	ROOT	_	_
2	's	_	PRON	PRP	_	3	nsubj	_	_
3	eat	_	VERB	VB	_	1	ccomp	_	_
4	,	_	.	,	_	3	punct	_	_
5	Grandma	_	NOUN	NNP	_	3	dobj	_	_
6	!	_	.	.	_	1	punct	_	_

1	Pardon	_	NOUN	NNP	_	6	nsubjpass	_	_
2	impossible	_	ADJ	JJ	_	1	acomp	_	_
3	,	_	.	,	_	6	punct	_	_
4	to	_	PRT	TO	_	6	aux	_	_
5	be	_	VERB	VB	_	6	auxpass	_	_
6	sent	_	VERB	VBN	_	0	ROOT	_	_
7	to	_	ADP	IN	_	6	prep	_	_
8	Siberia	_	NOUN	NNP	_	7	pobj	_	_
9	.	_	.	.	_	6	punct	_	_

1	Pardon	_	NOUN	NNP	_	0	ROOT	_	_
2	,	_	.	,	_	1	punct	_	_
3	impossible	_	ADJ	JJ	_	6	advmod	_	_
4	to	_	PRT	TO	_	6	aux	_	_
5	be	_	VERB	VB	_	6	auxpass	_	_
6	sent	_	VERB	VBN	_	1	dep	_	_
7	to	_	ADP	IN	_	6	prep	_	_
8	Siberia	_	NOUN	NNP	_	7	pobj	_	_
9	.	_	.	.	_	1	punct	_	_

1	Woman	_	NOUN	NN	_	6	nsubj	_	_
2	without	_	ADP	IN	_	1	prep	_	_
3	her	_	PRON	PRP$	_	4	poss	_	_
4	man	_	NOUN	NN	_	2	pobj	_	_
5	is	_	VERB	VBZ	_	6	cop	_	_
6	nothing	_	NOUN	NN	_	0	ROOT	_	_
7	.	_	.	.	_	6	punct	_	_

1	Woman	_	NOUN	NNP	_	8	nsubj	_	_
2	,	_	.	,	_	8	punct	_	_
3	without	_	ADP	IN	_	8	prep	_	_
4	her	_	PRON	PRP$	_	5	poss	_	_
5	man	_	NOUN	NN	_	3	pobj	_	_
6	,	_	.	,	_	8	punct	_	_
7	is	_	VERB	VBZ	_	8	cop	_	_
8	nothing	_	NOUN	NN	_	0	ROOT	_	_
9	.	_	.	.	_	8	punct	_	_

1	Woman	_	NOUN	NN	_	0	ROOT	_	_
2	:	_	.	:	_	1	punct	_	_
3	Without	_	ADP	IN	_	8	prep	_	_
4	her	_	PRON	PRP	_	3	pobj	_	_
5	,	_	.	,	_	8	punct	_	_
6	man	_	NOUN	NN	_	8	nsubj	_	_
7	is	_	VERB	VBZ	_	8	cop	_	_
8	nothing	_	NOUN	NN	_	1	dep	_	_
9	.	_	.	.	_	1	punct	_	_

Here every sentence has a single ROOT, and the terminal punctuation mark is always recognized as such.

In both tensorflow/syntaxnet and brianlow/syntaxnet images, demo.sh is originally identical. Both invoke the program bazel-bin/syntaxnet/parser_eval and use the parsey_mcparseface model. It seems to me there is a regression either in bazel-bin/syntaxnet/parser_eval or in the parsey_mcparseface model.

Is there any workaround for this bug? Where should I file a bug report, and against what? Please give me any suggestions you may have.
Thanks.",2,,[],2019-02-07 12:14:47,open,,,['models: research'],2019-02-14 23:58:06
192,tensorflow/models,models,6164,expelliarms,Is there a pre-trained model for deep_speech?,"On https://github.com/tensorflow/models/tree/master/research/deep_speech there are scripts for training deep speech 2 model on Librispeech dataset. Is there a pre-trained model of the same which I can use off the shelf for infrence?

Thank you.",1,,[],2019-02-07 04:13:55,open,,,"['models: research', 'stat:awaiting response']",2019-02-14 23:58:16
193,tensorflow/models,models,6160,domil,"InternalError (see above for traceback): Overlapping slices: existing slice = -:-, new slice = -:-          [[node save/restore_slice_9 (defined at /home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py:109)  = RestoreSlice[dt=DT_FLOAT, preferred_shard=0, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/restore_slice_9/tensor_name, save/restore_slice_9/shape_and_slice)]]","2 FMA
2019-02-06 12:58:29.314894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-06 12:58:29.315153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:1e.0
totalMemory: 15.75GiB freeMemory: 317.94MiB
2019-02-06 12:58:29.315182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-02-06 12:58:30.269515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-06 12:58:30.269563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2019-02-06 12:58:30.269579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2019-02-06 12:58:30.269745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
2019-02-06 12:58:30.381076: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381058: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381119: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381150: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381065: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381151: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381165: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381223: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381239: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381143: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381262: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381262: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381287: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381202: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381206: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381328: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381341: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381341: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381386: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381401: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381183: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381310: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381428: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381446: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381447: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381463: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381495: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381104: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381510: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381352: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381554: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381570: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381585: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381591: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381620: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381454: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381643: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381506: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381307: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381705: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381607: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381521: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381705: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381761: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381745: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381780: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381786: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381661: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381816: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381748: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381840: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381848: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381717: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381800: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381854: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381799: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381891: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381913: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381868: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381881: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381951: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381972: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381954: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381982: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382011: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382024: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381920: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382024: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381869: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382070: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382070: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382071: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381990: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382117: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382047: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382159: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382116: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382159: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382184: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.381980: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382201: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382216: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382167: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382201: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382248: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382231: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382278: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382308: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382136: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382231: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382345: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382356: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382368: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382394: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382411: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382202: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382432: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382445: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382460: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382469: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382329: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382371: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382286: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
2019-02-06 12:58:30.382248: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:175 : Internal: Overlapping slices: existing slice = -:-, new slice = -:-
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: Overlapping slices: existing slice = -:-, new slice = -:-
         [[{{node save/restore_slice_9}} = RestoreSlice[dt=DT_FLOAT, preferred_shard=0, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/restore_slice_9/tensor_name, save/restore_slice_9/shape_and_slice)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py"", line 302, in <module>
    tf.app.run()
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py"", line 292, in main
    _SampleModel(FLAGS.prefix, vocab)
  File ""/home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py"", line 170, in _SampleModel
    sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)
  File ""/home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py"", line 113, in _LoadModel
    sess.run('save/restore_all', {'save/Const:0': ckpt_file})
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Overlapping slices: existing slice = -:-, new slice = -:-
         [[node save/restore_slice_9 (defined at /home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py:109)  = RestoreSlice[dt=DT_FLOAT, preferred_shard=0, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/restore_slice_9/tensor_name, save/restore_slice_9/shape_and_slice)]]

Caused by op 'save/restore_slice_9', defined at:
  File ""/home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py"", line 302, in <module>
    tf.app.run()
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py"", line 292, in main
    _SampleModel(FLAGS.prefix, vocab)
  File ""/home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py"", line 170, in _SampleModel
    sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)
  File ""/home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py"", line 109, in _LoadModel
    'global_step:0'], name='')
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 234, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3440, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3440, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3299, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/home/ubuntu/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Overlapping slices: existing slice = -:-, new slice = -:-
         [[node save/restore_slice_9 (defined at /home/ubuntu/preTrainedModel/bazel-bin/lm_1b/lm_1b_eval.runfiles/__main__/lm_1b/lm_1b_eval.py:109)  = RestoreSlice[dt=DT_FLOAT, preferred_shard=0, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/restore_slice_9/tensor_name, save/restore_slice_9/shape_and_slice)]]
",1,,[],2019-02-06 13:09:57,open,,,['stat:awaiting response'],2019-02-07 12:19:27
194,tensorflow/models,models,6159,anilkunchalaece,Incomplete shape error while exporting graph from object detection API,"### System information
- **What is the top-level directory of the model you are using**:ObjectDetection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary via pip
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: No
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GeForce GT 730  and 4037MiB
- **Exact command to reproduce**:  python3 export_inference_graph.py --input_type image_tensor --pipeline_config_path training/inception_v2.config --trained_checkpoint_prefix training/model.ckpt-688 --output_directory trained-inference-graphs/output_inference_graph_v1

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I'm trying to train a custom object  using [object detection API](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html).  While trying to export trained model i'm getting the error 

**114 ops no flops stats due to incomplete shapes.**

Edit : The model is detecting with eval using 
python3 eval.py --logtostderr --pipeline_config_path=/home/ic/Documents/objectExtraction/workspace/training_demo/training/inception_v2.config --checkpoint_dir=/home/ic/Documents/objectExtraction/workspace/training_demo/training --eval_dir=/home/ic/Documents/objectExtraction/workspace/training_demo/eval

### Source code / logs
```
WARNING:tensorflow:From /home/ic/Documents/objectExtraction/models/research/object_detection/exporter.py:330: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
WARNING:tensorflow:From /home/ic/Documents/objectExtraction/models/research/object_detection/exporter.py:484: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.
Instructions for updating:
Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details
114 ops no flops stats due to incomplete shapes.
Parsing Inputs...
Incomplete shape.

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              0
-min_occurrence             0
-step                       -1
-order_by                   name
-account_type_regexes       _trainable_variables
-start_name_regexes         .*
-trim_name_regexes          .*BatchNorm.*
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     params
-output                     stdout:

==================Model Analysis Report======================
Incomplete shape.

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
param: Number of parameters (in the Variable).

Profile:
node name | # parameters
_TFProfRoot (--/5.49m params)
  BoxPredictor_0 (--/9.23k params)
    BoxPredictor_0/BoxEncodingPredictor (--/6.16k params)
      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)
      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x512x12, 6.14k/6.14k params)
    BoxPredictor_0/ClassPredictor (--/3.08k params)
      BoxPredictor_0/ClassPredictor/biases (6, 6/6 params)
      BoxPredictor_0/ClassPredictor/weights (1x1x512x6, 3.07k/3.07k params)
  BoxPredictor_1 (--/36.90k params)
    BoxPredictor_1/BoxEncodingPredictor (--/24.60k params)
      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)
      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1024x24, 24.58k/24.58k params)
    BoxPredictor_1/ClassPredictor (--/12.30k params)
      BoxPredictor_1/ClassPredictor/biases (12, 12/12 params)
      BoxPredictor_1/ClassPredictor/weights (1x1x1024x12, 12.29k/12.29k params)
  BoxPredictor_2 (--/18.47k params)
    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)
      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)
      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)
    BoxPredictor_2/ClassPredictor (--/6.16k params)
      BoxPredictor_2/ClassPredictor/biases (12, 12/12 params)
      BoxPredictor_2/ClassPredictor/weights (1x1x512x12, 6.14k/6.14k params)
  BoxPredictor_3 (--/9.25k params)
    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)
      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)
      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)
    BoxPredictor_3/ClassPredictor (--/3.08k params)
      BoxPredictor_3/ClassPredictor/biases (12, 12/12 params)
      BoxPredictor_3/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)
  BoxPredictor_4 (--/9.25k params)
    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)
      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)
      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)
    BoxPredictor_4/ClassPredictor (--/3.08k params)
      BoxPredictor_4/ClassPredictor/biases (12, 12/12 params)
      BoxPredictor_4/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)
  BoxPredictor_5 (--/4.64k params)
    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)
      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)
      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)
    BoxPredictor_5/ClassPredictor (--/1.55k params)
      BoxPredictor_5/ClassPredictor/biases (12, 12/12 params)
      BoxPredictor_5/ClassPredictor/weights (1x1x128x12, 1.54k/1.54k params)
  FeatureExtractor (--/5.41m params)
    FeatureExtractor/MobilenetV1 (--/5.41m params)
      FeatureExtractor/MobilenetV1/Conv2d_0 (--/864 params)
        FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_0/weights (3x3x3x32, 864/864 params)
      FeatureExtractor/MobilenetV1/Conv2d_10_depthwise (--/4.61k params)
        FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)
      FeatureExtractor/MobilenetV1/Conv2d_10_pointwise (--/262.14k params)
        FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights (1x1x512x512, 262.14k/262.14k params)
      FeatureExtractor/MobilenetV1/Conv2d_11_depthwise (--/4.61k params)
        FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)
      FeatureExtractor/MobilenetV1/Conv2d_11_pointwise (--/262.14k params)
        FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights (1x1x512x512, 262.14k/262.14k params)
      FeatureExtractor/MobilenetV1/Conv2d_12_depthwise (--/4.61k params)
        FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)
      FeatureExtractor/MobilenetV1/Conv2d_12_pointwise (--/524.29k params)
        FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights (1x1x512x1024, 524.29k/524.29k params)
      FeatureExtractor/MobilenetV1/Conv2d_13_depthwise (--/9.22k params)
        FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights (3x3x1024x1, 9.22k/9.22k params)
      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise (--/1.05m params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights (1x1x1024x1024, 1.05m/1.05m params)
      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256 (--/262.14k params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/weights (1x1x1024x256, 262.14k/262.14k params)
      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128 (--/65.54k params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)
      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128 (--/32.77k params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)
      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64 (--/16.38k params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)
      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512 (--/1.18m params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights (3x3x256x512, 1.18m/1.18m params)
      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256 (--/294.91k params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)
      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256 (--/294.91k params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)
      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128 (--/73.73k params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights (3x3x64x128, 73.73k/73.73k params)
      FeatureExtractor/MobilenetV1/Conv2d_1_depthwise (--/288 params)
        FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights (3x3x32x1, 288/288 params)
      FeatureExtractor/MobilenetV1/Conv2d_1_pointwise (--/2.05k params)
        FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights (1x1x32x64, 2.05k/2.05k params)
      FeatureExtractor/MobilenetV1/Conv2d_2_depthwise (--/576 params)
        FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights (3x3x64x1, 576/576 params)
      FeatureExtractor/MobilenetV1/Conv2d_2_pointwise (--/8.19k params)
        FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights (1x1x64x128, 8.19k/8.19k params)
      FeatureExtractor/MobilenetV1/Conv2d_3_depthwise (--/1.15k params)
        FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)
      FeatureExtractor/MobilenetV1/Conv2d_3_pointwise (--/16.38k params)
        FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights (1x1x128x128, 16.38k/16.38k params)
      FeatureExtractor/MobilenetV1/Conv2d_4_depthwise (--/1.15k params)
        FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)
      FeatureExtractor/MobilenetV1/Conv2d_4_pointwise (--/32.77k params)
        FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights (1x1x128x256, 32.77k/32.77k params)
      FeatureExtractor/MobilenetV1/Conv2d_5_depthwise (--/2.30k params)
        FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)
      FeatureExtractor/MobilenetV1/Conv2d_5_pointwise (--/65.54k params)
        FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights (1x1x256x256, 65.54k/65.54k params)
      FeatureExtractor/MobilenetV1/Conv2d_6_depthwise (--/2.30k params)
        FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)
      FeatureExtractor/MobilenetV1/Conv2d_6_pointwise (--/131.07k params)
        FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights (1x1x256x512, 131.07k/131.07k params)
      FeatureExtractor/MobilenetV1/Conv2d_7_depthwise (--/4.61k params)
        FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)
      FeatureExtractor/MobilenetV1/Conv2d_7_pointwise (--/262.14k params)
        FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights (1x1x512x512, 262.14k/262.14k params)
      FeatureExtractor/MobilenetV1/Conv2d_8_depthwise (--/4.61k params)
        FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)
      FeatureExtractor/MobilenetV1/Conv2d_8_pointwise (--/262.14k params)
        FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights (1x1x512x512, 262.14k/262.14k params)
      FeatureExtractor/MobilenetV1/Conv2d_9_depthwise (--/4.61k params)
        FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)
      FeatureExtractor/MobilenetV1/Conv2d_9_pointwise (--/262.14k params)
        FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights (1x1x512x512, 262.14k/262.14k params)

======================End of Report==========================
114 ops no flops stats due to incomplete shapes.
Parsing Inputs...
Incomplete shape.

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================
Incomplete shape.

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/17.62k flops)
  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)
  MultipleGridAnchorGenerator/add_2 (2.17k/2.17k flops)
  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)
  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)
  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)
  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)
  MultipleGridAnchorGenerator/add_5 (1.20k/1.20k flops)
  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)
  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)
  MultipleGridAnchorGenerator/mul_29 (600/600 flops)
  MultipleGridAnchorGenerator/add_8 (300/300 flops)
  MultipleGridAnchorGenerator/mul_36 (300/300 flops)
  MultipleGridAnchorGenerator/sub_2 (300/300 flops)
  MultipleGridAnchorGenerator/mul_35 (300/300 flops)
  MultipleGridAnchorGenerator/mul_37 (150/150 flops)
  MultipleGridAnchorGenerator/mul_44 (108/108 flops)
  MultipleGridAnchorGenerator/sub_3 (108/108 flops)
  MultipleGridAnchorGenerator/mul_43 (108/108 flops)
  MultipleGridAnchorGenerator/add_11 (108/108 flops)
  MultipleGridAnchorGenerator/mul_45 (54/54 flops)
  MultipleGridAnchorGenerator/sub_4 (48/48 flops)
  MultipleGridAnchorGenerator/add_14 (48/48 flops)
  MultipleGridAnchorGenerator/mul_51 (48/48 flops)
  MultipleGridAnchorGenerator/mul_52 (48/48 flops)
  MultipleGridAnchorGenerator/mul_53 (24/24 flops)
  MultipleGridAnchorGenerator/add (19/19 flops)
  MultipleGridAnchorGenerator/mul_18 (19/19 flops)
  MultipleGridAnchorGenerator/mul_17 (19/19 flops)
  MultipleGridAnchorGenerator/add_1 (19/19 flops)
  MultipleGridAnchorGenerator/mul_59 (12/12 flops)
  MultipleGridAnchorGenerator/mul_60 (12/12 flops)
  MultipleGridAnchorGenerator/sub_5 (12/12 flops)
  MultipleGridAnchorGenerator/add_17 (12/12 flops)
  MultipleGridAnchorGenerator/add_4 (10/10 flops)
  MultipleGridAnchorGenerator/add_3 (10/10 flops)
  MultipleGridAnchorGenerator/mul_26 (10/10 flops)
  MultipleGridAnchorGenerator/mul_25 (10/10 flops)
  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)
  MultipleGridAnchorGenerator/mul_55 (6/6 flops)
  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)
  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)
  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)
  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)
  MultipleGridAnchorGenerator/mul_38 (6/6 flops)
  MultipleGridAnchorGenerator/mul_39 (6/6 flops)
  MultipleGridAnchorGenerator/mul_40 (6/6 flops)
  MultipleGridAnchorGenerator/mul_54 (6/6 flops)
  MultipleGridAnchorGenerator/mul_61 (6/6 flops)
  MultipleGridAnchorGenerator/mul_46 (6/6 flops)
  MultipleGridAnchorGenerator/mul_47 (6/6 flops)
  MultipleGridAnchorGenerator/mul_48 (6/6 flops)
  MultipleGridAnchorGenerator/mul_56 (6/6 flops)
  MultipleGridAnchorGenerator/mul_31 (6/6 flops)
  MultipleGridAnchorGenerator/mul_30 (6/6 flops)
  MultipleGridAnchorGenerator/mul_24 (6/6 flops)
  MultipleGridAnchorGenerator/mul_23 (6/6 flops)
  MultipleGridAnchorGenerator/mul_22 (6/6 flops)
  MultipleGridAnchorGenerator/mul_32 (6/6 flops)
  MultipleGridAnchorGenerator/mul_33 (5/5 flops)
  MultipleGridAnchorGenerator/mul_34 (5/5 flops)
  MultipleGridAnchorGenerator/add_7 (5/5 flops)
  MultipleGridAnchorGenerator/add_6 (5/5 flops)
  MultipleGridAnchorGenerator/add_9 (3/3 flops)
  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)
  MultipleGridAnchorGenerator/mul_14 (3/3 flops)
  MultipleGridAnchorGenerator/mul_15 (3/3 flops)
  MultipleGridAnchorGenerator/mul_16 (3/3 flops)
  MultipleGridAnchorGenerator/mul_42 (3/3 flops)
  MultipleGridAnchorGenerator/mul_41 (3/3 flops)
  MultipleGridAnchorGenerator/add_10 (3/3 flops)
  MultipleGridAnchorGenerator/add_12 (2/2 flops)
  MultipleGridAnchorGenerator/add_13 (2/2 flops)
  MultipleGridAnchorGenerator/mul_50 (2/2 flops)
  MultipleGridAnchorGenerator/mul_49 (2/2 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add (1/1 flops)
  Preprocessor/map/while/add_1 (1/1 flops)
  Preprocessor/map/while/add (1/1 flops)
  Preprocessor/map/while/Less_1 (1/1 flops)
  Preprocessor/map/while/Less (1/1 flops)
  Postprocessor/Decode/transpose_1/sub (1/1 flops)
  Postprocessor/Decode/transpose/sub (1/1 flops)
  Postprocessor/Decode/get_center_coordinates_and_sizes/transpose/sub (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add_1 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)
  MultipleGridAnchorGenerator/mul_6 (1/1 flops)
  MultipleGridAnchorGenerator/add_19 (1/1 flops)
  MultipleGridAnchorGenerator/add_20 (1/1 flops)
  MultipleGridAnchorGenerator/add_21 (1/1 flops)
  MultipleGridAnchorGenerator/add_22 (1/1 flops)
  MultipleGridAnchorGenerator/add_23 (1/1 flops)
  MultipleGridAnchorGenerator/mul_9 (1/1 flops)
  MultipleGridAnchorGenerator/mul_8 (1/1 flops)
  MultipleGridAnchorGenerator/mul_7 (1/1 flops)
  MultipleGridAnchorGenerator/mul_12 (1/1 flops)
  MultipleGridAnchorGenerator/add_18 (1/1 flops)
  MultipleGridAnchorGenerator/mul_5 (1/1 flops)
  MultipleGridAnchorGenerator/mul_58 (1/1 flops)
  MultipleGridAnchorGenerator/mul_57 (1/1 flops)
  MultipleGridAnchorGenerator/assert_equal/Equal (1/1 flops)
  MultipleGridAnchorGenerator/Minimum (1/1 flops)
  MultipleGridAnchorGenerator/mul (1/1 flops)
  MultipleGridAnchorGenerator/mul_1 (1/1 flops)
  MultipleGridAnchorGenerator/mul_10 (1/1 flops)
  MultipleGridAnchorGenerator/mul_11 (1/1 flops)
  MultipleGridAnchorGenerator/mul_2 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)
  MultipleGridAnchorGenerator/mul_3 (1/1 flops)
  MultipleGridAnchorGenerator/mul_4 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)
  MultipleGridAnchorGenerator/add_15 (1/1 flops)
  MultipleGridAnchorGenerator/add_16 (1/1 flops)
  MultipleGridAnchorGenerator/mul_13 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)
  MultipleGridAnchorGenerator/truediv (1/1 flops)

======================End of Report==========================
2019-02-06 17:33:01.481753: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-06 17:33:01.543693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-06 17:33:01.544088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GT 730 major: 3 minor: 5 memoryClockRate(GHz): 0.9015
pciBusID: 0000:01:00.0
totalMemory: 3.94GiB freeMemory: 3.55GiB
2019-02-06 17:33:01.544103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-02-06 17:33:01.778208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-06 17:33:01.778236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-02-06 17:33:01.778242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-02-06 17:33:01.778359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3285 MB memory) -> physical GPU (device: 0, name: GeForce GT 730, pci bus id: 0000:01:00.0, compute capability: 3.5)
2019-02-06 17:33:02.694187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-02-06 17:33:02.694222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-06 17:33:02.694228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-02-06 17:33:02.694233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-02-06 17:33:02.694313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3285 MB memory) -> physical GPU (device: 0, name: GeForce GT 730, pci bus id: 0000:01:00.0, compute capability: 3.5)
2019-02-06 17:33:03.122006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-02-06 17:33:03.122046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-06 17:33:03.122052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-02-06 17:33:03.122056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-02-06 17:33:03.122144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3285 MB memory) -> physical GPU (device: 0, name: GeForce GT 730, pci bus id: 0000:01:00.0, compute capability: 3.5)

```",2,,[],2019-02-06 12:06:11,open,,,['models: research'],2019-03-04 10:24:31
195,tensorflow/models,models,6158,seemuch,Change the use_synth_data flag to data_source_type flag,...and added a cached_real_data option to that flag. ,5,,[],2019-02-05 20:19:41,open,,,['cla: yes'],2019-03-19 06:36:21
196,tensorflow/models,models,6156,morincl,Object Detection API: Fix the offline evaluation pipeline,"These fixes have been tested with Pascal VOC evaluation and MS COCO evaluation.

Fixes #3252 and #5369 among others.",0,,[],2019-02-05 17:19:00,open,,,['cla: yes'],2019-02-18 10:05:52
197,tensorflow/models,models,6155,aashish-0393,need to Deploy Object detection models to tensorflow servings.,"I need to convert Model trained on Object detection API to tensorflow servings.
The folder structure I get from saving a Object Detection model is ::
`model
         --checkpoint
        --frozen_inference_graph.pb
        --model.ckpt.data-00000-of-00001
       --model.ckpt.index
       --model.ckpt.meta
       --pipeline.config
       --saved_model
              --saved_model.pb
              --variables
                       --(empty)`

I need to convert above to tensorflow serving model structure

",4,,[],2019-02-05 10:53:49,open,,,"['models: research', 'type:support']",2019-04-09 18:26:23
198,tensorflow/models,models,6153,sebo313,Pretrained resnet for HPE,"### System information
Linux Ubuntu 16.04, Tensorflow 1.18 from binary, 
GPU GeForce GTX 1070

###Questions:

Which available pretrained tensorflow resnet has the highest accuracy in human pose estimation?

",2,,[],2019-02-04 20:13:25,open,,,['type:support'],2019-02-14 23:59:28
199,tensorflow/models,models,6152,HarshaPaladugu,How to add a new class to the existing model,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Hello,
I have a model ('frozen inference graph and checkpoints')  of 27 different types of logos now i want to add few more logs to my model but currently i don't have a data sets of pre-trained model that consists of 27 logos. Is their a way to add some more classes to the current model. Please ping me if any one have a solution.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-02-04 15:08:24,open,,,['type:support'],2019-02-05 23:24:47
200,tensorflow/models,models,6151,iHrushikesh,Updated typo in README.md,The `inference.py` expects arg as `kitti_video`,0,,[],2019-02-04 08:44:44,open,,,['cla: yes'],2019-02-04 08:45:53
201,tensorflow/models,models,6150,Ashokcharu,C- How to draw bounding-box using tensorflow c_api,"### In python 
I did object detection, Its working fine. Same thing I want to do in C

Here is the code to detect the object object in python

Calling boxes, like following:

```
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
num_detections = detection_graph.get_tensor_by_name('num_detections:0')
```
similarly for scores, and classes.

Then just called them in session run.

```
(boxes, scores, classes, num) = sess.run(
    [detection_boxes, detection_scores, detection_classes, num_detections],
    feed_dict={image_tensor: image_expanded})
```
Now Drawing the results of the detection

```
vis_util.visualize_boxes_and_labels_on_image_array(
    image,
    np.squeeze(boxes),
    np.squeeze(classes).astype(np.int32),
    np.squeeze(scores),
    category_index,
    use_normalized_coordinates=True,
    line_thickness=8,
    min_score_thresh=0.80)
```
Now I can able to view the image and detect the object(face).

`cv2.imshow('Object detector', image)`
And now above code is working great. and now same thing I wanted to do in C

### C Programming

Same like above i called boxes, scores, and classes.

```
  TF_Operation* image_tensor = TF_GraphOperationByName(graph, ""image_tensor"");
  TF_Operation* detection_boxes = TF_GraphOperationByName(graph, ""detection_boxes"");
  TF_Operation* detection_scores = TF_GraphOperationByName(graph, ""detection_scores"");
  TF_Operation* detection_classes= TF_GraphOperationByName(graph, ""detection_classes"");
  TF_Operation* num_detections = TF_GraphOperationByName(graph, ""num_detections"");
```
And created the dims

`int64_t dims[] = {1, img->width, img->height, num_channels };`
And passed image variable into TF_NewTensor

`  TF_Tensor * imgTensor = TF_NewTensor(TF_UINT8, dims, 4, img, img->imageSize, &deallocator, NULL);`
passing Input operation

```
  TF_Output image_input;
  image_input.oper = input_image_op;
  image_input.index = 0;
  TF_Output inputs[1] = {image_input};
```
passing OutPut operation

```
  TF_Output num_detection_output;
  num_detection_output.oper = num_detection_op;
  num_detection_output.index = 0;
  TF_Output outputs[1] = {num_detection_output};
  TF_Tensor* output_tensors[1];
```
And now passing this two input and output operation into TF_sessionRun

```
  TF_SessionRun(sess, NULL, 
                &image_input, &imgTensor, 1,
                outputs, output_tensors, 1,
                NULL, 0,
                NULL, status);
```
Viewing the image

` cvShowImage(""Face Detection"", img);`
Now,If I load the image, it's not detecting the object.

If I print the graph data i'm getting some outputs. For an example, If I print detection_classes. In result it showing 1, because I trained one Object.

Question :

Now my question is how can I Feed the image data's into graph and detect the object

Here is my full code In c

```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <tensorflow/c/c_api.h>

#include <opencv/highgui.h>
#include <opencv/cv.h>

  TF_Buffer* read_file(const char* file);

  void free_buffer(void* data, size_t length) { free(data); }

  void deallocator(void* ptr, size_t len, void* arg) { free((void*)ptr); }

  typedef unsigned char byte; 

  const int num_channels = 3;
  struct Image {
    byte* data;
    int width, height;
  };

  int main(int argc, char const* argv[]) {

  TF_Buffer* graph_def = read_file(""frozen_inference_graph.pb"");                      
  TF_Graph* graph = TF_NewGraph();
  TF_Status* status = TF_NewStatus();                                                     
  TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();                        
  TF_GraphImportGraphDef(graph, graph_def, opts, status);
  TF_DeleteImportGraphDefOptions(opts);
  if (TF_GetCode(status) != TF_OK) {
          fprintf(stderr, ""ERROR: Unable to import graph %s\n"", TF_Message(status));        
          return 1;
  }       
  fprintf(stdout, ""Successfully imported graph\n"");                                          
  TF_SessionOptions* opt = TF_NewSessionOptions();
  TF_Session* sess = TF_NewSession(graph, opt, status);
  //TF_DeleteSessionOptions(opt);
  if (TF_GetCode(status) != TF_OK) {
    fprintf(stderr, ""ERROR: Unable to create session %s\n"", TF_Message(status));
    return 1;
  }
  fprintf(stdout, ""Successfully session created\n"");

  IplImage* img = cvLoadImage(""Jb.jpg"", CV_LOAD_IMAGE_COLOR);
  if (!img)
  {
    printf(""Image can NOT Load!!!\n"");
    return 1;
  }

  TF_Operation* image_tensor = TF_GraphOperationByName(graph, ""image_tensor"");
  TF_Operation* detection_boxes = TF_GraphOperationByName(graph, ""detection_boxes"");
  TF_Operation* detection_scores = TF_GraphOperationByName(graph, ""detection_scores"");
  TF_Operation* detection_classes= TF_GraphOperationByName(graph, ""detection_classes"");
  TF_Operation* num_detections = TF_GraphOperationByName(graph, ""num_detections"");

  if (!image_tensor && !detection_boxes && detection_scores && detection_classes && num_detections){
    printf(""Image_tensor not loaded\n"");
    return 0;
  }

  int64_t dims[] = {1, img->width, img->height, num_channels };


  TF_Tensor * imgTensor = TF_NewTensor(TF_UINT8, dims, 4, img, img->imageSize, &deallocator, NULL);

  TF_Operation* input_image_op = TF_GraphOperationByName(graph, ""image_tensor"");
  if(!input_image_op) {
    printf(""Failed to find Op '%s'"", ""image_tensor"");
    return 1;
  }

  TF_Output image_input;
  image_input.oper = input_image_op;
  image_input.index = 0;
  TF_Output inputs[1] = {image_input};

  TF_Operation* num_detection_op = TF_GraphOperationByName(graph, ""detection_boxes"");
  if(!num_detection_op) {
    printf(""Failed to find Op '%s'"", ""num_detections"");
    return 1;
  }

  TF_Output num_detection_output;
  num_detection_output.oper = num_detection_op;
  num_detection_output.index = 0;

  TF_Output outputs[1] = {num_detection_output};
  TF_Tensor* output_tensors[1];

  TF_SessionRun(sess, NULL, 
                &image_input, &imgTensor, 1,
                outputs, output_tensors, 1,
                NULL, 0,
                NULL, status);

  TF_DeviceList* TF_SessionListDevices(sess, status);   

  printf(""%p"",&TF_SessionListDevices);           

  if(TF_GetCode(status) != TF_OK) {
    printf(""It's coming here"");
    printf(""%s\n"", TF_Message(status));
  }
  else {

    printf(""Ran successfully\n"");
    float* f = (float*)TF_TensorData(output_tensors[0]);
    printf(""TF data %f\n"", f[0]);

  }


  cvShowImage(""Face Detection"", img);
  cvWaitKey(0);
  cvReleaseImage(&img);

  TF_CloseSession(sess, status);
  TF_DeleteSession(sess, status);

  TF_DeleteStatus(status);
  TF_DeleteBuffer(graph_def);

  TF_DeleteGraph(graph);


  free((void*)img);

  return 0;
}

TF_Buffer* read_file(const char* file) {                                                  
  FILE *f = fopen(file, ""rb"");
  fseek(f, 0, SEEK_END);
  long fsize = ftell(f);                                                                  
  fseek(f, 0, SEEK_SET);  //same as rewind(f);                                            

  void* data = malloc(fsize);                                                             
  fread(data, fsize, 1, f);
  fclose(f);

  TF_Buffer* buf = TF_NewBuffer();                                                        
  buf->data = data;
  buf->length = fsize;                                                                    
  buf->data_deallocator = free_buffer;                                                    
  return buf;
} 
```",2,,[],2019-02-04 07:08:21,open,,,['type:support'],2019-02-09 12:15:14
202,tensorflow/models,models,6149,D-K-E,[object_detection] model_main generates export dir ?,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2015 NVIDIA Corporation
Built on Tue_Aug_11_14:27:32_CDT_2015
Cuda compilation tools, release 7.5, V7.5.17
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I have trained my own model using `model_main.py`. 
As far as I have understood it was suppose to generate model checkpoints from which we would need to generate a frozen graph using `export_inference_graph.py`.
However in my case it generated the following:

- checkpoint
- events.out.tfevents.....
- graph.pbtxt
- model.ckpt-XXX-.data-XXXX-of-XXXX
- model.ckpt-XXX-.index
- model.ckpt-XXX-.meta
- eval_0/events.out.tfevents....
- export/Servo/XXXX/saved_model.pb
- export/Servo/XXXX/variables/variables.index
- export/Servo/XXXX/variables/variables.data-XXXX-of-XXXX

This is not what I have understood from the docs, so I am slightly confused how to proceed next. Should I treat `saved_model.pb` as a frozen graph and use it in a session to classify new images, and what to do with the variables ?

**Edit**: `saved_model.pb` has more or less the same size of the resulting graph of `export_inference_graph.py`.

Thanks 
",3,,[],2019-02-04 03:20:59,open,,,['models: research'],2019-04-05 20:38:37
203,tensorflow/models,models,6146,leccyril,TypeError: 'InputReader' object does not support indexing in object_detection,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:


### System information
- **What is the top-level directory of the model you are using**:

**_object_detection_**

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

**_No_** 

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

**_Debian 9_**

- **TensorFlow installed from (source or binary)**:

**_intalled from github source_**

- **TensorFlow version (use command below)**:

**_1.8.0 (updated from git today)_**

- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

**_python legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config_**



### Describe the problem

train is not working anymore from github tensorflow last update

### Source code / logs


WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py:126: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
Traceback (most recent call last):
  File ""legacy/train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""legacy/train.py"", line 93, in main
    FLAGS.pipeline_config_path)
  File ""/home/tensorflow/models/research/object_detection/utils/config_util.py"", line 99, in get_configs_from_pipeline_file
    return create_configs_from_pipeline_proto(pipeline_config)
  File ""/home/tensorflow/models/research/object_detection/utils/config_util.py"", line 123, in create_configs_from_pipeline_proto
    configs[""eval_input_config""] = configs[""eval_input_configs""][0]
TypeError: 'InputReader' object does not support indexing
",8,,[],2019-02-03 15:39:29,open,,,['models: research'],2019-03-11 23:55:53
204,tensorflow/models,models,6145,adeebakausar,how to convert the mask into TF record ,"
![image1](https://user-images.githubusercontent.com/36010288/52176321-75da2200-2744-11e9-9b78-d3c04ba3803e.png)

",1,,[],2019-02-03 11:45:26,open,,,['type:support'],2019-02-05 23:26:30
205,tensorflow/models,models,6141,priyashkla,issue in convert the ImageNet data to native TFRecord format ,"
![image](https://user-images.githubusercontent.com/33479052/52167918-a459fc00-2748-11e9-88e4-c48c01813ca3.png)

",3,,[],2019-02-02 18:46:32,open,,,[],2019-03-08 23:10:32
206,tensorflow/models,models,6138,pooyadavoodi,Add Combined NMS,"CombinedNMS op was recently added to TensorFlow.
https://github.com/tensorflow/tensorflow/pull/23567

This PR adds CombinedNMS to the object detection API in TensorFlow.

The main difference between this new op and the old ones is that it does all of NMS in one op.
It's much more efficient, and it's also easier to swap out with a more efficient version of it from TF custom backends such as TensorRT and XLA.",4,,[],2019-02-01 21:50:08,open,,,['cla: yes'],2019-04-08 21:21:47
207,tensorflow/models,models,6137,mbufi,visualization_export_dir not saving images to specified folder when used in eval_config{} ,"### System information
- **What is the top-level directory of the model you are using**: Object Detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary 
- **TensorFlow version (use command below)**: tf-nightly-gpu
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 10/7.4
- **GPU model and memory**: 2080ti x 2 and 32 gig ram 
- **Exact command to reproduce**: 

### The problem

As the title states, I would like to save the detection vs. ground truth images of the eval from Tensorboard.

From the eval proto:


```
// Path to directory to store visualizations in. If empty, visualization
// images are not exported (only shown on Tensorboard).
optional string visualization_export_dir = 6 [default=""""];
```

Therefore I have added it to my model.config file as such. Keep in mind I am working from the /Object-Detection/ directory.

```
eval_config: {
  num_examples: 57

  visualization_export_dir: ""bevelgear_training/eval_images/""
  num_visualizations: 57
  metrics_set: ""pascal_voc_detection_metrics""
}

```

Training finishes as normal, I can see all the eval images in tensorboard BUT the `""bevelgear_training/eval_images/ ""` folder is empty.

Am I overlooking something? Anyone get this working?",4,,[],2019-02-01 21:32:14,open,,,['models: research'],2019-04-02 10:37:29
208,tensorflow/models,models,6135,satrya-sabeni,Mask rcnn training - whole bounding box is masked,"### System information
- **What is the top-level directory of the model you are using**: Object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: tf_record generated differently via tf record from a blog
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: CPU. 16GB ram (but using Google Cloud)
- **Exact command to reproduce**: 

### Problem description
I have successfully created an custom object detection model with this api before. Now I'm trying to train a mask rcnn model but when I look in tensorboard the mask just covers the whole bounding box with no shape whatsoever, like this:

![Image problem](https://i.imgur.com/cgMLydh.png)

After like 1500 steps no shape at all. I am using the 'mask_rcnn_resnet101_astrous_coco.config' and download the corresponding model to train from. I labeled the images using Rectlabel which generates a xml with bounding box and a mask png. Maybe it doesn't fully accept the png format?

Things I've already checked for:
- set only_faces: False when creating tf records 
- I have checked the tf records and it contains a png coded mask 'image/object/mask' ([1 example](https://pastebin.com/raw/T6KLx0w2))
- added ""metrics_set: ""coco_mask_metrics"" to ""eval_config""
- added mask_type: PNG_MASKS to both train and eval reader 

Possible causes:
- I am doing a test with just 37 images to see if it generates a segmentation prediction

At this point I really don't know what to do anymore, so any input or suggestion is appreciated. 

This is the tutorial I used: [https://towardsdatascience.com/building-a-custom-mask-rcnn-model-with-tensorflow-object-detection-952f5b0c7ab4](https://towardsdatascience.com/building-a-custom-mask-rcnn-model-with-tensorflow-object-detection-952f5b0c7ab4)",8,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2019-02-01 16:03:43,open,,,"['models: research', 'stat:contributions welcome']",2019-03-05 08:32:43
209,tensorflow/models,models,6134,tjulyz,details of training deeplabv3+ on cocovoc and cityscapes with coarse,"
### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**: tf1.6
- **Bazel version (if compiling from source)**:no
- **CUDA/cuDNN version**:cuda9.0 cudnn7
- **GPU model and memory**: xp(12g)
- **Exact command to reproduce**: no

### Describe the problem
Thanks for the codes!
I have some questions about the detail of the deeplabv3+
(1) can you share the results of deeplabv3+ on voc2012 testset without coco-pretraining?
(2) can you share the results of deeplabv3+ on cityscape testset without pretraining on coarse set? I have not found these results in your paper.
(3) I want to reproduce the coco-pretraining, so could you please share the details such as training iterations, batchsize for each gpu, learning rate for coco-pretraining and voc finetune, and whether have you used other bn methods?
(4) configurations about training on CItyscapes coarse set and finetune on fine set.

Thank you very very much!


### Source code / logs
no
",0,,[],2019-02-01 09:02:51,open,,,['models: research'],2019-02-01 21:43:11
210,tensorflow/models,models,6133,kalpalathika,How to find the bounding box coordinates,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-02-01 09:00:47,open,,,['type:support'],2019-02-05 23:37:49
211,tensorflow/models,models,6130,andreicatana,[Attention-OCR memorizing patterns],"Hello,

First of all I want to thank you for this awesome repository.
I tried to run attention-ocr on my dataset which contains images with dates in this format: 30/12/18. After 48k steps on 150k training dataset with a batch size of 32, even if the accuracy on the training dataset is almost 90%, on the qa the accuracy is below 5 percent. 
So, the problem is that the model is learning patterns and it cannot generalize. More exactly in the training data was images with dates only from 2017 and 2018, and the qa there was only dates from 2019. The datasets was created specially to verify this specific problem. 
So the question is: Is there something that can be done with the loss function or with other component in order to avoid this problem? 
Thank you a lot!",4,,[],2019-01-31 10:24:40,open,,,['models: research'],2019-02-06 07:55:59
212,tensorflow/models,models,6125,zengsn,fix issue - could not satisfy explicit device,"2nd time pull, see - https://github.com/tensorflow/models/pull/3641",1,,[],2019-01-31 05:57:33,open,,,['cla: yes'],2019-01-31 05:58:38
213,tensorflow/models,models,6123,PhilipMay,Release Imagenet Training Code for nasnet?,"Hi,
would it be possible to release the nasnet Training code for imagenet?
https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet

Or is it already released somewhere?

Thanks
Philip

### System information
- **What is the top-level directory of the model you are using**:
This info is not needed for this issue.
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
This info is not needed for this issue.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
This info is not needed for this issue.
- **TensorFlow installed from (source or binary)**:
This info is not needed for this issue.
- **TensorFlow version (use command below)**:
This info is not needed for this issue.
- **Bazel version (if compiling from source)**:
This info is not needed for this issue.
- **CUDA/cuDNN version**:
This info is not needed for this issue.
- **GPU model and memory**:
This info is not needed for this issue.
- **Exact command to reproduce**:
This info is not needed for this issue.",2,,[],2019-01-31 05:24:02,open,,,['models: research'],2019-02-02 12:15:00
214,tensorflow/models,models,6117,b7amine,Has anyone tried the same thing on floydhub?,"Has anyone tried the same thing on floydhub?
I am getting the modulenotfound error there. Can anyone help me?

_Originally posted by @uthiraa in https://github.com/tensorflow/models/issues/2577#issuecomment-406432072_",1,,[],2019-01-30 21:59:23,open,,,['stat:awaiting response'],2019-01-31 12:23:35
215,tensorflow/models,models,6116,mkhizeryounas,SavedModel export for NCF Recommender ,How can I SavedModel export for NCF Recommender?,1,,[],2019-01-30 21:58:06,open,,,['stat:awaiting response'],2019-01-31 12:23:30
216,tensorflow/models,models,6115,tarunluthra,ResourceExhaustedError OOM while running inference on a docker container,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**:  NVIDIA Docker: 2.0.3
- **Exact command to reproduce**:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I have created a docker container which has tensorflow's inference code embedded into it.  I am trying to send an image for inference and verify how many features are being detected by the ml model. 
at the moment the model is only able to handle a single image per inference. It craps out while running a second image with the error shown below.  Does anyone have insights on how to fix this?







### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,64,400,528] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
     [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at /opt/program/predictor.py:101)  = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise, FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
You can collect some of this information using our environment capture script:
",5,,[],2019-01-30 16:20:40,open,,,[],2019-02-04 16:28:33
217,tensorflow/models,models,6114,yeeyaa,official resnet with cifar10 dataset is not learning,"------------------------

### System information
- **What is the top-level directory of the model you are using**: official/resnet/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Red Hat Enterprise Linux Server release 7.5 (Maipo) for Power LE.
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**: N/A (not sure, I installed tf from IBM PowerAI)
- **CUDA/cuDNN version**: CUDA 10
- **GPU model and memory**: Tesla P100 16G
- **Exact command to reproduce**: python cifar10_main.py

### Describe the problem
This issue is very similar to #5380. However, the fix in that issue does not work for me.
I'm trying to train resnet on cifar10 dataset. By following the document I've downloaded the dataset and put it under default path. (/tmp/cifar10_data) And then started to train the model by running the cifar10_main.py script. However, even until the script finishes, my accuracy stays at 0. Same issue can be reproduced on my Macbook Pro and an IBM Power server.

I currently only has 3 GPUs as one of the 4 GPUs on the server is broken. However, I can reproduce this issue even with 1 GPU or no GPU (on my Mac).

### Source code / logs
```
(dlipy3) [simon@csldl01 resnet]$ python cifar10_main.py 
2019-01-30 18:49:09.774840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-30 18:49:09.836741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0003:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-30 18:49:09.893215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 2 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0007:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-30 18:49:09.893430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2
2019-01-30 18:49:10.550885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-30 18:49:10.550933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 
2019-01-30 18:49:10.550950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y N 
2019-01-30 18:49:10.550961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N N 
2019-01-30 18:49:10.550972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N 
2019-01-30 18:49:10.552264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 15129 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2019-01-30 18:49:10.552738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 15129 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0003:01:00.0, compute capability: 6.0)
2019-01-30 18:49:10.552985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:2 with 15129 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0007:01:00.0, compute capability: 6.0)
I0130 18:49:10.559577 70366661200528 tf_logging.py:115] Initializing RunConfig with distribution strategies.
I0130 18:49:10.559835 70366661200528 tf_logging.py:115] Not using Distribute Coordinator.
I0130 18:49:10.560448 70366661200528 tf_logging.py:115] Using config: {'_model_dir': '/tmp/cifar10_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 86400, '_session_config': allow_soft_placement: true
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.one_device_strategy.OneDeviceStrategy object at 0x3fff2415b908>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x3fff2415b978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}
2019-01-30 18:49:10.694470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2
2019-01-30 18:49:10.694757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-30 18:49:10.694771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 
2019-01-30 18:49:10.694783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y N 
2019-01-30 18:49:10.694794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N N 
2019-01-30 18:49:10.694805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N 
2019-01-30 18:49:10.696038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 15129 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2019-01-30 18:49:10.696226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 15129 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0003:01:00.0, compute capability: 6.0)
2019-01-30 18:49:10.696326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:2 with 15129 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0007:01:00.0, compute capability: 6.0)
I0130 18:49:10.780981 70366661200528 tf_logging.py:115] Benchmark run: {'model_name': 'resnet', 'dataset': {'name': 'CIFAR-10'}, 'machine_config': {'cpu_info': {'num_cores': 160, 'cpu_info': 'POWER8NVL (raw), altivec supported', 'mhz_per_cpu': 4023.0}, 'gpu_info': {'count': 3, 'model': 'Tesla P100-SXM2-16GB'}, 'memory_total': 544354402304, 'memory_available': 528438788096}, 'test_id': None, 'run_date': '2019-01-30T10:49:10.561312Z', 'tensorflow_version': {'version': '1.12.0', 'git_hash': ""b'unknown'""}, 'tensorflow_environment_variables': [], 'run_parameters': [{'name': 'batch_size', 'long_value': 128}, {'name': 'dtype', 'string_value': ""<dtype: 'float32'>""}, {'name': 'resnet_size', 'string_value': '56'}, {'name': 'resnet_version', 'string_value': '1'}, {'name': 'synthetic_data', 'bool_value': 'False'}, {'name': 'train_epochs', 'long_value': 182}]}
I0130 18:49:10.781345 70366661200528 tf_logging.py:115] Starting cycle: 0/19
W0130 18:49:10.793480 70366661200528 tf_logging.py:125] From /home/simon/workspace/models/official/resnet/resnet_run_loop.py:95: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.map_and_batch(...)`.
I0130 18:49:10.930330 70366661200528 tf_logging.py:115] Calling model_fn.
I0130 18:49:18.538796 70366661200528 tf_logging.py:115] Done calling model_fn.
I0130 18:49:18.796774 70366661200528 tf_logging.py:115] Create CheckpointSaverHook.
I0130 18:49:32.171143 70366661200528 tf_logging.py:115] Graph was finalized.
2019-01-30 18:49:32.171524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2
2019-01-30 18:49:32.171810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-30 18:49:32.171824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 
2019-01-30 18:49:32.171836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y N 
2019-01-30 18:49:32.171847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N N 
2019-01-30 18:49:32.171858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N 
2019-01-30 18:49:32.173070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15129 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2019-01-30 18:49:32.173357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15129 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0003:01:00.0, compute capability: 6.0)
2019-01-30 18:49:32.173492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15129 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0007:01:00.0, compute capability: 6.0)
I0130 18:49:32.179846 70366661200528 tf_logging.py:115] Restoring parameters from /tmp/cifar10_model/model.ckpt-0
I0130 18:49:32.981519 70366661200528 tf_logging.py:115] Running local_init_op.
I0130 18:49:33.065185 70366661200528 tf_logging.py:115] Done running local_init_op.
I0130 18:49:52.182665 70366661200528 tf_logging.py:115] Saving checkpoints for 0 into /tmp/cifar10_model/model.ckpt.
I0130 18:50:02.027058 70366661200528 tf_logging.py:115] Finalize system.
I0130 18:50:02.127228 70366661200528 tf_logging.py:115] Loss for final step: None.
I0130 18:50:02.127410 70366661200528 tf_logging.py:115] Starting to evaluate.
I0130 18:50:02.194000 70366661200528 tf_logging.py:115] Calling model_fn.
I0130 18:50:04.800140 70366661200528 tf_logging.py:115] Done calling model_fn.
I0130 18:50:04.827106 70366661200528 tf_logging.py:115] Starting evaluation at 2019-01-30-10:50:04
I0130 18:50:05.702091 70366661200528 tf_logging.py:115] Graph was finalized.
2019-01-30 18:50:05.702384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2
2019-01-30 18:50:05.702643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-30 18:50:05.702657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 
2019-01-30 18:50:05.702669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y N 
2019-01-30 18:50:05.702680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N N 
2019-01-30 18:50:05.702690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N 
2019-01-30 18:50:05.703666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15129 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2019-01-30 18:50:05.703836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15129 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0003:01:00.0, compute capability: 6.0)
2019-01-30 18:50:05.703963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15129 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0007:01:00.0, compute capability: 6.0)
I0130 18:50:05.704987 70366661200528 tf_logging.py:115] Restoring parameters from /tmp/cifar10_model/model.ckpt-0
I0130 18:50:06.097146 70366661200528 tf_logging.py:115] Running local_init_op.
I0130 18:50:06.136545 70366661200528 tf_logging.py:115] Done running local_init_op.
I0130 18:50:07.003525 70366661200528 tf_logging.py:115] Finished evaluation at 2019-01-30-10:50:06
I0130 18:50:07.003718 70366661200528 tf_logging.py:115] Saving dict for global step 0: accuracy = 0.0, accuracy_top_5 = 0.0, global_step = 0, loss = 0.0
I0130 18:50:12.970204 70366661200528 tf_logging.py:115] Saving 'checkpoint_path' summary for global step 0: /tmp/cifar10_model/model.ckpt-0
I0130 18:50:12.970948 70366661200528 tf_logging.py:115] Benchmark metric: {'name': 'accuracy', 'value': 0.0, 'unit': None, 'global_step': 0, 'timestamp': '2019-01-30T10:50:12.970906Z', 'extras': []}
I0130 18:50:12.971046 70366661200528 tf_logging.py:115] Benchmark metric: {'name': 'accuracy_top_5', 'value': 0.0, 'unit': None, 'global_step': 0, 'timestamp': '2019-01-30T10:50:12.971029Z', 'extras': []}
I0130 18:50:12.971127 70366661200528 tf_logging.py:115] Benchmark metric: {'name': 'loss', 'value': 0.0, 'unit': None, 'global_step': 0, 'timestamp': '2019-01-30T10:50:12.971111Z', 'extras': []}
I0130 18:50:12.971198 70366661200528 tf_logging.py:115] Starting cycle: 1/19
I0130 18:50:13.104621 70366661200528 tf_logging.py:115] Calling model_fn.
I0130 18:50:20.843732 70366661200528 tf_logging.py:115] Done calling model_fn.
I0130 18:50:21.102386 70366661200528 tf_logging.py:115] Create CheckpointSaverHook.
I0130 18:50:22.448599 70366661200528 tf_logging.py:115] Graph was finalized.
2019-01-30 18:50:22.448938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2
2019-01-30 18:50:22.449237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-30 18:50:22.449251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 
2019-01-30 18:50:22.449263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y N 
2019-01-30 18:50:22.449274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N N 
2019-01-30 18:50:22.449285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N 
2019-01-30 18:50:22.450381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15129 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2019-01-30 18:50:22.450598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15129 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0003:01:00.0, compute capability: 6.0)
2019-01-30 18:50:22.450719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15129 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0007:01:00.0, compute capability: 6.0)
I0130 18:50:22.452183 70366661200528 tf_logging.py:115] Restoring parameters from /tmp/cifar10_model/model.ckpt-0
I0130 18:50:23.205087 70366661200528 tf_logging.py:115] Running local_init_op.
I0130 18:50:23.288187 70366661200528 tf_logging.py:115] Done running local_init_op.
I0130 18:50:42.241658 70366661200528 tf_logging.py:115] Saving checkpoints for 0 into /tmp/cifar10_model/model.ckpt.
I0130 18:50:52.337174 70366661200528 tf_logging.py:115] Finalize system.
I0130 18:50:52.429791 70366661200528 tf_logging.py:115] Loss for final step: None.
I0130 18:50:52.429958 70366661200528 tf_logging.py:115] Starting to evaluate.
I0130 18:50:52.495685 70366661200528 tf_logging.py:115] Calling model_fn.
I0130 18:50:54.949359 70366661200528 tf_logging.py:115] Done calling model_fn.
I0130 18:50:54.984781 70366661200528 tf_logging.py:115] Starting evaluation at 2019-01-30-10:50:54
I0130 18:50:55.864446 70366661200528 tf_logging.py:115] Graph was finalized.
2019-01-30 18:50:55.864846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2
2019-01-30 18:50:55.865199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-30 18:50:55.865213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 
2019-01-30 18:50:55.865225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y N 
2019-01-30 18:50:55.865236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N N 
2019-01-30 18:50:55.865246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N 
2019-01-30 18:50:55.866270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15129 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2019-01-30 18:50:55.866434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15129 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0003:01:00.0, compute capability: 6.0)
2019-01-30 18:50:55.866554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15129 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0007:01:00.0, compute capability: 6.0)
I0130 18:50:55.867534 70366661200528 tf_logging.py:115] Restoring parameters from /tmp/cifar10_model/model.ckpt-0
I0130 18:50:56.259211 70366661200528 tf_logging.py:115] Running local_init_op.
I0130 18:50:56.298961 70366661200528 tf_logging.py:115] Done running local_init_op.
I0130 18:50:57.163550 70366661200528 tf_logging.py:115] Finished evaluation at 2019-01-30-10:50:57
I0130 18:50:57.163749 70366661200528 tf_logging.py:115] Saving dict for global step 0: accuracy = 0.0, accuracy_top_5 = 0.0, global_step = 0, loss = 0.0
I0130 18:50:57.164467 70366661200528 tf_logging.py:115] Saving 'checkpoint_path' summary for global step 0: /tmp/cifar10_model/model.ckpt-0
I0130 18:50:57.165109 70366661200528 tf_logging.py:115] Benchmark metric: {'name': 'accuracy', 'value': 0.0, 'unit': None, 'global_step': 0, 'timestamp': '2019-01-30T10:50:57.165076Z', 'extras': []}
I0130 18:50:57.165202 70366661200528 tf_logging.py:115] Benchmark metric: {'name': 'accuracy_top_5', 'value': 0.0, 'unit': None, 'global_step': 0, 'timestamp': '2019-01-30T10:50:57.165186Z', 'extras': []}
I0130 18:50:57.165279 70366661200528 tf_logging.py:115] Benchmark metric: {'name': 'loss', 'value': 0.0, 'unit': None, 'global_step': 0, 'timestamp': '2019-01-30T10:50:57.165264Z', 'extras': []}
I0130 18:50:57.165364 70366661200528 tf_logging.py:115] Starting cycle: 2/19
I0130 18:50:57.300551 70366661200528 tf_logging.py:115] Calling model_fn.
I0130 18:51:04.864876 70366661200528 tf_logging.py:115] Done calling model_fn.
I0130 18:51:05.124502 70366661200528 tf_logging.py:115] Create CheckpointSaverHook.
I0130 18:51:06.082098 70366661200528 tf_logging.py:115] Graph was finalized.
2019-01-30 18:51:06.082383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2
2019-01-30 18:51:06.082743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-30 18:51:06.082758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 
2019-01-30 18:51:06.082770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y N 
2019-01-30 18:51:06.082780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N N 
2019-01-30 18:51:06.082791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N 
2019-01-30 18:51:06.083754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15129 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2019-01-30 18:51:06.083940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15129 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0003:01:00.0, compute capability: 6.0)
2019-01-30 18:51:06.084069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15129 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0007:01:00.0, compute capability: 6.0)
I0130 18:51:06.085489 70366661200528 tf_logging.py:115] Restoring parameters from /tmp/cifar10_model/model.ckpt-0
I0130 18:51:06.834750 70366661200528 tf_logging.py:115] Running local_init_op.
I0130 18:51:06.917347 70366661200528 tf_logging.py:115] Done running local_init_op.
...
```",2,,[],2019-01-30 10:52:25,open,,,['models: official'],2019-02-28 00:13:31
218,tensorflow/models,models,6113,AlexeyV11,MobileNet training params to reproduce 70.9% top 1 accuracy,"Hi,

We have a pre-trained MobileNet_v1_1.0_224 model which achieves 70.9 imagenet accuracy - https://github.com/tensorflow/models/tree/master/research/slim

Was it trained with script train_image_classifier.py ? Could you please share hyper-params to reproduce this result?

Many thanks,
Alex",2,,[],2019-01-30 09:58:48,open,,,['models: research'],2019-02-01 21:53:39
219,tensorflow/models,models,6112,cumberb1tch,Incorrect quantized model ssd_mobilenet_v1_quantized_coco,"Object Detection model (ssd_mobilenet_v1_quantized_coco) from [this](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models) page is not fine-tuned with FakeQuantization.

Please see the Screen Shoot of model part: http://prntscr.com/me5nm6

We have pattern Conv ->  ReLU6 -> FakeQuantWithMinMaxVars -> Conv

And FakeQuantWithMinMaxVars have min and max range larger than 0-6 that comes from ReLU6. Which is impossible for trained model. Other quantized SSDs have ranges equal or smaller than 0-6 after ReLU6.

+ there are a lot of issues that this model shows low accuracy.


",1,,[],2019-01-30 09:51:58,open,,,['stat:awaiting response'],2019-01-31 00:19:22
220,tensorflow/models,models,6111,junjieliwhu,Solution for result = ''.join(_cescape_highbit_to_str[ord(c)] for c in result) IndexError: list index out of range,"hi guys, I met the problem a few days ago .now I solved it ,so shared with you

when I run model_mian.py, it occoured 
`result = ''.join(_cescape_highbit_to_str[ord(c)] for c in result) IndexError: list index out of range`

someone told that it may be caused by Chinese characters ,but I checked for many times ,and quite sure that there were not. so I found the code where threw error (text_encoding.py), and add try except, like this:
`  try:
      result = ''.join(_cescape_highbit_to_str[ord(c)] for c in result)
  except:
      print(result)`

then occured a new error: 
`'ascii' codec can't encode character u'\u202a' in position...`
and printed the path of train.record and test.record I wrote in pipeline config file(for me is ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config,'input_path','label_map_path')

the path is correct,but what is the strange code  '\u202a‘, I finally find that it is caused because I directly copy the file path in Windows and paste, which may add some strange code in the head of your real path. in windows, you think the path is 'data/train.record'
but in fact,when you copy the path and paste,may change to '\u202adata/train.record'
just delete and manually input，the problem will be solved!

This little bug wastes a lot of time of me, so I share with you and hope it works.

",1,,[],2019-01-30 07:44:40,open,,,['stat:awaiting response'],2019-01-31 00:19:18
221,tensorflow/models,models,6109,zsk423200,how to enable xla with estimator ,"cuda:9.0
tensorflow:1.12.0
What is the top-level directory of the model you are using： official/resnet
Have I written custom code：No
OS Platform and Distribution：Ubuntu16.04  MirroredStrategy
TensorFlow installed from：pip install
TensorFlow version：1.12.0
Bazel version：
CUDA/cuDNN version：cuda9.0
GPU model and memory：v100 32G
Exact command to reproduce : python imagenet_main.py --data_dir=/data/resnet-tf/tfrecord/ --batch_size=128 --ara='nccl' --num_gpus=2

in resnet_run_loop.py，i want to enable xla, so i add 
```
session_config = tf.ConfigProto()
session_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
```
but when i train by one gpu, and the distribute strategy is OneDeviceStrategy(""device:GPU:0""), xla is ok,
but when i train by two gpus, and the distribute strategy is MirroredStrategy, xla has error:
```
2019-01-30 10:55:06.902650: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:402] *** WARNING *** You are using ptxas 9.0.176, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
I0130 10:55:37.183267 140192737777408 tf_logging.py:115] Running local_init_op.
I0130 10:55:37.853480 140192737777408 tf_logging.py:115] Done running local_init_op.
I0130 10:56:25.948692 140192737777408 tf_logging.py:115] Saving checkpoints for 20020 into /tmp/model.ckpt.
2019-01-30 10:57:17.807557: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-01-30 10:57:17.807765: E tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc:338] Internal: All algorithms tried for convolution %custom-call = (f32[64,256,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,64,56,56]{3,2,1,0} %reduce-window.13518.10568, f32[1,1,64,256]{1,0,2,3} %copy.56), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convForward"", backend_config=""{\""convResultScale\"":1}"" failed.  Falling back to default algorithm.
```

or

```
*** WARNING *** You are using ptxas 9.0.176, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
I0130 11:05:42.691364 140448661317376 tf_logging.py:115] Running local_init_op.
I0130 11:05:43.474775 140448661317376 tf_logging.py:115] Done running local_init_op.
I0130 11:06:26.442031 140448661317376 tf_logging.py:115] Saving checkpoints for 20020 into /tmp/model.ckpt.
I0130 11:07:53.810114 140448661317376 tf_logging.py:115] cross_entropy = 2.8381975, learning_rate = 0.02560256, train_accuracy = 0.390625
I0130 11:07:53.812137 140448661317376 tf_logging.py:115] loss = 6.150483, step = 20020

E0130 11:09:08.093458 140448661317376 tf_logging.py:105] Model diverged with loss = NaN.

```



",1,,[],2019-01-30 02:57:01,open,,,['stat:awaiting response'],2019-01-31 01:24:58
222,tensorflow/models,models,6108,kelisiya,how to use center_loss in TF-slim  train_image_classifier.py,"when I use this [tensorflow-center_loss]( https://github.com/zoli333/Center-Loss )
`Traceback (most recent call last):
  File ""D:/Kelisiya/models-master/research/slim/train_image_classifier.py"", line 670, in <module>
    tf.app.run()
  File ""C:\Users\startdt\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:/Kelisiya/models-master/research/slim/train_image_classifier.py"", line 566, in main
    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])
  File ""D:\Kelisiya\models-master\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""D:/Kelisiya/models-master/research/slim/train_image_classifier.py"", line 559, in clone_fn
    center_loss,center = get_center_loss(logits,labels)
  File ""D:/Kelisiya/models-master/research/slim/train_image_classifier.py"", line 459, in get_center_loss
    loss = tf.nn.l2_loss(features - centers_batch)
  File ""C:\Users\startdt\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 862, in binary_op_wrapper
    return func(x, y, name=name)
  File ""C:\Users\startdt\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 8913, in sub
    ""Sub"", x=x, y=y, name=name)
  File ""C:\Users\startdt\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\startdt\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\startdt\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""C:\Users\startdt\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1790, in __init__
    control_input_ops)
  File ""C:\Users\startdt\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1629, in _create_c_op
    raise ValueError(str(e))
ValueError: Dimensions must be equal, but are 5 and 105 for 'sub_1' (op: 'Sub') with input shapes: [5,21], [105,21].`

I don't know how to use this code in train_image_classifier.py , have anyone tried with this code ever?",0,,[],2019-01-29 13:20:56,open,,,['models: research'],2019-01-29 20:44:20
223,tensorflow/models,models,6107,Jothisethu,AttributeError: 'FixedShapeResizer' object has no attribute 'resize_method',"When I run my new object_detection model,

python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_inception_v1_coco.config

I got this Error, How to fix this error, 
  File ""E:\DST\prog\objectdetection\Datasets\bero\models\research\object_detecti
on\builders\image_resizer_builder.py"", line 97, in build
    method = _tf_resize_method(fixed_shape_resizer_config.resize_method)
AttributeError: 'FixedShapeResizer' object has no attribute 'resize_method'

I have also attached the screen shot for this issue,

![ffff](https://user-images.githubusercontent.com/46806870/51896249-08d82c80-23d2-11e9-97e8-fb6f8badf773.jpg)


",0,,[],2019-01-29 08:58:05,open,,,['models: research'],2019-01-29 20:44:37
224,tensorflow/models,models,6106,KuribohG,Object detection validation very slow on custom dataset,"### System information
- **What is the top-level directory of the model you are using**: models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0/CuDNN 7.3
- **GPU model and memory**: TITAN Xp 12G
- **Exact command to reproduce**:
Run model_main.py on my custom dataset:
```
PIPELINE_CONFIG_PATH=""train.config""
MODEL_DIR=""models""
python3 ${HOME}/models/research/object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --alsologtostderr
```
train.config is exactly faster_rcnn_resnet101_pets.config, except I changed the num_classes to 1 (there's only one class in my dataset).

### Describe the problem
Although I set the `num_examples` exactly the same with pet dataset, the evaluation in the training process is extremely slow.
When `INFO:tensorflow:Done running local_init_op.` shows, it stuck for a long time. And the step `Evaluate annotation type *bbox*` takes about four hours.
Are there anyway reduce the evaluation time?

### Source code / logs
The eval part of train.config:
```
eval_config: {
  metrics_set: ""coco_detection_metrics""
  num_examples: 1101
}
```

Logs:
```
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2961: models/model.ckpt-2961
INFO:tensorflow:Saving checkpoints for 2962 into models/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2019-01-28-02:35:41
INFO:tensorflow:Graph was finalized.
2019-01-28 10:35:43.311144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-28 10:35:43.311247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-28 10:35:43.311260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-28 10:35:43.311267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-28 10:35:43.311647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11366 MB memory) ->
 physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:0c:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from models/model.ckpt-2962
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
WARNING:tensorflow:Ignoring ground truth with image id 1823103654 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1823103654 since it was previously added
creating index...
index created!
INFO:tensorflow:Loading and preparing annotation results...
INFO:tensorflow:DONE (t=28.37s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=13277.25s).
Accumulating evaluation results...
DONE (t=179.29s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.097
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.275
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.047
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.005
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.128
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.187
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.249
INFO:tensorflow:Finished evaluation at 2019-01-28-11:17:10
INFO:tensorflow:Saving dict for global step 2962: DetectionBoxes_Precision/mAP = 0.097453624, DetectionBoxes_Precision/mAP (large) = 0.12816702, DetectionBoxes_Precision/mAP (medium
) = 0.004950495, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.27519244, DetectionBoxes_Precision/mAP@.75IOU = 0.046542585, DetectionBoxes_Reca
ll/AR@1 = 0.009249808, DetectionBoxes_Recall/AR@10 = 0.06791596, DetectionBoxes_Recall/AR@100 = 0.18733716, DetectionBoxes_Recall/AR@100 (large) = 0.2487546, DetectionBoxes_Recall/A
R@100 (medium) = 0.0002144508, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/BoxClassifierLoss/classification_loss = 0.66864246, Loss/BoxClassifierLoss/localization_loss = 2.3871
467, Loss/RPNLoss/localization_loss = 1.0954195, Loss/RPNLoss/objectness_loss = 0.3143284, Loss/total_loss = 4.4655395, global_step = 2962, learning_rate = 1e-04, loss = 4.4655395
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2962: models/model.ckpt-2962
INFO:tensorflow:Saving checkpoints for 2963 into models/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2019-01-28-11:17:23
INFO:tensorflow:Graph was finalized.
2019-01-28 19:17:24.259390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-28 19:17:24.259465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-28 19:17:24.259475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-28 19:17:24.259482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-28 19:17:24.259647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11366 MB memory) ->
 physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:0c:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from models/model.ckpt-2963
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
WARNING:tensorflow:Ignoring ground truth with image id 1823103654 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1823103654 since it was previously added
creating index...
index created!
INFO:tensorflow:Loading and preparing annotation results...
INFO:tensorflow:DONE (t=27.72s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=11867.54s).
Accumulating evaluation results...
DONE (t=162.59s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.097
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.275
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.046
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.005
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.128
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.187
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.248
INFO:tensorflow:Finished evaluation at 2019-01-28-19:37:58
INFO:tensorflow:Saving dict for global step 2963: DetectionBoxes_Precision/mAP = 0.09728048, DetectionBoxes_Precision/mAP (large) = 0.12794043, DetectionBoxes_Precision/mAP (medium)
 = 0.004950495, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.27511117, DetectionBoxes_Precision/mAP@.75IOU = 0.045991823, DetectionBoxes_Recal
l/AR@1 = 0.009229408, DetectionBoxes_Recall/AR@10 = 0.06773527, DetectionBoxes_Recall/AR@100 = 0.18692453, DetectionBoxes_Recall/AR@100 (large) = 0.24820603, DetectionBoxes_Recall/A
R@100 (medium) = 0.00021755317, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/BoxClassifierLoss/classification_loss = 0.666948, Loss/BoxClassifierLoss/localization_loss = 2.38650
25, Loss/RPNLoss/localization_loss = 1.0936925, Loss/RPNLoss/objectness_loss = 0.31457162, Loss/total_loss = 4.4617558, global_step = 2963, learning_rate = 1e-04, loss = 4.4617558
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2963: models/model.ckpt-2963
INFO:tensorflow:Saving checkpoints for 2964 into models/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2019-01-28-19:38:17
INFO:tensorflow:Graph was finalized.
2019-01-29 03:38:18.362373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-29 03:38:18.362449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-29 03:38:18.362486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-29 03:38:18.362495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-29 03:38:18.362718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11366 MB memory) ->
 physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:0c:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from models/model.ckpt-2964
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
WARNING:tensorflow:Ignoring ground truth with image id 1823103654 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1823103654 since it was previously added
creating index...
index created!
INFO:tensorflow:Loading and preparing annotation results...
INFO:tensorflow:DONE (t=19.12s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
```
Maybe it is because evaluation step takes too long time, it evaluates at each train iteration.

And my dataset generation script:
```
import PIL.Image
import tensorflow as tf
import argparse
import hashlib
import io
import logging
import os
from lxml import etree
import random
from tqdm import tqdm
import contextlib2

from object_detection.utils import dataset_util
from object_detection.utils import label_map_util
from object_detection.dataset_tools import tf_record_creation_util

PBRS_ROOT = '/mnt/disk3/zzz/pbrs'
LABEL_MAP_PATH = '/mnt/disk3/zzz/pbrs/processed/2d_det/label_map.pbtxt'

parser = argparse.ArgumentParser()
parser.add_argument(""-o"", ""--output_path"", default=""/mnt/disk3/zzz/pbrs/processed/2d_det"", help=""Path to output TFRecord"")
args = parser.parse_args()

def create_tf_example(img_path, bbox_path, label_map_dict):
    with tf.gfile.GFile(img_path, 'rb') as fid:
        encoded_jpg = fid.read()
    encoded_jpg_io = io.BytesIO(encoded_jpg)
    image = PIL.Image.open(encoded_jpg_io)
    key = hashlib.sha256(encoded_jpg).hexdigest()
    
    width, height = 640, 480 

    xmin = []
    ymin = []
    xmax = []
    ymax = []
    classes = []
    classes_text = []

    f = open(bbox_path)
    lines = f.readlines()
    for line in lines:
        p = list(map(int, line.split()))
        xmin.append(float(p[2]) / width)
        ymin.append(float(p[1]) / height)
        xmax.append(float(p[4] + 1) / width)
        ymax.append(float(p[3] + 1) / height)
        classes_text.append('model'.encode('utf8'))
        classes.append(label_map_dict['model'])

    example = tf.train.Example(features=tf.train.Features(feature={
        'image/height': dataset_util.int64_feature(height),
        'image/width': dataset_util.int64_feature(width),
        'image/filename': dataset_util.bytes_feature(img_path.encode('utf8')),
        'image/source_id': dataset_util.bytes_feature(img_path.encode('utf8')),
        'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),
        'image/encoded': dataset_util.bytes_feature(encoded_jpg),
        'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),
        'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),
        'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),
        'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),
        'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),
        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
        'image/object/class/label': dataset_util.int64_list_feature(classes),
    }))
    return example


def create_tf_record(output_path, image_list, label_map_dict, num_shards):
    with contextlib2.ExitStack() as tf_record_close_stack:
        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
            tf_record_close_stack, output_path, num_shards)
        for idx, image in enumerate(tqdm(image_list)):
            bbox_path = os.path.join(PBRS_ROOT, '2d_bbox', image[0], '{}.txt'.format(image[1]))
            img_path = os.path.join(PBRS_ROOT, 'opengl_v2', image[0], '{}_color.jpg'.format(image[1]))
            tf_example = create_tf_example(img_path, bbox_path, label_map_dict)
            output_shard_index = idx % num_shards
            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())

def main():
    train_path = os.path.join(args.output_path, 'pbrs_2ddet_train.record')
    val_path = os.path.join(args.output_path, 'pbrs_2ddet_val.record')
    label_map_dict = label_map_util.get_label_map_dict(LABEL_MAP_PATH)

    house_id_list = os.listdir(os.path.join(PBRS_ROOT, 'node_v2'))
    image_list = []
    for house_id in house_id_list:
        camera_list = os.listdir(os.path.join(PBRS_ROOT, 'node_v2', house_id))
        camera_list = list(map(lambda x: x[:6], camera_list))
        for camera in camera_list:
            image_list.append((house_id, camera))
    random.seed(42)
    random.shuffle(image_list)

    num_examples = len(image_list)
    num_train = int(0.9 * num_examples)
    create_tf_record(train_path, image_list[:num_train], label_map_dict, num_shards=100)
    create_tf_record(val_path, image_list[num_train:], label_map_dict, num_shards=10)

if __name__ == '__main__':
    main()
```
There are about 500000 images in my train dataset, 50000 in val.",3,,[],2019-01-29 02:43:47,open,,,['models: research'],2019-03-09 01:12:02
225,tensorflow/models,models,6104,Walid-Ahmed,possible compatibility issue with object detection and Windows 10 ,"
------------------------

### System information
- **What is the top-level directory of the model you are using**:object-detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version **:1.10

- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh



### Describe the problem
Can not run the legacy\tain.py

### Error  log
tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open:  : The system cannot find the path specified.
; No such process
",1,,[],2019-01-28 14:36:21,open,,,['stat:awaiting response'],2019-01-29 12:17:09
226,tensorflow/models,models,6102,upwindflys,hanging when running estimator cifar10_main.py ,"I use tensorflow1.10,and run it with 1ps,1master,1worker,
then the worker hang after local_init_op
2019-01-28 15:59:36.770138: I tensorflow/core/distributed_runtime/master_session.cc:1165] Start master session a487f6a10e709c49 with config: gpu_options { per_process_gpu_memory_fraction: 1 force_gpu_compatible: true } allow_soft_placement: true
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
",1,"NamedUser(login=""ymodak"")","[NamedUser(login=""ymodak"")]",2019-01-28 08:15:32,open,,,"['models: official', 'stat:awaiting response']",2019-01-29 20:39:25
227,tensorflow/models,models,6100,Bahramudin,train.py and model_main.py,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Both Windows and Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX 1080 8GB
- **Exact command to reproduce**:

I want to ask, in the new version of the Object Detection API the train.py file has been moved to the legacy folder. And newly added model_main.py, but there is nothing said in the documentation, that why the train.py moved to the legacy folder and want we can use instead to train our own models?

And now which one is better to use, and how to use? And also why it is better than train.py.

This information is very necessary for us to know the difference between them in order to take advantage of the new version.
",6,,[],2019-01-27 14:20:48,open,,,['models: research'],2019-04-07 08:04:29
228,tensorflow/models,models,6099,NikolasMarkou,Fixed misleading logging messaging in object_detection,Logging reports that extra conv layers are added but that lies on a condition afterwards creating a wrong impression to anyone reading the logging,3,,[],2019-01-26 11:05:55,open,,,['cla: yes'],2019-01-26 11:08:34
229,tensorflow/models,models,6097,PiotrDabrowskey,Remove Python compiled files from deep_contextual_bandits,"Remove byte-compiled files from the repo.

By the way: `__pycache__/` is already added to `.gitignore`, so those `*.pyc` files must have been force added by mistake.",3,,[],2019-01-25 17:47:55,open,,,['cla: yes'],2019-01-25 17:50:24
230,tensorflow/models,models,6096,mrazekv,Imagenet downloader - update of data builder,"Imagenet downloader did not worked with Python 3. Following updates were implemented to the original code

JPEG file opening
-------
Without `rb` flag the UTF-8 error was raised

Index shuffling
------
In Python3 is not possible to shuffle `range`, must be converted to `list`

Bytes features
-------
Te features such as _colorspace_ are encoded as string. But the framework excepts them as bytes list.",0,,[],2019-01-25 12:38:26,open,,,['cla: yes'],2019-01-25 12:39:14
231,tensorflow/models,models,6095,yesmung,Define a variable named base_name in Prerequisites step,"Currently, a variable named base_name is not defined.
So, occur error while extract from downloaded model file.

Signed-off-by: MyungSung Kwak <yesmung@gmail.com>",2,,[],2019-01-25 05:30:19,open,,,['cla: yes'],2019-02-08 00:50:35
232,tensorflow/models,models,6094,wassimea,Issue evaluating mobilenet trained from slim/nets,"### System information
- **What is the top-level directory of the model you are using**: slim/nets
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**:  9.0 / 7.4.1
- **GPU model and memory**: GTX 980
- **Exact command to reproduce**:

### Describe the problem
I have trained mobilenet v2 from tensorflow slim/nets for a binary classification problem.

During training, I get decent validation accuracies (96%), but when I try to restore a saved checkpoint to run it on the same validation set, mobilenet returns the exact same probabilities regardless of the input image.

I believe it might be a problem with the model scope.

I use this exact code to restore other models that I have defined and they work without any problems. The problem is only when I try to restore a model trained from research/slim/nets where I get the exact same probabilities regardless of the input image.

### Source code / logs
I use this code to restore the trained mobilenet model

class Detector1(object):
def __init__(self, index,config):#, data_size, batch_size, model_path):

    model_path = '/logs/'
    graph = tf.Graph()
    with graph.as_default():
        self.image_op = tf.placeholder(tf.float32, shape=[1, 48, 48, 3], name='input_image')
        with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope(is_training=False)):
              self.cls_prob, endpoints = mobilenet_v2.mobilenet(self.image_op)
        self.sess = tf.Session(
            config=tf.ConfigProto(allow_soft_placement=True, gpu_options=tf.GPUOptions(allow_growth=True)))
        saver = tf.train.Saver()
        saver.restore(self.sess, '/logs/-600')
def predict(self,image):
    cls_prob = self.sess.run([self.cls_prob], feed_dict={self.image_op: image})
    return cls_prob


",0,,[],2019-01-24 18:42:31,open,,,['models: research'],2019-01-29 20:43:49
233,tensorflow/models,models,6093,lunasdejavu,Exception has occurred: tensorflow.python.framework.errors_impl.NotFoundError,"------------------------

### System information
- **What is the top-level directory of the model you are using**:C:\tf_od_api\mask_rcnn_restnet50
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 7x64
- **TensorFlow installed from (source or binary)**: binary  pip install
- **TensorFlow version (use command below)**:1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:CUDA9.0 cudnn 7.3.1
- **GPU model and memory**:nvidia GTX1060 6GB
- **Exact command to reproduce**:
python object_detection/legacy/train.py --train_dir=C:\\tf_od_api\\mask_rcnn_restnet50 --pipeline_config_path=C:\\tf_od_api\\mask_rcnn_restnet50\\mask_rcnn_resnet50_atrous_coco.config
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

I created the  TFrecords from [here](https://github.com/priya-dwivedi/Deep-Learning/blob/master/Custom_Mask_RCNN/create_pet_tf_record.py)
1 class and 1010 png images and Mask R-CNN with Resnet-50 (v1), Atrous version  model from [ here](http://download.tensorflow.org/models/object_detection/mask_rcnn_resnet50_atrous_coco_2018_01_28.tar.gz) config from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config)
I  modified the path and the tfrecord name in config and the image type,
when I used the command above, the errors showed up:


Exception has occurred: tensorflow.python.framework.errors_impl.NotFoundError
Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:  Key Conv/biases/Momentum not found in checkpoint    [[node save/RestoreV2 (defined at C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\legacy\trainer.py:377)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]  Caused by op 'save/RestoreV2', defined at:   File ""c:\Users\willy_sung\.vscode\extensions\ms-python.python-2018.12.1\pythonFiles\ptvsd_launcher.py"", line 45, in <module>     main(ptvsdArgs)   File ""c:\Users\willy_sung\.vscode\extensions\ms-python.python-2018.12.1\pythonFiles\lib\python\ptvsd\__main__.py"", line 265, in main     wait=args.wait)   File ""c:\Users\willy_sung\.vscode\extensions\ms-python.python-2018.12.1\pythonFiles\lib\python\ptvsd\__main__.py"", line 258, in handle_args     debug_main(addr, name, kind, *extra, **kwargs)   File ""c:\Users\willy_sung\.vscode\extensions\ms-python.python-2018.12.1\pythonFiles\lib\python\ptvsd\_local.py"", line 45, in debug_main     run_file(address, name, *extra, **kwargs)   File ""c:\Users\willy_sung\.vscode\extensions\ms-python.python-2018.12.1\pythonFiles\lib\python\ptvsd\_local.py"", line 79, in run_file     run(argv, addr, **kwargs)   File ""c:\Users\willy_sung\.vscode\extensions\ms-python.python-2018.12.1\pythonFiles\lib\python\ptvsd\_local.py"", line 140, in _run     _pydevd.main()   File ""c:\Users\willy_sung\.vscode\extensions\ms-python.python-2018.12.1\pythonFiles\lib\python\ptvsd\_vendored\pydevd\pydevd.py"", line 1925, in main     debugger.connect(host, port)   File ""c:\Users\willy_sung\.vscode\extensions\ms-python.python-2018.12.1\pythonFiles\lib\python\ptvsd\_vendored\pydevd\pydevd.py"", line 1283, in run     return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)   File ""c:\Users\willy_sung\.vscode\extensions\ms-python.python-2018.12.1\pythonFiles\lib\python\ptvsd\_vendored\pydevd\pydevd.py"", line 1290, in _exec     pydev_imports.execfile(file, globals, locals)  # execute the script   File ""c:\Users\willy_sung\.vscode\extensions\ms-python.python-2018.12.1\pythonFiles\lib\python\ptvsd\_vendored\pydevd\_pydev_imps\_pydev_execfile.py"", line 25, in execfile     exec(compile(contents+""\n"", file, 'exec'), glob, loc)   File ""c:\models\research\object_detection\legacy\train.py"", line 184, in <module>     tf.app.run()   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run     _sys.exit(main(argv))   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\util\deprecation.py"", line 306, in new_func     return func(*args, **kwargs)   File ""c:\models\research\object_detection\legacy\train.py"", line 180, in main     graph_hook_fn=graph_rewriter_fn)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\legacy\trainer.py"", line 377, in train     keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\training\saver.py"", line 1102, in __init__     self.build()   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\training\saver.py"", line 1114, in build     self._build(self._filename, build_save=True, build_restore=True)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\training\saver.py"", line 1151, in _build     build_save=build_save, build_restore=build_restore)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\training\saver.py"", line 795, in _build_internal     restore_sequentially, reshape)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\training\saver.py"", line 406, in _AddRestoreOps     restore_sequentially)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\training\saver.py"", line 862, in bulk_restore     return io_ops.restore_v2(filename_tensor, names, slices, dtypes)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1550, in restore_v2     shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper     op_def=op_def)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func     return func(*args, **kwargs)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\framework\ops.py"", line 3274, in create_op     op_def=op_def)   File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\framework\ops.py"", line 1770, in __init__     self._traceback = tf_stack.extract_stack()  NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:  Key Conv/biases/Momentum not found in checkpoint    [[node save/RestoreV2 (defined at C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\legacy\trainer.py:377)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]] 

the path of the data and model is in the image
<img width=""964"" alt=""tfodapiissue"" src=""https://user-images.githubusercontent.com/7753153/51660244-15324300-1fe8-11e9-9aee-2c04e02fd676.png"">
It seems the model is not right to the config file, 
but there is the same error when I try the maskrcnninceptionv2 model too.
can anyone help me how to solve this problem?",2,,[],2019-01-24 07:01:52,open,,,[],2019-01-25 09:42:16
234,tensorflow/models,models,6091,lgutzwil,[DeepLab] Reproducing frozen inference graph,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 9.0/cuDNN 7.0.5
- **GPU model and memory**: nVidia GeForce GTX 1080 8GB
- **Exact command to reproduce**: 
python deeplab/export_model.py --checkpoint_path=/data/tf_saved_models/deeplabv3_cityscapes_train/model.ckpt --export_path=/data/tf_saved_models/deeplabv3_cityscapes_train/frozen_inference_graph_new.pb --model_variant=""xception_65"" --atrous_rates=6 --atrous_rates=12 --atrous_rates=18 --output_stride=16 --decoder_output_stride=4 --train_crop_size=769 --train_crop_size=769 --num_classes=19
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Starting from the xception65 deeplabv3_cityscapes_train model checkpoint in the DeepLab model zoo, using the export_model.py script in the DeepLab folder, I can re-export the trained checkpoint as above to a new frozen inference graph, which I can load into a session and query successfully. However I notice that the graph I generate is not exactly identical to the frozen_inference_graph.pb supplied for that model: the graph I generate differs in size by about 0.1 MB. Is it possible for me to exactly reproduce the frozen inference graph supplied in the model zoo, using the export code in that folder?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-01-23 19:47:16,open,,"NamedUser(login=""ymodak"")",['models: research'],2019-01-29 20:46:37
235,tensorflow/models,models,6090,stephenjfox,Fix typos in morph_net/framework/README.md,"In reading the documentation, I found a reference to the [original name](https://arxiv.org/abs/1711.06798v1) of the paper and a typo regarding rank of a Tensor.

These are minor changes, but will prevent any confusion of future readers",0,,[],2019-01-23 18:35:45,open,,,['cla: yes'],2019-01-23 20:52:25
236,tensorflow/models,models,6089,filaPro,"[object detection] python3 compatibility, dict.iteritems","### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

Looks like there are still issues with python3 compatibility.
In this lines

https://github.com/tensorflow/models/blob/master/research/object_detection/metrics/coco_evaluation.py#L571

https://github.com/tensorflow/models/blob/master/research/object_detection/metrics/coco_evaluation.py#L585

`iteritems` method probably should be replaced with `items`.

Now I'm getting something like

```
UnknownError (see above for traceback): AttributeError: 'dict' object has no attribute 'iteritems'
Traceback (most recent call last):

  File ""/root/diagram-understanding/src/diagram_segmentation/venv/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 206, in __call__
    ret = func(*args)

  File ""/root/models/research/object_detection/metrics/coco_evaluation.py"", line 720, in first_value_func
    self._metrics = self.evaluate()

  File ""/root/models/research/object_detection/metrics/coco_evaluation.py"", line 570, in evaluate
    for image_id, shape in self._image_id_to_mask_shape_map.

AttributeError: 'dict' object has no attribute 'iteritems'
```
when trying to evaluate mask r-cnn.

",0,,[],2019-01-23 09:44:33,open,,,['models: research'],2019-01-29 20:41:25
237,tensorflow/models,models,6088,hongym7,model zoo ckpt file is not match,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I use ""ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config"" file.
in config file set below

  fine_tune_checkpoint: ""xxxxxxxxxx/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/model.ckpt""

and then run model_main.py

console :
WARNING:root:Variable [MobilenetV1/Conv2d_0/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_0/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_0/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_0/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_0/weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_10_depthwise/BatchNorm/beta] is not available in checkpoint

so print tensor name of ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/model.ckp

...
tensor_name:  FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/beta
...
tensor_name:  FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma
...

not match between model file and ckpt 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,[],2019-01-23 09:03:59,open,,,['type:support'],2019-02-06 22:16:47
238,tensorflow/models,models,6087,AakashKumarNain,Training stops after first few iterations,"### System information
- **What is the top-level directory of the model you are using**: Object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 10
- **GPU model and memory**: M60
- **Exact command to reproduce**:


### Describe the problem
I am trying to train the `ssd_mobilenet_v1_fpn_coco`. I followed the same steps as provided in the instructions. Training runs fine for few thousand iterations and then it freezes. Apart from the TFRecords path, I haven't changed anything in the config file. ",1,,[],2019-01-23 06:38:10,open,,,['models: research'],2019-02-15 07:38:33
239,tensorflow/models,models,6086,shahram95,Issues getting the intermediate layers output of ResNet50,"How can I visualize the architecture of my model, and tap-out intermediate layers from a pre-trained model, say ResNet-50 or MobileNetV1. ",1,,[],2019-01-23 05:01:55,open,,,"['stat:awaiting response', 'type:support']",2019-02-20 21:59:49
240,tensorflow/models,models,6085,wjnodejs,No matching distribution found for syntaxnet-with-tensorflow,"pip install 'ipython<6.0' protobuf numpy scipy jupyter syntaxnet-with-tensorflow

Requirement already satisfied: protobuf in /opt/software/anaconda3/lib/python3.6/site-packages (3.6.1)
Requirement already satisfied: numpy in /opt/software/anaconda3/lib/python3.6/site-packages (1.14.5)
Requirement already satisfied: scipy in /opt/software/anaconda3/lib/python3.6/site-packages (1.1.0)
Requirement already satisfied: jupyter in /opt/software/anaconda3/lib/python3.6/site-packages (1.0.0)
**Collecting syntaxnet-with-tensorflow
  Could not find a version that satisfies the requirement syntaxnet-with-tensorflow (from versions: )
No matching distribution found for syntaxnet-with-tensorflow**

",3,,[],2019-01-23 03:18:42,open,,,['models: research'],2019-02-20 21:58:43
241,tensorflow/models,models,6084,AliceDinh,Unable to deploy ssd-fpn model to mobile,"### System information
- **What is the top-level directory of the model you are using**:  object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Window 10
- **TensorFlow installed from (source or binary)**: binary, installed from pip: pip install tensorflow
- **TensorFlow version (use command below)**:  1.12.0
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None
### Describe the problem
I am not able to deploy ssd-fpn model to mobile.
I used TensorFlow Android Camera Demo, which is [TF Detect](https://github.com/tensorflow/tensorflow/tree/r1.9/tensorflow/examples/android), the ssd-fpn model is download from [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) but nothing detected.
",10,,[],2019-01-23 02:22:41,open,,,['models: research'],2019-02-21 02:50:31
242,tensorflow/models,models,6083,usamamuneeb,Update preprocessing_factory.py,"Quoting the README in `models/research/slim`: **ResNet V2 models use Inception pre-processing and input image size of 299 (use `--preprocessing_name inception --eval_image_size 299` when using `eval_image_classifier.py`).**

The file `preprocessing_factory.py` should then link `resnet_v2_*` models to `inception_preprocessing` and not `vgg_preprocessing`.",4,,[],2019-01-22 22:38:49,open,,,['cla: yes'],2019-03-30 08:28:49
243,tensorflow/models,models,6082,biswas62,TypeError: non_max_suppression() got an unexpected keyword argument 'score_threshold',"(tensorflow) C:\tensorflow\models\research\object_detection>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config
WARNING:tensorflow:From C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py:124: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
WARNING:tensorflow:From C:\tensorflow\models\research\object_detection\legacy\trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 136, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\tensorflow\models\research\object_detection\legacy\trainer.py"", line 291, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""C:\tensorflow\models\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""C:\tensorflow\models\research\object_detection\legacy\trainer.py"", line 204, in _create_losses
    prediction_dict = detection_model.predict(images, true_image_shapes)
  File ""C:\tensorflow\models\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 688, in predict
    self._anchors.get(), image_shape, true_image_shapes))
  File ""C:\tensorflow\models\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 775, in _predict_second_stage
    anchors, image_shape_2d, true_image_shapes)
  File ""C:\tensorflow\models\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 1285, in _postprocess_rpn
    clip_window=clip_window)
  File ""C:\tensorflow\models\research\object_detection\core\post_processing.py"", line 478, in batch_multiclass_non_max_suppression
    parallel_iterations=parallel_iterations)
  File ""C:\tensorflow\models\research\object_detection\utils\shape_utils.py"", line 228, in static_or_dynamic_map_fn
    return tf.map_fn(fn, elems, dtype, parallel_iterations, back_prop)
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\functional_ops.py"", line 409, in map_fn
    swap_memory=swap_memory)
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2934, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2720, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2662, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\functional_ops.py"", line 399, in compute
    packed_fn_values = fn(packed_values)
  File ""C:\tensorflow\models\research\object_detection\core\post_processing.py"", line 452, in _single_image_nms_fn
    additional_fields=per_image_additional_fields)
  File ""C:\tensorflow\models\research\object_detection\core\post_processing.py"", line 170, in multiclass_non_max_suppression
    score_threshold=score_thresh)
TypeError: non_max_suppression() got an unexpected keyword argument 'score_threshold'",4,,[],2019-01-22 18:20:51,open,,,['models: research'],2019-03-02 01:21:28
244,tensorflow/models,models,6080,bleedingfight,compile freeze_graph error,"I want compile  freeze_graph to freeze my trained model(`bazel build tensorflow/python/tools:freeze_graph`).but some error like this:
```
ERROR: /home/amax/tensorflow/tensorflow/core/kernels/BUILD:4355:1: C++ compilation of rule '//tensorflow/c
ore/kernels:scatter_op_gpu' failed (Exit 1)                                                               
clang: error: cannot find libdevice for sm_61. Provide path to different CUDA installation via --cuda-path
, or pass -nocudalib to build without linking with libdevice.                                             
Target //tensorflow/python/tools:freeze_graph failed to build                                             
Use --verbose_failures to see the command lines of failed build steps.                                    

```
my envs:
- Linux amax-Super-Server 4.13.0-32-generic #35~16.04.1-Ubuntu SMP Thu Jan 25 10:13:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux                                                                                        
- cuda-10.0
- cudnn7.3
- tensorflow1.12 from source
- GTX1080 8Gx2
- bazel 0.19.2
Not only freeze_graph can't be compile,but also any other c++ tool.What's wrong with my tensorflow?
",1,,[],2019-01-22 12:32:11,open,,,['stat:awaiting response'],2019-01-23 14:53:06
245,tensorflow/models,models,6079,trc2019,ValueError: 'images' must have either 3 or 4 dimensions,"I'm trying to export the model that i have customized as the flowing:
research/object_detection$ python3 export_inference_graph.py     --input_type image_tensor     --pipeline_config_path training/ssd_mobilenet_v1_coco.config     --trained_checkpoint_prefix training/model.ckpt-81100.data-00000-of-00001     --output_directory mm

I have got the following error : 
Traceback (most recent call last):
  File ""export_inference_graph.py"", line 150, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""export_inference_graph.py"", line 146, in main
    write_inference_graph=FLAGS.write_inference_graph)
  File ""/home/trc/Desktop/Object_Detector/models/research/object_detection/exporter.py"", line 455, in export_inference_graph
    write_inference_graph=write_inference_graph)
  File ""/home/trc/Desktop/Object_Detector/models/research/object_detection/exporter.py"", line 359, in _export_inference_graph
    graph_hook_fn=graph_hook_fn)
  File ""/home/trc/Desktop/Object_Detector/models/research/object_detection/exporter.py"", line 327, in _build_detection_graph
    output_collection_name=output_collection_name)
  File ""/home/trc/Desktop/Object_Detector/models/research/object_detection/exporter.py"", line 302, in _get_outputs_from_inputs
    preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
  File ""/home/trc/Desktop/Object_Detector/models/research/object_detection/meta_architectures/ssd_meta_arch.py"", line 477, in preprocess
    dtype=[tf.float32, tf.int32])
  File ""/home/trc/Desktop/Object_Detector/models/research/object_detection/utils/shape_utils.py"", line 237, in static_or_dynamic_map_fn
    outputs = [fn(arg) for arg in tf.unstack(elems)]
  File ""/home/trc/Desktop/Object_Detector/models/research/object_detection/utils/shape_utils.py"", line 237, in <listcomp>
    outputs = [fn(arg) for arg in tf.unstack(elems)]
  File ""/home/trc/Desktop/Object_Detector/models/research/object_detection/core/preprocessor.py"", line 2396, in resize_image
    align_corners=align_corners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py"", line 1005, in resize_images
    raise ValueError('\'images\' must have either 3 or 4 dimensions.')
ValueError: 'images' must have either 3 or 4 dimensions.

I have trid change the location of export_inference_graph.py but it is not help!!

Any help will be appreciated it",1,,[],2019-01-22 11:08:13,open,,,['stat:awaiting response'],2019-01-23 00:16:25
246,tensorflow/models,models,6078,ahundt,autoaugment pil cutout no-op fixed.,"@BarretZoph autoaugment's PIL cutout appears to never modify any values! Was this bug present when the original search algorithm was run? 

Notes on applying this fix: 
 - without re-running the autoaugment search may have a **negative impact** impact on results, since it trained on a no-op
- re-running the search and updating the [good_policies()](https://github.com/tensorflow/models/blob/903194c51d4798df25334dd5ccecc2604974efd9/research/autoaugment/policies.py) could have a  **positive impact**, since the search would run with the correct behavior. 👍
- runtime performance of a double for loop for image pixels is probably quite slow.



Here are the two key lines:
https://github.com/tensorflow/models/blob/903194c51d4798df25334dd5ccecc2604974efd9/research/autoaugment/augmentation_transforms.py#L405
https://github.com/tensorflow/models/blob/903194c51d4798df25334dd5ccecc2604974efd9/research/autoaugment/augmentation_transforms.py#L406

I tested similar code on the command line, see how no pixels are visited?
```python
$ coord = (15, 31)
$ print([i for i in range(coord[0], coord[0])])
[]
```
",0,,[],2019-01-21 21:53:38,open,,,['cla: yes'],2019-01-21 21:56:17
247,tensorflow/models,models,6076,sidharths,Training from pre-trained weights (Faster-rcnn/Mask-rcnn),"### System information
- **What is the top-level directory of the model you are using**:
onject detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04.5
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.12
- **Bazel version (if compiling from source)**:

- **CUDA/cuDNN version**:

- **GPU model and memory**: Tesla V100 16160MB

- **Exact command to reproduce**: Followed steps from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md to train locally

I have posted the same question on stackoverflow too with a bounty of 50 points. ([https://stackoverflow.com/questions/54162602/training-from-pre-trained-weights-faster-rcnn-mask-rcnn-object-detection-api](https://stackoverflow.com/questions/54162602/training-from-pre-trained-weights-faster-rcnn-mask-rcnn-object-detection-api))
I am training Mask-RCNN object detection API from pre-trained weights from the [object detection zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) with the coco 2017 dataset.

I have tried both running locally or using the legacy training script. As soon as the training starts, I am never able to see it resume from the mAP mentioned in the [Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for MaskRCNN or FasterRCNN.

What I want to do is resume training from the mAP of pretrained weights (mentioned 33 for mask_rcnn_resnet101_atrous_coco here at the [Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)), _is this possible_?

For instance the very first eval result for masks after 1200 iterations or so during training is given below (using script object_detection/model_main.py from [running locally](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md) approach).

 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.010
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.010
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.003
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.013

",3,,[],2019-01-21 13:42:40,open,,,['models: research'],2019-02-04 08:00:58
248,tensorflow/models,models,6075,leekinpo,update,,1,,[],2019-01-21 11:11:22,open,,,['cla: no'],2019-01-21 11:11:34
249,tensorflow/models,models,6073,lighTQ,train error when I added the FLAGS hparams_overrides ,"When I added the --hparams_overrides parameter when I used the tensorflow object detection API training, I reported the following error.

ERROR info:

Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 66, in main
    hparams=model_hparams.create_hparams(FLAGS.hparams_overrides),
  File ""/software/models/research/object_detection/model_hparams.py"", line 43, in create_hparams
    hparams = hparams.parse(hparams_overrides)
  File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/training/python/training/hparam.py"", line 542, in parse
    values_map = parse_values(values, type_map)
  File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/training/python/training/hparam.py"", line 263, in parse_values
    raise ValueError('Unknown hyperparameter type for %s' % name)
ValueError: Unknown hyperparameter type for label_map_path

executed scripts is:
 python object_detection/model_main.py     --pipeline_config_path=${PIPELINE_CONFIG_PATH}     --model_dir=${MODEL_DIR}     --num_train_steps=${NUM_TRAIN_STEPS}     --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES     --hparams_overrides='label_map_path=/home/pycharm_pj382/Datasets/tmp/label_map.pbtxt'
",2,,[],2019-01-21 08:20:30,open,,,['models: research'],2019-02-01 21:59:44
250,tensorflow/models,models,6070,yadinDean,absl.flags._exceptions.IllegalFlagValueError: flag --trained_checkpoint_prefix=None: Flag --trained_checkpoint_prefix must be specified.,"Im run
python3 export_inference_graph.py \ --input_type image_tensor \  --pipeline_config_path training/logo_v1.config \  ----trained_checkpoint_prefix training/model.ckpt-250000 \  --output_directory logo_deteksi

here is the log
Traceback (most recent call last):
  File ""export_inference_graph.py"", line 143, in <module>
    tf.app.run()
  File ""/home/dukundeath/.virtualenvs/cv/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 119, in run
    argv = flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)
  File ""/home/dukundeath/.virtualenvs/cv/lib/python3.5/site-packages/tensorflow/python/platform/flags.py"", line 112, in __call__
    return self.__dict__['__wrapped'].__call__(*args, **kwargs)
  File ""/home/dukundeath/.virtualenvs/cv/lib/python3.5/site-packages/absl/flags/_flagvalues.py"", line 635, in __call__
    self._assert_all_validators()
  File ""/home/dukundeath/.virtualenvs/cv/lib/python3.5/site-packages/absl/flags/_flagvalues.py"", line 509, in _assert_all_validators
    self._assert_validators(all_validators)
  File ""/home/dukundeath/.virtualenvs/cv/lib/python3.5/site-packages/absl/flags/_flagvalues.py"", line 530, in _assert_validators
    raise _exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))
absl.flags._exceptions.IllegalFlagValueError: flag --trained_checkpoint_prefix=None: Flag --trained_checkpoint_prefix must be specified.
how to fix it?
tks!
",2,,[],2019-01-20 11:15:07,open,,,['stat:awaiting response'],2019-03-05 05:15:46
251,tensorflow/models,models,6069,iliasmansouri,Evaluating on training data with the Object Detection API,"### System information
- **What is the top-level directory of the model you are using**:
research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes as in modifying the pipeline.config for my needs
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS with Cloud ML
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
TF1.9
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
gcloud ml-engine jobs submit training object_detection_`date +%m_%d_%Y_%H_%M_%S` \
    --runtime-version 1.9 \
    --job-dir=gs://${MODEL_DIR} \
    --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \
    --module-name object_detection.model_main \
    --region us-central1 \
    --config ${PATH_TO_LOCAL_YAML_FILE} \
    -- \
    --model_dir=gs://${MODEL_DIR} \
    --pipeline_config_path=gs://${PIPELINE_CONFIG_PATH}

### Describe the problem
Training on GCP and using the model for inference works perfectly. 
However, I'd like to visualise metrics, such as mAP, AR, classification loss and localization loss, on TensorBoard.

### Source code / logs
In ""model_main.py"", changing eval_on_train_data=True does not add the training data on TensorBoard
```
train_spec, eval_specs = 
model_lib.create_train_and_eval_specs(
            train_input_fn,
            eval_input_fns,
            eval_on_train_input_fn,
            predict_input_fn,
            train_steps,
            eval_on_train_data=True)
```

Furthermore, when looking at the snippet below I understand by providing a checkpoint_dir that it is possible to execute `model_lib.continuous_eval(estimator, FLAGS.checkpoint_dir, input_fn,                               train_steps, name)`
```
if FLAGS.checkpoint_dir:
    if FLAGS.eval_training_data:
      name = 'training_data'
      input_fn = eval_on_train_input_fn
    else:
      name = 'validation_data'
      # The first eval input will be evaluated.
      input_fn = eval_input_fns[0]
    if FLAGS.run_once:
      estimator.evaluate(input_fn,
                         num_eval_steps=None,
                         checkpoint_path=tf.train.latest_checkpoint(
                             FLAGS.checkpoint_dir))
    else:
      model_lib.continuous_eval(estimator, FLAGS.checkpoint_dir, input_fn,
                                train_steps, name)
  else:
    train_spec, eval_specs = model_lib.create_train_and_eval_specs(
        train_input_fn,
        eval_input_fns,
        eval_on_train_input_fn,
        predict_input_fn,
        train_steps,
        eval_on_train_data=False)
```
But the model_dir contains the checkpoints, so I am confused.
So, the question is: Can we evaluate on the training data with as goal to visualize and compare the metrics on Tensorboard?",1,,[],2019-01-18 18:22:10,open,,,['models: research'],2019-03-09 02:50:22
252,tensorflow/models,models,6068,ctessum,research/object_detection/dockerfiles/android: Dockerfile doesn't build,"### System information
- **What is the top-level directory of the model you are using**:
research/object_detection/dockerfiles/android
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04.1 LTS
- **TensorFlow installed from (source or binary)**:
N/A
- **TensorFlow version (use command below)**:
N/A
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
docker version
docker build --tag detect-tf .

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I am attempting to build a docker image using the Dockerfile in this directory. It fails to build.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

``` bash
$  lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 18.04.1 LTS
Release:	18.04
Codename:	bionic
$ docker version
Client:
 Version:      18.03.1-ce
 API version:  1.37
 Go version:   go1.9.5
 Git commit:   9ee9f40
 Built:        Thu Apr 26 07:17:38 2018
 OS/Arch:      linux/amd64
 Experimental: false
 Orchestrator: swarm

Server:
 Engine:
  Version:      18.03.1-ce
  API version:  1.37 (minimum version 1.12)
  Go version:   go1.9.5
  Git commit:   9ee9f40
  Built:        Thu Apr 26 07:15:45 2018
  OS/Arch:      linux/amd64
  Experimental: false
$ docker build --tag detect-tf .
Sending build context to Docker daemon  27.14kB
Step 1/21 : FROM tensorflow/tensorflow:nightly-devel
 ---> 5c0cfcf0facb
Step 2/21 : RUN git clone --depth 1 https://github.com/tensorflow/models.git &&     mv models /tensorflow/models
 ---> Using cache
 ---> 5bec1e897792
Step 3/21 : RUN export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"" &&     echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&     apt-get update -y && apt-get install google-cloud-sdk -y
 ---> Running in b36a2d4db863
deb http://packages.cloud.google.com/apt cloud-sdk-bionic main
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key output should not be parsed (stdout is not a terminal)
100  1326  100  1326    0     0   8392      0 --:--:-- --:--:-- --:--:--  8392
gpg: failed to start agent '/usr/bin/gpg-agent': No such file or directory
gpg: can't connect to the agent: No such file or directory
gpg: failed to start agent '/usr/bin/gpg-agent': No such file or directory
gpg: can't connect to the agent: No such file or directory
The command '/bin/sh -c export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"" &&     echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&     apt-get update -y && apt-get install google-cloud-sdk -y' returned a non-zero code: 2
```
",0,,[],2019-01-18 16:48:00,open,,,['models: research'],2019-02-01 22:01:00
253,tensorflow/models,models,6067,andrewsueg,[DeepLab]Training on a new dataset,"I would like to segment only one kind of object in a given image, but it has many of them. What is the right way to do it? 

I'm using labelme to segment them and convert to VOC. In my first attempt I segmented only a few of them and let the rest as background, because there are so many. When using the script to convert, I got _background_ as index 0 and object as index 1. In the segmentation_dataset.py I set num_classes=2 and ignore_label=0. Am I right to do that?

When training I get stuck in the same loss, and after training a get really poor results using vis.py.

Should I create a new class2 and give a few examples of background? Basically there are object and ground, but I can't segment all of the objects manually, some of them will stay in the background.",10,,[],2019-01-18 00:15:19,open,,,['stat:awaiting response'],2019-03-01 12:21:23
254,tensorflow/models,models,6064,TyrionZK,Can tensorflow object detection api  run on tensorflow 1.2 or lower version?,My other projects are running on tensorflow 1.2. But I just see the Object Detection API needs tensorflow 1.9 or higher version. So i want to know is it possibile to run the Object Detection API on tensorflow 1.2 or lower version? And how?,3,,[],2019-01-17 11:54:28,open,,,"['models: research', 'stat:awaiting response']",2019-02-01 22:01:29
255,tensorflow/models,models,6061,D-K-E,object_detection [Feature Request]: mirror repository possible ?,"What is the top-level directory of the model you are using
object_detection
Have I written custom code
No. I have not touched the interiors of the library, besides going through the tutorial they have provided
OS Platform and Distribution
N/A
TensorFlow installed from
N/A
TensorFlow version
N/A
Bazel version
N/A
CUDA/cuDNN version
N/A
GPU model and memory
N/A
Exact command to reproduce
N/A

Would it be possible for maintainers of the object_detection api to host their code in a mirror repository as well ?
It is difficult to use it as a dependency from this repository with all the other research going on in other apis as well. It also increases the size of the parent repository a lot. ",1,,[],2019-01-17 02:44:00,open,,,"['models: research', 'stat:awaiting response']",2019-02-01 22:01:42
256,tensorflow/models,models,6059,abhishek-niranjan,Fix issue #5976: Added data_preprocess script to train transformer model on own data,"#### Fix for issue no. #5976: Hassle free own data input to Transformer

Created a script to pre-process own data to train the transformer model on. Updated the README with the instructions on how to use it. ",2,,[],2019-01-16 18:55:42,open,,,['cla: yes'],2019-01-29 22:41:55
257,tensorflow/models,models,6057,sulashi,Struct2depth: The pretrained mdel,"System information
What is the top-level directory of the model you are using: models/research/struct2depth
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.11.0
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A
Describe the problem
1. The imagenet pretrained model on imagenet is needed for reapting the results of the paper, but I don't know which pretrained model the author used. Can you release the pretrained model or give the link to the pretrained model you used? Thank you!

This is the issue from: (it has been closed) 
https://github.com/tensorflow/models/issues/5888#issue-389212579
",0,,[],2019-01-16 03:17:04,open,,,['models: research'],2019-02-01 22:03:04
258,tensorflow/models,models,6056,vidit2011998,"My system is windows 10, python 3.6 and I install protoc-3.4.0-win32 not protoc-3.5.0-win32, **protoc-3.5 for windows has bug!!!**","My system is windows 10, python 3.6 and I install protoc-3.4.0-win32 not protoc-3.5.0-win32, **protoc-3.5 for windows has bug!!!**
After running protoc object_detection/protos/*.proto --python_out=.
just change 
`from utils import label_map_util`  
to 
`from object_detection.utils import label_map_util`

_Originally posted by @duancaohui in https://github.com/tensorflow/models/issues/1990#issuecomment-374976015_",1,,[],2019-01-15 20:22:50,open,,,['models: research'],2019-02-01 22:03:25
259,tensorflow/models,models,6055,vidit2011998,"After cloning tensorflow models, running `protoc object_detection/protos/*.proto --python_out=.`  and exporting PYTHONPATH  it still failed but changing the import helped:","After cloning tensorflow models, running `protoc object_detection/protos/*.proto --python_out=.`  and exporting PYTHONPATH  it still failed but changing the import helped: 
**```from object_detection.utils import label_map_util```**

_Originally posted by @mordka in https://github.com/tensorflow/models/issues/1990#issuecomment-351445424_",1,,[],2019-01-15 19:40:20,open,,,['models: research'],2019-02-01 22:04:34
260,tensorflow/models,models,6054,sumatege,I got a problem while train my on model on colab,"I'm trying to train with my own model step by step. Until training step, i got a problem. (I use ssd_mobilenet_v1_coco_11_06_2017)

python train.py \
        --logtostderr \
        --train_dir=bean_trained1 \
        --pipeline_config_path=ssd_mobilenet_v1_coco.config 

**This is error message,**

WARNING:tensorflow:From /content/drive/TestObjectdetection/drive/TestObjectdetection/object_detection/trainer.py:176: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:242: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:94: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From /content/drive/TestObjectdetection/drive/TestObjectdetection/object_detection/core/preprocessor.py:1922: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From /content/drive/TestObjectdetection/drive/TestObjectdetection/object_detection/core/batcher.py:96: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).
Traceback (most recent call last):
  File ""train.py"", line 198, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 194, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/content/drive/TestObjectdetection/drive/TestObjectdetection/object_detection/trainer.py"", line 192, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""/content/drive/TestObjectdetection/slim/deployment/model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""/content/drive/TestObjectdetection/drive/TestObjectdetection/object_detection/trainer.py"", line 133, in _create_losses
    losses_dict = detection_model.loss(prediction_dict)
  File ""/content/drive/TestObjectdetection/drive/TestObjectdetection/object_detection/meta_architectures/ssd_meta_arch.py"", line 431, in loss
    location_losses, cls_losses, prediction_dict, match_list)
  File ""/content/drive/TestObjectdetection/drive/TestObjectdetection/object_detection/meta_architectures/ssd_meta_arch.py"", line 565, in _apply_hard_mining
    match_list=match_list)
  File ""/content/drive/TestObjectdetection/drive/TestObjectdetection/object_detection/core/losses.py"", line 445, in __call__
    location_losses = tf.unstack(location_losses)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py"", line 1023, in unstack
    (axis, -value_shape.ndims, value_shape.ndims))
ValueError: axis = 0 not in [0, 0)",1,,[],2019-01-15 19:27:21,open,,,['models: research'],2019-02-01 22:04:57
261,tensorflow/models,models,6053,clstl,Resnet: DataLossError corrupted record,"### System information
- **What is the top-level directory of the model you are using**: `official/resnet`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: using stock
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**: Linux 4.4.0-141-generic #167-Ubuntu SMP x86_64 x86_64 x86_64 GNU/Linux
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9
- **GPU model and memory**: 3x GeForce GTX 1080 Ti 11GB
- **Exact command to reproduce**: `python -m official.resnet.imagenet_main --num_gpus 3 --data_dir=/dataset/final --model_dir=./resnet50_baseline/ --batch_size=33`

script output:
https://gist.github.com/clstl/033feafa092026aea8f321898225da85

### Describe the problem
I am trying to train the official Resnet50 from your repository, following the official documentation that you have there. The dataset in question is ImageNet 2012 that was successfully preprocessed from the tensorflow TPU repo. At the first few epoch, tensorflow throws a DataError with a corrupted record. To rootcause the issue, I found a script that goes through the damaged tfrecords and removes them, later accomodating the model code in order to avoid deleted records. I run the script at least twice for sanity check. After this routine, the model runs smoothly; however, if I am to start the training again (from scratch), tensorflow starts throwing the same exception, but now with a different damaged tfrecord that was perfectly fine before.

I am using nvidia-docker v2 to run tensorflow. The dataset is a volume that is bound to an HDD on a server. The training is done via MirroredStrategy using 3 1080 Tis. 

### Source code / logs
`TFRecords: DataLossError (see above for traceback): corrupted record at xxx`
Unfortunately, I cannot retrieve the full stack trace, as I reexporting the data. Nothing looks suspicious there, but I will post it once the reexporting is done.
",0,,[],2019-01-15 16:24:21,open,,,['models: official'],2019-02-01 22:05:11
262,tensorflow/models,models,6051,davidz123,> The FC layer should not be frozen. I guess this is what you are doing?,"Hi, I'm recently working on this feature and had some problem with it. Could you open-source your training code?",1,,[],2019-01-15 05:54:51,open,,,['stat:awaiting response'],2019-01-16 00:19:34
263,tensorflow/models,models,6050,chengsu99,cannot git clone,"Hello.
When I run data util, it shows that
Command 'git clone https://github.com/brendenlake/omniglot.git' returned non-zero exit status 1.",1,,[],2019-01-15 05:00:24,open,,,['cla: no'],2019-01-15 15:22:23
264,tensorflow/models,models,6048,YanZhiyuan0918,How to add ASPP and Decoder structure for mobilenet in this project?,"Does anyone know how to add ASPP and Decoder structure for mobilenet in this project? Please give some guidance, thanks in advance!",1,,[],2019-01-15 02:43:42,open,,,['stat:awaiting response'],2019-01-16 00:19:30
265,tensorflow/models,models,6047,kingstarcraft,How to run model_main.py with multi-GPU?,"### System information
- **What is the top-level directory of the model you are using**:Object Detection API
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows 10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0/7.1
- **GPU model and memory**:GTX1080TI 11GB
- **Exact command to reproduce**: 


### Describe the problem
how to run model_main.py with multi-GPU?
",2,,[],2019-01-15 01:51:29,open,,,['models: research'],2019-02-19 08:12:32
266,tensorflow/models,models,6046,HrithikMittal,Update Readme.md,This will help to know more about Tensorflow.,4,,[],2019-01-14 22:18:34,open,,"NamedUser(login=""HrithikMittal"")",['cla: yes'],2019-01-14 22:24:00
267,tensorflow/models,models,6044,bryanlimy,fix 'dict_keys' object does not support indexing,fix `'dict_keys' object does not support indexing` at `feature_map_generators.py`,3,,[],2019-01-14 15:40:18,open,,,['cla: yes'],2019-01-14 15:46:43
268,tensorflow/models,models,6043,kesinger,Struct2depth: NaNs in inf_norm,"System information

What is the top-level directory of the model you are using: models/research/struct2depth
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No (beyond fixing #5857 )
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: libcudart.so.9.0.176
GPU model and memory: 1080Ti 11Gb
Exact command to reproduce: python train.py 
--logtostderr 
--checkpoint_dir=ckpt 
--data_dir=/opt/bigdata/kitti_processed 
--architecture=resnet 
--imagenet_norm=true 
--joint_encoder=false --summary_freq=1
Describe the problem

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I am running into errors of the form ""LossTensor is inf or nan : Tensor had NaN values"". This appears to be from the inf_loss contribution:
losses = tf.map_fn(
get_losses, object_masks, dtype=tf.float32)
self.inf_loss += tf.reduce_mean(losses)

In particular, losses will occasionally contain a nan value, as shown here:
[array([ 0.92789054, 5.1210318 , 4.9717507 , 5.6173134 , 60.209503 ,
33.976864 , 0. ], dtype=float32), array([ 0.92788905, 4.3226395 , 4.3226404 , 4.3226247 , 60.209442 ,
33.97685 , nan], dtype=float32), array([ 0.9278935, 3.9967735, 3.8008199, 4.100856 , 60.20948 ,
29.60478 , 29.60479 ], dtype=float32)]

(As I understand it, these are the losses for j=0,1,2 in the loop ""for j in range(self.seq_length)"" )

Note that the nan in the j=1 row is paired with a 0 in the j=0 row, this appears to be consistent behavior.

The only thing I've found that triggers this is if the object mask is particularly small, but there doesn't seem to be an absolute threshold.",9,,[],2019-01-14 15:40:13,open,,,['models: research'],2019-03-19 02:57:45
269,tensorflow/models,models,6042,nistarlwc,How to change model structure of Mobilnet-SSD in Tensorflow Object Detection API?,"


I am doing to train a [Mobilnet-SSD](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config) for detect small objects.
So I want to add some box_predictor_layer in the front of the layer of network, like add a box_predictor_layer to connect 5/6th Convolution layer.

How to do it?
I read all .proto files, find [ssd.proto](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/ssd.proto), and [ssd_anchor_generator.proto](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/ssd_anchor_generator.proto), [box_predictor.proto](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/box_predictor.proto).
But I don't know how to change them, and graph of tensorboard is so disorderly, there are not any tutoria.
",2,,[],2019-01-14 14:41:10,open,,,"['models: research', 'stat:awaiting response']",2019-02-25 02:11:47
270,tensorflow/models,models,6041,ConnieTong, Could not find a version that satisfies the requirement syntaxnet-with-tensorflow (from versions: ),"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: syntaxnet
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.12.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:  10.0 
- **GPU model and memory**:
- **Exact command to reproduce**: pip install syntaxnet-with-tensorflow

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2,,[],2019-01-14 07:51:54,open,,,['models: research'],2019-03-18 19:41:51
271,tensorflow/models,models,6040,IvyGongoogle,can you mark the correct tensorflow version for the provided pre-trained models?,"hello, when I use 
`tf.train.import_meta_graph(""/xxx.meta"")` to 
load the pre-trained models in [here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models), I always get the error:

`  File ""/yyy/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 259, in import_graph_def
    raise ValueError('No op named %s in defined operations.' % node.op)
ValueError: No op named IteratorV2 in defined operations.`

similar problem mentioned in [here](https://github.com/tensorflow/models/issues/1564), but not get solved. So can you mark the correct tf version for the provided pre-trained models to not waster our time.",1,,[],2019-01-14 04:18:13,open,,,['stat:awaiting response'],2019-01-15 00:26:06
272,tensorflow/models,models,6039,hongym7,How to inference in tensorflow by using multi thread?,"### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
TF 1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
(GTX 1080 Ti 12GB ) * 2
- **Exact command to reproduce**:

### Describe the problem

How to ""object detection inference"" in tensorflow by using multi thread?
(example_file_pipeline.py / tensorflow_detection.py)  

I think that tf can't multi thread in inference time.
right?

one sample gpu job elapsed time : 2s
one sample gpu job elapsed time (in 3 thread) : 4s, 4s, 4s



### Source code / logs

import threading, requests, time
import numpy as np
import tensorflow as tf
import datetime

n = 3

A = np.random.rand(8000, 8000).astype('float32')
B = np.random.rand(8000, 8000).astype('float32')

def odThread():
    t1_1 = datetime.datetime.now()

    c1 = []

    def matpow(M, n):
        if n < 1:  # Abstract cases where n < 1
            return M
        else:
            return tf.matmul(M, matpow(M, n - 1))

    '''
    Single GPU computing
    '''

    a = tf.placeholder(tf.float32, [8000, 8000])
    b = tf.placeholder(tf.float32, [8000, 8000])

    c1.append(matpow(a, n))
    c1.append(matpow(b, n))

    with tf.device('/cpu:0'):
        sum = tf.add_n(c1)  # Addition of all elements in c1, i.e. A^n + B^n

    t1_1 = datetime.datetime.now()


    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.33)

    config = tf.ConfigProto(log_device_placement=True, gpu_options=gpu_options)

    with tf.Session(config=config) as sess:
        sess.run(sum, {a: A, b: B})

    t2_1 = datetime.datetime.now()

    print(""Single GPU computation time : "" + str(t2_1 - t1_1))
    print(""Total Elapsed time : "" + str(t2_1 - t1_1))


threads = [threading.Thread(target=odThread, args=()) for i in range(3)]
for t in threads:
    time.sleep(0.01)
    t.start()

",0,,[],2019-01-14 00:26:44,open,,,['models: research'],2019-02-01 21:55:21
273,tensorflow/models,models,6037,shubhank008,[DeepLab] Change Image/Input Size from 513 to 2k+,"System information
What is the top-level directory of the model you are using:
DeepLab v3

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10

TensorFlow installed from (source or binary):
With pip3 binary installation

TensorFlow version (use command below):
1.7 CPU only

Bazel version (if compiling from source):

CUDA/cuDNN version:
Not used

GPU model and memory:
Not used

Exact command to reproduce:

This is a followup from https://github.com/tensorflow/models/issues/4064 issue which was closed even though the proposed or though-of solution did not work.  
I am in same boat where I want to increase the size of image supported from 513px to atleast 2k or ideally 4k.  

I tried re-exporting the frozen graph set to 2052 crop_size  
    `python export_model.py --checkpoint_path deeplabv3_pascal_trainval/model.ckpt --export_path deeplabv3_pascal_trainval/1024/frozen_inference_graph.pb --crop_size 1024 --crop_size 1024 --atrous_rates 12 --atrous_rates 24 --atrous_rates 36 --model_variant=""xception_65""`

and changed the input_size in seg.py from 513 to 2052. The script executes without any errors but the exported file is blank/black and of 12KB instead.",3,,[],2019-01-12 15:30:40,open,,,['models: research'],2019-02-01 21:56:07
274,tensorflow/models,models,6033,1453042287,[TF-Slim] the link of the resnet v1 50's checkpoint may be wrong,"i can download all the checkpoints but the resnet v1 50's, maybe there is something wrong with the link",3,,[],2019-01-12 05:53:26,open,,,['models: research'],2019-02-01 21:56:18
275,tensorflow/models,models,6029,Work-jk-l,Deeplab Only for people segmentation,"We now have a project that needs the accurate edge of every people.But the edge of dataset(1020*1920) results are bad.
![image](https://user-images.githubusercontent.com/41975845/51036196-5ccbce80-15e7-11e9-8a9a-e8471d3a4b74.png)

I wonder whether the dataset(1080*1920) is big for the input size(513*513),So it will lose some information in the image.  And I wonder if I set the train datsets size smaller(near to 513*513),the result will be better? 

",2,,[],2019-01-11 13:28:50,open,,,['models: research'],2019-02-01 21:56:31
276,tensorflow/models,models,6028,qingchunlizhi,"model_fn at 0x7f2150274378>) includes params argument, but params are not passed to Estimator.","WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f2150274378>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
How can I fix these warning?

Have I written custom code： No
OS Platform and Distribution： Ubuntu18.04+1070ti GPU
TensorFlow installed from： pip
TensorFlow version：1.12.0
CUDA/cuDNN version：cuda9.0 cudnnV7.2
GPU model and memory ：8G
Exact command to reproduce：
python model_main.py \
    --pipeline_config_path=training/ssdlite_mobilenet_v2_coco.config \
    --model_dir=training \
    --num_train_steps=60000 \
    --num_eval_steps=20 \
    --alsologtostderr


",2,,[],2019-01-11 12:37:33,open,,,"['models: research', 'stat:awaiting response']",2019-02-01 21:56:42
277,tensorflow/models,models,6027,Freephi,Lack of documentation lstm_object_detection,"### System information
- **What is the top-level directory of the model you are using**: lstm_object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: 
- **Exact command to reproduce**:

### Describe the problem
This is **NOT** a bug issue, but a request for more documentation/information about the training process of the lstm_object_detection. I tired to find help on Stackoverflow, but now I hope for a response of the authors. It would help a lot if there would be a documentation on how to train or in particular on how to prepare the training data. A sample create_record.py file like the ones in object_detection would be great.



",4,"NamedUser(login=""dreamdragon"")","[NamedUser(login=""dreamdragon"")]",2019-01-11 11:27:38,open,,,"['stat:awaiting tensorflower', 'type:docs']",2019-02-22 19:21:32
278,tensorflow/models,models,6011,MirkoArnold1,Documentation outdated on evaluation metrics,"### System information
- **What is the top-level directory of the model you are using**:
N/A - Documentation issue
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
`object_detection/g3doc/evaluation_protocols.md`
mentions evaluation metrics that are not supported anymore.

### Source code / logs
`get_eval_metric_ops_for_evaluators` in `object_detection/eval_util.py` throws an error if any other metric than 'coco_detection_metrics' or 'coco_mask_metrics' are used.",1,,[],2019-01-08 09:57:46,open,,,['stat:awaiting response'],2019-01-10 10:40:52
279,tensorflow/models,models,6009,lintian06,Create a unit test for Keras application models in TF 2.0,"Create a unit test for Keras application models in TF 2.0. (Named models_v2_test.py)

Minor: move the shared model definition in models.py.

Tested in tf-nightly-2.0-preview. 
$ python official/keras_application_models/models_v2_test.py",1,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2019-01-08 08:55:12,open,,,['cla: yes'],2019-01-18 05:56:56
280,tensorflow/models,models,6007,npeirson,"Loss skyrockets randomly, cause and effect unkno","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. Really, they do. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow, per-say.

I left your pre-written message here so you could be as annoyed with it as I am.  

------------------------

### System information
- **What is the top-level directory of the model you are using**: `models/research/`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9 nightly
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: nightly
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: nightly
- **GPU model and memory**: Tesla K80
- **Exact command to reproduce**: follow your own ""pet detector"" tutorial. <-- seriously, just do this

You can collect some of this information using our environment capture script:

### Describe the problem
Loss leaps occasionally, scheduled learning rates shouldn't permit such dramatic changes. Not sure that they're actually causing a problem, but want to get your insight on what's going on here. Happens every ~10,000 steps, started after ~25k steps. Causes?

### Source code / logs
```
 master-replica-0 loss = 0.049381822, step = 62104 (56.439 sec) I  master-replica-0
 master-replica-0 loss = 0.20843819, step = 62204 (57.136 sec) I  master-replica-0
 master-replica-0 loss = 0.08859253, step = 62304 (56.564 sec) I  master-replica-0
 master-replica-0 loss = 0.14603062, step = 62404 (56.244 sec) I  master-replica-0
 master-replica-0 loss = 5676524600000000.0, step = 62504 (56.958 sec) I  master-replica-0
 master-replica-0 loss = 0.083275326, step = 62604 (56.345 sec) master-replica-0 
```",0,,[],2019-01-07 19:03:57,open,,,['models: research'],2019-02-20 22:01:04
281,tensorflow/models,models,6006,Poonamjo,model.ckpt is generated or we should have this file before running train.py,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2,"NamedUser(login=""jvishnuvardhan"")","[NamedUser(login=""jvishnuvardhan"")]",2019-01-07 09:53:57,open,,,"['stat:awaiting response', 'type:support']",2019-01-11 00:24:20
282,tensorflow/models,models,6005,adeebakausar," We have created the  xml file by matlab code which is give below ,output of xml file also mention below . both  manually created xml,file and imglabelling file are same , but we cant see the output boundry box on image on jupyter noteboook","*MATLAB CODE:*

image_name = textread('D:\MATLAB Code\Tanserflow_API\imagename.txt','%s ');
path1_name = textread('D:\MATLAB Code\Tanserflow_API\path_name.txt','%s ');
load('annotate221.mat');
Resultados='C:\Users\adeebakausar\Desktop\images\'
for i=1:30
docNode = com.mathworks.xml.XMLUtils.createDocument('annotation');
annotation = docNode.getDocumentElement;
img = docNode.createElement('folder');
img.appendChild(docNode.createTextNode('images'));
annotation.appendChild(img)

file_name = docNode.createElement('filename');
file_name.appendChild(docNode.createTextNode(sprintf('%s',image_name{i,1})));
annotation.appendChild(file_name);

path_name = docNode.createElement('path');
path_name.appendChild(docNode.createTextNode(sprintf('%s',path1_name{i,1})));
annotation.appendChild(path_name);

source_name = docNode.createElement('source');
annotation.appendChild(source_name);
database_name = docNode.createElement('database');
database_name.appendChild(docNode.createTextNode('Unknown'));
source_name.appendChild(database_name);

size_name = docNode.createElement('size');
annotation.appendChild(size_name)

width_name = docNode.createElement('width');
width_name.appendChild(docNode.createTextNode(sprintf('%d',640)));
size_name.appendChild(width_name);

height_name = docNode.createElement('height');
height_name.appendChild(docNode.createTextNode(sprintf('%d',364)));
size_name.appendChild(height_name);

depth_name=docNode.createElement('depth');
depth_name.appendChild(docNode.createTextNode(sprintf('%d',3)));
size_name.appendChild(depth_name);

segmented_name = docNode.createElement('segmented');
segmented_name.appendChild(docNode.createTextNode(sprintf('%d',0)));
annotation.appendChild(segmented_name);

[r,c]=size(annotate{i,1});
 a=annotate{i,1};
for kk=1:r
 product = docNode.createElement('object'); 
 annotation.appendChild(product)
 curr_node = docNode.createElement('name');
  curr_node.appendChild(docNode.createTextNode('MotorBike'));
 product.appendChild(curr_node);
 curr_node = docNode.createElement('pose');
  curr_node.appendChild(docNode.createTextNode('Unspecified'));
 product.appendChild(curr_node);
  curr_node = docNode.createElement('truncated');
  curr_node.appendChild(docNode.createTextNode(sprintf('%d',0)));
 product.appendChild(curr_node);
 curr_node = docNode.createElement('difficult')
 curr_node.appendChild(docNode.createTextNode(sprintf('%d',0)));
  product.appendChild(curr_node);
  curr_node1 = docNode.createElement('bndbox');
  product.appendChild(curr_node1);
 nod3 = docNode.createElement('xmin'); 
 nod3.appendChild(docNode.createTextNode(sprintf('%d',a(kk,1))));
 curr_node1.appendChild(nod3)
nod3 = docNode.createElement('ymin'); 
nod3.appendChild(docNode.createTextNode(sprintf('%d',a(kk,2))));
curr_node1.appendChild( nod3)
nod3 = docNode.createElement('xmax'); 
nod3.appendChild(docNode.createTextNode(sprintf('%d',a(kk,3))));
curr_node1.appendChild( nod3)
nod3 = docNode.createElement('ymax'); 
nod3.appendChild(docNode.createTextNode(sprintf('%d',a(kk,4))));
curr_node1.appendChild( nod3)
end  
a=sprintf('%s',image_name{i,1})
a=regexp(a,'j','split')
s1=a{1,1}
s2='xml'
s = strcat(s1,s2)
xmlwrite([Resultados,s],docNode);
type([Resultados,s])
end


 **Manually created file:**

<annotation>
   <folder>images</folder>
   <filename>000001.jpg</filename>
   <path>C:\Users\adeebakausar\Desktop\object-detection\images\000001.jpg</path>
   <source>
      <database>Unknown</database>
   </source>
   <size>
      <width>640</width>
      <height>364</height>
      <depth>3</depth>
   </size>
   <segmented>0</segmented>
   <object>
      <name>MotorBike</name>
      <pose>Unspecified</pose>
      <truncated>0</truncated>
      <difficult>0</difficult>
      <bndbox>
         <xmin>565</xmin>
         <ymin>209</ymin>
         <xmax>639</xmax>
         <ymax>303</ymax>
      </bndbox>
   </object>
   <object>
      <name>MotorBike</name>
      <pose>Unspecified</pose>
      <truncated>0</truncated>
      <difficult>0</difficult>
      <bndbox>
         <xmin>484</xmin>
         <ymin>181</ymin>
         <xmax>530</xmax>
         <ymax>208</ymax>
      </bndbox>
   </object>
   <object>
      <name>MotorBike</name>
      <pose>Unspecified</pose>
      <truncated>0</truncated>
      <difficult>0</difficult>
      <bndbox>
         <xmin>408</xmin>
         <ymin>158</ymin>
         <xmax>458</xmax>
         <ymax>219</ymax>
      </bndbox>
   </object>
   <object>
      <name>MotorBike</name>
      <pose>Unspecified</pose>
      <truncated>0</truncated>
      <difficult>0</difficult>
      <bndbox>
         <xmin>308</xmin>
         <ymin>165</ymin>
         <xmax>345</xmax>
         <ymax>240</ymax>
      </bndbox>
   </object>
   <object>
      <name>MotorBike</name>
      <pose>Unspecified</pose>
      <truncated>0</truncated>
      <difficult>0</difficult>
      <bndbox>
         <xmin>259</xmin>
         <ymin>141</ymin>
         <xmax>286</xmax>
         <ymax>193</ymax>
      </bndbox>
   </object>
   <object>
      <name>MotorBike</name>
      <pose>Unspecified</pose>
      <truncated>0</truncated>
      <difficult>0</difficult>
      <bndbox>
         <xmin>349</xmin>
         <ymin>134</ymin>
         <xmax>371</xmax>
         <ymax>160</ymax>
      </bndbox>
   </object>
   <object>
      <name>MotorBike</name>
      <pose>Unspecified</pose>
      <truncated>0</truncated>
      <difficult>0</difficult>
      <bndbox>
         <xmin>197</xmin>
         <ymin>111</ymin>
         <xmax>221</xmax>
         <ymax>143</ymax>
      </bndbox>
   </object>
   <object>
      <name>MotorBike</name>
      <pose>Unspecified</pose>
      <truncated>0</truncated>
      <difficult>0</difficult>
      <bndbox>
         <xmin>317</xmin>
         <ymin>127</ymin>
         <xmax>341</xmax>
         <ymax>152</ymax>
      </bndbox>
   </object>
</annotation>

**Imglabelling:**

 <folder>images</folder>
   <filename>000001.jpg</filename>
   <path>C:\Users\adeebakausar\Desktop\object-detection\images\000001.jpg</path>
   <source>
      <database>Unknown</database>
   </source>
   <size>
      <width>640</width>
      <height>364</height>
      <depth>3</depth>
   </size>
   <segmented>0</segmented>

<annotation>
	<object>
		<name>MotorBike</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>409</xmin>
			<ymin>157</ymin>
			<xmax>453</xmax>
			<ymax>218</ymax>
		</bndbox>
	</object>
	<object>
		<name>MotorBike</name>
		<pose>Unspecified</pose>
		<truncated>1</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>567</xmin>
			<ymin>203</ymin>
			<xmax>640</xmax>
			<ymax>299</ymax>
		</bndbox>
	</object>
	<object>
		<name>MotorBike</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>494</xmin>
			<ymin>177</ymin>
			<xmax>522</xmax>
			<ymax>207</ymax>
		</bndbox>
	</object>
	<object>
		<name>MotorBike</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>305</xmin>
			<ymin>165</ymin>
			<xmax>348</xmax>
			<ymax>240</ymax>
		</bndbox>
	</object>
	<object>
		<name>MotorBike</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>260</xmin>
			<ymin>141</ymin>
			<xmax>288</xmax>
			<ymax>196</ymax>
		</bndbox>
	</object>
	<object>
		<name>MotorBike</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>353</xmin>
			<ymin>138</ymin>
			<xmax>376</xmax>
			<ymax>168</ymax>
		</bndbox>
	</object>
	<object>
		<name>MotorBike</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>315</xmin>
			<ymin>125</ymin>
			<xmax>340</xmax>
			<ymax>155</ymax>
		</bndbox>
	</object>
</annotation>




   



",1,,[],2019-01-07 09:47:48,open,,,['stat:awaiting response'],2019-01-08 00:17:15
283,tensorflow/models,models,6004,GzuPark,Modified a link address,,0,,[],2019-01-07 07:55:24,open,,,['cla: yes'],2019-01-07 07:55:26
284,tensorflow/models,models,6003,msarfrazcss,How to capture unique object from webcam?,"Hey guys,

I have trained my model for single class, i want to capture(Count) the same object from webcam once, means if same object detecting in frame then it should capture as one until it leave the frame.

In my case it captures the same object multiple times(means it saves the multiple images for same object) but i want single image for single object.

Anyone Can tell me please how can we do this in Darknet/Darkflow using python.

Thanks in advance.",1,,[],2019-01-07 07:54:51,open,,,"['stat:awaiting response', 'type:support']",2019-01-09 18:25:13
285,tensorflow/models,models,6002,Poonamjo,"accoon_dataset_master\data\object_detection.pbtxt : The filename, directory name, or volume label syntax is incorrect. ; Unknown error","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2,,[],2019-01-07 07:50:44,open,,,[],2019-03-17 05:02:37
286,tensorflow/models,models,6000,hongym7,Inference Time between Tesla K80 and GTX 1080 in Tensorflow,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Hi all. I have a question. I run object detection application in tensorflow But K80 inference time is higher than gtx 1080. (by example_file_pipeline.py)

Common : CUDA 9.0, Python 3.5, Tensorflow 1.12
A : Tesla K80 1, Windows Server 2012 R2
B : GTX 1080 1, Windows 7
C : GTX 1080 Ti 1, Ubuntu 16.04

A inference time is 3.2s ~ 3.4s
B inference time is 1.6s ~ 1.8s
C inference time is 1.0s ~ 1.1s

Why ?





### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,[],2019-01-07 02:00:02,open,,,['type:support'],2019-01-11 19:06:49
287,tensorflow/models,models,5999,emirismail,google.protobuf.text_format.ParseError: 101:3 : Couldn't parse string: 'utf-8' codec can't decode byte 0xe9 in position 30: invalid continuation byte,"i tried to train the  Faster R-CNN + ResNet-101 architecture on the LISA Traffic Signs
dataset  but i got this error,

google.protobuf.text_format.ParseError: 101:3 : Couldn't parse string: 'utf-8' codec can't decode byte 0xe9 in position 30: invalid continuation byte

line command : python /home/emir/models/research/object_detection/train.py --logtostderr --pipeline_config_path=rcnn/lisa/experiments/training/faster_rcnn_lisa.config --train_dir=rcnn/lisa/experiments/training
",3,,[],2019-01-06 17:03:20,open,,,['models: research'],2019-02-20 22:01:16
288,tensorflow/models,models,5997,RamishaRaniK,"google.protobuf.text_format.ParseError: 127:1 : Message type ""object_detection.protos.InputReader"" has no field named ""eval_config"".","I am getting this error,


'C:\tensorflow1\models\research\object_detection>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config
WARNING:tensorflow:From C:\Anaconda3\envs\Helmet\lib\site-packages\tensorflow\python\platform\app.py:124: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Anaconda3\envs\Helmet\lib\site-packages\tensorflow\python\platform\app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""C:\Anaconda3\envs\Helmet\lib\site-packages\tensorflow\python\util\deprecation.py"", line 136, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 93, in main
    FLAGS.pipeline_config_path)
  File ""C:\tensorflow1\models\research\object_detection\utils\config_util.py"", line 96, in get_configs_from_pipeline_file
    text_format.Merge(proto_str, pipeline_config)
  File ""C:\Anaconda3\envs\Helmet\lib\site-packages\google\protobuf\text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\Anaconda3\envs\Helmet\lib\site-packages\google\protobuf\text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\Anaconda3\envs\Helmet\lib\site-packages\google\protobuf\text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\Anaconda3\envs\Helmet\lib\site-packages\google\protobuf\text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\Anaconda3\envs\Helmet\lib\site-packages\google\protobuf\text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Anaconda3\envs\Helmet\lib\site-packages\google\protobuf\text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""C:\Anaconda3\envs\Helmet\lib\site-packages\google\protobuf\text_format.py"", line 730, in _MergeField
    (message_descriptor.full_name, name))
google.protobuf.text_format.ParseError: 127:1 : Message type ""object_detection.protos.InputReader"" has no field named ""eval_config"".'

",4,,[],2019-01-05 20:36:47,open,,,['models: research'],2019-02-20 22:01:26
289,tensorflow/models,models,5996,pSolT,ssd_mobilenetv2_oidv4 download link not working,"Re - rising the issue as some random dude self-assigned it and blocked it for two weeks:

The download link of the ssd_mobilenetv2_oidv4 model (from the Tensorflow detection model zoo page) trained on open images is not working.

Original issue:
https://github.com/tensorflow/models/issues/5920

Page where the link can be found: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md

Link: http://download.tensorflow.org/models/object_detection/ssd_mobilenetv2_oidv4_2018_10_30.tar.gz

Error message:
```
<Error>
<Code>AccessDenied</Code>
<Message>Access denied.</Message>
<Details>
Anonymous caller does not have storage.objects.get access to download.tensorflow.org/models/object_detection/ssd_mobilenetv2_oidv4_2018_10_30.tar.gz.
</Details>
</Error>
````",1,,[],2019-01-05 16:48:45,open,,,['stat:awaiting response'],2019-01-06 12:13:51
290,tensorflow/models,models,5995,boluodehaiwangzi,This project is so unfriendly to beginner.,"Firstly, I show my best honor to all the contributor of this project. Your works help me a lot. But, I have some problem, I found this project is unfriendly for beginner. In my case, I just want to try to understand the wide&deep model. But before I can get to understand how the model run, I have to get myself accustom to the flag, logging, core, hooks_helper, it took me a lot of time. Tensorflow is not easy to learn, and these make it harder. There must be some ways to simplify it. ",1,,[],2019-01-05 15:49:58,open,,,['stat:awaiting response'],2019-01-06 12:13:46
291,tensorflow/models,models,5989,zhinengshidai,ERROR:root:config_name: lmap_Msc.clip5.sbpd_d_r2r+bench_test,"when run the command:   sh scripts/script_test_pretrained_models.sh
occur the below problem: 
ERROR:root:config_name: lmap_Msc.clip5.sbpd_d_r2r+bench_test
ERROR:root:arch_str: lmap_Msc
ERROR:root:navtask_str: sbpd_d_r2r
ERROR:root:solver_str: clip5
ERROR:root:mode_str: bench_test
ERROR:root:solver_vars: adam_eps: aeps1en8
...
is someone know how to solve it? thanks
",2,,[],2019-01-04 03:00:15,open,,,[],2019-01-06 00:13:13
292,tensorflow/models,models,5986,WeiyiLi,GPUs are not fully occupied when reproduce the training of mobilenet_v2,"What is the top-level directory of the model you are using: N/A
Have I written custom code: No
OS Platform and Distribution: ubuntu 16
TensorFlow installed from: pip3
TensorFlow version: 1.12.0
Bazel version
CUDA/cuDNN version: 9.0.176
GPU model and memory: Eight GPUs, each is Tesla V100. 32502MiB


Hi, I would like to reproduce training mobilenet_v2 and followed the training command in 

https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet

I also used 8 gpu. The command I finally use is 

`python3 train_image_classifier.py --model_name=""mobilenet_v2"" --learning_rate=0.36 --preprocessing_name=""inception_v2"" --label_smoothing=0.1 --moving_average_decay=0.9999 --batch_size=96 --num_clones=8 --learning_rate_decay_factor=0.98 --num_epochs_per_decay=0.3125 --dataset_dir=""/home/dataset_dir/train/"" --train_dir=""/home/ckpt2/""
`
But when I checked my gpu util, it shows 
![image](https://user-images.githubusercontent.com/6508944/50656646-c04f5f80-0f48-11e9-9453-4fc2557f5eb4.png)

All the parameters I used is the same as the example but as you can see most of the gpu util are 0 or very low. How can I fully take use of the GPUs to speed up the training?  

Thanks for help!",1,,[],2019-01-03 18:55:14,open,,,['stat:awaiting response'],2019-01-04 17:13:19
293,tensorflow/models,models,5985,DecaK,Shortened code optimization,,1,,[],2019-01-03 18:45:36,open,,,['cla: no'],2019-01-03 18:52:58
294,tensorflow/models,models,5984,Venka97,Input 2 images at once,"
### System information
- **What is the top-level directory of the model you are using**: /home/models/research/deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 9.0 / 7.1.4 
- **GPU model and memory**: gtx 1080ti
- **Exact command to reproduce**: NA


### Describe the problem
How do I input two images at once into the model?  Eg. one iteration of input would take (image1, image2, label )  as input in place of (image, label). My use case is change detection between the two images. How can I modify the network to do the same?
",2,"NamedUser(login=""jvishnuvardhan"")","[NamedUser(login=""jvishnuvardhan"")]",2019-01-03 12:28:37,open,,,['type:support'],2019-01-15 12:19:23
295,tensorflow/models,models,5982,netanel-s,[Object Detection][Feature Request] Allow more than one eval_spec,"- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 11GB
- **Exact command to reproduce**: Evaluating on more than one dataset/metrics_set while training 

### Describe the problem
In [training and evaluation](https://github.com/tensorflow/models/blob/master/research/object_detection/model_main.py#L105) case of `model_main` you restrict to a single evaluation spec. 
It would be very appreciated if could support more than one evaluation spec.
This would mean another evaluation sets (e.g. validation sets of several datasets), with corresponding protocols (metrics_set).

Thanks in advance.",1,,[],2019-01-03 09:14:37,open,,,['models: research'],2019-02-20 22:01:48
296,tensorflow/models,models,5979,xxllp,object detection predict error ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
can't show the image 

### Source code / logs

QObject::moveToThread: Current thread (0x7f3a44ba2800) is not the object's thread (0x7f3a4dd7a900).
Cannot move to target thread (0x7f3a44ba2800)

QPixmap: Must construct a QApplication before a QPaintDevice
Aborted (core dumped)

",1,"NamedUser(login=""jvishnuvardhan"")","[NamedUser(login=""jvishnuvardhan"")]",2019-01-02 04:23:08,open,,,"['models: research', 'stat:awaiting response', 'type:bug/performance']",2019-02-06 22:18:48
297,tensorflow/models,models,5977,1icas,Object Detection | can not print loss in command line when training,"tensorflow version: 1.10.0
des: when i use the object detection program to train the coco dataset. I find that there isn't print any loss information in the command line. Only print the ap information in the command line for val dataset. Is this a problem? ",2,"NamedUser(login=""jvishnuvardhan"")","[NamedUser(login=""jvishnuvardhan"")]",2019-01-02 03:33:33,open,,,"['stat:awaiting response', 'type:support']",2019-01-11 21:15:59
298,tensorflow/models,models,5975,godsme,"Why does transformer share same embedding weights within encoder, decoder and logits?","After reading through the transformer code, one thing I don't understand is that, since the model is for translation task from one language to another, the vocabulary of source and target language is different, but the implementation use the same embedding input between encoder and decoder, even the projection of output.  did I misunderstand anything?",1,,[],2018-12-31 08:49:24,open,,,[],2019-01-03 06:46:15
299,tensorflow/models,models,5974,Henruiz,Having trouble when trying to train my images. Weird Error. ,"###System information
Home/models/research/object_detection
I have configured all the necessary files up until this point. 
Using a VM on a Macbook Pro for Ubunut 18.04 LTS
Tensorflow was installed from git source

### Describe the problem
I am trying to run [ python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config ] So i can train my images. 

I downloaded the train.py file and copied it into the directory. 

### Source code / logs
This is the error I get 
![image](https://user-images.githubusercontent.com/28213504/50556443-cde1bc80-0c9e-11e9-813f-c2de2cf0d814.png)

",2,,[],2018-12-31 07:54:35,open,,,['stat:awaiting response'],2019-01-07 08:32:01
300,tensorflow/models,models,5971,tjhgit,Simplifying Faster RCNN and SSD to one class and one anchor box,"Thanks a lot for sharing the object detection code to everyone! It really works well in combination with ML engine in google cloud.

I have an object detection task with a single class and a fixed size object. Therefore I would like to simplify the Faster R CNN network to this problem. 

For Faster R CNN I did the following:
1. wrote a custom pipeline.config:
```
first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [1.0]
        aspect_ratios: [1.0, 2.0]
        height: 64 
        width: 256 
        height_stride: 16 
        width_stride: 16 
      }
    }
....
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 15
        max_total_detections: 30
      }
```
I also tried to reduce `first_stage_max_proposals` to 30 however this fails. Somewhere else in the network architecture, I need to change something accordingly, but where?
 
2. As I understand I do not need the classifier (that is the second stage at all). How can I get rid of it? 

For SSD I tried the following. 
1. Changed mainly the following in the config:
```
      ssd_anchor_generator {
        num_layers: 1
        scales: 0.1
        aspect_ratios: 3.0
        height_stride: 32
        width_stride: 64
      }
```
However I get the error 
> ValueError: Number of feature maps is expected to equal the length of `num_anchors_per_location`.

Since the generated feature maps are : [(20, 20), (10, 10), (5, 5), (3, 3), (2, 2), (1, 1)]
How can I only generate one single feature map, to be compatible with the above `num_layers=1` definition. 
Where in the code are the feature maps defined?

Generally it would help to better understand how to design a new network architecture, what parts need to be adapted, etc... Is there somewhere information about this?

Thanks for your help !
",2,,[],2018-12-29 09:06:01,open,,,[],2019-01-04 12:19:59
301,tensorflow/models,models,5970,sugadevtop,ImportError: No module named quantize.python - Training on GoogleCloud ML,"### System information
- **What is the top-level directory of the model you are using**: faster_rcnn_resnet101_coco_11_06_2017
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: set PATH_TO_CONFIGURE to setup the config file
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.14.2
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: No
- **CUDA/cuDNN version**: No
- **GPU model and memory**: No
- **Exact command to reproduce**: 

> gcloud ml-engine jobs submit training `whoami`_object_detection_`date +%s` \
    --job-dir=${YOUR_GCS_BUCKET}/train \
    --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,dist/pycocotools-2.0.tar.gz \
    --module-name object_detection.model_main \

### Describe the problem
I'm trying to follow these steps on [https://cloud.google.com/blog/products/gcp/training-an-object-detector-using-cloud-machine-learning-engine](url) to train a object detector using Google Cloud ML

Everything works fine until it says `ImportError: No module named quantize.python`

### Source code / logs
> ERROR	2018-12-29 10:28:11 +0700	master-replica-0		Traceback (most recent call last):
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		  File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		    ""__main__"", fname, loader, pkg_name)
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		    exec code in run_globals
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		  File ""/root/.local/lib/python2.7/site-packages/object_detection/model_main.py"", line 26, in <module>
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		    from object_detection import model_lib
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		  File ""/root/.local/lib/python2.7/site-packages/object_detection/model_lib.py"", line 28, in <module>
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		    from object_detection import exporter as exporter_lib
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		  File ""/root/.local/lib/python2.7/site-packages/object_detection/exporter.py"", line 20, in <module>
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		    from tensorflow.contrib.quantize.python import graph_matcher
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		ImportError: No module named quantize.python
ERROR	2018-12-29 10:28:11 +0700	master-replica-0		Command '['python', '-m', u'object_detection.model_main', '--job-dir', u'gs://street-detection-ml/train']' returned non-zero exit status 1
INFO	2018-12-29 10:28:11 +0700	master-replica-0		Module completed; cleaning up.
INFO	2018-12-29 10:28:11 +0700	master-replica-0		Clean up finished.
ERROR	2018-12-29 10:28:25 +0700	service		The replica master 0 exited with a non-zero status of 1. 
ERROR	2018-12-29 10:28:25 +0700	service		Traceback (most recent call last):
ERROR	2018-12-29 10:28:25 +0700	service		  File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
ERROR	2018-12-29 10:28:25 +0700	service		    ""__main__"", fname, loader, pkg_name)
ERROR	2018-12-29 10:28:25 +0700	service		  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
ERROR	2018-12-29 10:28:25 +0700	service		    exec code in run_globals
ERROR	2018-12-29 10:28:25 +0700	service		  File ""/root/.local/lib/python2.7/site-packages/object_detection/model_main.py"", line 26, in <module>
ERROR	2018-12-29 10:28:25 +0700	service		    from object_detection import model_lib
ERROR	2018-12-29 10:28:25 +0700	service		  File ""/root/.local/lib/python2.7/site-packages/object_detection/model_lib.py"", line 28, in <module>
ERROR	2018-12-29 10:28:25 +0700	service		    from object_detection import exporter as exporter_lib
ERROR	2018-12-29 10:28:25 +0700	service		  File ""/root/.local/lib/python2.7/site-packages/object_detection/exporter.py"", line 20, in <module>
ERROR	2018-12-29 10:28:25 +0700	service		    from tensorflow.contrib.quantize.python import graph_matcher
ERROR	2018-12-29 10:28:25 +0700	service		ImportError: No module named quantize.python
ERROR	2018-12-29 10:28:25 +0700	service		
ERROR	2018-12-29 10:28:25 +0700	service		To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=1035448617633&resource=ml_job%2Fjob_id%2Fhungcao_object_detection_1546053952&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22hungcao_object_detection_1546053952%22

",3,,[],2018-12-29 03:53:33,open,,,[],2019-04-04 15:51:02
302,tensorflow/models,models,5968,llevkova,SSD MobileNet v2 quantized on COCO,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I tried to evaluate the provided ssd_mobilenet_v2 quantized model from the model zoo and obtained mAP = 8.3% on the COCO validation set using the provided pipeline.config. In the model zoo table the mAP is reported as 22%. Any ideas why this discrepancy is happening?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

",2,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-28 22:40:32,open,,,['type:docs'],2019-01-03 12:22:34
303,tensorflow/models,models,5963,laserljy,"Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)","Hi there,

I have searched and tried many solutions for this issue, but unfortunately I still have this error.

Basically, when I tried to retrain Google Inception V3 model on my machine, I had the following error messages:

```
2018-12-25 22:24:36.468376: E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 7102 (compatibility version 7100) but source was compiled with 7004 (compatiblity version 7000). If using a binary install, upgrade your CuDNN library to match. If building from sourcs, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
2018-12-25 22:24:37.034472: F tensorflow/core/kernels/conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)
Aborted (core dumped)

```

Here is the system info of my machine:
- Ubuntu 18.04
- Used pip3 to install TensorFlow
`pip3 install tensorflow_gpu==1.6.0`
- tensorflow_gpu==1.6.0
- CUDA9.0, cuDNN7.1.3
- Python 3.6.7
- 2 GPUs, both are: 1080Ti, 11G MEM
- Exact command to reproduce: `python retrain.py --images /home/laser/Desktop/AI_models/Data/MedicalImages`

What I have tried:
1. Following this issue: https://github.com/tensorflow/tensorflow/issues/16965, I added the code provided by @jart, it did not work.
2. In the error messages shown above, it told about CuDNN compatibility, so I tried install: CUDA9.1 + cuDNN7.0.3, but I still had the same errors. 
3. Then I installed the tensorflow cpu version: `pip3 install tensorflow==1.6.0` the program run correctly. But no GPUs are used, instead all CPUs are fully loaded, which took 4 hours to finished training the model. 

Really feel stressed, hope someone could help. Many thanks.

",2,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-27 04:37:39,open,,,['type:support'],2018-12-28 23:08:24
304,tensorflow/models,models,5962,alimaan2935,Key Conv/biases/Momentum not found in checkpoint when trying to retrain faster_rcnn_inception_resnet_v2_atrous_lowproposals_oidv2,"
### System information
- **What is the top-level directory of the model you are using**:
/models/research/object_detection

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
1.12.0 CPU

- **Exact command to reproduce**:
`python /mnt/dev_files/models/research/object_detection/model_main.py \
    --pipeline_config_path=models/models/model/pipeline.config \
    --model_dir=models/models/model \
    --num_train_steps=8000200 \
    --sample_1_of_n_eval_examples=2 \
    --alsologtostderr`


### Describe the problem
I am tryinf to retrain ""faster_rcnn_inception_resnet_v2_atrous_lowproposals_oidv2"" model.
I downloaded the model from [here](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid_2018_01_28.tar.gz).
changed the config from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid.config) with custom number of classes and correct paths to tfrecords and downloaded model files.
Also, i added `from_detection_checkpoint: true` in config file to suppress the warnings about some nodes not found in checkpoint file.
When i run the above command to train the model, i get the error `Key Conv/biases/Momentum not found in checkpoint`.
I have tested with a bunch of different dowsnloaded models for coco and oid as well and the issue is the same.
However, when i repeat the same procedure with ssd models, the training starts. But i don't need ssd models.
How can i fix this issue?

### Source code / logs
`WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
INFO:tensorflow:Maybe overwriting train_steps: 8000200
INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 2
INFO:tensorflow:Maybe overwriting eval_num_epochs: 1
INFO:tensorflow:Maybe overwriting load_pretrained: True
INFO:tensorflow:Ignoring config override key: load_pretrained
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False
INFO:tensorflow:Using config: {'_model_dir': 'models/models/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f85c5a6c518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f85c5a7a0d0>) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:Not using Distribute Coordinator.
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.
WARNING:tensorflow:num_readers has been reduced to 10 to match input file shards.
WARNING:tensorflow:From /mnt/dev_files/models/research/object_detection/builders/dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
WARNING:tensorflow:From /home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From /mnt/dev_files/models/research/object_detection/builders/dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From /mnt/dev_files/models/research/object_detection/predictors/heads/box_head.py:93: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From /mnt/dev_files/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2236: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
WARNING:tensorflow:From /mnt/dev_files/models/research/object_detection/core/losses.py:345: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2018-12-27 02:58:33.713707: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:tensorflow:Restoring parameters from models/models/model/model.ckpt
2018-12-27 02:58:35.536259: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Conv/biases/Momentum not found in checkpoint
Traceback (most recent call last):
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Key Conv/biases/Momentum not found in checkpoint


         [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1546, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key Conv/biases/Momentum not found in checkpoint


         [[node save/RestoreV2 (defined at /mnt/dev_files/models/research/object_detection/model_lib.py:475)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""/mnt/dev_files/models/research/object_detection/model_main.py"", line 111, in <module>
    tf.app.run()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/mnt/dev_files/models/research/object_detection/model_main.py"", line 107, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1237, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/mnt/dev_files/models/research/object_detection/model_lib.py"", line 475, in model_fn
    save_relative_paths=True)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1102, in __init__
    self.build()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1114, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1151, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 789, in _build_internal
    restore_sequentially, reshape)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps
    name=""restore_shard""))
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps
    restore_sequentially)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 862, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

NotFoundError (see above for traceback): Key Conv/biases/Momentum not found in checkpoint



         [[node save/RestoreV2 (defined at /mnt/dev_files/models/research/object_detection/model_lib.py:475)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1556, in restore
    names_to_keys = object_graph_key_mapping(save_path)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1830, in object_graph_key_mapping
    checkpointable.OBJECT_GRAPH_PROTO_KEY)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 371, in get_tensor
    status)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/mnt/dev_files/models/research/object_detection/model_main.py"", line 111, in <module>
    tf.app.run()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/mnt/dev_files/models/research/object_detection/model_main.py"", line 107, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
    saving_listeners)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1468, in _train_with_estimator_spec
    log_step_count_steps=log_step_count_steps) as mon_sess:
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session
    return self._sess_creator.create_session()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 288, in prepare_session
    config=config)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 218, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1562, in restore
    err, ""a Variable name or other graph key that is missing"")
tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key Conv/biases/Momentum not found in checkpoint

         [[node save/RestoreV2 (defined at /mnt/dev_files/models/research/object_detection/model_lib.py:475)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""/mnt/dev_files/models/research/object_detection/model_main.py"", line 111, in <module>
    tf.app.run()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/mnt/dev_files/models/research/object_detection/model_main.py"", line 107, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1237, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/mnt/dev_files/models/research/object_detection/model_lib.py"", line 475, in model_fn
    save_relative_paths=True)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1102, in __init__
    self.build()
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1114, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1151, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 789, in _build_internal
    restore_sequentially, reshape)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps
    name=""restore_shard""))
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps
    restore_sequentially)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 862, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key Conv/biases/Momentum not found in checkpoint

         [[node save/RestoreV2 (defined at /mnt/dev_files/models/research/object_detection/model_lib.py:475)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'report_uninitialized_variables_1/boolean_mask/GatherV2:0' shape=(?,) dtype=string>
If you want to mark it as used call its ""mark_used()"" method.
It was originally created here:
  File ""/mnt/dev_files/models/research/object_detection/model_main.py"", line 111, in <module>
    tf.app.run()  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))  File ""/mnt/dev_files/models/research/object_detection/model_main.py"", line 107, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    return self  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
    saving_listeners)  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1468, in _train_with_estimator_spec
    log_step_count_steps=log_step_count_steps) as mon_sess:  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__
    stop_grace_period_secs=stop_grace_period_secs)  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__
    _WrappedSession.__init__(self, self._create_session())  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1121, in _create_session
    'the job. Error: %s', e)  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session
    self.tf_sess = self._session_creator.create_session()  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session
    init_fn=self._scaffold.init_fn)  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 219, in finalize
    return self  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 268, in get_or_default
    return op  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 199, in default_ready_for_local_init_op
    variables.global_variables())  File ""/home/ali1234/anaconda3/envs/recsys/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 189, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
==================================`",3,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-27 03:14:21,open,,,['type:support'],2018-12-30 23:38:30
305,tensorflow/models,models,5960,Matskevichivan,Have an error TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int32'> to <dtype: 'float32'>,"https://stackoverflow.com/questions/53922231/have-an-error-typeerror-cannot-convert-a-list-containing-a-tensor-of-dtype-dty

I want to make my own tensorflow object detection api. Use this guide http://www.gradient-ascent.com/blog/2018/7/24/8tdkwi9iwmds0ws0e1fvhuauw8zxdt And when i want to train, i have this error. Use 3 config files,but the error remains. Please help me!

```
user@pc:~/anaconda3/lib/python3.6/site-packages/tensorflow/models$ python research/object_detection/train.py \
>     --logtostderr \
>     --train_dir=train \
>     --pipeline_config_path=ssd_mobilenet_v2_coco.config
WARNING:tensorflow:From /home/user/anaconda3/lib/python3.6/site-packages/tensorflow/models/research/object_detection/trainer.py:176: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:From /home/user/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py:242: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.
.
```
A lot of warning (7-8)

```
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From /home/user/anaconda3/lib/python3.6/site-packages/tensorflow/models/research/object_detection/core/batcher.py:96: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).
Traceback (most recent call last):
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 455, in _apply_op_helper
    as_ref=input_arg.is_ref)
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1211, in internal_convert_n_to_tensor
    ctx=ctx))
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 971, in _autopacking_conversion_function
    return _autopacking_helper(v, dtype, name or ""packed"")
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 902, in _autopacking_helper
    elem))
TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int32'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Preprocessor/stack_1:0' shape=(1, 3) dtype=int32>)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""research/object_detection/train.py"", line 198, in <module>
    tf.app.run()
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""research/object_detection/train.py"", line 194, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/models/research/object_detection/trainer.py"", line 192, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/models/research/object_detection/trainer.py"", line 124, in _create_losses
    images = tf.concat(images, 0)
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1124, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1033, in concat_v2
    ""ConcatV2"", values=values, axis=axis, name=name)
  File ""/home/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 483, in _apply_op_helper
    raise TypeError(""%s that don't all match."" % prefix)
TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [<NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>,...] that don't all match.
```


",3,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-25 21:17:19,open,,,['type:support'],2018-12-28 23:04:47
306,tensorflow/models,models,5959,UltronAI,[struct2depth] out of memory even if batch_size=1 and using two 1080Ti,"### System information
- **What is the top-level directory of the model you are using**: struct2depth
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: do some changes, but not a lot.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: cuda 9.0
- **GPU model and memory**: Nvidia 1080Ti (11GB) x 2 
- **Exact command to reproduce**: 
        python train.py \
          --checkpoint_dir $ckpy_dir \
          --data_dir $kitti_data \
          --architecture resnet \
          --imagenet_norm false \
          --batch_size 1

### Describe the problem
I want to run struct2depth training code on my own machine with two 1080ti, but failed.
I have set batch_size=1, and enabled handle_motion, but I still have no enough memory to train, which really confuses me.
How should I do to train struct2depth? Or how much gpus should I use to do it?

Besids, I always get that LossTensor is inf or nan, and find many people tell that it's related to lr and batch_size. 

#### What can I do to solve these problems? Thanks so much!
",3,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-25 11:38:51,open,,,['type:docs'],2019-01-11 20:35:19
307,tensorflow/models,models,5958,tabrisweapon,looking for possible way to train official resnet from scratch with BN unfreezed,"I've went through the official/resnet implement. It seems that freezed batch normalization is the default setting and there is no way to train resnet from scratch and update the BN variable at the same time by simply setting a flag. Programmers have to change the official code and give the loss_filter_fn a value to make the exclude_batch_norm function out of function.  
corresponding codes are in official/resnet/resnet_run_loop.resnet_model_fn.exclude_batch_norm
If I am right, wouldn't be better to have a flag responsible for this function.


### System information
- **What is the top-level directory of the model you are using**: official/resnet
- **Have I written custom code**: no
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**:binary
- **TensorFlow version**:1.13
- **Bazel version (if compiling from source)**:N/A
- **CUDA/cuDNN version**: cudnn7
- **GPU model and memory**:NVIDIA 1080Ti
- **Exact command to reproduce**:N/A",2,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-25 10:33:02,open,,,['type:docs'],2018-12-28 23:01:07
308,tensorflow/models,models,5957,yogurfrul,A bug in pcl_rl replay_buffer code,"it would raise this ValueEroor when n = max_size in https://github.com/tensorflow/models/blob/master/research/pcl_rl/replay_buffer.py#L136

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py"", line 757, in argpartition
    return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py"", line 51, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
ValueError: kth(=10) out of bounds (10)
```",1,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-25 07:35:26,open,,,['type:support'],2018-12-28 22:55:08
309,tensorflow/models,models,5956,yogurfrul,fix bug in replay_buffer.py,"it would raise this ValueEroor when n = max_size 
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py"", line 757, in argpartition
    return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py"", line 51, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
ValueError: kth(=10) out of bounds (10)
```",1,,[],2018-12-25 07:32:46,open,,,['cla: no'],2018-12-25 23:23:02
310,tensorflow/models,models,5955,zhangxg,fix slim exported lenet graph can not be frozen,"the exported Lenet graph cannot be frozen due to below error: 

```
InvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Assign requires shapes of both tensors to match. lhs shape= [5,5,3,32] rhs shape= [5,5,1,32]
```
The used scripts as following: 
**export graph**
```
python export_inference_graph.py \
  --alsologtostderr \
  --model_name=lenet \
  --output_file=/tmp/lenet-model/lenet.pb \
  --dataset_name=mnist
```
**frozen graph** 
```
python -m tensorflow.python.tools.freeze_graph \
  --input_graph=/tmp/lenet-model/lenet.pb \
  --input_checkpoint=/tmp/lenet-model/model.ckpt-600 \
  --input_binary=true \
  --output_graph=/tmp/lenet-model/lenet_freeze.pb \
  --output_node_names=Predictions/Reshape_1
```
",3,,[],2018-12-25 03:26:34,open,,,['cla: yes'],2019-01-15 08:28:12
311,tensorflow/models,models,5953,wenliwyan,move flags validator before tf.app.run(),"absl.flags validation happens at tf.app.run(). See https://github.com/abseil/abseil-py/blob/master/absl/flags/_validators.py#L329

The flags validator was not working in the previous code, and the error message would be ""TypeError: Expected binary or unicode string, got None"" when required flags were not correctly provided in arguments, which is not very helpful for debugging.
After the modification, the error message will be like ""absl.flags._exceptions.IllegalFlagValueError: flag --pipeline_config_path=None: Flag --pipeline_config_path must be specified"".",4,,[],2018-12-24 03:47:17,open,,,['cla: yes'],2019-03-25 13:49:18
312,tensorflow/models,models,5951,JaosonMa,ssdlite-mobilenet_v2  Increase input size from 300 to 900? loss dont convergence,"ssdlite_mobilenet_v2_coco.config
i changed 
model {
  ssd {
    num_classes: 185
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 900  # Increase input size from 300 to 900
        width: 900 # 900
      }
    }
...

____________________
i Increase input size from 300 to 900 ,because i want to detect samll object in the image,
but , loss not convergence. i don/t know why?


INFO:tensorflow:global step 441: loss = 37.9318 (0.351 sec/step)
I1223 15:40:34.394506 140216558724928 tf_logging.py:115] global step 441: loss = 37.9318 (0.351 sec/step)
INFO:tensorflow:global step 442: loss = 79.1737 (0.333 sec/step)
I1223 15:40:34.729128 140216558724928 tf_logging.py:115] global step 442: loss = 79.1737 (0.333 sec/step)
INFO:tensorflow:global step 443: loss = 29.9729 (0.290 sec/step)
I1223 15:40:35.027728 140216558724928 tf_logging.py:115] global step 443: loss = 29.9729 (0.290 sec/step)
INFO:tensorflow:global step 444: loss = 34.6217 (0.298 sec/step)
I1223 15:40:35.327668 140216558724928 tf_logging.py:115] global step 444: loss = 34.6217 (0.298 sec/step)
INFO:tensorflow:global step 445: loss = 78.8400 (0.310 sec/step)
I1223 15:40:35.639422 140216558724928 tf_logging.py:115] global step 445: loss = 78.8400 (0.310 sec/step)
INFO:tensorflow:global step 446: loss = 22.9451 (0.296 sec/step)
I1223 15:40:35.936698 140216558724928 tf_logging.py:115] global step 446: loss = 22.9451 (0.296 sec/step)
INFO:tensorflow:global step 447: loss = 77.7216 (0.276 sec/step)
I1223 15:40:36.214318 140216558724928 tf_logging.py:115] global step 447: loss = 77.7216 (0.276 sec/step)
INFO:tensorflow:global step 448: loss = 38.1520 (0.361 sec/step)
I1223 15:40:36.577134 140216558724928 tf_logging.py:115] global step 448: loss = 38.1520 (0.361 sec/step)
INFO:tensorflow:global step 449: loss = 26.1242 (0.296 sec/step)
I1223 15:40:36.874693 140216558724928 tf_logging.py:115] global step 449: loss = 26.1242 (0.296 sec/step)
INFO:tensorflow:global step 450: loss = 21.5703 (0.345 sec/step)
I1223 15:40:37.221611 140216558724928 tf_logging.py:115] global step 450: loss = 21.5703 (0.345 sec/step)
INFO:tensorflow:global step 451: loss = 149.9978 (0.327 sec/step)
I1223 15:40:37.550590 140216558724928 tf_logging.py:115] global step 451: loss = 149.9978 (0.327 sec/step)


loss continue jump ,

how can i trained the 900*900 image with ssdlite_mobilenet_v2?
@drpngx 
@tensorflow-jenkins 

",2,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-23 07:43:26,open,,,['type:support'],2018-12-28 22:45:38
313,tensorflow/models,models,5950,Faaiza01,I am running the tensorflow object detection API i m getting these errors :3,"Traceback (most recent call last):   File ""legacy/train.py"", line 51, in <module>     from object_detection.builders import model_builder    File ""C:\Users\Faaiza Rasheed\Anaconda3\lib\site-packages\object_detection-0.1 -py3.6.egg\object_detection\builders\model_builder.py"", line 34, in <module>     from object_detection.meta_architectures import ssd_meta_arch     File ""C:\Users\Faaiza Rasheed\Anaconda3\lib\site-packages\object_detection-0.1 -py3.6.egg\object_detection\meta_architectures\ssd_meta_arch.py"", line 31, in <m odule>     from object_detection.utils import visualization_utils     File ""C:\Users\Faaiza Rasheed\Anaconda3\lib\site-packages\object_detection-0.1 -py3.6.egg\object_detection\utils\visualization_utils.py"", line 28, in <module>     import matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top     File ""C:\Users\Faaiza Rasheed\Anaconda3\lib\site-packages\matplotlib\pyplot.py "", line 31, in <module>     import matplotlib.colorbar     File ""C:\Users\Faaiza Rasheed\Anaconda3\lib\site-packages\matplotlib\colorbar. py"", line 32, in <module> import matplotlib.artist as martist    File ""C:\Users\Faaiza Rasheed\Anaconda3\lib\site-packages\matplotlib\artist.py "", line 16, in <module>      from .path import Path   File ""C:\Users\Faaiza Rasheed\Anaconda3\lib\site-packages\matplotlib\path.py"",  line 21, in <module>      from . import _path, rcParams ImportError: cannot import name '_path'   S",1,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-22 12:36:26,open,,,['type:support'],2018-12-28 22:42:37
314,tensorflow/models,models,5949,rogerthat94,Update running_pets.md,Added a link to the using_your_own_dataset instructions.,0,,[],2018-12-21 20:16:11,open,,,['cla: yes'],2018-12-21 20:16:32
315,tensorflow/models,models,5948,cclauss,Python 3 fixes for cognitive_planning,"Python 3 treats legacy __print__ statements as syntax errors but __print()__ function works as expected in both Python 2 and Python 3.
* __xrange()__ --> __six.moves.xrange()__  # https://six.readthedocs.io
* __raw_input()__ --> __six.moves.input()__
* import logging
",0,,[],2018-12-21 11:26:07,open,,,['cla: yes'],2018-12-21 11:26:09
316,tensorflow/models,models,5946,ihafsa,no module name scripts,"hey i am new in tensorflow, i just tried to run its first example i.e of flower dataset. when i tried to run the command : 
python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --how_many_training_steps=500 \
  --model_dir=tf_files/models/ \
  --summaries_dir=tf_files/training_summaries/""${ARCHITECTURE}"" \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --architecture=""${ARCHITECTURE}"" \
  --image_dir=tf_files/flower_photos
it is giving me the error **/usr/bin/python: no module name scripts**. i
please help me to resolve this issue",2,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-21 06:40:13,open,,,['type:support'],2019-01-10 15:19:53
317,tensorflow/models,models,5944,mayurnewase,Fast Decode for transformer model,"Can we get greedy decoder in transformer model like in tensor2tensor library has.

",1,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-20 14:59:12,open,,,['type:docs'],2018-12-28 22:37:23
318,tensorflow/models,models,5942,gunxiaoshi,Perform py-3 premade_estimator.py error reporting,"E:\Python\Tensorflow\models\samples\core\get_started>py -3  premade_estimator.py
Traceback (most recent call last):
  File ""premade_estimator.py"", line 88, in <module>
    tf.app.run(main)
  File ""D:\Program Files\Programs\Python\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""premade_estimator.py"", line 34, in main
    (train_x, train_y), (test_x, test_y) = iris_data.load_data()
  File ""E:\Python\Tensorflow\models\samples\core\get_started\iris_data.py"", line 21, in load_data
    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)
  File ""D:\Program Files\Programs\Python\Python36\lib\site-packages\pandas\io\parsers.py"", line 678, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""D:\Program Files\Programs\Python\Python36\lib\site-packages\pandas\io\parsers.py"", line 440, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File ""D:\Program Files\Programs\Python\Python36\lib\site-packages\pandas\io\parsers.py"", line 787, in __init__
    self._make_engine(self.engine)
  File ""D:\Program Files\Programs\Python\Python36\lib\site-packages\pandas\io\parsers.py"", line 1014, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File ""D:\Program Files\Programs\Python\Python36\lib\site-packages\pandas\io\parsers.py"", line 1708, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File ""pandas\_libs\parsers.pyx"", line 384, in pandas._libs.parsers.TextReader.__cinit__
  File ""pandas\_libs\parsers.pyx"", line 697, in pandas._libs.parsers.TextReader._setup_parser_source
OSError: Initializing from file failed

E:\Python\Tensorflow\models\samples\core\get_started>
",1,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-20 09:28:39,open,,,[],2018-12-28 20:29:42
319,tensorflow/models,models,5938,00krishna,Is there a way to encode data in a TFRecord format using C++ instead of python?,"### System information
- **What is the top-level directory of the model you are using**:

This question relates to both the official and research models. 

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Not applicable.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04, but issue is cross-platform

- **TensorFlow installed from (source or binary)**:
TF version 1.11 from binary. 

- **TensorFlow version (use command below)**:

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'unknown' 1.12.0


- **Bazel version (if compiling from source)**:
NA, not using Bazel.

- **CUDA/cuDNN version**:
CUDA 10. cuDNN 7.4

- **GPU model and memory**:
NA - GTX 1080 Ti, 11GB memory.

- **Exact command to reproduce**:
NA 
### Describe the problem

This question points to an omission in the documentation. The question is whether there is a way to encode data in a TFRecord format using only C++, and not using the the python functions from `tf.train.` We know that there are python functions to encode data in TFRecord format, including the `tf.train.Int64List()` and `tf.train.BytesList().` But are there similar function in C++? Are there any code samples of anyone doing something like this? Thanks for reviewing my question.

What is the top-level directory of the model you are using
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce


**What have I tried**:
I have posted on multiple sites, but with no responses.

I posted this question on Stack Exchange:
https://stackoverflow.com/questions/53769229/tensorflow-can-i-create-a-tfrecords-file-using-c

I have posted this question on Reddit:
https://www.reddit.com/r/tensorflow/comments/a7jd0e/does_anyone_know_how_to_create_a_tfrecord_file/

I have posted this question on Quora:
https://www.quora.com/unanswered/Can-I-create-a-TensorFlow-TFRecord-file-from-C-instead-of-Python?__filter__=&__nsrc__=2&__snid3__=3644653184

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

NA",3,,[],2018-12-19 21:19:29,open,,,['type:docs'],2019-02-01 16:36:32
320,tensorflow/models,models,5933,jmgrn61,What is the correct command to reproduce the accuracy results in the paper?,"**Have I written custom code: No
OS Platform and Distribution: Ubuntu 16.04 LTS
TensorFlow installed from: https://www.tensorflow.org
TensorFlow version: Tensorflow 1.8
Bazel version: NA
CUDA/cuDNN version: cudnn-8.0-linux-x64-v6.0
GPU model and memory: Tesla K80 and 24GB
Exact command to reproduce: python eval.py --dataset_dir=/Users/andy/attention_ocr/models/research/attention_ocr/python/datasets/data/fsns/ --split=test --batch_size=204 --num_batches 1000**

I believe many people will have similar question with mine.  According to the instruction in the readme file, I downloaded the pre-trained model ""attention_ocr_2017_08_09.tar.gz"", and I guess I should run eval.py to get the testing accuracy.  

However, the sequence accuracy easily got above 93% with this downloaded model. It seems the default implementation in this code, the num_batches is set to 100, so the accuracy 93% must be based on some subset of images in the training set.

So my question is, what is the exact command here to evaluate a model (i.e. attention_ocr_2017_08_09.tar.gz) on the whole test data and reproduce ~84% of accuracy? 
",2,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-19 07:15:47,open,,,['type:support'],2018-12-28 20:12:18
321,tensorflow/models,models,5932,yaceben,[Feature request]:  models/research/object_detection load tfrecords from regex for infer_detections.py,"### System information
- **What is the top-level directory of the model you are using**: tensorflow/models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, using stock from git
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux  4.19.8-arch1-1-ARCH Antergos
- **Python version**: Python 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) 
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 10.0/7.4
- **GPU model and memory**: Nvidia Quadro K2100M 2GB
- **Exact command to reproduce**: 
```sh
python object_detection/inference/infer_detections.py \
--input_tfrecord_paths=data/labels/airbus_boats_test.tfrecord-00000-of-00004,data/labels/airbus_boats_test.tfrecord-00000-of-00004,data/labels/airbus_boats_test.tfrecord-00001-of-00004,data/labels/airbus_boats_test.tfrecord-00002-of-00004,data/labels/airbus_boats_test.tfrecord-00003-of-00004 \
--output_tfrecord_path=eval/detections.tfrecord \
--inference_graph=models/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14/frozen_inference_graph.pb
```
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

We must pass a comma separated list of tfrecords to --input_tf_records_path but it is inconvenient when we have alot of tfrecords. It would be best if we could pass a proto config file like for the model_main.py file. but a regexed or pseudo regexed implementation would also do

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

I devised a simple solution for myself:

```python
    input_tfrecord_paths = [os.path.join(input_tfrecord_root_path,f) 
                            for f in os.listdir(input_tfrecord_root_path) 
                            if re.match(input_tfrecord_fname, f)]
```
instead of 
```python
input_tfrecord_paths = [
       v for v in FLAGS.input_tfrecord_paths.split(',') if v]
```

So now i can use
```sh
python object_detection/inference/infer_detections.py \
--input_tfrecord_paths=data/labels/airbus_boats_test.tfrecord-00[0-9]+-of-00128 \
--output_tfrecord_path=eval/detections.tfrecord \
--inference_graph=models/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14/frozen_inference_graph.pb
```",1,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-19 01:04:44,open,,,"['stat:contributions welcome', 'type:feature']",2019-01-01 03:03:11
322,tensorflow/models,models,5931,yaceben,Update object_detection_evaluation.py for some fixes relating to issue #5924,"fixed unicode category_name for compatibility with python 3.x
fixed bug where `groundtruth_dict[standard_fields.InputDataFields.groundtruth_group_of]` of NoneType would throw an error in the if statement",6,,[],2018-12-18 22:15:31,open,,,['cla: yes'],2019-03-06 17:55:34
323,tensorflow/models,models,5930,rogerthat94,Update installation.md,"When I read these instructions, I thought that the models/research directory was something that was located in my tensorflow directory that pip created. I did not realize that this repository had to be cloned. It seems that other people were confused by this as well: https://github.com/tensorflow/models/issues/2253

I have updated the instructions to try to indicate that the models directory is the root of this repository, and not part of pip's TensorFlow installation.",0,,[],2018-12-18 19:39:59,open,,,['cla: yes'],2018-12-21 19:45:51
324,tensorflow/models,models,5927,sed0724963,"The different ""size "" image output will turn left when I was testing them.","I had trained a detection model and My training data is used picture from iphone 6s plus , it size is  768x1024 and Resolution is 72dpi. (vertical photo)
Model type is"" faster rcnn resnet 101"".

My Questions is :If I used the different cell-phone pictures (e.s. samsung) to testing ,that's output will turn left and it's only turn left it will not turn right or other's way,why??

I'm not only used samsung but also used HTC for testing ,although that problem is not much  ,but it's still have this problem from HTC .and the samsung almost have this problem.

samsung picture size is 2322x4128 and Resolution is 72dpi. (vertical photo)
HTC picture size is 2368x4160 and Resolution is 72dpi. (vertical photo)

At the beginning,I assume this is size problem,but most HTC image is still available to testing.although the output is not necessarily correct but is will not to turn left.In addition If I used the picture with 4128x2322 size(horizontal photo) ,it's will be available,although the output is not necessarily correct but is will not to turn left too. 

so I don't know  what problem it is? If it's a problem by different size. HTC's size is bigger than samsung,but the samsung is more problem than HTC. and If used the horizontal photo it's will no problem even it's size is bigger than iphone's picture.

***Currently, horizontal photo is not influences, this problem is only happened at vertical photo.

***I solvation this problem by change image size,and it's work,but I still want to know why some image will turn left??

------------------------

### System information
- **What is the top-level directory of the model you are using**: ../model/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- OS Platform and Distribution: win10
- TensorFlow installed from : pip command line on cmd
- TensorFlow version (use command below):tensorflow-GPU 1.8
- CUDA/cuDNN version : cuda 9.0 and cudnn 7.1.4
- GPU model and memory: ASUS ROG Strix GeForce® GTX 1080 GDDR5X 8GB



",1,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-18 10:40:24,open,,,['type:support'],2018-12-28 19:59:50
325,tensorflow/models,models,5924,yaceben,Bug: if input_config.WhichOneof('input_reader') == 'tf_record_input_reader': AttributeError: 'list' object has no attribute 'WhichOneof',"### System information
- **What is the top-level directory of the model you are using**: tensorflow/models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, using stock from git
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux  4.19.8-arch1-1-ARCH Antergos
- **Python version**: Python 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) 
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 10.0/7.4
- **GPU model and memory**: Nvidia Quadro K2100M 2GB
- **Exact command to reproduce**: 
```
python object_detection/metrics/offline_eval_map_corloc.py metrics/offline_eval_map_corloc.py --eval_dir=/home/user/PycharmProjects/SomeProject/eval_metrics --eval_config_path=/home/user/PycharmProjects/SomeProject/config/test_eval_config.pbtxt --input_config_path=/home/user/PycharmProjects/SomeProject/config/test_input_config.pbtxt
```

### Describe the problem
Getting error `if input_config.WhichOneof('input_reader') == 'tf_record_input_reader': AttributeError: 'list' object has no attribute 'WhichOneof'` while trying to run the Computing evaluation measures example from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/oid_inference_and_evaluation.md

also fixed a typo line 162 offline_eval_map_corloc.py `  input_config = configs['eval_input_configs']` (eval_input_config**s**), at least for coco_metrics

### Source code / logs
# Traceback
```
/home/user/anaconda3/envs/tensorflow_gpuenv/bin/python /opt/JetBrains/apps/PyCharm-P/ch-0/183.4284.139/helpers/pydev/pydevd.py --multiproc --qt-support=auto --client 127.0.0.1 --port 35603 --file /home/user/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/models/research/object_detection/metrics/offline_eval_map_corloc.py --eval_dir=/home/user/PycharmProjects/SomeProject/eval_metrics --eval_config_path=/home/user/PycharmProjects/SomeProject/config/test_eval_config.pbtxt --input_config_path=/home/user/PycharmProjects/SomeProject/config/test_input_config.pbtxt
pydev debugger: process 6999 is connecting

Connected to pydev debugger (build 183.4284.139)
/home/user/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/models/research/object_detection/utils/visualization_utils.py:26: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.
  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
Traceback (most recent call last):
  File ""/opt/JetBrains/apps/PyCharm-P/ch-0/183.4284.139/helpers/pydev/pydevd.py"", line 1689, in <module>
    main()
  File ""/opt/JetBrains/apps/PyCharm-P/ch-0/183.4284.139/helpers/pydev/pydevd.py"", line 1683, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/opt/JetBrains/apps/PyCharm-P/ch-0/183.4284.139/helpers/pydev/pydevd.py"", line 1083, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/opt/JetBrains/apps/PyCharm-P/ch-0/183.4284.139/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/user/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/models/research/object_detection/metrics/offline_eval_map_corloc.py"", line 171, in <module>
    tf.app.run(main)
  File ""/home/user/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/user/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/models/research/object_detection/metrics/offline_eval_map_corloc.py"", line 164, in main
    metrics = read_data_and_evaluate(input_config, eval_config)
  File ""/home/user/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/models/research/object_detection/metrics/offline_eval_map_corloc.py"", line 91, in read_data_and_evaluate
    if input_config.WhichOneof('input_reader') == 'tf_record_input_reader':
AttributeError: 'list' object has no attribute 'WhichOneof'

Process finished with exit code 1
```

## Contents of test_eval_config.pbtxt
```
metrics_set: 'coco_detection_metrics'
```
## Contents of test_input_config.pbtxt
```
label_map_path: ""/home/user/PycharmProjects/SomeProject/config/mscoco_label_map.pbtxt""

tf_record_input_reader: {
    input_path: ""/home/user/PycharmProjects/SomeProject/eval/detections.tfrecord""
}
```
If more info is needed please don't hesitate,
Thanks",13,,[],2018-12-18 01:24:22,open,,,['type:bug/performance'],2019-03-07 12:52:13
326,tensorflow/models,models,5923,atsyplikhin,Update tensorrt.py,"Corrected typo in metavar=""<IN>""",3,,[],2018-12-18 00:44:58,open,,,['cla: yes'],2018-12-18 00:46:47
327,tensorflow/models,models,5920,bocsiboti,ssd_mobilenetv2_oidv4 download link not working,"The download link of the ssd_mobilenetv2_oidv4 model (from the Tensorflow detection model zoo page) trained on open images is not working.

Page where the link can be found: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md

Link: http://download.tensorflow.org/models/object_detection/ssd_mobilenetv2_oidv4_2018_10_30.tar.gz

Error message: 

```
<Error>
<Code>AccessDenied</Code>
<Message>Access denied.</Message>
<Details>
Anonymous caller does not have storage.objects.get access to download.tensorflow.org/models/object_detection/ssd_mobilenetv2_oidv4_2018_10_30.tar.gz.
</Details>
</Error>
```",3,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-17 08:21:53,open,,,['type:bug/performance'],2018-12-29 09:49:51
328,tensorflow/models,models,5918,Keepmoving-ZXY,suitable distrubute strategy for NeuMF model.,"### System information
- **What is the top-level directory of the model you are using**:models/official/recommendation
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:16.04.1-Ubuntu SMP
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.12.0-rc0
- **Bazel version (if compiling from source)**:0.17.1
- **CUDA/cuDNN version**:7.2.1.38-1+cuda9.0
- **GPU model and memory**:Tesla V100-PCIE 32480MiB
- **Exact command to reproduce**:N/A

### Describe the problem
I train NeuMF model in the following case:
`1.1 process owns 1 GPU with PS strategy in a single server.`
`2.1 process owns 4 GPU with PS strategy in the same server.`
the following sheet show the result of above two case:
| case | global_step/s | example/s |
| case 1 | 8.35019 | 8482156.913395628 |
| case 2 | 1.14396 | 4707933.963351771 |
During train in case 1 and 2, I use ml-20m dataset and repeat dataset five times, ensure that every GPU's batch size is a fixed number. I found `global_step/s` of `case 2` is much lower than `case 1`, and `example/s`
of `case 2` is much lower than `case 1`.

I think the PS strategy is not suitable for NeuMF model,so I replace PS strategy with Mirror strategy and re-train the model
in the following case
`1.1 process owns 4 GPU with Mirror strategy in a single server.`
`2.2 process in two server owns 4 GPU each with Mirror strategy.`
the following sheet show the result of above two case:
| case | global_step/s | example/s |
| case 1 | 5.69666 | 25478880.81903213 |
| case 2 | 1.87715 | 15181016.607434036 |
During training in case 1 and 2, I use ml-20m dataset and repeat dataset five times,ensure that every GPU's batch size is a fixed number and is same as train with PS strategy. The result is also confused  that performance of 8 GPU in two worker is much lower than that in 4 GPU in a single worker. 

I am very confused about the result, and I want to know which distribute strategy is suitable for NeuMF model, and why performance result of NeuMF model with PS and Mirror strategy  is so confused. 
Thanks.
### Source code / logs
change `construct_estimator` function in `ncf_main.py` as follows:

     distribute_strategy = params['distribute_strategy']
	 if distribute_strategy == 'ParameterServer':
         distribution = tf.contrib.distribute.ParameterServerStrategy(num_gpus_per_worker=num_gpus)
         tf.logging.info(""distribute strategy is ParameterServer"")
     elif distribute_strategy == 'Mirror':
         tf.logging.info(""distribute strategy is Mirror"")
         distribution = tf.contrib.distribute.MirroredStrategy(num_gpus_per_worker=num_gpus)
     else:
         tf.logging.info(""No distribute strategy found,exit"")
         exit(1)

     run_config = tf.estimator.RunConfig(
         model_dir=model_dir,
         log_step_count_steps=10,
         train_distribute=distribution,
         eval_distribute=distribution
     )
     estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir,
                                     config=run_config, params=params)
     return estimator

  replace `train_estimator.train(input_fn=train_input_fn, hooks=train_hooks,steps=num_train_steps)` in `run_ncf` function in`ncf_main.py` with the following code:
    `train_spec = tf.estimator.TrainSpec(train_input_fn,max_steps=5000,hooks=train_hooks)`
    `eval_spec = tf.estimator.EvalSpec(eval_input_fn,steps=100)`
    `tf.estimator.train_and_evaluate(estimator,train_spec,eval_spec)`",0,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-17 04:29:11,open,,,['type:support'],2018-12-28 03:23:32
329,tensorflow/models,models,5917,sonfiree,Deletes checkpoint ,"**System information**

- What is the top-level directory of the model you are using: Object_Detection
- Have I written custom code: Modified Config file
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: build
- TensorFlow version: 1.12.0
- CUDA/cuDNN version: CUDA 10 / cuDNN 7.3
- GPU model and memory: gtx 1060 6gb

Object_Detection deletes my checkpoint.
How not to deleete them?",2,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-17 03:04:38,open,,,['type:support'],2018-12-28 03:20:53
330,tensorflow/models,models,5916,jsdd25,Cognitive_planning run train_supervised_active_vision.py show: cudaGetDevice() failed.,"Hi!
I have encountered some problems when I run train_supervised_active_vision.py in cognitive_planning. Please give me some advice. Thank you very much!

------------------------

### System information
- **What is the top-level directory of the model you are using**: cognitive_planning
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: anaconda
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: No
- **CUDA/cuDNN version**: 8.0.61
- **GPU model and memory**: GTX1080 8112MiB
- ** Exact command to reproduce**: 
```
~/cognitive_planning$ python train_supervised_active_vision.py \
>   --mode='train' \
>   --logdir=$CHECKPOINT_DIR \
>   --modality_types='det' \
>   --batch_size=8 \
>   --train_iters=200000 \
>   --lstm_cell_size=2048 \
>   --policy_fc_size=2048 \
>   --sequence_length=20 \
>   --max_eval_episode_length=100 \
>   --test_iters=194 \
>   --gin_config=envs/configs/active_vision_config.gin \
>   --logtostderr
```


### Describe the problem
```
(gymlab) jin@jin-System-Product-Name:~/cognitive_planning$ python train_supervised_active_vision.py \
>   --mode='train' \
>   --logdir=$CHECKPOINT_DIR \
>   --modality_types='det' \
>   --batch_size=8 \
>   --train_iters=200000 \
>   --lstm_cell_size=2048 \
>   --policy_fc_size=2048 \
>   --sequence_length=20 \
>   --max_eval_episode_length=100 \
>   --test_iters=194 \
>   --gin_config=envs/configs/active_vision_config.gin \
>   --logtostderr
I1217 09:06:55.325814 139683146712832 train_supervised_active_vision.py:377] modality types: [<ModalityTypes.OBJECT_DETECTION: 2>]
W1217 09:06:55.326095 139683146712832 active_vision_dataset_env.py:389] environment does not terminate the episode if the agent is too close to the environment
I1217 09:06:55.484458 139683146712832 active_vision_dataset_env.py:253] loading targets: /home/jin/cognitive_planning/AVD_Minimal/Meta/annotated_targets.npy
I1217 09:06:55.494143 139683146712832 active_vision_dataset_env.py:258] loading depth: /home/jin/cognitive_planning/AVD_Minimal/Meta/depth_imgs.npy
I1217 09:07:09.453788 139683146712832 active_vision_dataset_env.py:262] processing depth
I1217 09:07:10.269463 139683146712832 active_vision_dataset_env.py:277] loading sseg: /home/jin/cognitive_planning/AVD_Minimal/Meta/sseg_crf.npy
I1217 09:07:14.758609 139683146712832 active_vision_dataset_env.py:281] processing sseg
I1217 09:07:15.061821 139683146712832 active_vision_dataset_env.py:293] loading imgs: /home/jin/cognitive_planning/AVD_Minimal/Meta/imgs.npy
I1217 09:07:26.205753 139683146712832 active_vision_dataset_env.py:302] logging done in 30.721297 seconds
/home/jin/gym/gym/logger.py:30: UserWarning: WARN: gym.spaces.Box autodetected dtype as <type 'numpy.uint8'>. Please provide explicit dtype.
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
/home/jin/gym/gym/logger.py:30: UserWarning: WARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
OrderedDict([(<ModalityTypes.OBJECT_DETECTION: 2>, (tf.float32, [20, 64, 64, 90])), (<ModalityTypes.GOAL: 4>, (tf.float32, [20, 5])), (<ModalityTypes.PREV_ACTION: 5>, (tf.float32, [20, 8]))])
300
300

W1217 09:07:27.327918 139683146712832 tf_logging.py:125] From /home/jin/cognitive_planning/policies.py:150: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').

W1217 09:07:27.809431 139683146712832 tf_logging.py:125] From /home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/learning.py:737: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-12-17 09:07:27.963019: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-12-17 09:07:28.064821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-17 09:07:28.065167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8475
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.60GiB
2018-12-17 09:07:28.065181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
I1217 09:07:28.065444 139683146712832 tf_logging.py:115] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
Traceback (most recent call last):
  File ""train_supervised_active_vision.py"", line 503, in <module>
    app.run(main)
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""train_supervised_active_vision.py"", line 497, in main
    train()
  File ""train_supervised_active_vision.py"", line 490, in train
    session_config=tf.ConfigProto(allow_soft_placement=True),
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 748, in train
    master, start_standard_services=False, config=session_config) as sess:
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 1004, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 832, in stop
    ignore_live_threads=ignore_live_threads)
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 993, in managed_session
    start_standard_services=start_standard_services)
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 730, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 288, in prepare_session
    config=config)
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 185, in _restore_checkpoint
    sess = session.Session(self._target, graph=self._graph, config=config)
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1551, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/jin/anaconda2/envs/gymlab/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 676, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
```
Then I check the version of my CUDA and nvidia driver version.
```
jin@jin-System-Product-Name:~$ nvidia-smi
Mon Dec 17 09:14:27 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.130                Driver Version: 384.130
```
```
jin@jin-System-Product-Name:~$ cat /usr/local/cuda/version.txt
CUDA Version 8.0.61
```",4,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-17 01:19:24,open,,,['type:support'],2018-12-28 03:17:32
331,tensorflow/models,models,5914,toli-belo,"interactive_text_analyzer notebook error: ""Key lookahead/cell/cell_0/layer_norm_basic_lstm_cell/kernel not found""","### System information
- **What is the top-level directory of the model you are using**: models/research/syntaxnet/examples/dragnn
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: stock example below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.11.0-0-gc19e29306c', '1.11.0')
- **Bazel version (if compiling from source)**: 0.15.2
- **CUDA/cuDNN version**: 10.0 / 7.3.1.20
- **GPU model and memory**: RTX 2080 ti 11GB
- **Exact command to reproduce**: run 1st cell in: models/research/syntaxnet/examples/dragnn/interactive_text_analyzer.ipynb

### Describe the problem
I am unable to run jupyter notebook ""interactive_text_analyzer.ipynb"", located in models/research/syntaxnet/examples/dragnn. 
It returns the error shown below. can someone advise on how to get the interactive_text_analyzer notebook to run?
`NotFoundError: Key lookahead/cell/cell_0/layer_norm_basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]`
`	 [[Node: save/RestoreV2/_17 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_24_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]`

I saw a reference to using a checkpoint converter ([here](https://github.com/KranthiGV/Pretrained-Show-and-Tell-model/issues/9#issuecomment-366816238)), and ran converter like this:
```
anatolii@ubun:~/models/research/syntaxnet/examples/dragnn$ python2 ~/models/research/syntaxnet/tensorflow/tensorflow/contrib/rnn/python/tools/checkpoint_convert.py ./data/en/segmenter/checkpoint ./data-new/en/segmenter/
2018-12-15 16:54:22.748978: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-12-15 16:54:22.930786: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.665
pciBusID: 0000:05:00.0
totalMemory: 10.73GiB freeMemory: 10.03GiB
2018-12-15 16:54:22.930811: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-12-15 16:54:23.143174: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-15 16:54:23.143205: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-12-15 16:54:23.143212: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-12-15 16:54:23.143415: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9689 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5)
```
but it results in a small `checkpoint` file 59 bytes long with this content:
```
model_checkpoint_path: "".""
all_model_checkpoint_paths: "".""
```

### Source Code of the notebook cell that's failing:
```
import os
import ipywidgets as widgets
import tensorflow as tf
from IPython import display
from dragnn.protos import spec_pb2
from dragnn.python import graph_builder
from dragnn.python import spec_builder
from dragnn.python import load_dragnn_cc_impl  # This loads the actual op definitions
from dragnn.python import render_parse_tree_graphviz
from dragnn.python import visualization
from google.protobuf import text_format
from syntaxnet import load_parser_ops  # This loads the actual op definitions
from syntaxnet import sentence_pb2
from syntaxnet.ops import gen_parser_ops
from tensorflow.python.platform import tf_logging as logging

def load_model(base_dir, master_spec_name, checkpoint_name):
    # Read the master spec
    master_spec = spec_pb2.MasterSpec()
    with open(os.path.join(base_dir, master_spec_name), ""r"") as f:
        text_format.Merge(f.read(), master_spec)
    spec_builder.complete_master_spec(master_spec, None, base_dir)
    logging.set_verbosity(logging.WARN)  # Turn off TensorFlow spam.

    # Initialize a graph
    graph = tf.Graph()
    with graph.as_default():
        hyperparam_config = spec_pb2.GridPoint()
        builder = graph_builder.MasterBuilder(master_spec, hyperparam_config)
        # This is the component that will annotate test sentences.
        annotator = builder.add_annotation(enable_tracing=True)
        builder.add_saver()  # ""Savers"" can save and load models; here, we're only going to load.

    sess = tf.Session(graph=graph)
    with graph.as_default():
        #sess.run(tf.global_variables_initializer())
        #sess.run('save/restore_all', {'save/Const:0': os.path.join(base_dir, checkpoint_name)})
        checkpointPath = os.path.join(base_dir, checkpoint_name)
        print (""calling saver.restore on"",checkpointPath)
        builder.saver.restore(sess, checkpointPath)
        
    def annotate_sentence(sentence):
        with graph.as_default():
            return sess.run([annotator['annotations'], annotator['traces']],
                            feed_dict={annotator['input_batch']: [sentence]})
    return annotate_sentence

segmenter_model = load_model(""data/en/segmenter"", ""spec.textproto"", ""checkpoint"")
parser_model = load_model(""data/en"", ""parser_spec.textproto"", ""checkpoint"")
```
### Output of the cell:
```
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
('calling saver.restore on', 'data/en/segmenter/checkpoint')
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-1-6469534a5d58> in <module>()
     48     return annotate_sentence
     49 
---> 50 segmenter_model = load_model(""data/en/segmenter"", ""spec.textproto"", ""checkpoint"")
     51 parser_model = load_model(""data/en"", ""parser_spec.textproto"", ""checkpoint"")

<ipython-input-1-6469534a5d58> in load_model(base_dir, master_spec_name, checkpoint_name)
     40         checkpointPath = os.path.join(base_dir, checkpoint_name)
     41         print (""calling saver.restore on"",checkpointPath)
---> 42         builder.saver.restore(sess, checkpointPath)
     43 
     44     def annotate_sentence(sentence):

/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/training/saver.pyc in restore(self, sess, save_path)
   1800     else:
   1801       sess.run(self.saver_def.restore_op_name,
-> 1802                {self.saver_def.filename_tensor_name: save_path})
   1803 
   1804   @staticmethod

/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    898     try:
    899       result = self._run(None, fetches, feed_dict, options_ptr,
--> 900                          run_metadata_ptr)
    901       if run_metadata:
    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1133     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1134       results = self._do_run(handle, final_targets, final_fetches,
-> 1135                              feed_dict_tensor, options, run_metadata)
   1136     else:
   1137       results = []

/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1314     if handle is None:
   1315       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1316                            run_metadata)
   1317     else:
   1318       return self._do_call(_prun_fn, handle, feeds, fetches)

/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
   1333         except KeyError:
   1334           pass
-> 1335       raise type(e)(node_def, op, message)
   1336 
   1337   def _extend_graph(self):

NotFoundError: Key lookahead/cell/cell_0/layer_norm_basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_17 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_24_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op u'save/RestoreV2', defined at:
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py"", line 499, in start
    self.io_loop.start()
  File ""/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py"", line 1073, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2714, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2818, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py"", line 2878, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-6469534a5d58>"", line 50, in <module>
    segmenter_model = load_model(""data/en/segmenter"", ""spec.textproto"", ""checkpoint"")
  File ""<ipython-input-1-6469534a5d58>"", line 32, in load_model
    builder.add_saver()  # ""Savers"" can save and load models; here, we're only going to load.
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/__main__/dragnn/python/graph_builder.py"", line 720, in add_saver
    write_version=saver_pb2.SaverDef.V1)
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1338, in __init__
    self.build()
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 835, in _build_internal
    restore_sequentially, reshape)
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 472, in _AddRestoreOps
    restore_sequentially)
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 886, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/home/anatolii/models/research/syntaxnet/bazel-bin/dragnn/tools/oss_notebook_launcher.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key lookahead/cell/cell_0/layer_norm_basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_17 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_24_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
```
",0,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-16 01:04:14,open,,,['type:bug/performance'],2018-12-28 03:13:03
332,tensorflow/models,models,5909,IronE-G-G,models/research/skip_thoughts/skip_thoughts/skip_thoughts_model.py:Why does the skip thought vector was put in the initial_state in the function build_decoders,"Hello,
I wonder whether there is a mistake in the decoder of skipthoughts. 
The file is **tensorflow/models/research/skip_thoughts/skip_thoughts/skip_thoughts_model.py**

### Describe the problem
Why does the skip thought vector was put in the initial_state in the function **build_decoders** when it use the function **_build_decoder** in **line 333**? I have read the skip-thought paper and the skip-thought vector was input in every time step in the decoder in the paper.  
### code
<code>
  def _build_decoder(self, name, embeddings, targets, mask, initial_state,
                     reuse_logits):
...
</code>

    
<code>
  def build_decoders(self):
  
    """"""Builds the sentence decoders.

    Inputs:
      self.decode_pre_emb
      self.decode_post_emb
      self.decode_pre_ids
      self.decode_post_ids
      self.decode_pre_mask
      self.decode_post_mask
      self.thought_vectors

    Outputs:
      self.target_cross_entropy_losses
      self.target_cross_entropy_loss_weights
    """"""
    if self.mode != ""encode"":
      # Pre-sentence decoder.
      self._build_decoder(""decoder_pre"", self.decode_pre_emb,
                          self.decode_pre_ids, self.decode_pre_mask,
                          self.thought_vectors, False)

      # Post-sentence decoder. Logits weights are reused.
      self._build_decoder(""decoder_post"", self.decode_post_emb,
                          self.decode_post_ids, self.decode_post_mask,
                          self.thought_vectors, True)
</code>

### system information
**What is the top-level directory of the model you are using tensorflow/models/research/skip_thoughts**
**Have I written custom code:No**
**OS Platform and Distribution: N/A** 
**TensorFlow installed from: N/A**
**TensorFlow version :N/A**
**Bazel version :N/A**
**CUDA/cuDNN version: NO**
**GPU model and memory: NO**
",1,,[],2018-12-14 06:14:39,open,,,"['models: research', 'type:docs']",2019-02-06 22:26:45
333,tensorflow/models,models,5906,pierrot0,TF cifar10 cnn tutorial: use tensorflow-datasets to load the data.,,1,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-12-13 15:05:01,open,,,['cla: yes'],2019-01-18 05:29:08
334,tensorflow/models,models,5903,chkam05,ImportError: libcublas.so.9.0,"Python 3.6.7
Ubuntu 18.04 subsystem for Windows
This version of python is required becouse objects i traning is required to use in windows AI project

kamil@MSI-PC:/mnt/d/Users/models/research/object_detection$ python3 ./legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config
Traceback (most recent call last):
  File ""/home/kamil/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/kamil/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/kamil/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./legacy/train.py"", line 47, in <module>
    import tensorflow as tf
  File ""/home/kamil/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/kamil/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/kamil/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/kamil/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/kamil/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/kamil/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.",5,"NamedUser(login=""ymodak"")","[NamedUser(login=""ymodak"")]",2018-12-12 18:25:21,open,,,['type:build/install'],2018-12-27 01:06:05
335,tensorflow/models,models,5901,rodasoares,Can Not Replicate Transformer Base Bleu Scores,"### System information
- **What is the top-level directory of the model you are using**: /models/official/transformer
- **Have I written custom code** : No
- **OS Platform and Distribution** :Ubuntu 16.04.5 LTS
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version** : v1.12.0-0-ga6d8ffae09 1.12.0
- **CUDA/cuDNN version**: release 9.0, V9.0.176
- **GPU model and memory**: Tesla K40m/12GB
- **Exact command to reproduce**: Official Instructions
- **Bazel version**: N/A

### Describe the problem
I have been trying to replicate models/offical/tensorflow/ **Base**. I followed the official instructions  yet I was faced with two problems:
1- The size of the generated vocabulary was bigger than the one defined in /models/offical/tranformer/model/model_params.py. The program would not work, I fixed it by changing the value in model_params.py to the actual vocabulary size of 33945.
2- The second problem, and the one I could not solve,  is that Blue scores after **10 epochs** are not consistent with what is reported in models/official/tensorflow.  Running, as instructed,  compute_bleu.py gives **case-insensitive** Bleu scores of **26.04** far bellow the ""promised"" **27.7** Bleu for **Base** Transformer.

I have trained **3** models and even though there are fluctuation in Bleu scores these are minimal being the biggest difference **0,1 Bleu**.   

### Source code / logs
python compute_bleu.py --translation=translation.en --reference=test_data/newstest2014.de                                                          
I1212 11:03:35.484694 140343540246272 tf_logging.py:115] Case-insensitive results: 26.038009
I1212 11:03:40.145630 140343540246272 tf_logging.py:115] Case-sensitive results: 25.506699

",4,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-12 13:31:14,open,,,['type:bug/performance'],2019-01-18 18:58:54
336,tensorflow/models,models,5895,jianchao-li,Use distutils.version.StrictVersion for version comparisons,"Now, version comparison is performed in the following way.

```python
""1.4"" <= tf_version
```

This is problematic. For example, when `tf_version` is `1.10.1`, we would expect the above statement to return `True`. However, it will return `False` since it is using string comparison.

```python
>>> import tensorflow as tf
>>> tf_version = tf.__version__
>>> print(tf_version)
1.10.1
>>> ""1.4"" <= tf_version
False
```

To fix this bug, we need to use some version comparison packages for Python. And in `object_detection_tutorial.ipynb`, `distutils.version.StrictVersion` is used. So it is also used here to be consistent.",4,,[],2018-12-11 10:29:42,open,,"NamedUser(login=""jianchao-li"")",['cla: yes'],2018-12-29 02:39:58
337,tensorflow/models,models,5894,netanel-s,[Feature Request] Support Group normalization in OD API,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 11GB
- **Exact command to reproduce**: N/A

### Describe the problem
Please add support to use group normalization ([1](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/group_norm), [2](https://arxiv.org/abs/1803.08494)) through Object Detection API.

Thanks in advance.",1,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-11 09:43:53,open,,,['type:feature'],2018-12-28 02:29:15
338,tensorflow/models,models,5887,hongym7,about earlystopping,"I run object detection API.
I wanna use 'early stopping' API.

Below 

    early_stopping = tf.contrib.estimator.stop_if_no_decrease_hook(
        estimator,
        metric_name='loss_1',
        max_steps_without_decrease=1
    )

    train_spec, eval_specs = model_lib.create_train_and_eval_specs(
        train_input_fn,
        early_stopping,
        eval_input_fns,
        eval_on_train_input_fn,
        predict_input_fn,
        train_steps,
        eval_on_train_data=False)

I changed metric_name to 'loss_1', 'total_loss', 'loss'.
But I can't use early stopping function.
Help me.



Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,[],2018-12-10 09:13:42,open,,,['type:support'],2019-02-22 08:13:28
339,tensorflow/models,models,5886,Anubhav2017,Tensorboard not showing accuracy and total loss in object detection API,"Kindly answer the following question on stackoverflow:

https://stackoverflow.com/questions/53699497/tensorflow-object-detection-evaluation-accuracy-not-showing-on-tensorboard",2,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-10 05:01:11,open,,,['type:support'],2018-12-28 01:47:50
340,tensorflow/models,models,5884,freekoy,Cannot find module 'data_provider',"### System information
- **What is the top-level directory of the model you are using**:research/gan/pix2pix
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.4.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:CUDA 8 / cuDNN6
- **GPU model and memory**:8G
- **Exact command to reproduce**:python train_test.py

`Traceback (most recent call last):
  File ""train_test.py"", line 26, in <module>
    import train
  File ""/work/models/research/gan/pix2pix/train.py"", line 25, in <module>
    import data_provider
ModuleNotFoundError: No module named 'data_provider'
`",0,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-10 03:44:07,open,,,['type:support'],2018-12-28 01:43:33
341,tensorflow/models,models,5883,gfjiyue,"[variables_helper.py:141]Variable [SecondStageBoxPredictor/BoxEncodingPredictor/biases] is available in checkpoint, but has an incompatible shape with model.","## System information
**What is the top-level directory of the model you are using**: Object detection
**Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, but I just edited the **model_main.py** at line 62: 
```
config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir,  
 save_checkpoints_steps=500,
 tf_random_seed=1230, 
 keep_checkpoint_max=100)
```
**OS Platform and Distribution**: Ubuntu ""16.04.2 LTS (Xenial Xerus)""
**TensorFlow installed from (source or binary)**: installed with Python's pip package
**TensorFlow version (use command below)**: ('v1.9.0-0-g25c197e023', '1.9.0')
**Bazel version**: N/A
**CUDA/cuDNN version**: CUDA 8.0
**GPU model and memory**:  Tesla P40    22919MiB 
**Exact command to reproduce**:
   export CUDA_VISIBLE_DEVICES=3
   python object_detection/model_main_my.py \
    --pipeline_config_path=${train_path}/data/faster_rcnn_resnet101_t.config \
    --model_dir=${train_path}/model \
    --num_train_steps=50000 \
    --sample_1_of_n_eval_examples=1 \
    --alsologtostderr
## Describe the problem
There always show warnings in the secondstage. I want to know if this in relation with the pretrain model and if it will influence the box detection precision.
## Warnings:
W1209 21:57:14.465106 140428642035456 variables_helper.py:141] Variable [SecondStageBoxPredictor/BoxEncodingPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[360]], model variable shape: [[4]]. This variable will not be initialized from the checkpoint.
W1209 21:57:14.465503 140428642035456 variables_helper.py:141] Variable [SecondStageBoxPredictor/BoxEncodingPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[2048, 360]], model variable shape: [[2048, 4]]. This variable will not be initialized from the checkpoint.
W1209 21:57:14.465818 140428642035456 variables_helper.py:141] Variable [SecondStageBoxPredictor/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[91]], model variable shape: [[2]]. This variable will not be initialized from the checkpoint.
W1209 21:57:14.466134 140428642035456 variables_helper.py:141] Variable [SecondStageBoxPredictor/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[2048, 91]], model variable shape: [[2048, 2]]. This variable will not be initialized from the checkpoint.
",2,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-09 13:09:48,open,,,['type:support'],2018-12-28 00:47:30
342,tensorflow/models,models,5882,Shameendra,mAP does not show up on tensorboard for Object detection API,"Hi i am using the Google Object Detection API to train on my own data. However, when i run the eval.py(from legacy folder) in order to see evaluation results and then run tensorboard, just images and graphs show up but no scalars like mAP. the command i am using is

`python eval.py --logtostderr --checkpoint_dir=training/ --eval_dir=evaluation/ --pipeline_config_path=training/faster_rcnn_inception_v2_pedestrians.config`

Can anyone please tell me how do i resolve this? Do i need to add some extra lines in eval.py or is there some other things that i should do?

",4,,[],2018-12-08 14:02:27,open,,,['type:support'],2019-03-23 03:28:16
343,tensorflow/models,models,5881,abdullahalsaidi16,Model not predicting anything even on the training set,"I've trained mobilenet v2 ,v1 and faster_rcnn to detect only one object 
when training the loss reach to **1.0 or 0.01**(on faster_rcnn) but when I export a frozen graph and interfere some photos the probability is always 0 for finding the object
I used for training the ""train.py"" script in models/research/object_detection/legacy and my photo size is 572x432
I've been trying to solve that for about a month  :( 

### System information
I worked on google colab and this the notebook link:
https://drive.google.com/open?id=1fdOwwOaIlr7650UM6nLWtXrnTShaR_go

**folders in colab**
- data/( train.record test.record)
- pipeline.conf
- train.py
- label_map.pbtxt
- pretrained( check points for pretrained model)
- train( saving train progress)


**Tensorflow version:**
v1.12.0-0-ga6d8ffae09 1.12.0



#### this is my pipeline config file:

```
model {
  faster_rcnn {
    num_classes: 1
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
      }
    }
    feature_extractor {
      type: ""faster_rcnn_resnet50""
      first_stage_features_stride: 16
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        height_stride: 16
        width_stride: 16
        scales: 0.25
        scales: 0.5
        scales: 1.0
        scales: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 1.0
        aspect_ratios: 2.0
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.00999999977648
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.699999988079
    first_stage_max_proposals: 100
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
        use_dropout: false
        dropout_keep_probability: 1.0
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.300000011921
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}
train_config {
  batch_size: 1
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  optimizer {
    momentum_optimizer {
      learning_rate {
        manual_step_learning_rate {
          initial_learning_rate: 0.000300000014249
          
          schedule {
            step: 900000
            learning_rate: 2.99999992421e-05
          }
          schedule {
            step: 1200000
            learning_rate: 3.00000010611e-06
          }
        }
      }
      momentum_optimizer_value: 0.899999976158
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""pretrained/model.ckpt""
  from_detection_checkpoint: true
  num_steps: 200000
}
train_input_reader {
  label_map_path: ""label_map.pbtxt""
  tf_record_input_reader {
    input_path: ""data/train.record""
  }
}
eval_config {
  num_examples: 8000
  max_evals: 10
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""label_map.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""data/test.record""
  }
}
```
#####




",1,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-07 20:45:40,open,,,['type:support'],2018-12-28 00:39:35
344,tensorflow/models,models,5879,a2bc,Save positive and negative crop samples,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/53671993/save-positive-and-negative-samples-used-in-tensorflow-object-detection-api

I use Tensorflow Object Detection API with MobilenetV2 as network backbone and SSD as meta-structure to do the object detection job.

In SSD, for each anchor point, we make several candidate bounding boxes with different aspect_ratios. For each bounding box, if its intersection with the bounding box ground-truth is greater than a threshold, we say that this bounding box is positive. Otherwise, it is negative. And then we use these positive and negative to do the training. (So it is important to note that it is NOT the entire image is used to train, but only one (or several) crops of these images are used)

To debug, I'd like to save these positive and negative crops to hard disk to see what are really samples that the algorithm uses to train.

I read the python code of Tensorflow Object Detection API but I'm lost :(

If you have any hint, please show me !

Thanks !

------------------------

### System information
- **What is the top-level directory of the model you are using**: model_main.py in folder object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.10.1 (GPU)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 9.0.176, cuDNN 7.2.1
- **GPU model and memory**: GeForce GTX 105, 4GRam
- **Exact command to reproduce**:
",1,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-07 15:24:34,open,,,['type:support'],2018-12-28 01:08:17
345,tensorflow/models,models,5878,swxhss,load resnet_v2_50.ckpt has error,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:1.9.0 tensorlfow-gpu
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:cuda-9.0
- **GPU model and memory**:16GB
- **Exact command to reproduce**:

### Describe the problem
when I restore the pre_trained resnet_v2_50.ckpt,it shows can not find batchnorm/beta.

### Source code / logs
ITraceback (most recent call last):
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Tensor name ""resnet_v2_50/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta"" not found in checkpoint files ./resnet_v2_50.ckpt
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_301 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_306_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/swx/head_detection/myssd/trans_resnet50_from_slim.py"", line 18, in <module>
    saver.restore(sess,'./resnet_v2_50.ckpt')
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1768, in restore
    six.reraise(exception_type, exception_value, exception_traceback)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1752, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Tensor name ""resnet_v2_50/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta"" not found in checkpoint files ./resnet_v2_50.ckpt
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_301 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_306_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op 'save/RestoreV2', defined at:
  File ""/home/swx/head_detection/myssd/trans_resnet50_from_slim.py"", line 16, in <module>
    saver  = tf.train.Saver()
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1284, in __init__
    self.build()
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1296, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1333, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal
    restore_sequentially, reshape)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 400, in _AddRestoreOps
    restore_sequentially)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 832, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/home/swx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Tensor name ""resnet_v2_50/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta"" not found in checkpoint files ./resnet_v2_50.ckpt
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_301 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_306_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

",0,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-07 04:04:55,open,,,['type:support'],2018-12-28 01:09:39
346,tensorflow/models,models,5876,atyshka,"Tensorflow-TensorRT ""Engine buffer is full""","------------------------

### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 10/7
- **GPU model and memory**: Jetson Xavier 16GB shared w/ RAM
- **Exact command to reproduce**: sess.run()

### Describe the problem

I'm trying to convert the frozen weights from faster_rcnn_resnet50_coco to TensorRT and I'm getting the following error when I call session.run():

`2018-12-06 12:21:53.405304: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:260] Engine buffer is full. buffer limit=1, current entries=1, requested batch=100
2018-12-06 12:21:53.405458: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:277] Failed to get engine batch, running native segment for my_trt_op_1`

Is this a bug? If not, what is the cause of this error?

### Source code / logs

Here is a minimal example of the code:

    trt_graph = trt.create_inference_graph(
        input_graph_def=frozen_graph,
        outputs=output_names,
        max_batch_size=1,
        max_workspace_size_bytes=4000000000,
        precision_mode='FP16',
        minimum_segment_size=50
    )
    
    tf_config = tf.ConfigProto()
    tf_config.gpu_options.allow_growth = True
    tf_sess = tf.Session(config=tf_config)
    tf.import_graph_def(trt_graph, name='')\
    scores, boxes, classes, num_detections = tf_sess.run([tf_scores, tf_boxes, tf_classes, tf_num_detections], feed_dict={
        tf_input: image_resized[None, ...]
    })

I'm following [this](https://github.com/NVIDIA-AI-IOT/tf_trt_models/blob/master/examples/classification/classification.ipynb) guide in an NVIDIA repo if you want to see the complete code. Supposedly someone else got the same faster-rcnn working so it should work.",7,,[],2018-12-06 19:04:29,open,,,"['stat:awaiting tensorflower', 'type:bug/performance']",2019-02-18 06:51:22
347,tensorflow/models,models,5875,prismformore,models/research/delf/: The use of Product Quantization,"DELF is used for Image Retrieval but it seems that the code for retrieval is not provided.

In the DELF paper: *We encode each descriptor to a 50-bit code using PQ, where each 40D feature descriptor is split into 10 subvectors with equal dimensions, and we identify 25 centroids per subvector by k-means clustering to achieve 50-bit encoding. We perform asymmetric distance calculation, where the query descriptors are not encoded to improve the accuracy of nearest neighbor retrieval. To speed up the
nearest neighbor search, we construct an inverted index for descriptors, using a codebook of size 8K. To reduce encoding errors, a KD-tree is used to partition each Voronoi cell, and a Locally Optimized Product Quantizer [20] is employed for each subtree with fewer than 30K features.*

This description seems that the authors used PQ +Inverted Index + KD-TREE + LOPQ(on KD-tree), with no specific example and sound complicated. Would that be possible to provide some example codes related to the image retrieval system? Thank you. @andrefaraujo 


",4,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-06 16:02:55,open,,,['type:support'],2019-01-10 23:50:22
348,tensorflow/models,models,5874,eddyli5,[object detection] TypeError: restore_map() got an unexpected keyword argument 'from_detection_checkpoint',"## System information
- **What is the top-level directory of the model you are using**: object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: from pip 
- **TensorFlow version (use command below)**:  1.10.0
- **Bazel version (if compiling from source)**:  0.15.0
- **CUDA/cuDNN version**:    9.0.176
- **GPU model and memory**:   Tesla K80  ， 12G
- **Exact command to reproduce**: 

Describe the problem
(tensorflow_p36) ubuntu@ip-172-31-9-197:~/models/research/object_detection$ python train.py --train_dir=V70_output --pipeline_config_path=V70_output/ssd_mobilenet_v1_coco.config --logtostderr
WARNING:tensorflow:From /home/ubuntu/models/research/object_detection/trainer.py:228: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
W1206 05:05:42.226124 140603508307712 tf_logging.py:125] From /home/ubuntu/models/research/object_detection/trainer.py:228: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1206 05:05:42.245491 140603508307712 tf_logging.py:125] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /home/ubuntu/models/research/object_detection/core/preprocessor.py:1208: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
W1206 05:05:42.535090 140603508307712 tf_logging.py:125] From /home/ubuntu/models/research/object_detection/core/preprocessor.py:1208: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
INFO:tensorflow:depth of additional conv before box predictor: 0
I1206 05:05:46.581395 140603508307712 tf_logging.py:115] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I1206 05:05:46.618006 140603508307712 tf_logging.py:115] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I1206 05:05:46.654365 140603508307712 tf_logging.py:115] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I1206 05:05:46.691074 140603508307712 tf_logging.py:115] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I1206 05:05:46.811589 140603508307712 tf_logging.py:115] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I1206 05:05:46.848017 140603508307712 tf_logging.py:115] depth of additional conv before box predictor: 0
Traceback (most recent call last):
  File ""train.py"", line 168, in <module>
    tf.app.run()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 164, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/ubuntu/models/research/object_detection/trainer.py"", line 273, in train
    train_config.load_all_detection_checkpoint_vars))
**TypeError: restore_map() got an unexpected keyword argument 'from_detection_checkpoint'**
",1,"NamedUser(login=""msymp"")","[NamedUser(login=""msymp"")]",2018-12-06 10:23:06,open,,,['type:support'],2018-12-28 01:07:31
349,tensorflow/models,models,5868,hamadafath,"When training attention_ocr model on my cpu i had error: Assign requires the size of two tensors to be the same. lhs shape =[265,26]  rhs shape = [265,134] but when i train it on gpu this issue disappear. So, how could i solve that issue please??",,1,"NamedUser(login=""alexgorban"")","[NamedUser(login=""alexgorban"")]",2018-12-05 10:31:24,open,,,['stat:awaiting response'],2018-12-12 00:06:00
350,tensorflow/models,models,5867,monckxqq,[object detection] AttributeError: 'module' object has no attribute 'CollectiveAllReduceStrategy',"### System information
- **What is the top-level directory of the model you are using**:object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Centos 7.5
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.9.0
- **Bazel version (if compiling from source)**:0.19
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:Titan 1080 Ti / 8G
- **Exact command to reproduce**:No
- **Distribute environment**:2 machines with 8 GPUs each

### Describe the problem
I want to run a distributed training on object detection.
At first, I run the file 'model_main.py' by simply setting the 'TF_CONFIG' in the command and it runs smoothly.  Is this a correct way to run a distributed training?
But this way seems to be an asynchronous training.
And in order to run a synchronous training,  I config the 'RunConfig' as blow, as it mentioned at [Distribution Strategy](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute):
```
config = tf.estimator.RunConfig(train_distribute=tf.contrib.distribute.CollectiveAllReduceStrategy())
```
it reported:
```
AttributeError: 'module' object has no attribute 'CollectiveAllReduceStrategy'.
```
How can I start a synchronous training on object detection?
",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-12-05 07:08:04,open,,,[],2018-12-07 14:06:21
351,tensorflow/models,models,5865,zhinengshidai,How to install graph-tool?,"########################Problem####################################
-----------------------------------------------------------------------------------------------------------
when I run ""scripts/script_preprocess_annoations_S3DIS.sh"" 
The result show :
Traceback (most recent call last):
  File ""scripts/script_preprocess_annoations_S3DIS.py"", line 23, in <module>
    from datasets import nav_env
  File ""/media/rosfun/work/CMP/cognitive_mapping_and_planning/datasets/nav_env.py"", line 40, in <module>
    import graph_tool as gt
ModuleNotFoundError: No module named 'graph_tool'
--------------------------------------------------------------------------------------------------------------
Then i run ""conda install graph-tool""
The result show:
Solving environment: failed

CondaHTTPError: HTTP 404 NOT FOUND for url <https://conda.anaconda.org/rwes/noarch/repodata.json>
Elapsed: 00:00.481398
CF-RAY: 483f1ec10ef25402-LAX

The remote server could not find the noarch directory for the
requested channel with url: https://conda.anaconda.org/rwes

As of conda 4.3, a valid channel must contain a `noarch/repodata.json` and
associated `noarch/repodata.json.bz2` file, even if `noarch/repodata.json` is
empty. please request that the channel administrator create
`noarch/repodata.json` and associated `noarch/repodata.json.bz2` files.
$ mkdir noarch
$ echo '{}' > noarch/repodata.json
$ bzip2 -k noarch/repodata.json

You will need to adjust your conda configuration to proceed.
Use `conda config --show channels` to view your configuration's current state.
Further configuration help can be found at <https://conda.io/docs/config.html>.
-------------------------------------------------------------------------------------------------------------------

### Comput system:(without GPU)
ubunt16
conda 4.5.8
",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-12-05 01:28:29,open,,,['stat:awaiting response'],2018-12-07 00:21:43
352,tensorflow/models,models,5864,maddyobrienjones,Object Detection API - Gradient Clipping,"### System information
- **What is the top-level directory of the model you are using**: /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/models/facessd_mobilenet_v2_quantized_320x320_open_image_v4
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 19.0
- **TensorFlow installed from (source or binary)**: Deep Learning AMI on AWS
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: V9.0.176
- **GPU model and memory**: p2.xlarge
- **Exact command to reproduce**: PIPELINE_CONFIG_PATH=/home/ubuntu/anaconda3/lib/python3.6/site-packag
es/tensorflow/models/research/facessd_mobilenet_v2_quantized_320x320_open_image_v4/pipeline.config
MODEL_DIR=/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorfl
ow/models/research/facessd_mobilenet_v2_quantized_320x320_open_image_v4
NUM_TRAIN_STEPS=50000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python object_detection/model_main.py \
>     --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
>     --model_dir=${MODEL_DIR} \
>     --num_train_steps=${NUM_TRAIN_STEPS} \
>     —sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
>     --alsologtostderr

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am working on a face detection problem detecting small faces in large cluttered images. Every time I have tried training the model, my loss has exploded and become a NaN value. I've tried reorganizing my directories, double checking the coordinates of my bounding boxes, resizing my images, using both the FaceSSD model and the Faster RCNN Inception V2 Coco model, changing from RMS prop optimizer to Adam optimizer, decreasing my learning rate, and using legacy/train.py instead of model_main.py.

I am looking for advice on how to integrate gradient clipping into my model to avoid this happening. Is this done through the pipeline.config file or through the optimizerbuilder.py file? Any help/advice would be appreciated.

### Source code / logs
Traceback: 
ERROR:tensorflow:Model diverged with loss = NaN.
E1204 15:30:06.767873 140283027941120 tf_logging.py:105] Model diverged with loss = NaN.
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
    saving_listeners)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1471, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1156, in run
    run_metadata=run_metadata)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    raise six.reraise(*original_exc_info)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1240, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1320, in run
    run_metadata=run_metadata))
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 754, in after_run
    raise NanLossDuringTrainingError
tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.
",0,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-12-05 00:44:03,open,,,[],2018-12-07 00:21:23
353,tensorflow/models,models,5863,biboffff,"mIoU Calculation Cityscapes, class weights","### System information
- **What is the top-level directory of the model you are using**: Deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12
- **CUDA/cuDNN version**: 9.0

Do you use any class weights for the mIoU calculation for the cityscapes validation set?

I try to calculate the IoU per image with a custom python script. For a tfrecord containing a single image I got the exact confusion matrix as tensorflow calculates during validation, however the final/printed mIoU value differs from mine.

best regards,
chris



",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-12-04 19:52:56,open,,,[],2018-12-07 12:14:41
354,tensorflow/models,models,5857,yghcats,python inference.py error?,"System information
What is the top-level directory of the model you are using: ~/models/research/struct2depth
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below):1.4
Bazel version (if compiling from source): N/A
CUDA/cuDNN version:CUDA:8.0,cudnn:8.0-v6.0
GPU model and memory:GeForce GTX 12206MiB
Exact command to reproduce:python inference.py --input_dir ~/models/research/struct2depth/data --output_dir ~/models/research/struct2depth/output --model_ckpt ~/models/struct2depth/model/model.ckpt-56286 --file_extension png --depth --egomotion true

I try to set up:LC_ALL=en_US.UTF-8
But it has error:

  File ""inference.py"", line 157, in _run_inference
    joint_encoder=joint_encoder)
  File ""/home/launchx-500-16/models/research/struct2depth/model.py"", line 166, in __init__
    util.count_parameters()
  File ""/home/launchx-500-16/models/research/struct2depth/util.py"", line 151, in count_parameters
    format_number(shape.num_elements()))
  File ""/home/launchx-500-16/models/research/struct2depth/util.py"", line 236, in format_number
    locale.setlocale(locale.LC_ALL, 'en_US')
  File ""/usr/lib/python2.7/locale.py"", line 581, in setlocale
    return _setlocale(category, locale)
locale.Error: unsupported locale setting",4,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-12-04 03:51:55,open,,,[],2018-12-17 08:44:19
355,tensorflow/models,models,5856,louisquinn,Object Detection use_bfloat16 error.,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Installed with pip
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.1
- **GPU model and memory**: 1080ti 
- **Exact command to reproduce**: Standard training job with use_bfloat16=true in pipeline.config

### Describe the problem
I am attempting to train and object detection model using bfloat16 ops.
When setting ""use_bfloat16"" to ""true"" from protos/train.proto with Faster RCNN meta, I am getting the following exception:

**_TypeError: Value passed to parameter 'image' has DataType bfloat16 not in list of allowed values: uint8, uint16, int8, int16, int32, int64, float16, float32, float64_**

After some quick debugging, this is coming from when tf.image.crop_and_resize is called, which does not support bfloat16 images. 

### Source code / logs
_TypeError: Value passed to parameter 'image' has DataType bfloat16 not in list of allowed values: uint8, uint16, int8, int16, int32, int64, float16, float32, float64_

**in file:**

tensorflow-env-1.12-python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint
",0,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-12-04 03:50:22,open,,,[],2018-12-05 00:17:52
356,tensorflow/models,models,5855,WeiyiLi,NanLossDuringTrainingError when training mnist,"### System information
- **What is the top-level directory of the model you are using**: /models/official/mnist
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 18.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:1.12
- **Bazel version (if compiling from source)**: 0.15.0
- **CUDA/cuDNN version**:CPU-ONLY
- **GPU model and memory**:

### Describe the problem
I tried the mnist in [models/official/mnist](https://github.com/tensorflow/models/tree/master/official/mnist) and run it using command  `python3 mnist.py --export_dir ./mnist_saved_models
`
But it has error 
```
raise NanLossDuringTrainingError
tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.
```
I tried to reduce the learning rate in mnist.py but this error still exists. 
The source code is from https://github.com/tensorflow/tensorflow/releases/tag/v1.12.0
### Source code / logs
```
I1203 15:30:05.617134 140284692195136 tf_logging.py:115] Initializing RunConfig with distribution strategies.
I1203 15:30:05.617341 140284692195136 tf_logging.py:115] Not using Distribute Coordinator.
I1203 15:30:05.617615 140284692195136 tf_logging.py:115] Using config: {'_model_dir': '/tmp/mnist_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.one_device_strategy.OneDeviceStrategy object at 0x7f95f2f2f780>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f95f2f2f7f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}
I1203 15:30:05.661948 140284692195136 tf_logging.py:115] Calling model_fn.
I1203 15:30:06.011193 140284692195136 tf_logging.py:115] Done calling model_fn.
I1203 15:30:06.041876 140284692195136 tf_logging.py:115] Create CheckpointSaverHook.
I1203 15:30:06.174833 140284692195136 tf_logging.py:115] Graph was finalized.
I1203 15:30:06.178840 140284692195136 tf_logging.py:115] Restoring parameters from /tmp/mnist_model/model.ckpt-0
I1203 15:30:06.292510 140284692195136 tf_logging.py:115] Running local_init_op.
I1203 15:30:06.299429 140284692195136 tf_logging.py:115] Done running local_init_op.
I1203 15:30:06.514888 140284692195136 tf_logging.py:115] Saving checkpoints for 0 into /tmp/mnist_model/model.ckpt.
I1203 15:30:10.928804 140284692195136 tf_logging.py:115] cross_entropy = 5.2012495e+22, learning_rate = 1e-04, train_accuracy = 0.15
I1203 15:30:10.929588 140284692195136 tf_logging.py:115] loss = 5.2012495e+22, step = 0
E1203 15:30:11.142082 140284692195136 tf_logging.py:105] Model diverged with loss = NaN.
2018-12-03 15:30:11.148204: W tensorflow/core/kernels/data/cache_dataset_ops.cc:770] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the datasetwill be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Traceback (most recent call last):
  File ""mnist.py"", line 236, in <module>
    absl_app.run(main)
  File ""/home/kathy/.local/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/kathy/.local/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""mnist.py"", line 230, in main
    run_mnist(flags.FLAGS)
  File ""mnist.py"", line 211, in run_mnist
    mnist_classifier.train(input_fn=train_input_fn, hooks=train_hooks)
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1205, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1352, in _train_model_distributed
    saving_listeners)
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1471, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1156, in run
    run_metadata=run_metadata)
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    raise six.reraise(*original_exc_info)
  File ""/home/kathy/.local/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1240, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1320, in run
    run_metadata=run_metadata))
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 753, in after_run
    raise NanLossDuringTrainingError
tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.
```
Please help out!",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-12-03 23:32:38,open,,,['stat:awaiting response'],2018-12-05 00:18:15
357,tensorflow/models,models,5852,canozcivelek,ImportError: No module named utils When running the tutorial notebook,"Hi!

Just installed tensorflow and dependencies etc. Now I'm trying to just run the tutorial notebook via Google Colab. However, even though I have installed ""utils"" package, it gives me the ""ImportError: No module named utils"" what am I missing here?

Thanks!",4,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-12-02 18:31:37,open,,,[],2018-12-04 12:16:56
358,tensorflow/models,models,5851,Jooseba,Missing images at the end of object_detection_tutorial.ipynb,"So I have been straight now trying to install tensorflow with object_detection about 20h for two computers without any succsess. I don't get any errors but I haven't seen the dog and beach picture at all. Any suggestions why it is not working? Anyone else has this problem?
### System informationg
- **Tenso\models**:
- **Win 10 pro 64-bit both computers**:
- **This site**:
- **1.12 GPU**:
- **CUDA/cuDNN version 9/7**:
- **GTX970 4GB and GTX1060 6GB mobile**:


### Describe the problem
-I can start the object_detection_tutorial.ipynb, but I don't get the test images neither any error messages. It goes well to end of the script but the images are missing.

### Source code / logs
-orginal code, straight from here.
",6,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-12-02 16:36:04,open,,,['stat:awaiting response'],2019-03-20 05:54:39
359,tensorflow/models,models,5850,blairhan,fix error when gradient is None,"When I trained ResNet in FP16, it gave an error since some gradients' values are None and cannot be divided by the loss_scale. ",4,,[],2018-12-02 15:58:41,open,,,['cla: no'],2019-03-24 12:02:34
360,tensorflow/models,models,5849,thedirtyyugo,cant install software on python using code.,"- **What is the top-level directory of the model you are using**: python 3.7
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: a little here and there on lenux
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:64-bit OS, x64 based processor **TensorFlow installed from (source or binary)**:  tensorflow-1.12.0-cp36-cp36m-win_amd64.whl (website)
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**:N/A
- **CUDA/cuDNN version**: cudnn-9.0-windows10-x64-v7.4.1.
- **GPU model and memory**:NVIDIA GeForce GTX 1070
- **Exact command to reproduce**: I used the code below, python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION) and got a syntax error  EOL while scanning string literal.

2018-12-01 22:15:38.357265: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_dnn.cc:378] Loaded runtime CuDNN library: 7401 (compatibility version 7400) but source was compiled with 7003 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.

I was also wondering if anyone knows what the code above means?
------------------------

### System information


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""


",6,"NamedUser(login=""gunan"")","[NamedUser(login=""gunan"")]",2018-12-02 03:16:56,open,,,[],2018-12-17 23:00:01
361,tensorflow/models,models,5848,quackmonger,[deeplab] Input/Output tensors,How do we find out the names of the Input/Output tensors? I am trying to run inference (forward pass) on a sample input and I have the frozen inference graph.,1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-12-02 02:42:25,open,,,['stat:awaiting response'],2018-12-03 00:17:05
362,tensorflow/models,models,5847,quackmonger,[deeplab] 16-bit Grayscale Images,How can we adapt the base implementation of Deeplab v3 (in tensorflow/models) for 16 bit grayscale png images? It seems like it's expecting 8 bit RGB jpg images. ,1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-12-02 02:40:02,open,,,['stat:awaiting response'],2018-12-03 00:17:01
363,tensorflow/models,models,5846,ghost,Get multi class detection precision in COCO evaluation,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: /home/trlab/Documents/Research/python_research/models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I changed /home/trlab/Documents/Research/python_research/models/research/object_detection/anchor_generators/grid_anchor_generator.py to get my custom size anchor. 

'''
  def __init__(self,
               scales=(0.5, 1.0, 2.0),
               aspect_ratios=(0.5, 1.0, 2.0),
'''
and 
'''
    # Handle argument defaults
    if base_anchor_size is None:
      base_anchor_size = [128, 64]
    base_anchor_size = tf.to_float(tf.convert_to_tensor(base_anchor_size))
'''
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: installed from pip (I guess it is binary)
- **TensorFlow version (use command below)**: 1.9.0
- **Bazel version (if compiling from source)**: (I guess I am on binary)
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Tesla V100 16GB
- **Exact command to reproduce**: CUDA_VISIBLE_DEVICES=0  python3 model_main.py --logtostderr --model_dir=training/chenoho/chenho_multi/cheonho_multi_200/1  --pipeline_config_path=training/faster_rcnn_resnet50_coco.config


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

This is not a bug; however, I am trying to use some function that is probably innate in tensorflow object detection API or MS COCO evaluation.

I use customized training and evaluation set. It has two classes; 'vehicle' and 'heavyVehicle'. When I run the code above, the results show like below.

![image](https://user-images.githubusercontent.com/30972892/49324389-d2b43300-f56f-11e8-811f-3f5c386f377f.png)

However, what I want to see is the precision metric for each of two classes. And also, it will be greatful if I can store the result as a txt file. 

I guess it is something related with 'coco_tools.py' and 'coco_evaluation.py' . These are on 'models/research/object_detection/metrics' folder.

There are some lines that seems to be affect the evaluation results. Such as 'include_metrics_per_category' or 'all_metrics_per_category'.

There might be more jobs needed to get a result. Simply changing those two factor into 'True' does not work. It produces 'Category stats do not exist' error.

Could you please help me? Thank you.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",4,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-12-01 04:52:23,open,,,[],2019-03-19 12:37:32
364,tensorflow/models,models,5845,christyChenYa,The replica ps 0 exited with a non-zero status of 1. Termination reason: Error. The replica ps 1 exited with a non-zero status of 1. Termination reason: Error. The replica ps 2 exited with a non-zero status of 1. Termination reason: Error. ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: Object Detection- fasterRCNN
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS
- **TensorFlow installed from (source or binary)**: official website
- **TensorFlow version (use command below)**: 1.11.0 (This is the version on my PC, but I wanted to train on Google Cloud)
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
gcloud ml-engine jobs submit training `whoami`_waterpolo_`date +%m_%d_%Y_%H_%M_%S` \
    --job-dir=gs://waterpolo_datax \
    --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz \
    --module-name object_detection.train \
    --config object_detection/samples/cloud/cloud.yml \
    -- \
    --pipeline_config_path=gs://waterpolo_datax/pipeline.config

**- **Source code:** 
import functools
import json
import os
import tensorflow as tf

from google.protobuf import text_format

from object_detection.legacy import trainer
from object_detection.builders import input_reader_builder
from object_detection.builders import model_builder
from object_detection.protos import input_reader_pb2
from object_detection.protos import model_pb2
from object_detection.protos import pipeline_pb2
from object_detection.protos import train_pb2

tf.logging.set_verbosity(tf.logging.INFO)

flags = tf.app.flags
flags.DEFINE_string('master', '', 'BNS name of the TensorFlow master to use.')
flags.DEFINE_integer('task', 0, 'task id')
flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')
flags.DEFINE_boolean('clone_on_cpu', False,
                     'Force clones to be deployed on CPU.  Note that even if '
                     'set to False (allowing ops to run on gpu), some ops may '
                     'still be run on the CPU if they have no GPU kernel.')
flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '
                     'replicas.')
flags.DEFINE_integer('ps_tasks', 0,
                     'Number of parameter server tasks. If None, does not use '
                     'a parameter server.')
flags.DEFINE_string('train_dir', '',
                    'Directory to save the checkpoints and training summaries.')

flags.DEFINE_string('pipeline_config_path', '',
                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '
                    'file. If provided, other configs are ignored')

flags.DEFINE_string('train_config_path', '',
                    'Path to a train_pb2.TrainConfig config file.')
flags.DEFINE_string('input_config_path', '',
                    'Path to an input_reader_pb2.InputReader config file.')
flags.DEFINE_string('model_config_path', '',
                    'Path to a model_pb2.DetectionModel config file.')

FLAGS = flags.FLAGS


def get_configs_from_pipeline_file():
  """"""Reads training configuration from a pipeline_pb2.TrainEvalPipelineConfig.

  Reads training config from file specified by pipeline_config_path flag.

  Returns:
    model_config: model_pb2.DetectionModel
    train_config: train_pb2.TrainConfig
    input_config: input_reader_pb2.InputReader
  """"""
  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
  with tf.gfile.GFile(FLAGS.pipeline_config_path, 'r') as f:
    text_format.Merge(f.read(), pipeline_config)

  model_config = pipeline_config.model
  train_config = pipeline_config.train_config
  input_config = pipeline_config.train_input_reader

  return model_config, train_config, input_config


def get_configs_from_multiple_files():
  """"""Reads training configuration from multiple config files.

  Reads the training config from the following files:
    model_config: Read from --model_config_path
    train_config: Read from --train_config_path
    input_config: Read from --input_config_path

  Returns:
    model_config: model_pb2.DetectionModel
    train_config: train_pb2.TrainConfig
    input_config: input_reader_pb2.InputReader
  """"""
  train_config = train_pb2.TrainConfig()
  with tf.gfile.GFile(FLAGS.train_config_path, 'r') as f:
    text_format.Merge(f.read(), train_config)

  model_config = model_pb2.DetectionModel()
  with tf.gfile.GFile(FLAGS.model_config_path, 'r') as f:
    text_format.Merge(f.read(), model_config)

  input_config = input_reader_pb2.InputReader()
  with tf.gfile.GFile(FLAGS.input_config_path, 'r') as f:
    text_format.Merge(f.read(), input_config)

  return model_config, train_config, input_config


def main(_):
  assert FLAGS.train_dir, '`train_dir` is missing.'
  if FLAGS.pipeline_config_path:
    model_config, train_config, input_config = get_configs_from_pipeline_file()
  else:
    model_config, train_config, input_config = get_configs_from_multiple_files()

  model_fn = functools.partial(
      model_builder.build,
      model_config=model_config,
      is_training=True)

  create_input_dict_fn = functools.partial(
      input_reader_builder.build, input_config)

  env = json.loads(os.environ.get('TF_CONFIG', '{}'))
  cluster_data = env.get('cluster', None)
  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None
  task_data = env.get('task', None) or {'type': 'master', 'index': 0}
  task_info = type('TaskSpec', (object,), task_data)
  
  # Parameters for a single worker.
  ps_tasks = 0
  worker_replicas = 1
  worker_job_name = 'lonely_worker'
  task = 0
  is_chief = True
  master = ''

  if cluster_data and 'worker' in cluster_data:
    # Number of total worker replicas include ""worker""s and the ""master"".
    worker_replicas = len(cluster_data['worker']) + 1
  if cluster_data and 'ps' in cluster_data:
    ps_tasks = len(cluster_data['ps'])

  if worker_replicas > 1 and ps_tasks < 1:
    raise ValueError('At least 1 ps task is needed for distributed training.')

  if worker_replicas >= 1 and ps_tasks > 0:
    # Set up distributed training.
    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',
                             job_name=task_info.type,
                             task_index=task_info.index)
    if task_info.type == 'ps':
      server.join()
      return

    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)
    task = task_info.index
    is_chief = (task_info.type == 'master')
    master = server.target

  trainer.train(create_input_dict_fn, model_fn, train_config, master, task,
                FLAGS.num_clones, worker_replicas, FLAGS.clone_on_cpu, ps_tasks,
                worker_job_name, is_chief, FLAGS.train_dir)


if __name__ == '__main__':
  tf.app.run()

",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-30 20:28:08,open,,,[],2019-02-16 10:11:47
365,tensorflow/models,models,5841,tech509201941,word2vec: AttributeError: 'module' object has no attribute 'get_compile_flags',"I think it's a bug, please delete if not.
I try to start the following model word2vec example from the repository:

https://github.com/tensorflow/models/tree/master/tutorials/embedding

The data preparation works:

```
curl http://mattmahoney.net/dc/text8.zip > text8.zip
unzip text8.zip
curl https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip > source-archive.zip
unzip -p source-archive.zip  word2vec/trunk/questions-words.txt > questions-words.txt
rm text8.zip source-archive.zip
```

But the attempt to add the custom tensorflow operation fails like described in the following:

```
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
```

Results in:

```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'get_compile_flags'
```

The newest tensorflow version (```v1.12.0```) is installed via pip:

```
pip install --upgrade tensorflow
```

The python version is ```v2.7.15```.
",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-30 12:08:27,open,,,['stat:awaiting response'],2018-12-01 12:14:15
366,tensorflow/models,models,5837,wangxianrui,Quantized model in detection_model_zoo,"### System information
- **What is the top-level directory of the model you are using**:
`/models/research`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
`NO`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
` Linux Ubuntu 16.04
`- **TensorFlow installed from (source or binary)**:
`binary`
- **TensorFlow version (use command below)**:
`1.12`
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
`cuda9.0`
- **GPU model and memory**:
`1080Ti
`- **Exact command to reproduce**:

### Describe the problem
In the detection_model_zoo https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md, I found some quantized models such as `ssd_mobilenet_v2_coco` and `ssd_mobilenet_v2_quantized_coco`, So.. How can I get the quantized model from origional model? which method should I use,` tf.contrib.quantize` and `tf.contrib.lite` ?
But the saved model named `tflite_graph.pb`, not suffix with '.tflite',  may be another quantiz method?",6,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1""), NamedUser(login=""pkulzc"")]",2018-11-30 01:15:56,open,,,[],2018-12-10 16:52:15
367,tensorflow/models,models,5835,JakeWade5,No OPKernel was registered to support Op 'Equal' with these attrs,"Im getting the following error when trying to load my TensorFlow model, im using the following tools:

Xcode (Latest)
TensorFlow Experimental V1.1.1 (Installed using pods)
C++

What is the top-level directory of the model you are using: /Model
Have I written custom code: No custom Tensorflow code, just c++ to run the model
OS Platform and Distribution: iOS
TensorFlow installed from: Coca Pods
TensorFlow version: Tensorflow Experimental latest (installed from pods)
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: iOS device integrated gpu
Exact command to reproduce: The following code below

Im able to create the TensorFlow session with no issues, and I'm able to read the TensorFlow model into a GraphDef, but when I try to create the model using the active TensorFlow session I get the following issue.

> Invalid argument: No OpKernel was registered to support Op 'Equal' with these attrs.  

> Registered devices: 

> [CPU]

> Registered kernels: 
> 
device='CPU'; T in [DT_FLOAT] [[Node: Equal = Equal[T=DT_INT32, _output_shapes=[[]]](Rank, Equal)]]

This is how my code currently looks

    tensorflow::Session* sessionPointer = nullptr;
    const tensorflow::SessionOptions options;
    
    tensorflow::Status newSessionStatus = tensorflow::NewSession(options, &sessionPointer);
    
    [MachineLearning checkTensorflowStatus:newSessionStatus];
    
    std::unique_ptr<tensorflow::Session> session(sessionPointer);
    
    // Create a point to the tensorflow graph
    tensorflow::GraphDef tensorflowGraph;
    
    // Get the model file path
    NSString* filePath = FilePathForResourceName(@""frozen_inference_graph"", @""pb"");
    
    // Read the data into the TensorflowGraph
    auto fileConverteredToProto = PortableReadFileToProto([filePath UTF8String], &tensorflowGraph);
    
    // Create the tensorflow graph
    tensorflow::Status createGraphStatus = session->Create(tensorflowGraph);
    
    // Check the status of the tensorflow graph
    [MachineLearning checkTensorflowStatus:createGraphStatus];

I have checked that the file path it is getting is correct and it seems fine with loading the model data into the GraphDef. This is how I do it

    // Finds the file path for the given file name
    NSString* FilePathForResourceName(NSString* name, NSString* extension)
    {
        NSString* filePath = [[NSBundle mainBundle] pathForResource:name ofType:extension];
        if (filePath == NULL)
        {
            LOG(ERROR) << ""Could not find the TensorFlow model"";
        }
        return filePath;
    }

    // Reads the file into the protobuf. This was taken from the TensorFlow example code
    bool PortableReadFileToProto(const std::string& fileName, ::google::protobuf::MessageLite* proto)
    {
        // Create the input stream with the given file name
        ::google::protobuf::io::CopyingInputStreamAdaptor stream(new IfstreamInputStream(fileName));
    
        // Set that this object owns the copying stream
        stream.SetOwnsCopyingStream(true);
    
        // Create a coded input stream
        ::google::protobuf::io::CodedInputStream codedStream(&stream);

        // Set the byte limit, aparently having protos too big can causes issues, so we limit it.
        codedStream.SetTotalBytesLimit(1024LL << 20, 512LL << 20);
    
        // Parse the coded stream
        return proto->ParseFromCodedStream(&codedStream);
    }



I did some reading about OP's not been supported in other posts, but Equal is already supported but it doesn't work in the way that I need it too for my TensorFlow model.

I was also reading this guide on how to add a new OP.
[Tensorflow how to add a new OP][1]


  [1]: https://www.tensorflow.org/guide/extend/op

But I don't really understand how it all works, so im struggling to get my model running.

I would have preferred to convert my TensorFlow model to CoreML and used the Swift CoreML tools, but there is an issue with the conversion, where the Slice OP isn't fully supported and it can't convert my model.",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-11-29 11:28:37,open,,,[],2018-12-01 00:22:43
368,tensorflow/models,models,5834,ElizaNami,error on compiling object_detection_tutorial,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",3,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-11-29 06:31:51,open,,,[],2019-01-02 17:26:53
369,tensorflow/models,models,5833,linmelon,batch_non_max_suppression score_threshold ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

why the parameters batch_non_max_suppression {score_threshold:1e-8} in samples  is different from the model zoo config(batch_non_max_suppression {score_threshold: 0.3)?",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-11-29 03:18:58,open,,,['stat:awaiting response'],2018-12-03 16:45:57
370,tensorflow/models,models,5831,muralisb,"Mask RCNN with tensorflow API: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(""arg0:0"", shape=(), dtype=float32, device=/device:CPU:0)'","
### System information
- **What is the top-level directory of the model you are using**:
My Repo is in E: drive and I have created a folder with a path ""E:\Object_Detection\Masked-RCNN\Custom-Mask-RCNN-using-Tensorfow-Object-detection-API-master""

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I am trying to replicate the work done by vijendra and this is the lnk to his repo
https://github.com/vijendra1125/Custom-Mask-RCNN-using-Tensorfow-Object-detection-API

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
I am working on windows 10 platform
- **TensorFlow installed from (source or binary)**:
tensorflow was installed using pip install command (tensorflow-1.9.0-cp36-cp36m-win_amd64.whl)

- **TensorFlow version (use command below)**: 'v1.9.0-0-g25c197e023' 1.9.0

- **GPU model and memory**:
my laptop has 16 gb RAM and 1 TB memory and no GPU

- **Exact command to reproduce**:
(base) C:\Users\nVIALP2\Documents\Python_Scripts\Tensorflow\models\research>python object_detection/legacy/train.py --train_dir=E:\Object_Detection\Masked-RCNN\Custom-Mask-RCNN-using-Tensorfow-Object-detection-API-master\CP --pipeline_config_path=E:\Object_Detection\Masked-RCNN\Custom-Mask-RCNN-using-Tensorfow-Object-detection-API-master\mask_rcnn_inception_v2_coco.config

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

### Describe the problem
I am trying to build a Custom Mask RCNN using Tensorfow Object detection API that uses used pre-trained mask RCNN which is trained with inception V2 as feature extracter. I have used pixel annotation tool for creaking mask of hand images and was trying to train the model using my own dataset.

### Source code / logs


WARNING:tensorflow:From C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py:125: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
W1128 16:57:42.430089 15960 tf_logging.py:125] From C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py:125: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
WARNING:tensorflow:From C:\Users\nVIALP2\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\legacy\trainer.py:265: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
W1128 16:57:42.542089 15960 tf_logging.py:125] From C:\Users\nVIALP2\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\legacy\trainer.py:265: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 0 to match input file shards.
W1128 16:57:42.570088 15960 tf_logging.py:125] num_readers has been reduced to 0 to match input file shards.
Traceback (most recent call last):
  File ""object_detection/legacy/train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""object_detection/legacy/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\legacy\trainer.py"", line 279, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\legacy\trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""object_detection/legacy/train.py"", line 121, in get_next
    dataset_builder.build(config)).get_next()
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\dataset_builder.py"", line 134, in build
    config.input_path[:], input_reader_config)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\dataset_builder.py"", line 80, in read_dataset
    sloppy=config.shuffle))
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1002, in apply
    dataset = transformation_func(self)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\contrib\data\python\ops\interleave_ops.py"", line 88, in _apply_fn
    buffer_output_elements, prefetch_input_elements)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\data\ops\readers.py"", line 130, in __init__
    cycle_length, block_length)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1988, in __init__
    super(InterleaveDataset, self).__init__(input_dataset, map_func)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1957, in __init__
    self._map_func.add_to_graph(ops.get_default_graph())
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\framework\function.py"", line 475, in add_to_graph
    self._create_definition_if_needed()
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\framework\function.py"", line 331, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\framework\function.py"", line 340, in _create_definition_if_needed_impl
    self._capture_by_value, self._caller_device)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\framework\function.py"", line 804, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1945, in tf_map_func
    dataset = map_func(nested_args)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\data\ops\readers.py"", line 196, in __init__
    filenames = ops.convert_to_tensor(filenames, dtype=dtypes.string)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1011, in convert_to_tensor
    as_ref=False)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1107, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\nVIALP2\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 944, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(""arg0:0"", shape=(), dtype=float32, device=/device:CPU:0)'
",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-11-28 22:13:05,open,,,['stat:awaiting response'],2019-02-05 15:56:35
371,tensorflow/models,models,5829,joeyguerra,`.vscode` isn't ignored in Git repo,Just adding `.vscode` to .gitignore.,1,,[],2018-11-28 19:17:22,open,,,['cla: no'],2018-11-28 19:17:27
372,tensorflow/models,models,5828,TitusTom,Change IoU threshold for mAP calculation when using the Object detection API for evaluation.,"## System information

- **Have I written custom code:**  
`No`
- **OS Platform and Distribution** : 
`Ubuntu 16.06`
- **TensorFlow installed from** : 
`pip install tensorflow-cpu==1.5.1`
- **TensorFlow version**: 
`1.5.1`
- **Bazel version:** 
`N/A`
- **CUDA/cuDNN version:**  
`9.0`
- **GPU model and memory:** 
`N/A`
- **Exact command to reproduce** : 
```
python object_detection/eval.py \
	--logtostderr \
	--pipeline_config_path=object_detection/training/faster_rcnn_inception_v2_pets.config \
	--checkpoint_dir=object_detection/training/ \
	--eval_dir=object_detection/training/
```

## Describe the problem

After scouring the documentation, github and SO , I cannot find clear institutions on how to change the IoU threshold for evaluation. This is set at 0.5. How would one go about changing the same?

I noticed utils/object_detection_evaluation is what controls the metrics calculation. Do i change up all the iou values in all the metrics here? Also when running eval.py, by default are all the metrics there used to calcualte mAP?

I am not explicitly defining any metric in the config file.

Edit: 
I changed up the threshold from 0.5 to different values like 0.9 and 0.6 in utils/object_detection_evaluation.py . I also tried setting the metric to pascalvoc in my confic file using the following metrics_set:""pascal_voc_detection_metrics"" . Both of these changes do not show any change in the results in tensorboard.

```
eval.config below
eval_config: {
  num_examples: 800
  max_evals: 1
  metrics_set:""pascal_voc_detection_metrics""
}
",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-28 14:18:05,open,,,[],2019-01-19 00:13:44
373,tensorflow/models,models,5827,egg-west,fix a bug in rnn\ptb model that deals with '\n' incorrectly,"In line:33 and line:35, if `'\n'` is replaced by '<eof>', the the `<eof>` will be concatenated with the word before it and after it.
**for example:**
>I would like\n
>Do you?

After processed in this initial code, it would become `like<eof>Do`
And this token makes no sense.

So I modify it to ' <eof> ' with two space before and after it.
Then after being splited, the example mentioned above would be 
> I would like \<eof\> Do you",4,,[],2018-11-28 12:46:46,open,,,['cla: yes'],2018-11-28 13:08:45
374,tensorflow/models,models,5826,baihualinxin,ssd_mobilenet_v1_300x300_coco14_sync Unable to train,"System information
What is the top-level directory of the model you are using:models-master\research\object_detection
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 16.04 windwos 10
TensorFlow installed from (source or binary):pip
TensorFlow version (use command below):1.10.0
Bazel version (if compiling from source):0.17.01
CUDA/cuDNN version:cuda 9 cudnn 8
GPU model and memory:GTX 1080Ti
Exact command to reproduce:
You can collect some of this information using our environment capture script:
b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0

https://github.com/tensorflow/models/commits/master/research/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config

```
 SSD with Mobilenet v1 feature extractor and focal loss.
 Trained on COCO14, initialized from Imagenet classification checkpoint

 Achieves 19.3 mAP on COCO14 minival dataset. Doubling the number of training
 steps gets to 20.6 mAP.

 This config is TPU compatible

model {
  ssd {
    inplace_batchnorm_update: true
    freeze_batchnorm: false
    num_classes: 1
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    encode_background_as_zeros: true
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        class_prediction_bias_init: -4.6
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            random_normal_initializer {
              stddev: 0.01
              mean: 0.0
            }
          }
          batch_norm {
            train: true,
            scale: true,
            center: true,
            decay: 0.97,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v1'
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.97,
          epsilon: 0.001,
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    loss {
      classification_loss {
        weighted_sigmoid_focal {
          alpha: 0.75,
          gamma: 2.0
        }
      }
      localization_loss {
        weighted_smooth_l1 {
          delta: 1.0
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    normalize_loc_loss_by_codesize: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  #fine_tune_checkpoint: ""PATH_TO_BE_CONFIGURED/model.ckpt""
  batch_size: 16
  sync_replicas: true
  startup_delay_steps: 0
  replicas_to_aggregate: 8
  num_steps: 10000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  #data_augmentation_options {
    #ssd_random_crop {
    #}
  #}
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: 0.9
          total_steps: 10000
          warmup_learning_rate: 0.3
          warmup_steps: 300
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""G:/image/train.record""
  }
  label_map_path: ""G:/kitti_label_map.pbtxt""
}

eval_config: {
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
  num_examples: 8000
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""G:/test/image/train.record""
  }
  label_map_path: ""G:/kitti_label_map.pbtxt""
  shuffle: false
  num_readers: 1
}

```


![default](https://user-images.githubusercontent.com/13042809/49150205-23951300-f347-11e8-9914-2a87e074f384.PNG)
",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-11-28 11:54:39,open,,,[],2018-12-04 03:01:08
375,tensorflow/models,models,5823,stoneyang,"How to use summary graph tools when installing tf through pip, with --user option?","As one working on a linux server with other people, tensorflow is installed through `pip`, with `--user` option:
```
pip install tensorflow-gpu==versionNum --user
```
When using `summary_graph`, I cannot found it at the directory where tf was installed. 

Any suggestions?

Thanks!",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-28 08:22:09,open,,,[],2018-11-30 00:18:05
376,tensorflow/models,models,5822,xuexue577,I wan to ask some questions about《Facial Landmark Detection with Tweaked Convolutional Neural Networks》,"Hello, would it be convenient for you to give me your email address?I have some questions to ask you about the recurrence of this article.",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-11-28 02:17:25,open,,,['stat:awaiting response'],2018-11-29 00:17:14
377,tensorflow/models,models,5821,cclauss,unicode() was removed in Python 3,__unicode()__ was removed in Python 3 because all __str__ are Unicode.  This PR proposes using [__six.text_type__](https://six.readthedocs.io/#six.text_type) instead.,0,,[],2018-11-27 18:22:32,open,,,['cla: yes'],2018-11-27 23:34:20
378,tensorflow/models,models,5820,cclauss,syntaxnet: Old style exceptions are syntax errors in Python 3,@calberti @markomernick Can we please get your review on this simple change that is compatible with both Python 2 and Python 3.  This is a syntax error.,0,,[],2018-11-27 16:34:58,open,,,['cla: yes'],2018-12-05 17:47:33
379,tensorflow/models,models,5819,cclauss,learning: print() is a function in Python 3,@lukemetz,0,,[],2018-11-27 16:29:56,open,,,['cla: yes'],2018-11-27 16:37:01
380,tensorflow/models,models,5817,niceyang123,changed activation,"For the faster_rcnn_inception_resnet_v2 model, can I use selu or other activations other than relu and relu_6?
",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-11-27 10:23:17,open,,,[],2018-11-30 00:51:21
381,tensorflow/models,models,5813,JingLiJJ,attention_ocr: run the whole test dataset,"System information
What is the top-level directory of the model you are using: attention_ocr
TensorFlow version (use command below): 1.8.0
CUDA/cuDNN version: 7.1.2

Describe the problem
Thanks a lot for your work, I have a question about eval.py. 
I know that you use 100*32 images to test model randomly. Now, I want to test model on the whole test datasets. So, I change tf.train.shuffle_batch to tf.train.batch in data_provider.py. Now, the data would be chosen in order. Then, I set num_batches to  dataset_size/batch_size. Am I right? Can the model test the whole test dataset now?

",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-26 09:48:57,open,,,['stat:awaiting response'],2018-11-27 12:19:41
382,tensorflow/models,models,5812,xinyuwl,limiting number of CPUs with intra_op_parallelism_threads ,"I'm trying to run models/official/mnist/mnist.py on CPU.
 I tried to limit the number of CPU threads used by Tensorflow to 1 by creating config like:
 
session_config = tf.ConfigProto(
      inter_op_parallelism_threads=1,#flags_obj.inter_op_parallelism_threads,
      intra_op_parallelism_threads,1,#flags_obj.intra_op_parallelism_threads,
      allow_soft_placement=True)

But unfortunately, that didn't help. I've tried various variations on this, However, none of it seems to work: all 48 logical cores on the cluster are used. In my opinion, the main problem we are having here is the fact that we are not able to control the number of threads here. Although we set it to 1 with various TF options you can actually see that this job is creating many more threads on the node.

P.S. I'm using Python 2.7 and Tensorflow 1.11.0.",3,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-11-26 07:30:29,open,,,"['models: official', 'stat:awaiting response']",2019-02-04 21:53:54
383,tensorflow/models,models,5809,cailiang9,There should be 2 ways for fast_mode?,https://github.com/tensorflow/models/blob/b2522f9bb9a559e148502a67ebd9a6592320cb0f/research/slim/preprocessing/inception_preprocessing.py#L230,2,,[],2018-11-23 09:54:24,open,,,[],2018-11-28 12:17:49
384,tensorflow/models,models,5807,dinghuanghao,TFLite have not support standard NMS in TFLite_Detection_PostProcess,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Darwin Kernel Version 17.7.0
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:v1.12.0-rc2
- **Bazel version (if compiling from source)**:0.18.1
- **CUDA/cuDNN version**:
- **GPU model and memory**:mobilenet-ssd
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The TFLite_Detection_PostProcess Operation is not support standard NMS（Non Maximal Suppression）, it use the fast NMS by default.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

TfLiteStatus NonMaxSuppressionMultiClass(TfLiteContext* context,
                                         TfLiteNode* node, OpData* op_data) {

  …………
  NonMaxSuppressionMultiClassFastHelper(context, node, op_data,
                                        GetTensorData<float>(scores));
  return kTfLiteOk;
}

// This function implements a fast version of Non Maximal Suppression for
// multiple classes where
// 1) we keep the top-k scores for each anchor and
// 2) during NMS, each anchor only uses the highest class score for sorting.
// 3) Compared to standard NMS, the worst runtime of this version is O(N^2)
// instead of O(KN^2) where N is the number of anchors and K the number of
// classes.
TfLiteStatus NonMaxSuppressionMultiClassFastHelper(TfLiteContext* context,
                                                   TfLiteNode* node,
                                                   OpData* op_data,
                                                   const float* scores){……}
",0,,[],2018-11-23 08:29:47,open,,,[],2018-11-23 08:29:47
385,tensorflow/models,models,5804,nareshmungpara,For how may epoch dose open image model on faster rcnn is trained?,"For how many steps does `faster_rcnn_inception_resnet_v2_atrous_oid` model is trained ?

I am trying to retrain it with open image data set for 600 classes and after around 1 lack steps i tried to test it using `object_detection_tutorial.ipynb` with that 2 images of dog and beach but there is no object detected.

So is this expected behaviour or there is some other issue?",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-11-22 12:05:49,open,,,['stat:awaiting response'],2018-11-23 00:15:28
386,tensorflow/models,models,5803,traderandreas,Tensorflow API Object Detection train.py crashes after INFO:tensorflow:Starting Queues.,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:tensorflow1
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:na
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows 10
- **TensorFlow installed from (source or binary)**:na
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:na
- **CUDA/cuDNN version**:9 and cudnn 9
- **GPU model and memory**:gtx 1050 4G
- **Exact command to reproduce**:python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""1.12

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I run the above command and get the following output

(tensorflow1) C:\Tensorflow1\models\research\object_detection>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\trainer.py:228: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\utils\dataset_util.py:130: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\ops\sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\core\preprocessor.py:2962: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\core\batcher.py:96: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).
WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\input.py:751: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\input.py:751: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\core\box_predictor.py:391: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\core\losses.py:306: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\meta_architectures\faster_rcnn_meta_arch.py:1952: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\ops\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
WARNING:tensorflow:From C:\Users\Andreas Shepley\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py:737: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-11-22 17:35:40.639199: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-11-22 17:35:42.000975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.30GiB
2018-11-22 17:35:42.065095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-22 17:35:45.385849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-22 17:35:45.412676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2018-11-22 17:35:45.437288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2018-11-22 17:35:45.455739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3020 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from C:/tensorflow1/models/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Starting Queues.

The program does not go any further? It stops at this point.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-11-22 08:28:21,open,,,[],2019-02-11 16:32:57
387,tensorflow/models,models,5802,tree89,"Error when training ""mask_rcnn_resnet101_pets"" with train.py","System information
What is the top-level directory of the model you are using: /home/user
Have I written custom code: No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.10.1
Bazel version (if compiling from source): I don't use it
CUDA/cuDNN version: CUDA 9.0 / cuDNN: 7.1
GPU model and memory: Geforce GTX 1070
Exact command to reproduce: Exact command to reproduce: 
python3 train.py --logtostderr --train_dir=data3 --pipeline_config_path=data3/mask_rcnn_resnet101_pets.config


#############################################################################
### [Situation]

### I just tried to train ""mask_rcnn_resnet101_pets"" with train.py
I made my training dataset(for test training to check that I can train it)
My test set is made by using ""create_pet_tf_record.py"" with that dataset image

http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz

I erased everything except the ""Bengal"" from image, annotation, all txt files.
(I just wanna check how to make own dataset and how to train it)

### I changed ""create_pet_tf_record.py"" little bit like under.
I added this code 

label_map_dict['Bengal'] = 1

on line# 159, cuz this code on line# 263

label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)

make only output empty dictionary {}

### so, I got a dataset serise for checking 

pets_fullbody_with_masks_train.record-00000-of-00010
~
pets_fullbody_with_masks_train.record-00009-of-00010

and 
pets_fullbody_with_masks_val.record-00000-of-00010
~
pets_fullbody_with_masks_val.record-00009-of-00010

### and I also changed mask_rcnn_resnet101_pets.config file and pet_label_map.pbtxt file like this

train_input_reader: {
  tf_record_input_reader {
    input_path: ###""data3/pet_fullbody_with_masks_train.record-?????-of-00010""
  }
  label_map_path: ###""data3/pet_label_map.pbtxt""
  load_instance_masks: true
  mask_type: PNG_MASKS
}

eval_config: {
  num_examples: 50
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ###""data3/pet_fullbody_with_masks_val.record-?????-of-00010""
  }
  label_map_path: ###""data3/pet_label_map.pbtxt""
  load_instance_masks: true
  mask_type: PNG_MASKS
  shuffle: false
  num_readers: 1
}

and

item {
  id: 1
  name: 'Bengal'
}

### I typed 
python3 train.py --logtostderr --train_dir=data3 --pipeline_config_path=data3/mask_rcnn_resnet101_pets.config
on my terminal

### I got error message
/home/cb/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
WARNING:tensorflow:From /home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py:125: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
WARNING:tensorflow:From /home/cb/Work/models/research/object_detection/legacy/trainer.py:265: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 0 to match input file shards.
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 272, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/home/cb/Work/models/research/object_detection/legacy/trainer.py"", line 279, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""/home/cb/Work/models/research/object_detection/legacy/trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""train.py"", line 121, in get_next
    dataset_builder.build(config)).get_next()
  File ""/home/cb/Work/models/research/object_detection/builders/dataset_builder.py"", line 134, in build
    config.input_path[:], input_reader_config)
  File ""/home/cb/Work/models/research/object_detection/builders/dataset_builder.py"", line 80, in read_dataset
    sloppy=config.shuffle))
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1109, in apply
    dataset = transformation_func(self)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/interleave_ops.py"", line 87, in _apply_fn
    buffer_output_elements, prefetch_input_elements)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py"", line 128, in __init__
    cycle_length, block_length)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2294, in __init__
    super(InterleaveDataset, self).__init__(input_dataset, map_func)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2257, in __init__
    experimental_nested_dataset_support=True)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1454, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 481, in add_to_graph
    self._create_definition_if_needed()
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 337, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 346, in _create_definition_if_needed_impl
    self._capture_by_value, self._caller_device)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 863, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1392, in tf_data_structured_function_wrapper
    ret = func(*nested_args)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py"", line 194, in __init__
    filenames = ops.convert_to_tensor(filenames, dtype=dtypes.string)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 998, in convert_to_tensor
    as_ref=False)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1094, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/cb/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 931, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(""arg0:0"", shape=(), dtype=float32, device=/device:CPU:0)'

### [Question]

How can I solve this problem? I don't know what is my mistake.
I have totally no idea, please help.

Any help be appriciated
Thanks
",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-22 07:51:53,open,,,[],2018-11-23 00:15:00
388,tensorflow/models,models,5801,D-X-Y,How to skip the evaluation procedure in object detection?,"### System information
- **What is the top-level directory of the model you are using**: `models/research/object_detection/`
- **OS Platform and Distribution **: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: `conda install tensorflow-gpu`
- **TensorFlow version (use command below)**: 1.12.0
- **CUDA/cuDNN version**: 9.0 / directly install tensorflow-gpu without manually installing 
- **GPU model and memory**: Titan 1080Ti
- **Have I written custom code**: NO
- **Bazel version**: N/A
- **Exact command to reproduce**: See below

### Describe the problem
I want to skip the evaluation procedure when training object detection model.
I use the official training codes as follows:
```
python object_detection/model_main.py \
    --pipeline_config_path ${config} \
    --model_dir ${save_dir} \
    --alsologtostderr
```
The evaluation configuration is 
```
eval_config: {
  num_examples: 1
  num_visualizations: 1
  metrics_set: ""coco_detection_metrics""
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  eval_interval_secs: 999999999
}
```
I think setting the eval_interval_secs of 999999999 will lead two evaluation procedures to have a gap of more than one month. But actually, two procedures only gap less than one hour. Is there anything wrong? Can anyone give some suggestions about `eval_config` if I want to skip the evaluation procedure in object detection.",4,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-11-22 06:37:12,open,,,['stat:awaiting owner'],2018-12-26 05:33:01
389,tensorflow/models,models,5800,msarfrazcss,How to feed grabbed frame from webcam into Tensorflow,"Hi everyone, I am able to get image/frame from webcam but problem is that how to feed/pass this frame into Tensorflow, to do so i am not getting success, can you help me please by giving some line of code to do so.
**Below is the code** i am using, and want to pass grabbed frames from webcam into tensorflow code( as we do in python):

        Loader.load(opencv_objdetect.class);
        FrameGrabber grabber = FrameGrabber.createDefault(0);
        grabber.start();
        OpenCVFrameConverter.ToMat converter = new OpenCVFrameConverter.ToMat();
        
        Mat grabbedImage = converter.convert(grabber.grab());
        CanvasFrame frame = new CanvasFrame(""Some Title"", CanvasFrame.getDefaultGamma()/grabber.getGamma());
        
        while (frame.isVisible() && (grabbedImage = converter.convert(grabber.grab())) != null) {
            Frame myFrame = converter.convert(grabbedImage);
            frame.showImage(myFrame);
        }
        frame.dispose();
        grabber.stop();",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-22 05:37:30,open,,,[],2018-11-27 12:18:47
390,tensorflow/models,models,5799,dorp92,What PCL version did you use for compiling the icp op?,@rezama What PCL version did you use for compiling the icp op?,1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-11-21 22:01:17,open,,,['stat:awaiting response'],2018-11-23 00:15:14
391,tensorflow/models,models,5798,kulkarnivishal,Images are not getting added in tf.event files,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
TPU
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.9
- **Bazel version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
NA (Cloud ML engine TPU)
- **GPU model and memory**:
TPU basic tier
- **Exact command to reproduce**:
Training: 
```gcloud ml-engine jobs submit training $JOB_NAME_`date +%m_%d_%Y_%H_%M_%S` --job-dir=${MODEL_DIR} --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_tpu_main --runtime-version 1.9 --scale-tier BASIC_TPU --region us-central1 -- \ --tpu_zone us-central1 --model_dir=${MODEL_DIR} --pipeline_config_path=${PIPELINE_CONFIG_PATH}```
Evaluation: 
```gcloud ml-engine jobs submit training $JOB_NAME_eval_`date +%m_%d_%Y_%H_%M_%S` --runtime-version 1.9 --job-dir=${MODEL_DIR}    --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz --module-name object_detection.model_main --region us-central1 --scale-tier BASIC_GPU -- \ --model_dir=${MODEL_DIR} --pipeline_config_path=${PIPELINE_CONFIG_PATH} --checkpoint_dir=${MODEL_DIR}```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
**Images are not getting added to event files and because of this the tensorboard shows no information apart from loss and learning rate.** 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
![image](https://user-images.githubusercontent.com/36867201/48858825-3e78ed80-ed71-11e8-926e-f9eda3744879.png)
",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-21 17:40:00,open,,,[],2018-11-22 12:18:29
392,tensorflow/models,models,5796,tree89,"error with label_map_util.get_label_map_dict method, output is only empty dictionary","System information
What is the top-level directory of the model you are using: /home/user
Have I written custom code: No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.10.1
Bazel version (if compiling from source): I don't use it
CUDA/cuDNN version: CUDA 9.0 / cuDNN: 7.1
GPU model and memory: Geforce GTX 1070
Exact command to reproduce: No

#############################################################################
### [Question]

When I try to use this method ""get_label_map_dict"" like here with input ""face_label_map.pbtxt"" using create_pet_tf_record.py

label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)

the output is just empty dictionary {}, I tried other pbtxt files on this google object detection API but always same result.

Anyone could tell me what's my mistake?

Thanks to read this
#############################################################################
This is face_label_map.pbtxt

item {
  name: ""face""
  id: 1
  display_name: ""face""
}",6,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-11-21 08:34:07,open,,,[],2018-12-03 16:59:27
393,tensorflow/models,models,5795,Luoice,    from nets import inception_resnet_v2 ModuleNotFoundError: No module named 'nets',"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-21 07:17:33,open,,,[],2018-11-22 00:23:10
394,tensorflow/models,models,5791,baihualinxin,ssd_mobilenet_v1_quantized_300x300_coco14_sync unavailable fine_tune_checkpoint,"------------------------

### System information
- **What is the top-level directory of the model you are using**:models-master\research\object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04 windwos 10
- **TensorFlow installed from (source or binary)**:pip
- **TensorFlow version (use command below)**:1.10.0
- **Bazel version (if compiling from source)**:0.17.01
- **CUDA/cuDNN version**:cuda 9 cudnn 8
- **GPU model and memory**:GTX 1080Ti
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:
b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0


### Describe the problem
```
# SSD with Mobilenet v1 with quantized training.
# Trained on COCO, initialized from Imagenet classification checkpoint

# Achieves 18.2 mAP on coco14 minival dataset.

# This config is TPU compatible

model {
  ssd {
    inplace_batchnorm_update: true
    freeze_batchnorm: false
    num_classes: 1
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    encode_background_as_zeros: true
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        class_prediction_bias_init: -4.6
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            random_normal_initializer {
              stddev: 0.01
              mean: 0.0
            }
          }
          batch_norm {
            scale: true,
            center: true,
            decay: 0.97,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v1'
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          random_normal_initializer {
            stddev: 0.01
            mean: 0.0
          }
        }
        batch_norm {
          scale: true,
          center: true,
          decay: 0.97,
          epsilon: 0.001,
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    loss {
      classification_loss {
        weighted_sigmoid_focal {
          alpha: 0.75,
          gamma: 2.0
        }
      }
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    normalize_loc_loss_by_codesize: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  fine_tune_checkpoint: ""G:/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18/model.ckpt""
  batch_size: 16
  sync_replicas: true
  startup_delay_steps: 0
  replicas_to_aggregate: 8
  num_steps: 500000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  #data_augmentation_options {
    #ssd_random_crop {
    #}
  #}
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: .2
          total_steps: 500000
          warmup_learning_rate: 0.06
          warmup_steps: 2000
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}
train_input_reader: {
  tf_record_input_reader {
    input_path: ""G:/train/data_object_image_2/training/image_2/train.record""
  }
  label_map_path: ""G:/kitti_label_map.pbtxt""
}

eval_config: {
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
  num_examples: 93
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""G:/val/data_object_image_2/training/image_2/train.record""
  }
  label_map_path: ""G:/kitti_label_map.pbtxt""
  shuffle: false
  num_readers: 1
}

graph_rewriter {
  quantization {
    delay: 300
    activation_bits: 8
    weight_bits: 8
  }
}


```

### I use it ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18 ckpt 

### Continuing training fine_tune_checkpoint
### Report errors

```
W1121 10:59:54.874578 32804 variables_helper.py:144] Variable [MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/max/biased] is not available in checkpoint
W1121 10:59:54.874578 32804 variables_helper.py:144] Variable [MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/max/local_step] is not available in checkpoint
W1121 10:59:54.874578 32804 variables_helper.py:144] Variable [MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/min/biased] is not available in checkpoint
W1121 10:59:54.874578 32804 variables_helper.py:144] Variable [MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/min/local_step] is not available in checkpoint
W1121 10:59:54.874578 32804 variables_helper.py:144] Variable [MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/max] is not available in checkpoint
W1121 10:59:54.874578 32804 variables_helper.py:144] Variable [MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/min] is not available in checkpoint
W1121 10:59:54.874578 32804 variables_helper.py:144] Variable [MobilenetV1/MobilenetV1/Conv2d_9_pointwise/weights_quant/max] is not available in checkpoint
W1121 10:59:54.874578 32804 variables_helper.py:144] Variable [MobilenetV1/MobilenetV1/Conv2d_9_pointwise/weights_quant/min] is not available in checkpoint
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""D:\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\util\deprecation.py"", line 272, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""D:\models-master\research\object_detection\legacy\trainer.py"", line 397, in train
    init_saver = tf.train.Saver(available_var_map)
  File ""D:\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\training\saver.py"", line 1281, in __init__
    self.build()
  File ""D:\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\training\saver.py"", line 1293, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""D:\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\training\saver.py"", line 1318, in _build
    raise ValueError(""No variables to save"")
ValueError: No variables to save

```


### fine_tune_checkpoint Set up your own training CKPT report error.
### quantized  The model does not support fine_tune_checkpoint operation

### Where does config need to be set up?",5,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-21 03:23:27,open,,,[],2018-11-28 00:19:05
395,tensorflow/models,models,5789,shortcipher3,Fixed broken config and improved readability,Fixed broken configuration and improved readability to make it easier to adjust if configuration options change.,0,,[],2018-11-20 17:02:04,open,,,['cla: yes'],2018-11-20 17:02:07
396,tensorflow/models,models,5788,fayeshine,tutorials cifar10 is broken,"tutorials cifar10 is broken, may need some update to be compatible with TF newest version.",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-11-20 09:29:38,open,,,"['models: official', 'stat:awaiting response']",2019-02-01 22:12:44
397,tensorflow/models,models,5787,Spatialghost,IndexError: list index out of range faster_rcnn_inception_v2_coco_2018_01_28,"Hi, i try to train object detection model with faster_rcnn_inception_v2_coco_2018_01_28 use for custom dataset, when i run the training in Windows 10 with comand python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_coco.config i have next error:
WARNING:tensorflow:From C:\tensorflow\lib\site-packages\tensorflow\python\platform\app.py:124: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
W1120 12:40:18.033661  7896 tf_logging.py:118] From C:\tensorflow\lib\site-packages\tensorflow\python\platform\app.py:124: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
WARNING:tensorflow:From C:\tensorflow\models\research\object_detection\legacy\trainer.py:265: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
W1120 12:40:18.079681  7896 tf_logging.py:118] From C:\tensorflow\models\research\object_detection\legacy\trainer.py:265: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""C:\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 136, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\tensorflow\models\research\object_detection\legacy\trainer.py"", line 279, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""C:\tensorflow\models\research\object_detection\legacy\trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""train.py"", line 121, in get_next
    dataset_builder.build(config)).get_next()
  File ""C:\tensorflow\models\research\object_detection\builders\dataset_builder.py"", line 123, in build
    num_additional_channels=input_reader_config.num_additional_channels)
  File ""C:\tensorflow\models\research\object_detection\data_decoders\tf_example_decoder.py"", line 297, in __init__
    default_value=''),
  File ""C:\tensorflow\models\research\object_detection\data_decoders\tf_example_decoder.py"", line 59, in __init__
    label_map_proto_file, use_display_name=False)
  File ""C:\tensorflow\models\research\object_detection\utils\label_map_util.py"", line 164, in get_label_map_dict
    label_map = load_labelmap(label_map_path)
  File ""C:\tensorflow\models\research\object_detection\utils\label_map_util.py"", line 136, in load_labelmap
    text_format.Merge(label_map_string, label_map)
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 888, in _MergeScalarField
    value = tokenizer.ConsumeString()
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1251, in ConsumeString
    the_bytes = self.ConsumeByteString()
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1266, in ConsumeByteString
    the_list = [self._ConsumeSingleByteString()]
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1291, in _ConsumeSingleByteString
    result = text_encoding.CUnescape(text[1:-1])
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_encoding.py"", line 103, in CUnescape
    result = ''.join(_cescape_highbit_to_str[ord(c)] for c in result)
  File ""C:\tensorflow\lib\site-packages\google\protobuf\text_encoding.py"", line 103, in <genexpr>
    result = ''.join(_cescape_highbit_to_str[ord(c)] for c in result)
IndexError: list index out of range

i use tensorflow 1.12 and protobuf 3.6.1.
Please help to solve this problem.
Great Thanks!",4,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-11-20 08:43:31,open,,,[],2018-12-21 07:14:11
398,tensorflow/models,models,5785,spencerkraisler,"google.protobuf.text_format.ParseError: 174:3 : Message type ""object_detection.protos.InputReader"" has no field named ""td_record_input_reader"".","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: tensorflow object detection api 
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS high Sierra
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.18 (in python 3.6)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
PIPELINE_CONFIG_PATH=/Users/spencerkraisler/Desktop/raccoon/models/model/ssd_mobilenet_v2_coco.config
MODEL_DIR=/Users/spencerkraisler/Desktop/raccoon/models/model
NUM_TRAIN_STEPS=1000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python3 object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderrs

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
PIPELINE_CONFIG_PATH=/Users/spencerkraisler/Desktop/raccoon/models/model/ssd_mobilenet_v2_coco.config
MODEL_DIR=/Users/spencerkraisler/Desktop/raccoon/models/model
NUM_TRAIN_STEPS=1000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python3 object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderrs

This produces error when I run model_main.py 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-11-20 01:41:00,open,,,[],2018-11-21 00:22:58
399,tensorflow/models,models,5784,jillelajitta,model_main.py Coco evaluation is unable to handle bigger validation sets.,"model_main.py Coco evaluation is unable to handle bigger validation sets. I have validation dataset of 200,000 images, It is evaluating well till the end, at the end it is killing my terminal. When I tried evaluating just with 10,000 images, it's going well. Could someone please help me.",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-11-20 01:40:54,open,,,['stat:awaiting response'],2018-11-30 11:12:42
400,tensorflow/models,models,5782,scooter4j,How does one properly configure fine_tuning_checkpoint for Google CLME implementation?,"I'm confused about how to configure fine_tune_checkpoint for training on Google Cloud Machine Learning Engine. Using ssd_mobilenet_v2_coco from the object-detection zoo, and  I'm seeing three model.ckpt.xxx files in the checkpoint download (but no model.ckpt file).

I see where people training the model locally can specify ""/path/model.ckpt"" and the training works (apparently it recognizes the three checkpoint files and does some magic). But this doesn't work with the Google Cloud MLE.

My pipeline config file for fine_tune_checkpoint is as follows:
```
.
.
.
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""gs://training-model-smokey/model.ckpt""
  fine_tune_checkpoint_type:  ""detection""

  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
.
.
.
```
but the Google CMLE fails looking for model.ckpt, as shown:
```
ERROR	2018-11-18 22:41:03 -0700	service		InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on gs://training-model-smokey/model.ckpt: Not found: Error executing an HTTP request: HTTP response code 404 with body '{
ERROR	2018-11-18 22:41:03 -0700	service		 ""error"": {
ERROR	2018-11-18 22:41:03 -0700	service		  ""errors"": [
ERROR	2018-11-18 22:41:03 -0700	service		   {
ERROR	2018-11-18 22:41:03 -0700	service		    ""domain"": ""global"",
ERROR	2018-11-18 22:41:03 -0700	service		    ""reason"": ""notFound"",
ERROR	2018-11-18 22:41:03 -0700	service		    ""message"": ""Not Found""
ERROR	2018-11-18 22:41:03 -0700	service		   }
ERROR	2018-11-18 22:41:03 -0700	service		  ],
ERROR	2018-11-18 22:41:03 -0700	service		  ""code"": 404,
ERROR	2018-11-18 22:41:03 -0700	service		  ""message"": ""Not Found""
ERROR	2018-11-18 22:41:03 -0700	service		 }
ERROR	2018-11-18 22:41:03 -0700	service		}
ERROR	2018-11-18 22:41:03 -0700	service		'
ERROR	2018-11-18 22:41:03 -0700	service			 when reading gs://training-model-smokey/
```
How does one properly configure fine_tuning_checkpoint for Google CLME implementation?",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-19 16:41:11,open,,,['stat:awaiting response'],2018-11-20 12:20:47
401,tensorflow/models,models,5774,1453042287,fine_tune_batch_norm=true and batch_size=12 gives 'Loss is inf or nan',"### System information
- **What is the top-level directory of the model you are using**:
deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
pypi
- **TensorFlow version (use command below)**:
1.11
- **Bazel version (if compiling from source)**:
none
- **CUDA/cuDNN version**:
CUDA 9.0 / cuDNN 7.3
- **GPU model and memory**:
2 *Geforce 1080 Ti, 11GB
- **Exact command to reproduce**:
CUDA_VISIBLE_DEVICES=5,4 python deeplab/train.py --logtostderr --training_number_of_steps=50000 --train_split=""train"" --model_variant=""mobilenet_v2"" --atrous_rates=12 --atrous_rates=24 --atrous_rates=36 --output_stride=8 --decoder_output_stride=4 --dataset=""pascal_voc_seg"" --tf_initial_checkpoint=deeplab/logs-for-bdd-1-1/model.ckpt-150000 --train_logdir=deeplab/logs-for-bdd-1-2 --dataset_dir=deeplab/datasets/pascal_voc_seg_for_bdd/tfrecord --train_batch_size=12 --num_clones=2 --fine_tune_batch_norm=False --slow_start_step=200
You can collect some of this information using our environment capture script:

### Describe the problem
hello, when i use the model to train bdd100k, after i train the model with a output_stride=16 for about 30k and then i try to train the model with the output_stride=8 for another 30k just like the deeplabv3's paper said, but when i set the fine_tune_batch_norm=False, the loss is not convergent, so the question is dose the fine_tune_batch_norm=False means i must use a small batch_size like 1?

### Source code / logs
INFO:tensorflow:global step 10: loss = 0.2214 (1.030 sec/step)
INFO:tensorflow:global step 20: loss = 0.5442 (0.555 sec/step)
INFO:tensorflow:global step 30: loss = 0.2877 (0.660 sec/step)
INFO:tensorflow:global step 40: loss = 0.6611 (1.050 sec/step)
INFO:tensorflow:global step 50: loss = 0.9771 (1.060 sec/step)
INFO:tensorflow:global step 60: loss = 0.3172 (0.807 sec/step)
INFO:tensorflow:global step 70: loss = 0.6717 (0.571 sec/step)
INFO:tensorflow:global step 80: loss = 0.3134 (0.755 sec/step)
INFO:tensorflow:global step 90: loss = 0.4875 (0.567 sec/step)
INFO:tensorflow:global step 100: loss = 1.0767 (0.551 sec/step)
INFO:tensorflow:global step 110: loss = 0.3748 (1.099 sec/step)
INFO:tensorflow:global step 120: loss = 0.2953 (1.087 sec/step)
INFO:tensorflow:global step 130: loss = 0.2860 (0.818 sec/step)
INFO:tensorflow:global step 140: loss = 0.3499 (0.741 sec/step)
INFO:tensorflow:global step 150: loss = 0.3700 (0.765 sec/step)
INFO:tensorflow:global step 160: loss = 0.3467 (0.515 sec/step)
INFO:tensorflow:global step 170: loss = 0.3890 (0.541 sec/step)
INFO:tensorflow:global step 180: loss = 0.5042 (0.577 sec/step)
INFO:tensorflow:global step 190: loss = 0.4484 (0.965 sec/step)
INFO:tensorflow:global step 200: loss = 0.5706 (0.569 sec/step)
INFO:tensorflow:global step 210: loss = 131.9931 (0.562 sec/step)
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Loss is inf or nan. : Tensor had Inf values

",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-18 08:36:30,open,,,[],2019-01-03 01:50:24
402,tensorflow/models,models,5772,1453042287,(deeplabv3+)multi-gpu and BN?,"### System information
- **What is the top-level directory of the model you are using**:
deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
pypi
- **TensorFlow version (use command below)**:
1.11
- **Bazel version (if compiling from source)**:
none
- **CUDA/cuDNN version**:
CUDA 9.0 / cuDNN 7.3
- **GPU model and memory**:
Titan xp / 12GB 
- **Exact command to reproduce**:
CUDA_VISIBLE_DEVICES=7,6,5,4 nohup python deeplab/train.py --logtostderr  --num_clones=4 --training_number_of_steps=150000 --train_split=""train"" --model_variant=""mobilenet_v2"" --atrous_rates=6 --atrous_rates=12 --atrous_rates=18 --output_stride=16 --decoder_output_stride=4 --dataset=""pascal_voc_seg"" --tf_initial_checkpoint=deeplab/logs-for-carla/model.ckpt-200000 --initialize_last_layer=False --train_logdir=deeplab/logs --dataset_dir=deeplab/datasets/pascal_voc_seg_for_bdd/tfrecord --train_batch_size=32 --slow_start_step=200 --slow_start_learning_rate=1e-5 >>deeplab/logs/nohup.out &

### Describe the problem
hi there, when i use multi-gpu to train the model, like train_batch_size=32 and num_clone=4, that means each gpu would has a batch_size=8, but what about the batch_normalization? is it calculated by a batch_size=8 or batch_size=32?
",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-18 02:05:44,open,,,[],2019-02-02 12:41:56
403,tensorflow/models,models,5771,HelgeS,Fixed broken link,,0,,[],2018-11-17 14:26:05,open,,,['cla: yes'],2018-11-17 14:26:07
404,tensorflow/models,models,5769,MaesIT,"Mask R-CNN with Inception Resnet v2, Atrous version; ValueError(""Shapes %s and %s are incompatible"" % (self, other))","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:models/research/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.11.0
- **Bazel version (if compiling from source)**: /
- **CUDA/cuDNN version**: 
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Nov__3_21:07:56_CDT_2017
Cuda compilation tools, release 9.1, V9.1.85
- **GPU model and memory**:
GeForce GTX 1080 (12gb)
Memory:
                    total        used        free      shared  buff/cache   available
Mem:          32084        2966       24809          96        4307       28565
Swap:          2047        1850         197


- **Exact command to reproduce**:
python object_detection/model_main.py --pipeline_config_path=object_detection/models/scratches/mask_rcnn_inception_resnet_v2_atrous_coco.config --model_dir=object_detection/models/scratches/train/ --num_train_steps=1000000 --sample_1_of_n_eval_examples=100 --alsologtostderr



### Describe the problem
When using the ""Mask R-CNN with Inception Resnet v2, Atrous version"" config on a custom dataset (not a pretrained model) the training runs well for 10 minutes, but at the evaluation step the process stops with the following error:
File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1190, in boolean_mask
    shape_tensor[axis:axis + ndims_mask].assert_is_compatible_with(shape_mask)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 848, in assert_is_compatible_with
    raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
ValueError: Shapes (100, 91) and (300, 91) are incompatible

**the complete error trace is shown in the logs below the source code**

Dataset: 1000 training images with mask provided (possible multiple per image), 100 evaluation images also with masks provided.

I have used exactly the same dataset to train & eval on a ""Mask R-CNN with Inception V2"" config, this works fine (already trained for 175k steps & evaluated tons of times). But I would like to train the same data on the inception_resnet_v2 model to see if there is difference in accuracy.

I have also tried running the legacy train.py with the Inception Resnet v2, this works fine, but when I try the legacy eval.py on the trained data it gives me the same error.




### Source code 
# Mask R-CNN with Inception Resnet v2, Atrous version

model {
  faster_rcnn {
    num_classes: 90
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 800
        max_dimension: 1365
      }
    }
    number_of_stages: 3
    feature_extractor {
      type: 'faster_rcnn_inception_resnet_v2'
      first_stage_features_stride: 8
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 8
        width_stride: 8
      }
    }
    first_stage_atrous_rate: 2
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 17
    maxpool_kernel_size: 1
    maxpool_stride: 1
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        predict_instance_masks: true
        mask_height: 33
        mask_width: 33
        mask_prediction_conv_depth: 0
        mask_prediction_num_conv_layers: 4
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
        conv_hyperparams {
          op: CONV
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.01
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
    second_stage_mask_prediction_loss_weight: 4.0
  }
}

train_config: {
  batch_size: 1
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.003
          schedule {
            step: 50000
            learning_rate: .0003
          }
          schedule {
            step: 100000
            learning_rate: .00003
          }
          schedule {
            step: 150000
            learning_rate: .000003
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  ##fine_tune_checkpoint: ""object_detection/models/scratches/train/model.ckpt-44.index""
  from_detection_checkpoint: true

  num_steps: 1000000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""object_detection/data/scratchestrain.record""
  }
  label_map_path: ""object_detection/data/label_map.pbtxt""
  load_instance_masks: true
  mask_type: PNG_MASKS
}

eval_config: {
  num_examples:100
  ##max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""object_detection/data/scratcheseval.record""
  }
  label_map_path: ""object_detection/data/label_map.pbtxt""
  load_instance_masks: true
  mask_type: PNG_MASKS
  shuffle: false
  num_readers: 1
}



### Logs
(tf) dietermaes@PCSooi:~/Documents/Tensorflowv2/models/research$ python object_detection/model_main.py --pipeline_config_path=object_detection/models/scratches/mask_rcnn_inception_resnet_v2_atrous_coco.config --model_dir=object_detection/models/scratches/train/ --num_train_steps=1000000 --sample_1_of_n_eval_examples=100 --alsologtostderr
/home/dietermaes/Documents/Tensorflowv2/models/research/object_detection/utils/visualization_utils.py:27: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.
  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
W1117 12:44:48.994960 140311471769408 tf_logging.py:125] Forced number of epochs for all eval validations to be 1.
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
W1117 12:44:48.995174 140311471769408 tf_logging.py:125] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f9c27b96e18>) includes params argument, but params are not passed to Estimator.
W1117 12:44:48.995556 140311471769408 tf_logging.py:125] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f9c27b96e18>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1117 12:44:49.015062 140311471769408 tf_logging.py:125] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /home/dietermaes/Documents/Tensorflowv2/models/research/object_detection/builders/dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
W1117 12:44:49.702305 140311471769408 tf_logging.py:125] From /home/dietermaes/Documents/Tensorflowv2/models/research/object_detection/builders/dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
WARNING:tensorflow:From /home/dietermaes/Documents/Tensorflowv2/models/research/object_detection/predictors/heads/box_head.py:93: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
W1117 12:44:58.801386 140311471769408 tf_logging.py:125] From /home/dietermaes/Documents/Tensorflowv2/models/research/object_detection/predictors/heads/box_head.py:93: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/dietermaes/Documents/Tensorflowv2/models/research/object_detection/core/losses.py:345: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

W1117 12:44:59.133028 140311471769408 tf_logging.py:125] From /home/dietermaes/Documents/Tensorflowv2/models/research/object_detection/core/losses.py:345: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
2018-11-17 12:45:12.597581: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1181, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1215, in _train_model_default
    saving_listeners)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1409, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1148, in run
    run_metadata=run_metadata)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1239, in run
    raise six.reraise(*original_exc_info)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1224, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1304, in run
    run_metadata=run_metadata))
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 581, in after_run
    if self._save(run_context.session, global_step):
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 606, in _save
    if l.after_save(session, step):
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 517, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 537, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 912, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 476, in evaluate
    return _evaluate()
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 462, in _evaluate
    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1422, in _evaluate_build_graph
    self._call_model_fn_eval(input_fn, self.config))
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1458, in _call_model_fn_eval
    features, labels, model_fn_lib.ModeKeys.EVAL, config)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1169, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/dietermaes/Documents/Tensorflowv2/models/research/object_detection/model_lib.py"", line 307, in model_fn
    prediction_dict, features[fields.InputDataFields.true_image_shape])
  File ""/home/dietermaes/Documents/Tensorflowv2/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1710, in loss
    groundtruth_masks_list,
  File ""/home/dietermaes/Documents/Tensorflowv2/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1983, in _loss_box_classifier
    tf.greater(one_hot_flat_cls_targets_with_background, 0))
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1190, in boolean_mask
    shape_tensor[axis:axis + ndims_mask].assert_is_compatible_with(shape_mask)
  File ""/home/dietermaes/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 848, in assert_is_compatible_with
    raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
ValueError: Shapes (100, 91) and (300, 91) are incompatible

I'm looking for solutions but have not found them on stackoverflow/here.

I am guessing that during the training images & masks are resized, but that this doesn't happen on the eval data which results in an incompatible shape? 
Please let me know if you also have encountered this issue + how can it be solved.

Thanks in advance,
Dieter Maes",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-11-17 12:07:41,open,,,[],2018-11-19 22:21:26
405,tensorflow/models,models,5768,thashim,Sampling code in lm_1b sets incorrect BOS token for character embeddings.,"### System information
- **What is the top-level directory of the model you are using**: https://github.com/tensorflow/models/tree/master/research/lm_1b
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: titanx, 12G
- **Exact command to reproduce**: run lm_1b_eval in sampling mode.

### Describe the problem

Sampling mode incorrectly sets the character beginning of sentence token, resulting in incorrect generations. 

The error is at (https://github.com/tensorflow/models/blob/master/research/lm_1b/lm_1b_eval.py#L180), which maps `<S>` (BOS token) to the character representation of `'<' 'S' '>'`. This causes the model to think it's not at the start of the sentence, and results in incoherent samples. 

Manually setting the first tokens to BOS in both prefix vectors solves the problem.

This does not affect perplexity calculations, since the batch loader manually sets the BOS tokens instead of using word-> character conversions.

### Source code / logs

Example from current code
```
--they can 't tell who 's whose ( ask who you 're with ) . </S> 
This is clear this isn 't your ordinary small city economy -- how to even make one dress ? </S> 
is pushing to print one , ERROR , the trade volume born once a decade -- Your tax dollars are being invested to help resolve the problem . -- Global banks have been uniformly reduced to wilderness status -- limiting what 's set . -- Making again earnings statements very graphic . -- Credit history . -- Best ever touch in credit cards . -The best you can see is expensive stimulus of <UNK> stock . -- Leverage in don 't go too far , you 'll get nearer to the bottom . </S> 
```

Example after manually setting BOS tokens
```
Tragically enough , her luck turned out to be more mass murderer than New Castle County Sheriff . </S> 
But stay home on four days , and the researchers say American men may be on the verge of the thousand American grand mal fathers going a mile without a smoker . </S> 
More than a dozen smaller cities , including San Diego and San Diego , were flooded and a few more were submerged , causing more evacuations Monday . </S> 
```

",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-16 21:26:02,open,,,[],2018-11-17 12:15:13
406,tensorflow/models,models,5767,SanthoshRajendiran,"Depth Multiplier becomes 0, on conversion to TFLITE","MODEL: DeeplabV3+ MobilenetV2 Pretrained with Pascal VOC 2012
Tensorflow Version: 1.12.0
TFLITE Version: 1.12.0
Bazel Version: 0.17.2
OS version: Ubuntu 16.04
GPU Version: GTX 1080 Ti
Command to reproduce:

bazel run //tensorflow/lite/toco:toco -- \
--input_file=frozen_inference_graph.pb \
--output_file=converted.tflite \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--input_arrays=ImageTensor \
--output_arrays=SemanticPredictions \
--input_shapes=1,513,513,3 \
--allow_custom_ops \
--inference_type=FLOAT \
--inference_input_type=QUANTIZED_UINT8

(or)

CUDA_VISIBLE_DEVICES=""0"" \
tflite_convert \
--output_file=converted.tflite \
--graph_def_file=frozen_inference_graph.pb \
--inference_type=FLOAT \
--inference_input_type=QUANTIZED_UINT8 \
--input_arrays=ImageTensor \
--output_arrays=SemanticPredictions \
--input_shapes=1,513,513,3 \
--mean_values=513 \
--std_dev_values=513

Benchmarking Command:
bazel run -c opt tensorflow/lite/tools/benchmark:benchmark_model -- --graph='realpath converted.tflite'

Conversion of pre-trained models provided on the [model zoo](https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md) to TFLITE, on benchmarking and running on Android provided issues on depth multiplier value getting converted to 0.

Stack Overflow Issue: [https://stackoverflow.com/questions/53228969/unable-to-test-and-deploy-a-deeplabv3-mobilenetv2-tensorflow-lite-segmentation-m](https://stackoverflow.com/questions/53228969/unable-to-test-and-deploy-a-deeplabv3-mobilenetv2-tensorflow-lite-segmentation-m)",2,"NamedUser(login=""haozha111"")","[NamedUser(login=""haozha111"")]",2018-11-16 13:43:42,open,,,"['comp:lite', 'stat:awaiting tensorflower']",2019-03-19 03:30:56
407,tensorflow/models,models,5766,cannguyen275,[Object Detection API] Show wrong detection,"### System information
- **What is the top-level directory of the model you are using**:
models/research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.04
- **TensorFlow installed from (source or binary)**:
conda install
- **TensorFlow version (use command below)**:
1.7.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA® Toolkit 9.0; cuDNN v7.1
- **GPU model and memory**:
GForce GTX 1080Ti (11GB)
- **Exact command to reproduce**:

```
python eval.py --logtostderr --pipeline_config_path=training/faster_rcnn_inception_resnet_v2_atrous_coco.config --checkpoint_dir=training_point/ --eval_dir=eval/
```
```
tensorboard --logdir=training:training_point/,testing:eval/
```
### Describe the problem

I've trained a model using the object detection API. Using eval.py, I can evaluate my model with my validation set. The mAP is ~96%.
I want to see the wrong prediction. It is 4% left and my validation set is 10000.
Is there any way to see that?
Thanks,",3,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-11-16 11:00:52,open,,,[],2018-11-20 08:35:24
408,tensorflow/models,models,5764,zbyuan,  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-16 08:24:46,open,,,[],2018-11-17 00:15:28
409,tensorflow/models,models,5762,zhly0,model trained on  PASCAL VOC 2012,"Hi,
in the model_zoo,you provide model trained on  PASCAL VOC 2012,but this dataset has less than 3k images use for segmentation,is the training number too small?
Thanks",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-16 02:08:05,open,,,['stat:awaiting response'],2018-11-17 00:15:37
410,tensorflow/models,models,5761,rootkitchao,Feature Request:MNASNET on tf-slim,"### System information
- **What is the top-level directory of the model you are using**:tensorflow/models/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:MS Windows 10 Pro 64bit Build 17763
- **TensorFlow installed from (source or binary)**:binary（tensorflow-gpu)
- **TensorFlow version (use command below)**:1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Bazel version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0/7.4
- **GPU model and memory**:NVIDIA Geforce RTX2080TI 11GB
- **Exact command to reproduce**:N/A

### Source code / logs
N/A

Dear tensorflower:
I have read the paper on MNASNET(https://arxiv.org/abs/1807.11626), but I have not found the code.I am planning to write a custom code for MNASNET.But before that, I want to ask if there is any plan to add MNASNET to TF-Slim.Thanks.",14,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-11-15 19:09:38,open,,,['type:feature'],2019-03-20 12:09:12
411,tensorflow/models,models,5760,SanthoshRajendiran,Feature Request: Sample application for TFLITE Deeplab,"Feature request, requesting a sample application for DeeplabV3+ ### tag:feature_template

**System information**

- TensorFlow version: Tensorflow 1.12 [tensorflow-lite-1.12 ]
- Model Information: DeeplabV3+ MobilenetV2 (with depth multiplier 0.5)
- Are you willing to contribute it (Yes/No): No
- Android API Version: API 24+ (Android Nougat)

**Feature Current Behavior**
There is no mobile application to test out the working of Deeplab tflite model in Android or IOS. This seems as a direct need for developers and it will be helpful for knowing the parsing mechanism for tflite where we get semantic predictions as an output, as there is an unclear way of parsing the specific data type in android and ios(prediction?) as it involves pixel data.

**Current API:** Need of change
We are in need of new API model to help in with parsing the model input. Could be released as a subsequent fix.

**Beneficiaries**
People who are developing camera applications can directly benefit from this as it involves playing with segmentation on android devices.

**Other information.**
We are trying building the application tweaking with available applications. We are currently facing up some issues. Hereby with, we are attaching the issue links.

[https://stackoverflow.com/questions/53228969/unable-to-test-and-deploy-a-deeplabv3-mobilenetv2-tensorflow-lite-segmentation-m](https://stackoverflow.com/questions/53228969/unable-to-test-and-deploy-a-deeplabv3-mobilenetv2-tensorflow-lite-segmentation-m)

[https://stackoverflow.com/questions/53236290/unable-to-load-tflite-deeplab-segmentation-model-in-android-application-error](https://stackoverflow.com/questions/53236290/unable-to-load-tflite-deeplab-segmentation-model-in-android-application-error)",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-15 13:30:40,open,,,[],2018-11-17 12:15:18
412,tensorflow/models,models,5759,ztwe,ssdlite_mobilenet_v2_coco android tflite error,"tensorflow version: 1.9
model: ssdlite_mobilenet_v2_coco_2018_05_09
tflite_export_tool:  object_detection/export_tflite_ssd_graph.py
tflite-library: 1.10.0/1.9.0


```
Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:74)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:54)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)
```",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-11-15 09:03:45,open,,,['stat:awaiting response'],2018-11-21 06:31:40
413,tensorflow/models,models,5758,spencerkraisler,Python stalls (infinite bouncing rocket) when I try to use model_main.py ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: High Sierra Mac os 
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:using CPU
- **Exact command to reproduce**:
PIPELINE_CONFIG_PATH=/Users/kraisler/Desktop/raccoon_tutorial/models/model/ssd_mobilenet_v1_pets.config
MODEL_DIR=/Users/kraisler/Desktop/raccoon_tutorial/models/model
NUM_TRAIN_STEPS=50000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am following this tutorial: https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9
I try to run model_main.py to train a model but it just prints this out and stalls. I've trained other models before, but not SSD's like mobile net (the one I'm using).
It stays like this forever. 

note: this works whether I use python 3.6 or python 2.7. 

### Source code / logs
OUGL-2-NW-C04-M:research kraisler$ PIPELINE_CONFIG_PATH=/Users/kraisler/Desktop/raccoon_tutorial/models/model/ssd_mobilenet_v1_pets.config
OUGL-2-NW-C04-M:research kraisler$ MODEL_DIR=/Users/kraisler/Desktop/raccoon_tutorial/models/model
OUGL-2-NW-C04-M:research kraisler$ NUM_TRAIN_STEPS=50000
OUGL-2-NW-C04-M:research kraisler$ SAMPLE_1_OF_N_EVAL_EXAMPLES=1
OUGL-2-NW-C04-M:research kraisler$ python object_detection/model_main.py \
>     --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
>     --model_dir=${MODEL_DIR} \
>     --num_train_steps=${NUM_TRAIN_STEPS} \
>     --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
>     --alsologtostderr
/Users/kraisler/models/research/object_detection/utils/visualization_utils.py:27: UserWarning: 
This call to matplotlib.use() has no effect because the backend has already
been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

The backend was *originally* set to 'MacOSX' by the following code:
  File ""object_detection/model_main.py"", line 26, in <module>
    from object_detection import model_lib
  File ""/Users/kraisler/models/research/object_detection/model_lib.py"", line 27, in <module>
    from object_detection import eval_util
  File ""/Users/kraisler/models/research/object_detection/eval_util.py"", line 27, in <module>
    from object_detection.metrics import coco_evaluation
  File ""/Users/kraisler/models/research/object_detection/metrics/coco_evaluation.py"", line 20, in <module>
    from object_detection.metrics import coco_tools
  File ""/Users/kraisler/models/research/object_detection/metrics/coco_tools.py"", line 47, in <module>
    from pycocotools import coco
  File ""/usr/local/lib/python2.7/site-packages/pycocotools/coco.py"", line 49, in <module>
    import matplotlib.pyplot as plt
  File ""/usr/local/lib/python2.7/site-packages/matplotlib/pyplot.py"", line 71, in <module>
    from matplotlib.backends import pylab_setup
  File ""/usr/local/lib/python2.7/site-packages/matplotlib/backends/__init__.py"", line 16, in <module>
    line for line in traceback.format_stack()


  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
W1114 20:46:22.522630 140735970734976 tf_logging.py:125] Forced number of epochs for all eval validations to be 1.
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
W1114 20:46:22.522892 140735970734976 tf_logging.py:125] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x12f965488>) includes params argument, but params are not passed to Estimator.
W1114 20:46:22.523379 140735970734976 tf_logging.py:125] Estimator's model_fn (<function model_fn at 0x12f965488>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1114 20:46:22.552421 140735970734976 tf_logging.py:125] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /Users/kraisler/models/research/object_detection/builders/dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
W1114 20:46:22.618475 140735970734976 tf_logging.py:125] From /Users/kraisler/models/research/object_detection/builders/dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
W1114 20:46:22.779803 140735970734976 tf_logging.py:125] From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From /Users/kraisler/models/research/object_detection/core/preprocessor.py:1208: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
W1114 20:46:22.908727 140735970734976 tf_logging.py:125] From /Users/kraisler/models/research/object_detection/core/preprocessor.py:1208: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From /Users/kraisler/models/research/object_detection/builders/dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
W1114 20:46:24.049441 140735970734976 tf_logging.py:125] From /Users/kraisler/models/research/object_detection/builders/dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
W1114 20:46:26.257921 140735970734976 variables_helper.py:141] Variable [BoxPredictor_0/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[6]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.258060 140735970734976 variables_helper.py:141] Variable [BoxPredictor_0/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 273]], model variable shape: [[1, 1, 512, 6]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.258198 140735970734976 variables_helper.py:141] Variable [BoxPredictor_1/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.258268 140735970734976 variables_helper.py:141] Variable [BoxPredictor_1/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 1024, 546]], model variable shape: [[1, 1, 1024, 12]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.258414 140735970734976 variables_helper.py:141] Variable [BoxPredictor_2/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.258490 140735970734976 variables_helper.py:141] Variable [BoxPredictor_2/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 546]], model variable shape: [[1, 1, 512, 12]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.258668 140735970734976 variables_helper.py:141] Variable [BoxPredictor_3/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.258763 140735970734976 variables_helper.py:141] Variable [BoxPredictor_3/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.258913 140735970734976 variables_helper.py:141] Variable [BoxPredictor_4/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.258995 140735970734976 variables_helper.py:141] Variable [BoxPredictor_4/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.259133 140735970734976 variables_helper.py:141] Variable [BoxPredictor_5/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.259224 140735970734976 variables_helper.py:141] Variable [BoxPredictor_5/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 546]], model variable shape: [[1, 1, 128, 12]]. This variable will not be initialized from the checkpoint.
W1114 20:46:26.262506 140735970734976 variables_helper.py:144] Variable [global_step] is not available in checkpoint
2018-11-14 20:46:42.850174: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA",3,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-11-15 04:53:11,open,,,[],2019-02-10 05:14:57
414,tensorflow/models,models,5757,spencerkraisler,"ValueError: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(""arg0:0"", shape=(), dtype=float32, device=/device:CPU:0)' Keep getting this error. Been at this for nearly 4 hours now. Really stressed.","
### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: No GPU
- **Exact command to reproduce**:

PIPELINE_CONFIG_PATH=/Users/kraisler/Desktop/raccoon_tutorial/models/model/ssd_mobilenet_v1_pets.config
MODEL_DIR=/Users/kraisler/Desktop/raccoon_tutorial/models/model
NUM_TRAIN_STEPS=50000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am following this tutorial: https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9

I did everything. Protoc is installed, the object detector test tensor flow provided worked, everything. I enter the following at the research directory in the object detection API:

PIPELINE_CONFIG_PATH=/Users/kraisler/Desktop/raccoon_tutorial/models/model/ssd_mobilenet_v1_pets.config
MODEL_DIR=/Users/kraisler/Desktop/raccoon_tutorial/models/model
NUM_TRAIN_STEPS=50000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr

and I get the error in the title. I have no idea what this means. 
### Source code / logs

OUGL-2-NW-C04-M:research kraisler$ PIPELINE_CONFIG_PATH=/Users/kraisler/Desktop/raccoon_tutorial/models/model/ssd_mobilenet_v1_pets.config
OUGL-2-NW-C04-M:research kraisler$ MODEL_DIR=/Users/kraisler/Desktop/raccoon_tutorial/models/model
OUGL-2-NW-C04-M:research kraisler$ NUM_TRAIN_STEPS=50000
OUGL-2-NW-C04-M:research kraisler$ SAMPLE_1_OF_N_EVAL_EXAMPLES=1
OUGL-2-NW-C04-M:research kraisler$ python object_detection/model_main.py \
>     --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
>     --model_dir=${MODEL_DIR} \
>     --num_train_steps=${NUM_TRAIN_STEPS} \
>     --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
>     --alsologtostderr
/Users/kraisler/models/research/object_detection/utils/visualization_utils.py:27: UserWarning: 
This call to matplotlib.use() has no effect because the backend has already
been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

The backend was *originally* set to 'MacOSX' by the following code:
  File ""object_detection/model_main.py"", line 26, in <module>
    from object_detection import model_lib
  File ""/Users/kraisler/models/research/object_detection/model_lib.py"", line 27, in <module>
    from object_detection import eval_util
  File ""/Users/kraisler/models/research/object_detection/eval_util.py"", line 27, in <module>
    from object_detection.metrics import coco_evaluation
  File ""/Users/kraisler/models/research/object_detection/metrics/coco_evaluation.py"", line 20, in <module>
    from object_detection.metrics import coco_tools
  File ""/Users/kraisler/models/research/object_detection/metrics/coco_tools.py"", line 47, in <module>
    from pycocotools import coco
  File ""/usr/local/lib/python2.7/site-packages/pycocotools/coco.py"", line 49, in <module>
    import matplotlib.pyplot as plt
  File ""/usr/local/lib/python2.7/site-packages/matplotlib/pyplot.py"", line 71, in <module>
    from matplotlib.backends import pylab_setup
  File ""/usr/local/lib/python2.7/site-packages/matplotlib/backends/__init__.py"", line 16, in <module>
    line for line in traceback.format_stack()


  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
W1114 20:29:31.596975 140735970734976 tf_logging.py:125] Forced number of epochs for all eval validations to be 1.
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
W1114 20:29:31.597275 140735970734976 tf_logging.py:125] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x126e94488>) includes params argument, but params are not passed to Estimator.
W1114 20:29:31.597842 140735970734976 tf_logging.py:125] Estimator's model_fn (<function model_fn at 0x126e94488>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 0 to match input file shards.
W1114 20:29:31.619033 140735970734976 tf_logging.py:125] num_readers has been reduced to 0 to match input file shards.
WARNING:tensorflow:From /Users/kraisler/models/research/object_detection/builders/dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
W1114 20:29:31.685348 140735970734976 tf_logging.py:125] From /Users/kraisler/models/research/object_detection/builders/dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1234, in _train_model_default
    input_fn, model_fn_lib.ModeKeys.TRAIN))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1075, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1162, in _call_input_fn
    return input_fn(**kwargs)
  File ""/Users/kraisler/models/research/object_detection/inputs.py"", line 479, in _train_input_fn
    batch_size=params['batch_size'] if params else train_config.batch_size)
  File ""/Users/kraisler/models/research/object_detection/builders/dataset_builder.py"", line 134, in build
    config.input_path[:], input_reader_config)
  File ""/Users/kraisler/models/research/object_detection/builders/dataset_builder.py"", line 80, in read_dataset
    sloppy=config.shuffle))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1190, in apply
    dataset = transformation_func(self)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/data/experimental/ops/interleave_ops.py"", line 87, in _apply_fn
    buffer_output_elements, prefetch_input_elements)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/data/ops/readers.py"", line 134, in __init__
    cycle_length, block_length)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2714, in __init__
    super(InterleaveDataset, self).__init__(input_dataset, map_func)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2677, in __init__
    experimental_nested_dataset_support=True)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1860, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/function.py"", line 479, in add_to_graph
    self._create_definition_if_needed()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/function.py"", line 335, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/function.py"", line 344, in _create_definition_if_needed_impl
    self._capture_by_value, self._caller_device)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/function.py"", line 864, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1794, in tf_data_structured_function_wrapper
    ret = func(*nested_args)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/data/ops/readers.py"", line 200, in __init__
    filenames = ops.convert_to_tensor(filenames, dtype=dtypes.string)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1050, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 983, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(""arg0:0"", shape=(), dtype=float32, device=/device:CPU:0)'


I just want to say I have been at this for a very very long time. I'm kind aggravated. I would greatly greatly greatly appreciate it if anyone helped me. Yes, I did search for this error and no none of the methods I found worked. ",6,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-11-15 04:32:45,open,,,[],2019-03-30 01:35:30
415,tensorflow/models,models,5756,MeowsQAQ,Where could i get slim?,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel""), NamedUser(login=""jvishnuvardhan"")]",2018-11-15 04:30:40,open,,,['type:support'],2019-01-09 19:38:08
416,tensorflow/models,models,5753,Sajjadmanal,Unable to train model using train.py,"System information
What is the top-level directory of the model you are using: models\research\object_detection
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Little change
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.12.0
Bazel version (if compiling from source):
CUDA/cuDNN version: NA
GPU model and memory: CPU 16 GB
Exact command to reproduce: `python legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config`

faster_rcnn_inception_v2_pets.config file:

```
# Faster R-CNN with Inception v2, configured for Oxford-IIIT Pets Dataset.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for ""PATH_TO_BE_CONFIGURED"" to find the fields that
# should be configured.

model {
  faster_rcnn {
    num_classes: 6
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
      }
    }
    feature_extractor {
      type: 'faster_rcnn_inception_v2'
      first_stage_features_stride: 16
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 16
        width_stride: 16
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 300
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 1
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.0002
          schedule {
            step: 0
            learning_rate: .0002
          }
          schedule {
            step: 900000
            learning_rate: .00002
          }
          schedule {
            step: 1200000
            learning_rate: .000002
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""C:/tensorflow1/models/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt""
  from_detection_checkpoint: true
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}


train_input_reader: {
  tf_record_input_reader {
    input_path: ""C:/tensorflow1/models/research/object_detection/train.record""
  }
  label_map_path: ""C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt""
}

eval_config: {
  num_examples: 67
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""C:/tensorflow1/models/research/object_detection/test.record""
  }
  label_map_path: ""C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt""
  shuffle: false
  num_readers: 1
}
````


Error:
````
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\sm50014\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\Users\sm50014\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 306, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Users\sm50014\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\legacy\trainer.py"", line 248, in train
    detection_model = create_model_fn()
  File ""C:\Users\sm50014\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\model_builder.py"", line 121, in build
    add_summaries)
  File ""C:\Users\sm50014\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\model_builder.py"", line 430, in _build_faster_rcnn_model
    num_classes=num_classes)
  File ""C:\Users\sm50014\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\box_predictor_builder.py"", line 617, in build
    config_box_predictor.convolve_then_upsample_masks))
AttributeError: 'MaskRCNNBoxPredictor' object has no attribute 'convolve_then_upsample_masks'
````
",3,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-11-14 07:38:02,open,,,[],2019-04-02 17:19:28
417,tensorflow/models,models,5751,ChildrenGreens,SyntaxNet Bazel Test Fail,"INFO: From Compiling external/org_tensorflow/tensorflow/core/kernels/boosted_trees/training_ops.cc [for host]:
external/org_tensorflow/tensorflow/core/kernels/boosted_trees/training_ops.cc: In member function 'virtual void tensorflow::BoostedTreesUpdateEnsembleOp::Compute(tensorflow::OpKernelContext*)':
external/org_tensorflow/tensorflow/core/kernels/boosted_trees/training_ops.cc:159:57: warning: 'ensemble_resource' may be used uninitialized in this function [-Wmaybe-uninitialized]
           ensemble_resource->PostPruneTree(current_tree);
                                                         ^
ERROR: /root/models/research/syntaxnet/bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/external/grpc/BUILD:1287:1: C++ compilation of rule '@grpc//:grpc_resolver_dns_ares' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 55 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:30:18: fatal error: ares.h: No such file or directory
compilation terminated.
",3,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-14 03:07:23,open,,,[],2018-12-13 15:21:37
418,tensorflow/models,models,5750,seanbae,Fix setup steps in mobilenet_example.ipynb,"Fix the setup steps in `mobilenet_example.ipynb`.

1. The existing Jupyter notebook interchangeably uses `base_name` and `checkpoint_name`, but `base_name` is never defined.

This leads to the vanilla setup to fail with `NameError`:
<img width=""922"" alt=""screenshot 2018-11-13 13 07 12"" src=""https://user-images.githubusercontent.com/1895508/48443272-63ec7280-e745-11e8-83d2-8b0833c46f71.png"">

This PR replaces all `base_name` references to `checkpoint_name`.

2. Fix an incorrect `slim` path.
",0,,[],2018-11-13 20:57:25,open,,"NamedUser(login=""seanbae"")",['cla: yes'],2018-11-13 21:15:10
419,tensorflow/models,models,5749,lychenpan,Quantized tflite model always predict class as zero for any images ,"Generated quantized tflite model predicts all the same.  
------------------------

### System information
- **What is the top-level directory of the model you are using**:  tensorflow/models/research/  
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  
- **TensorFlow installed from (source or binary)**: keras-docker
- **TensorFlow version (use command below)**:   1.11.0
- **Bazel version (if compiling from source)**:  
- **CUDA/cuDNN version**:  
- **GPU model and memory**: 11G
- **Exact command to reproduce**:  

### Describe the problem
I want to use quantized model to detect object on mobile.  1. I fine-tune a model from ssdlite-mobile-v2-coco model, (it has no problem when testing it on PC)  2. I use ""object_detection/export_tflite_ssd_graph.py ""  to export  *.pb and *.pbtxt from the checkpoint folder  3. use ""tflite_convert"" commnad to convert *pb file to tflite(both float and quantized are tried). 
When i covert *pb to float tflite model, the generated model works well on mobile;  but when convert it to quantized tflite model,  it's class prediction are always zero, and their score is 0.5,  for any image input. 

### Source code / logs
1.  script to export *pb file from check point folder.

export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
CUDA_VISIBLE_DEVICES=1 \
python object_detection/export_tflite_ssd_graph.py \
--pipeline_config_path=train/carpart_base/pipeline.config \
--trained_checkpoint_prefix=train/carpart_base/model.ckpt-600000 \
--output_directory=output2 \
--add_postprocessing_op=true

2. script to covert pb file to tflite model
tflite_convert \
--output_file=/src/workspace/models/research/output3/float1.tflite \
--graph_def_file=/src/workspace/models/research/output3/tflite_graph.pb \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--input_type=FLOAT \
--inference_type=FLOAT \
--change_concat_input_ranges=false \
--allow_custom_ops
Above float model works well on mobile.

tflite_convert \
--output_file=/src/workspace/models/research/output2/quant4.tflite \
--graph_def_file=/src/workspace/models/research/output2/tflite_graph.pb \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='raw_outputs/box_encodings','raw_outputs/class_predictions' \
--input_type=QUANTIZED_UINT8 \
--inference_type=QUANTIZED_UINT8 \
--change_concat_input_ranges=false \
--mean_values=0 \
--std_dev_values=1 \
--default_ranges_min=0 \
--default_ranges_max=6 \
This quantized model predicts all the same for class and class score.  And i have tried different mean/std(128,128), which not works either. 
Can anyone tell me about the error or how to calculate the mean/std parameters?
Thanks!
",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-11-13 15:29:21,open,,,[],2018-11-19 02:30:26
420,tensorflow/models,models,5748,dorp92,bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so : fatal error LNK1169: one or more multiply defined symbols found,"### System information
- **What is the top-level directory of the model you are using**: C:\Inuitive\tensorflow-1.11.0
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 7
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.11.0
- **Bazel version (if compiling from source)**:0.19.1
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:gtx 1070 8gb
- **Exact command to reproduce**:bazel build //tensorflow/tools/pip_package:build_pip_package -c opt --verbose_failures

tensorflow build fails:

> bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so : fatal erro
r LNK1169: one or more multiply defined symbols found
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 31.780s, Critical Path: 30.43s, Remote (0.00% of the time):
[queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully",0,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-11-13 14:46:10,open,,,[],2018-11-14 12:17:03
421,tensorflow/models,models,5744,qbilius,fixed scaling of channels,"When loading weights for channel-scaled versions of mobilenet, such as `mobilenet_v2_0.75_224`, I get an error:

`Assign requires shapes of both tensors to match. lhs shape= [1,1,240,960] rhs shape= [1,1,240,1280]`.

This seems to stem from the fact that the layer just prior to avgpool is supposed to always have no less than 1280 channels -- yet it is incorrectly scaled down to 960 instead when the mutiplier is .75.

This PR fixes the issue that mobilenets appear to work as expected.",3,,[],2018-11-12 23:42:09,open,,,['cla: yes'],2018-11-12 23:44:52
422,tensorflow/models,models,5743,bhavani-subramanian,NCF: Enabled inter/intra-op parallelism threads by default.,"If these flags are not passed, a default value of 0 will be used.",2,,[],2018-11-12 21:18:59,open,,,['cla: yes'],2018-11-13 18:59:25
423,tensorflow/models,models,5742,jrbtaylor,DeepLab Xception checkpoints are broken,"### System information
- **What is the top-level directory of the model you are using**: research/deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.9
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Titan V
- **Exact command to reproduce**: see below

The following applies to both ""xception65_cityscapes_trainfine"" and ""xception71_dpc_cityscapes_trainval"" checkpoints from research/deeplab/model_zoo. If I build the graph with the included functions, i.e. 

`xception.xception_71(input_tensor, num_classes=30, is_training=False, global_pool=False, output_stride=8, scope='xception71')`

and try to restore from the checkpoints in the model zoo with `tf.train.Saver().restore(sess, ckpt_path)`, where ckpt_path is the unzipped checkpoint file, I get the following error:

> DataLossError (see above for traceback): Unable to open table file /segmentation.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

This occurs for both the xception 65 and 71 models, and I was also unable to restore from the frozen_inference_graph.pb file (though I will exclude that because I would rather use the ckpt). ",0,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-11-12 21:05:46,open,,,[],2018-11-13 12:16:46
424,tensorflow/models,models,5741,WolfNiu,Asking for Controller Code of AutoAugment,"Could someone let me know whether the controller part of the code is released? By inspecting the autoaugment folder, I only found an already trained policy. Thank you.",12,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-11-12 14:43:12,open,,,[],2019-02-07 05:48:15
425,tensorflow/models,models,5740,dscha09,Why does `num_detections` value vary from image to image,"This is not really an ""error"" I'm encountering. However, I wonder why the total number of detections from image to image varies.

For example, for one video, the total number of detections is always 100 (some have really low scores tho), and in some videos this number changes.

Why is this?",2,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-11-12 07:14:38,open,,,['stat:awaiting response'],2018-12-19 10:25:41
426,tensorflow/models,models,5739,jiaswee1995,Failed to export trained model (Illegal Instruction Code Dumped) TF1.9,"![image](https://user-images.githubusercontent.com/22536079/48321840-5957ad80-e65f-11e8-9a70-cb60a2abc5dc.png)

### System information
- $models/research/object_detection
- Linux Ubuntu 18.04
- pip install tensorflow==1.9.*
- Tensorflow 1.9
- Using Paperspace GPU and CPU
- Exact command to reproduce: as shown as above picture",3,"NamedUser(login=""achowdhery"")","[NamedUser(login=""achowdhery""), NamedUser(login=""pkulzc"")]",2018-11-12 01:47:50,open,,,[],2018-11-18 16:16:32
427,tensorflow/models,models,5735,1varun,Update installation.md,Second last step to confirm correct installation for windows was missing.,3,,[],2018-11-10 19:55:46,open,,,['cla: yes'],2018-11-16 19:04:54
428,tensorflow/models,models,5733,xjtuljy,is it correct for random_image_scale to use tf.image.resize_image for data augmentation?,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: docker image
- **TensorFlow version (use command below)**: v1.11.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: V100
- **Exact command to reproduce**:

### Describe the problem
I tried to add some new operations to data augmentation, so I checked some examples of data_augmentation_options in preprocesser.py. 

for the function random_image_scale [https://github.com/tensorflow/models/blob/d7ce21fa4d3b8b204530873ade75637e1313b760/research/object_detection/core/preprocessor.py#L767](url) however, it uses tf.image.resize_images to scale the image. And object_detection/legacy/trainer.py shows that, this augmentation scales the input queue, which will soon be scaled to a desired shape before being fed into feature extractor (eg. ssd_meta_arch includes a ""preprocess"" step to scale the input queue to 300x300). So, it seems to me that, this random scale operation in the data augmentation doesn't make sense. Instead, shall the operation tf.image.resize_images be followed tf.image.resize_image_with_crop_or_pad, so that the image tensor is really scaled.

Please let me know if I am wrong. Thanks for your help.





",0,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-11-09 19:04:45,open,,"NamedUser(login=""xjtuljy"")",[],2018-11-22 01:12:26
429,tensorflow/models,models,5732,krips89,Test for object detection (model_builder_test.py) fails,"### System information
- **What is the top-level directory of the model you are using**: research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: ('v1.12.0-rc0-1231-g0af3d95', '1.12.0-rc0')
- **Bazel version (if compiling from source)**: 0.18.0
- **CUDA/cuDNN version**: 7.3
- **GPU model and memory**: GTX 1070, 8GB
- **Exact command to reproduce**: python object_detection/builders/model_builder_test.py from research folder.

### Describe the problem
After going through the the object detection installation steps (in https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) the test (the final step of `python object_detection/builders/model_builder_test.py`) is failing.

### Source code / logs
```
..........E......E....
======================================================================
ERROR: test_create_ssd_inception_v2_model_from_config (__main__.ModelBuilderTest)
test_create_ssd_inception_v2_model_from_config (__main__.ModelBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor
    yield
  File ""/usr/local/lib/python2.7/dist-packages/absl/third_party/unittest3_backport/case.py"", line 162, in run
    testMethod()
  File ""object_detection/builders/model_builder_test.py"", line 158, in test_create_ssd_inception_v2_model_from_config
    text_format.Merge(model_text_proto, model_proto)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 730, in _MergeField
    (message_descriptor.full_name, name))
ParseError: 65:9 : Message type ""object_detection.protos.Ssd"" has no field named ""min_num_negative_samples"".

======================================================================
ERROR: test_create_ssd_mobilenet_v2_keras_model_from_config (__main__.ModelBuilderTest)
test_create_ssd_mobilenet_v2_keras_model_from_config (__main__.ModelBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor
    yield
  File ""/usr/local/lib/python2.7/dist-packages/absl/third_party/unittest3_backport/case.py"", line 162, in run
    testMethod()
  File ""object_detection/builders/model_builder_test.py"", line 792, in test_create_ssd_mobilenet_v2_keras_model_from_config
    model = self.create_model(model_proto)
  File ""object_detection/builders/model_builder_test.py"", line 87, in create_model
    return model_builder.build(model_config, is_training=True)
  File ""/home/sarkar/exp/tensorflow_sources/models/research/object_detection/builders/model_builder.py"", line 117, in build
    add_background_class)
  File ""/home/sarkar/exp/tensorflow_sources/models/research/object_detection/builders/model_builder.py"", line 213, in _build_ssd_model
    is_training=is_training)
  File ""/home/sarkar/exp/tensorflow_sources/models/research/object_detection/builders/model_builder.py"", line 151, in _build_ssd_feature_extractor
    raise ValueError('Unknown ssd feature_extractor: {}'.format(feature_type))
ValueError: Unknown ssd feature_extractor: ssd_mobilenet_v2_keras

----------------------------------------------------------------------
Ran 22 tests in 0.138s

FAILED (errors=2)
```

Further, if I ignore this, the training steps fails with the following error in almost all the cases:
```
OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint
```
",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-11-09 19:04:24,open,,,[],2019-01-08 12:13:59
430,tensorflow/models,models,5729,rohitsaluja22,"unable to reach 84.2 test seq accuracy on fsns dataset with pretrained inception_resnet_v2 encoder, batch size = 16","What is the top-level directory of the model you are using: attention_ocr/python
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): pip install --upgrade tensorflow-gpu
TensorFlow version (use command below): 1.4.1.
Bazel version: N/A
CUDA/cuDNN version: cuda/8.0 cudnn/7.1.2
GPU model and memory: 4 x GeForce GTX 1080
Exact command to reproduce: python ../eval.py --dataset_dir=/home/ayush/OCR/fsns_ayush/data/fsns/ --train_log_dir=/dev/saved_models_h4/ --split=test --batch_size=204

Hi I modified the model.py to use inception_resnet_v2 and enabled co-ordinate encoding.
I used pretrained inception resnet_v2, but using batch size of 16:-
CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --batch_size=16 --checkpoint_inception=./inception_resnet_v2_2016_08_30.ckpt

Still I am not able to reach accuracy of 84.2 on fsns dataset as shown in paper, even after training for 853k iterations.

So I have 3 questions:-
1. Will batch size = 32 give me 84.2 accuracy?
2. or/and did you used random initialization of incoder instead of pretrained inception_resnet_v2?
3. Shall I change eval.py at test time, is default code randomly cropping 80% of image and resizing during test time as well?

Basically I want to know how can I exactly replicate the training process to get 84.2 % sequence level accuracy on fsns dataset?

",5,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-09 10:43:27,open,,,[],2018-12-20 19:34:40
431,tensorflow/models,models,5728,NamanChuriwala,Towered Optimizer in Transformer_main.py fails,"Hi,
I tried implemented distributed training in Tensorflow using Towered Optimizer in transformer_main.py by wrapping the optimizer and model_fn  as follows:
  **optimizer = tf.contrib.estimator.TowerOptimizer(optimizer)
  model_fn=tf.contrib.estimator.replicate_model_fn(model_fn)**

**But while training and evaluating, I get the following error:
**InvalidArgumentError (see above for traceback): Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 113) and num_split 2
 [[Node: split_inputs/split = Split[T=DT_INT64, num_split=2, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](split_inputs/split_1/split_dim, FunctionBufferingResourceGetNext/_4681)]]
 [[Node:tower_1/model/Transformer/decode/decoder_stack/layer_0/ffn/feed_foward_network/filter_layer/Tensordot/GatherV2/_5917 = _HostRecv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_8704_...t/GatherV2"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]****",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-11-09 10:26:23,open,,,['stat:awaiting response'],2018-11-10 00:33:13
432,tensorflow/models,models,5727,1453042287,Object Detection API:how to inference from image directly?,"### System information
- **What is the top-level directory of the model you are using**:
object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA10  cuDNN7.3
- **GPU model and memory**:
rxt2080ti 11G
- **Exact command to reproduce**:
none
### Describe the problem
like the title, how can i use the model to detect on images directly instead of transforming the images to tfrecord firstly?

### Source code / logs
none
",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-09 08:44:17,open,,,[],2018-11-15 01:01:53
433,tensorflow/models,models,5726,anonym24,*_pets.config vs *_coco.config - what should I use for training my own classes?,"I want to retrain coco mobilenet v1 (to add my own classes) - http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz

there are two configs:

- ssd_mobilenet_v1_coco.config
- ssd_mobilenet_v1_pets.config

which one should I use?

does suffix **pets** means that config should used to train with your own custom classes or what?
so should I choose **pets** config?",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-11-09 08:26:24,open,,,['stat:awaiting response'],2018-11-10 00:33:09
434,tensorflow/models,models,5724,YknZhu,number of steps for training SSD inception on CoCo,"Sorry to hijack this post
Could you please tell me how many steps are required to train COCO dataset on SSD Inception V2 model? I have checked tensorflow object detection api paper, it is mentioned LR rate was decreased by 0.95 for every 800k steps. But what is the total number of steps required to train COCO dataset? 
Please help me.
Thank you.

_Originally posted by @jillelajitta in https://github.com/tensorflow/models/issues/5662#issuecomment-437139417_",7,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-08 21:04:18,open,,,[],2018-12-20 10:30:17
435,tensorflow/models,models,5721,Alexcdut,demo,pull isiosia,1,,[],2018-11-08 14:31:37,open,,,['cla: no'],2018-11-08 14:31:42
436,tensorflow/models,models,5719,anonym24,"model_main.py - eats a lot of cpu, doesn't use that match gpu, trains slowly","It never starts any training, but just loads my CPU by 90%

![image](https://user-images.githubusercontent.com/8851301/48186005-6adb4580-e33f-11e8-9946-b372f3955065.png)

```
python object_detection\model_main.py --pipeline_config_path=object_detection\training\ssd_mobilenet_v1_coco.config --model_dir=object_detection\images --num_train_steps=50000 --sample_1_of_n_eval_examples=1 --alsologtostderr
WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
W1108 10:06:58.010473  8132 tf_logging.py:125] Forced number of epochs for all eval validations to be 1.
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
W1108 10:06:58.010473  8132 tf_logging.py:125] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x0000023D23E837B8>) includes params argument, but params are not passed to Estimator.
W1108 10:06:58.026091  8132 tf_logging.py:125] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x0000023D23E837B8>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1108 10:06:58.162420  8132 tf_logging.py:125] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From C:\tensorflow1\models\research\object_detection\builders\dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
W1108 10:06:58.271738  8132 tf_logging.py:125] From C:\tensorflow1\models\research\object_detection\builders\dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
WARNING:tensorflow:From C:\Users\Admin\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\ops\sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
W1108 10:06:58.412351  8132 tf_logging.py:125] From C:\Users\Admin\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\ops\sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From C:\tensorflow1\models\research\object_detection\core\preprocessor.py:1207: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
W1108 10:06:58.568533  8132 tf_logging.py:125] From C:\tensorflow1\models\research\object_detection\core\preprocessor.py:1207: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From C:\tensorflow1\models\research\object_detection\builders\dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
W1108 10:06:59.552674  8132 tf_logging.py:125] From C:\tensorflow1\models\research\object_detection\builders\dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
2018-11-08 10:07:17.951339: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-11-08 10:07:18.276994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7465
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 4.97GiB
2018-11-08 10:07:18.281700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-08 10:07:24.611608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-08 10:07:24.616268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2018-11-08 10:07:24.617424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2018-11-08 10:07:24.618709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4727 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
```

### System information
- **What is the top-level directory of the model you are using**: http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10
- **TensorFlow installed from (source or binary)**: pip tensorflow-gpu https://www.tensorflow.org/install/pip
- **TensorFlow version (use command below)**: 1.12.0
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: GTX 1060 6GB
",15,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-11-08 08:17:48,open,,"NamedUser(login=""anonym24"")",[],2019-03-20 14:29:15
437,tensorflow/models,models,5718,baihualinxin,objcet detection on  Android  is label The correct value cannot be used,"
------------------------

### System information
- **What is the top-level directory of the model you are using**:models-master\research\object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04 windwos 10
- **TensorFlow installed from (source or binary)**:pip
- **TensorFlow version (use command below)**:1.10.0
- **Bazel version (if compiling from source)**:0.17.01
- **CUDA/cuDNN version**:cuda 9 cudnn 8
- **GPU model and memory**:GTX 1080Ti
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:
b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0

### Describe the problem
Android demo in run Mobile phone label Fixed value
![792162767640418762](https://user-images.githubusercontent.com/13042809/48185098-fae4b380-e36e-11e8-99dc-1be27065de96.jpg)
![251355634106298905](https://user-images.githubusercontent.com/13042809/48185101-fc15e080-e36e-11e8-93e7-b933b7777d27.jpg)

label is  Identifying anything is 0.73


data set: klitti data
config:ssd_mobilenet_v1_quantized_300x300_coco14_sync.config
pbtxt：kitti_label_map.pbtxt
train：models-master\research\object_detection\legacy\train.py
        --logtostderr \
        --train_dir=path/to/train_dir \
        --pipeline_config_path=pipeline_config.pb

ckpt turn pb

models-master\research\object_detection\export_tflite_ssd_graph.py

python object_detection/export_tflite_ssd_graph \
    --pipeline_config_path path/to/pipeline.config \
    --trained_checkpoint_prefix path/to/model.ckpt-20000 \
    --output_directory path/to/exported_model_directory


pb turn tflite

bazel run --config=opt tensorflow/contrib/lite/toco:toco -- \
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops

Explain:
mobilenet_v1_quantized_300x300_coco14_sync.config parameter
model {
  ssd {
    inplace_batchnorm_update: true
    freeze_batchnorm: false
    num_classes: 2
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    encode_background_as_zeros: true
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        class_prediction_bias_init: -4.6
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            random_normal_initializer {
              stddev: 0.01
              mean: 0.0
            }
          }
          batch_norm {
            scale: true,
            center: true,
            decay: 0.97,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v1'
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          random_normal_initializer {
            stddev: 0.01
            mean: 0.0
          }
        }
        batch_norm {
          scale: true,
          center: true,
          decay: 0.97,
          epsilon: 0.001,
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    loss {
      classification_loss {
        weighted_sigmoid_focal {
          alpha: 0.75,
          gamma: 2.0
        }
      }
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    normalize_loc_loss_by_codesize: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  #fine_tune_checkpoint: ""PATH_TO_BE_CONFIGURED/model.ckpt""
  batch_size: 32
  sync_replicas: true
  startup_delay_steps: 0
  replicas_to_aggregate: 8
  num_steps: 50000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  #data_augmentation_options {
   # ssd_random_crop {
   # }
  #}
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: .2
          total_steps: 50000
          warmup_learning_rate: 0.06
          warmup_steps: 2000
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}
train_input_reader: {
  tf_record_input_reader {
    input_path: ""D:/SSD_KITTI/data/test_data/kitti_train.tfrecord""
  }
  label_map_path: ""G:/KITTI_record/tfrecord/kitti_label_map.pbtxt""
}

eval_config: {
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
  num_examples: 8000
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""D:/SSD_KITTI/data/test_data/kitti_val.tfrecord""
  }
  label_map_path: ""G:/KITTI_record/tfrecord/kitti_label_map.pbtxt""
  shuffle: false
  num_readers: 1
}

graph_rewriter {
  quantization {
    delay: 48000
    activation_bits: 8
    weight_bits: 8
  }
}

problem:
1.config parameter  Is there a problem?

2.Is it necessary to modify?
export_tflite_ssd_graph.py 
 flags.DEFINE_integer('max_detections', 10,
                     'Maximum number of detections (boxes) to show.')
flags.DEFINE_integer('max_classes_per_detection', 1,
                     'Number of classes to display per detection box.')

3.I don't know what else is wrong？
Or is there a problem with the model?


4.Android demo 
  private static final String TF_OD_API_MODEL_FILE = ""detect.tflite"";
  private static final String TF_OD_API_LABELS_FILE = ""file:///android_asset/labels_list.txt"";

What parameters do I need to change?
",13,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-08 08:16:13,open,,,[],2019-03-28 10:15:39
438,tensorflow/models,models,5713,anonym24,export_tflite_ssd_graph.py - doesn't create anything in /tmp/tflite ,"from the next link I'm trying to get the frozen graph

https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md

```
object_detection/export_tflite_ssd_graph.py \
--pipeline_config_path=/home/user/Downloads/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18/pipeline.config \
--trained_checkpoint_prefix=/home/user/Downloads/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18/model.ckpt \
--output_directory=/tmp/tflite \
--add_postprocessing_op=true
```

Logs:

```
object_detection/export_tflite_ssd_graph.py \
> --pipeline_config_path=/home/user/Downloads/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18/pipeline.config \
> --trained_checkpoint_prefix=/home/user/Downloads/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18/model.ckpt \
> --output_directory=/tmp \
> --add_postprocessing_op=true
object_detection/export_tflite_ssd_graph.py: line 81: preprocess: command not found
object_detection/export_tflite_ssd_graph.py: line 81: config_override: command not found
object_detection/export_tflite_ssd_graph.py: line 81: rExports an SSD detection model to use with tf-lite.

Outputs file:
* A tflite compatible frozen graph - /tflite_graph.pb

The exported graph has the following input and output nodes.

Inputs:
'normalized_input_image_tensor': a float32 tensor of shape
[1, height, width, 3] containing the normalized input image. Note that the
height and width must be compatible with the height and width configured in
the fixed_shape_image resizer options in the pipeline config proto.

In floating point Mobilenet model, 'normalized_image_tensor' has values
between [-1,1). This typically means mapping each pixel (linearly)
to a value between [-1, 1]. Input image
values between 0 and 255 are scaled by (1/128.0) and then a value of
-1 is added to them to ensure the range is [-1,1).
In quantized Mobilenet model, 'normalized_image_tensor' has values between [0,
255].
In general, see the  function defined in the feature extractor class
in the object_detection/models directory.

Outputs:
If add_postprocessing_op is true: frozen graph adds a
  TFLite_Detection_PostProcess custom op node has four outputs:
  detection_boxes: a float32 tensor of shape [1, num_boxes, 4] with box
  locations
  detection_classes: a float32 tensor of shape [1, num_boxes]
  with class indices
  detection_scores: a float32 tensor of shape [1, num_boxes]
  with class scores
  num_boxes: a float32 tensor of size 1 containing the number of detected boxes
else:
  the graph has two outputs:
   'raw_outputs/box_encodings': a float32 tensor of shape [1, num_anchors, 4]
    containing the encoded box predictions.
   'raw_outputs/class_predictions': a float32 tensor of shape
    [1, num_anchors, num_classes] containing the class scores for each anchor
    after applying score conversion.

Example Usage:
--------------
python object_detection/export_tflite_ssd_graph     --pipeline_config_path path/to/ssd_mobilenet.config     --trained_checkpoint_prefix path/to/model.ckpt     --output_directory path/to/exported_model_directory

The expected output would be in the directory
path/to/exported_model_directory (which is created if it does not exist)
with contents:
 - tflite_graph.pbtxt
 - tflite_graph.pb
Config overrides (see the  flag) are text protobufs
(also of type pipeline_pb2.TrainEvalPipelineConfig) which are used to override
certain fields in the provided pipeline_config_path.  These are useful for
making small changes to the inference graph that differ from the training or
eval config.

Example Usage (in which we change the NMS iou_threshold to be 0.5 and
NMS score_threshold to be 0.0):
python object_detection/export_tflite_ssd_graph     --pipeline_config_path path/to/ssd_mobilenet.config     --trained_checkpoint_prefix path/to/model.ckpt     --output_directory path/to/exported_model_directory
    --config_override : No such file or directory
import-im6.q16: not authorized `tf' @ error/constitute.c/WriteImage/1037.
from: can't read /var/mail/google.protobuf
from: can't read /var/mail/object_detection
from: can't read /var/mail/object_detection.protos
object_detection/export_tflite_ssd_graph.py: line 99: flags: command not found
object_detection/export_tflite_ssd_graph.py: line 100: syntax error near unexpected token `'output_directory','
object_detection/export_tflite_ssd_graph.py: line 100: `flags.DEFINE_string('output_directory', None, 'Path to write outputs.')'

```

### System information
- **What is the top-level directory of the model you are using**: http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: python pip tensorflow-gpu
- **TensorFlow version (use command below)**: 1.11
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: GTX 1060 6GB
",9,"NamedUser(login=""achowdhery"")","[NamedUser(login=""achowdhery"")]",2018-11-07 11:44:44,open,,,[],2019-03-18 20:06:49
439,tensorflow/models,models,5711,bellycat77,Bugfix in config_util.py,Bug in config_util.py `within get_configs_from_multiple_files()`,0,,[],2018-11-07 08:57:56,open,,,['cla: yes'],2018-11-07 08:57:58
440,tensorflow/models,models,5710,xiaohu2015,questions about the data augmentation of  object detection API.,"Hi, I have read the core/preprocessor.py file, I have some questions about the ssd_random_crop, 

```
def ssd_random_crop(image,
                    boxes,
                    labels,
                    label_scores=None,
                    multiclass_scores=None,
                    masks=None,
                    keypoints=None,
                    min_object_covered=(0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0),
                    aspect_ratio_range=((0.5, 2.0),) * 7,
                    area_range=((0.1, 1.0),) * 7,
                    overlap_thresh=(0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0),
                    random_coef=(0.15,) * 7,
                    seed=None,
                    preprocess_vars_cache=None):

```

Why you use the same value for  'overlap_thresh' when set the `min_object_covered`, I think it is not normal. for example, when `min_object_covered`=1.0, maybe a object with overlap_thresh=0.8 is filterd. If you assign it as background, is that reasonable?
In original SSD paper, the strategy is to keep overlapped part if the center of it is in the sample patch.

@jch1 @tombstone @raymond-yuan ",1,"NamedUser(login=""wt-huang"")","[NamedUser(login=""wt-huang"")]",2018-11-07 08:48:26,open,,,[],2018-11-07 15:28:54
441,tensorflow/models,models,5709,wujsy,use deep_speech to train chinese model error,"
### System information
- **What is the top-level directory of the model you are using**:/path/models/research/deep_speech
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:""Ubuntu 16.04.4 LTS""
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.10.0-gpu
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:CUDA9 & cuDNN7
- **GPU model and memory**:Tesla v100,16G
- **Exact command to reproduce**:
#!/bin/bash
export PYTHONPATH=""$PYTHONPATH:/data/workspace/models""

CUDA_VISIBLE_DEVICES=0,1,2,3 \
python -u deep_speech.py \
  --model_dir='./models/aishell' \
  --train_data_dir='/data/workspace/models/research/deep_speech/data/aishell/manifest.train' \
  --eval_data_dir='/data/workspace/models/research/deep_speech/data/aishell/manifest.dev' \
  --vocabulary_file='/data/workspace/models/research/deep_speech/data/aishell/vocab_normal.txt' \
  --num_gpus=4 \
  --batch_size=64 \
  --train_epochs=100 
### Describe the problem
Hi, when i used deep_speech to train aishell dataset, errors occured, it looks like key error and the terminal info is as follows:
### Source code / logs
tensorflow.python.framework.errors_impl.UnknownError: KeyError: ' '
Traceback (most recent call last):

  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/script_ops.py"", line 206, in __call__
    ret = func(*args)

  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 416, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""/data/workspace/models/research/deep_speech/data/dataset.py"", line 227, in _gen_data
    transcript, text_featurizer.token_to_index)

  File ""/data/workspace/models/research/deep_speech/data/featurizer.py"", line 91, in compute_label_feature
    feats = [token_to_idx[token] for token in tokens]

  File ""/data/workspace/models/research/deep_speech/data/featurizer.py"", line 91, in <listcomp>
    feats = [token_to_idx[token] for token in tokens]

KeyError: ' '


	 [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_FLOAT, DT_INT32, DT_INT32, DT_INT32], token=""pyfunc_1"", _device=""/device:CPU:0""](arg0)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,161,1], [?,1], [?,1], [?,?]], output_types=[DT_FLOAT, DT_INT32, DT_INT32, DT_INT32]](IteratorFromStringHandle)]]
	 [[Node: FunctionBufferingResourceGetNext = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT32, DT_INT32, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FunctionBufferingResource)]]
	 [[Node: boolean_mask_1/Reshape/_5133 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2634_boolean_mask_1/Reshape"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
> /data/workspace/models/research/deep_speech/deep_speech.py(285)run_deep_speech()
-> estimator.train(input_fn=input_fn_train, hooks=train_hooks)

any ideas? thanks
",4,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-11-07 07:47:29,open,,,[],2019-03-06 06:37:38
442,tensorflow/models,models,5707,786694836,after run model_main.py,"It show me this : 

creating index...
index created!
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=39.81s).
Accumulating evaluation results...
DONE (t=11.59s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.334
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.548
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.350
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.110
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.455
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.357
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.490
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.514
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.638
2018-11-07 04:40:20.876549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-07 04:40:20.876662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-07 04:40:20.876675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-07 04:40:20.876682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-07 04:40:20.877056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7942 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:03:00.0, compute capability: 6.0)
WARNING:tensorflow:From /home-ex/tclsz/miniconda3/envs/tensorflow-abb/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py:1018: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.
Instructions for updating:
Pass your op to the equivalent parameter main_op instead.
W1107 04:40:21.531219 140403135842112 tf_logging.py:125] From /home-ex/tclsz/miniconda3/envs/tensorflow-abb/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py:1018: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.
Instructions for updating:
Pass your op to the equivalent parameter main_op instead.

and then It stopped.

I don't know what it means.

So who can tell me what the right method to train the ssdlite model that can show me the steps and the loss ?",16,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-11-07 01:40:24,open,,,[],2019-01-22 09:29:20
443,tensorflow/models,models,5700,netanel-s,[Feature Request] Better interface for classes and dataset handling ,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**: N/A

### Describe the problem
Best to my knowledge (and you're more than welcome to correct me), there's no convenient interface for class and dataset handling. For example, say you have COCO dataset in TFrecords format, but you don't want to use all its classes, but only a subset. Then the best current solution I found is to convert the entire dataset while filtering the unwanted classes, and shifting the IDs of the remaining classes, so that the `num_classes` in the config file can be reduced correspondingly as well (since it has to be as the largest ID number).

This is a very non-efficient way to achieve such thing, instead I suggest that there would be a way to do such thing through the API without reconverting the dataset. For example, the label map can have both `original_id` and `used_id` to distinguish between the original ID number in the dataset, and the new wished ID number, and then the `num_classes` would be the largest `used_id`. Another way is to simply use only the categories in the label map, without filling in the gaps, and then `num_classes` would be the number of classes left in the label map.

In any case, the dataset should have an option to be filtered while training or evaluating, so that examples with only unwanted classes wouldn't be included. It shouldn't be the only option, since you still might want to use images without any positive example (e.g. when I want to detect motorcycle, I would also put bicycle images so it would learn the difference).

As reconverting the entire dataset is not wanted (it takes both time and space, since different versions might need different copies with different class set), allow adding new examples with new classes after initial conversion, by simply converting the new examples, and using both TFrecord files to train/evaluate.

Thanks in advance.",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-06 09:31:45,open,,,[],2018-11-07 00:19:58
444,tensorflow/models,models,5699,oscartackstrom,Fix bug in dataset.py when batch_size==max_length.,"Each batch contains sequences of length in [buckets_min, buckets_max), so that the longest instance in a batch will have length buckets_max - 1. Without this fix, instances with length == batch_size will result in buckets of zero elements, causing the dataset iterator to crash.

As a bonus this adds an extra instance per batch.",3,,[],2018-11-06 09:21:41,open,,,['cla: yes'],2018-11-06 09:36:26
445,tensorflow/models,models,5698,netanel-s,[Feature Request/Suggestions] FPN architecture modifications and user preferences,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**: training FPN architecture

### Describe the problem
I would like to suggest 3 things regarding FPN architecture:

1. the FPN-MobileNetV2 feature extractor uses the output of the lower bottlenecks. It's first projects 192 to 32 and then goes back up to 256 in layer 7, and 576 to 96 and then back up to 256 in level 14. This acts as a serious bottleneck, and I think it should be considered whether to use the expanded tensors directly i.e. 192->256, 576->256. On the other hand, 576->256 is pretty large (2 times as 576->96+96->256), so maybe only use layer 7's expanded tensor? I would be glad to hear your opinion about it.
2. Currently, the feature pyramid only aggregates features from the backbone levels (e.g. 3,4,5), and not from the additional layers which follow. I think it should be added as an option to the configuration of FPN to also aggregate the features from the additional layers.
3. Currently, the up-sampling procedure is hard-coded to use the nearest neighbors method. I suggest to let the method be another option of the FPN configuration, and in particular to allow learned weights for the up-sampling (transposed convolution).

Thanks in advance.",1,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-11-06 09:05:50,open,,,"['stat:awaiting owner', 'type:feature']",2018-12-03 21:17:03
446,tensorflow/models,models,5697,Happy-Yu,NMS（non maximum suppression）is really used in object_detection??? ,"My question is posted at 
https://stackoverflow.com/questions/53166961/nms-non-maximum-suppression-is-really-used-in-object-detection


------------------------

### System information
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:


### Describe the problem
We are doing an project based on research->object_detection, applying ssd+mobilenet. When we focus on NMS（non maximum suppression）, we found that NMS module doesn't work.  We are curious whether NMS is used.

",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-06 06:51:05,open,,,['stat:awaiting response'],2018-12-13 09:31:58
447,tensorflow/models,models,5696,89douner,Training faster rcnn batch size,"System information
What is the top-level directory of the model you are using: /home/user
Have I written custom code: No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.10.1
Bazel version (if compiling from source): I don't use it
CUDA/cuDNN version: CUDA 9.0 / cuDNN: 7.1
GPU model and memory: Geforce GTX 1070
Exact command to reproduce: No





### Describe the problem
When I modified batch size from 1 to 64, 32, 16, 8, 4, or 2, I got an message.
 ![3](https://user-images.githubusercontent.com/31752297/48043786-be2c8700-e1cb-11e8-82a9-7c3580c3d63f.png)

![1](https://user-images.githubusercontent.com/31752297/48043791-c5ec2b80-e1cb-11e8-90e2-eb653f166598.png)


![2](https://user-images.githubusercontent.com/31752297/48043795-c7b5ef00-e1cb-11e8-9491-a4a5d3ef93ab.png)

When I fixed the batch size to 1, the error message didn't appear. 
Could you tell me the reason or the solution for batch size 64 ??






",1,"NamedUser(login=""wt-huang"")","[NamedUser(login=""wt-huang"")]",2018-11-06 04:58:53,open,,,[],2018-11-07 04:29:09
448,tensorflow/models,models,5688,laobadao,faster_rcnn_resnet101_coco_2018_01_28 in Model zoo,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): A little (object_detection_tutorial.ipynb pb path )
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.10.0
- Python version: python3
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No

### Describe the problem

http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz

Is this faster_rcnn_resnet101_coco_2018_01_28 forzen pb a offical version?

1. It is not match with faster_rcnn_resnet101_coco.config ,such as num_classes: 90 actually is 20 
2. can not run with object_detection_tutorial.ipynb , does'nt have num_detections', 'detection_boxes', 'detection_scores',  'detection_classes', outputs. 

Is there something wrong ?



",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-11-05 07:52:25,open,,,['stat:awaiting response'],2018-11-06 00:52:43
449,tensorflow/models,models,5680,dambuck,learned Prior,"Hi,
Its actually not and issue...

after reading the paper I had a look at the code and was surprised to see that the prior is learned aswell. I could not find any information about this process in the related paper.

As I am not really proficient with tensorflow code and mostly just fooling around with keras, I was wondering how the prior is learned? 
The KL loss is computed between prior and posterior, while the posterior is simpy the output of the encoder. Could you explain how the learning is done?

Could I implement it in the form of a random initialized layer and compute the KL between this layer an the posterior? Or might it work to connect the encoder to another layer(additional to the the sampling and feed forward to the generator) and use this layers output as the prior in the KL loss?

Using keras 2.2.2
tf 1.10.1 no gpu support
custom code



Thank you ",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-04 18:31:45,open,,,['stat:awaiting response'],2018-11-05 13:46:27
450,tensorflow/models,models,5676,valmunos,Feature Request [Object Detection]: Training  from Scratch,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
What is the top-level directory of the model you are using: N/A
Have I written custom code: No
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from: pip
TensorFlow version: 1.10.1
Bazel version: N/A
CUDA/cuDNN version: 9.0
GPU model and memory: Titan Xp / 12 GiB
Exact command to reproduce: N/A

### Describe the problem
Feature Request:

It's already possible to train feature extractors from scratch, and implement them as part of an object detection architecture.  It could be helpful train the CNNs in the zoo from scratch without having to implement the network and the functions described in the [""Defining Your Own Model""](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/defining_your_own_model.md) readme.  Training the extractor and network simultaneously would cut down on training and debugging time, and make life simpler for people looking to train from scratch.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-11-03 09:30:15,open,,,[],2018-11-07 12:17:34
451,tensorflow/models,models,5675,GL-RuiLa,export savedmodel for AWS sagemaker inference endpoint ,"
### System information
- **What is the top-level directory of the model you are using**:
    object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
    No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    Windows 10
- **TensorFlow installed from (source or binary)**:
    source
- **TensorFlow version (use command below)**:
     b'v1.11.0-rc2-4-gc19e29306c' 1.11.0
- **Bazel version (if compiling from source)**:
    
- **CUDA/cuDNN version**:
    cuda 8.0
- **GPU model and memory**:
    GRX 1060 
- **Exact command to reproduce**:
python object_detection/export_inference_graph.py --input_type image_tensor --pipeline_config_path object_detection/models/model/rfcn_resnet101_pedestrain.config --checkpoint_path object_detection/models/model/train/model.ckpt-573563 --inference_graph_path object_detection/models/model/pedestrain --export_as_saved_model=True
You can collect some of this information using our environment capture script:

### Describe the problem
I want to use od-API to train my model but do data inference on AWS SageMaker. The SM bring your own model platform gave a example of export inference graph from estimator using export_savedmodel() function and generate saved_model which can be used on AWS sagemaker. 
https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/tensorflow_iris_byom/tensorflow_BYOM_iris.ipynb

I know od-API doesn't export_savedmodel() function to easily export serving graph but the output generated by above command also has a saved_model folder which has a similar structure. I also followed this thread below 

https://github.com/tensorflow/models/issues/1988

to generate saved_model wieh non-empty variables directory which has exactly same structure as SageMaker required (including variables.data-00000-of-00001 and variables.index files). But the deployment is failed. I am wondering if there is anyone who has experience in tensorflow od-API and AWS sagemaker could help me with that. Thanks. 

",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-11-03 00:50:48,open,,,[],2018-11-04 00:14:06
452,tensorflow/models,models,5672,zyc4me,"object detection:  I changed the score converter to SOFTMAX,and class...loss: weighted_softmax","but, why does the regulization losses continue increase during training and finally it is very big than other two loss?  however,the classification loss and loc loss is normal. AND when i use SIGMOID converter and weighted _sigmoid  classification loss,  i do not have this problom.
![image](https://user-images.githubusercontent.com/34511872/47916531-75ca4c00-dee1-11e8-90be-6f8d90e4213e.png)

and my config is as follow:


model {
  ssd {
    num_classes: 2
    image_resizer {
      fixed_shape_resizer {
        height: 256
        width: 256
      }
    }
    feature_extractor {
      type: ""ssd_mobilenet_v1""
      depth_multiplier: 0.5
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.99999989895e-05
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.0299999993294
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.999700009823
          center: true
          scale: true
          epsilon: 0.0010000000475
          train: true
        }
      }
      use_depthwise: true
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.99999989895e-05
            }
          }
          initializer {
            truncated_normal_initializer {
              mean: 0.0
              stddev: 0.0299999993294
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.999700009823
            center: true
            scale: true
            epsilon: 0.0010000000475
            train: true
          }
        }
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.800000011921
        kernel_size: 3
        box_code_size: 4
        apply_sigmoid_to_scores: false
        use_depthwise: true
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 5
        min_scale: 0.20000000298
        max_scale: 0.949999988079
        aspect_ratios: 1.0
        aspect_ratios: 0.10000000149
        aspect_ratios: 0.5
        aspect_ratios: 0.20000000298
        aspect_ratios: 0.333299994469
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0100000118837
        iou_threshold: 0.5
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SOFTMAX
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_softmax {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.990000009537
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 3
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
  }
}
train_config {
  batch_size: 64
  data_augmentation_options {
    random_adjust_brightness {
    }
  }
  data_augmentation_options {
    random_adjust_contrast {
    }
  }
  data_augmentation_options {
    random_adjust_hue {
    }
  }
  data_augmentation_options {
    random_adjust_saturation {
    }
  }
  data_augmentation_options {
    random_distort_color {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  optimizer {
    momentum_optimizer {
      learning_rate {
        manual_step_learning_rate {
          schedule {
            step: 1
            learning_rate: 0.0010000000475
          }
          schedule {
            step: 80000
            learning_rate: 0.000500000023749
          }
          schedule {
            step: 150000
            learning_rate: 0.000250000011874
          }
          schedule {
            step: 200000
            learning_rate: 0.000125000005937
          }
          schedule {
            step: 240000
            learning_rate: 1.24999996842e-05
          }
        }
      }
      momentum_optimizer_value: 0.899999976158
    }
  }
  fine_tune_checkpoint: ""/home/zyc/tensorflow/coco_train/snapshot_coco_ssdlite_mobilenetv1_256_0.5x_5/model.ckpt-614942""
  from_detection_checkpoint: true
  num_steps: 2000000
}
train_input_reader {
  label_map_path: ""/home/zyc/tensorflow/PersonDataMerge4/data/person_label_map.pbtxt""
  tf_record_input_reader {
    input_path: ""/home/zyc/tensorflow/PersonDataMerge4/data/person_train.record""
  }
}
eval_config {
  num_examples: 1797
  use_moving_averages: false
  retain_original_images: true
}
eval_input_reader {
  label_map_path: ""/home/zyc/tensorflow/PersonDataMerge4/data/person_label_map.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""/home/zyc/tensorflow/PersonDataMerge4/data/person_test.record""
  }
}
",0,"NamedUser(login=""wt-huang"")","[NamedUser(login=""wt-huang"")]",2018-11-02 12:56:29,open,,,[],2018-11-03 04:09:11
453,tensorflow/models,models,5671,joyhuang9473,Add keypoint setting support to dataset builder,"Take `num_keypoints` setting in input_reader_config to read groundtruth_keypoints
with TfExampleDecoder.

Ref:

- https://github.com/tensorflow/models/blob/master/research/object_detection/data_decoders/tf_example_decoder.py#L263",0,,[],2018-11-02 08:54:14,open,,,['cla: yes'],2018-11-02 08:54:17
454,tensorflow/models,models,5668,moderato,"[Object Detection] TF trains 1 epoch and evaluates indefinitely, even when a limit is set","### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.11
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.2/cudnn 7.3.0
- **GPU model and memory**: GTX 1050 Ti 4G
- **Exact command to reproduce**:
```bash
PIPELINE_CONFIG_PATH=/home/zhongyilin/Documents/models/research/object_detection/trained_models/bdd100k_ssdlite_mobilenet_v2/ssdlite_mobilenet_v2_bdd100k.config
MODEL_DIR=/home/zhongyilin/Documents/models/research/object_detection/trained_models/bdd100k_ssdlite_mobilenet_v2/train_logs
NUM_TRAIN_STEPS=200000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python object_detection/model_main.py \
	--pipeline_config_path=${PIPELINE_CONFIG_PATH} \
	--model_dir=${MODEL_DIR} --num_train_steps=${NUM_TRAIN_STEPS} \
	--sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
	--alsologtostderr
```

Config file:
```
# SSDLite with Mobilenet v2 configuration for BDD100K Dataset.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for ""PATH_TO_BE_CONFIGURED"" to find the fields that
# should be configured.

model {
  ssd {
    num_classes: 10
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 534
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 3
        use_depthwise: true
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.03
              mean: 0.0
            }
          }
          batch_norm {
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v2'
      min_depth: 16
      depth_multiplier: 1.0
      use_depthwise: true
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.9997,
          epsilon: 0.001,
        }
      }
    }
    loss {
      classification_loss {
        weighted_sigmoid {
        }
      }
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 3
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  batch_size: 8
  batch_queue_capacity: 2
  prefetch_queue_capacity: 2
  optimizer {
    rms_prop_optimizer: {
      learning_rate: {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.004
          decay_steps: 800720
          decay_factor: 0.95
        }
      }
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  # fine_tune_checkpoint: ""/home/zhongyilin/Documents/models/research/object_detection/trained_models/bdd100k_ssdlite_mobilenet_v2/train_logs/model.ckpt""
  # fine_tune_checkpoint_type: ""detection""
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""/home/zhongyilin/Documents/data/BDD/tfrecord/100k/train/output_train100k_000000.tfrecord""
  }
  label_map_path: ""object_detection/data/bdd100k_label_map.pbtxt""
}

eval_config: {
  num_examples: 10000
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 1
  eval_interval_secs: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""/home/zhongyilin/Documents/data/BDD/tfrecord/100k/val/output_val100k_000000.tfrecord""
  }
  label_map_path: ""object_detection/data/bdd100k_label_map.pbtxt""
  shuffle: false
  num_readers: 1
}
```

### Describe the problem
As the title said. From Tensorboard it looks like the training only runs for 1 epoch and it keeps running eval all the time. The training loss doesn't get updated, while the eval loss updates for a bunch of data points. I have reset the parameters in ""eval_config"" with different values but it doesn't work.

Plus, it looks like the ""fine_tune_checkpoint"" in ""train_config"" doesn't work when it's set to PATH/model.ckpt. It shows an error of 
```
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /home/zhongyilin/Documents/models/research/object_detection/trained_models/bdd100k_ssdlite_mobilenet_v2/train_logs/model.ckpt
```
### Source code / logs
```
zhongyilin@zhongyilin-ECE:~/Documents/models/research/scripts$ ./bdd100k_mobilenet_v2_ssdlite.sh 
/home/zhongyilin/Documents/models/research/object_detection/utils/visualization_utils.py:27: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.
  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
W1101 21:06:28.286998 139715550816000 tf_logging.py:125] Forced number of epochs for all eval validations to be 1.
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
W1101 21:06:28.287134 139715550816000 tf_logging.py:125] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f1181c42378>) includes params argument, but params are not passed to Estimator.
W1101 21:06:28.287409 139715550816000 tf_logging.py:125] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f1181c42378>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1101 21:06:28.308489 139715550816000 tf_logging.py:125] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /home/zhongyilin/Documents/models/research/object_detection/core/preprocessor.py:1207: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
W1101 21:06:28.501330 139715550816000 tf_logging.py:125] From /home/zhongyilin/Documents/models/research/object_detection/core/preprocessor.py:1207: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From /home/zhongyilin/Documents/models/research/object_detection/builders/dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
W1101 21:06:29.325152 139715550816000 tf_logging.py:125] From /home/zhongyilin/Documents/models/research/object_detection/builders/dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
2018-11-01 21:06:42.719184: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-01 21:06:42.787760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-01 21:06:42.788059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.468
pciBusID: 0000:02:00.0
totalMemory: 3.94GiB freeMemory: 3.60GiB
2018-11-01 21:06:42.788071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-01 21:06:42.928717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-01 21:06:42.928742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-01 21:06:42.928748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-01 21:06:42.928843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3316 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-11-01 21:06:58.060117: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-01 21:06:58.396333: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.49GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-01 21:06:58.399570: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-01 21:06:58.574441: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.88GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-01 21:06:59.115525: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-01 21:16:58.264482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-01 21:16:58.264513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-01 21:16:58.264518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-01 21:16:58.264521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-01 21:16:58.264591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3316 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
creating index...
index created!
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=96.65s).
Accumulating evaluation results...
DONE (t=8.93s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004
2018-11-01 21:33:05.603130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-01 21:33:05.603162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-01 21:33:05.603166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-01 21:33:05.603169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-01 21:33:05.603231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3316 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
creating index...
index created!
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=83.82s).
Accumulating evaluation results...
DONE (t=8.13s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004
2018-11-01 21:49:09.087981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-01 21:49:09.088014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-01 21:49:09.088019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-01 21:49:09.088022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-01 21:49:09.088090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3316 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
^CTraceback (most recent call last):
  File ""object_detection/model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 1181, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 1215, in _train_model_default
    saving_listeners)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 1409, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1148, in run
    run_metadata=run_metadata)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1224, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1304, in run
    run_metadata=run_metadata))
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 581, in after_run
    if self._save(run_context.session, global_step):
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 606, in _save
    if l.after_save(session, step):
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 517, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 537, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 912, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 476, in evaluate
    return _evaluate()
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 469, in _evaluate
    output_dir=self.eval_dir(name))
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 1528, in _evaluate_run
    config=self._session_config)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/evaluation.py"", line 212, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1148, in run
    run_metadata=run_metadata)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1224, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1296, in run
    run_metadata=run_metadata)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1076, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1292, in _do_call
    return fn(*args)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/zhongyilin/miniconda3/envs/neon/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1367, in _call_tf_sessionrun
    run_metadata)
KeyboardInterrupt
```
Above shows the log produced by a normal run using the script and config file provided. It's been Ctrl+C at the end.

Anyone has an idea how to solve it? Thanks in advance!",4,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-11-02 07:42:47,open,,,[],2018-12-22 08:01:49
455,tensorflow/models,models,5666,rahenri,Large model from ptb_word_lm.py performs badly,"### System information
- **What is the top-level directory of the model you are using**:
tutorials/rnn/ptb

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
Binary

- **TensorFlow version (use command below)**:
1.11.0

- **Bazel version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
CUDA V9.1.85
cuDNN 7.0

- **GPU model and memory**:
1080 TI, 11Gb

- **Exact command to reproduce**:
python ptb_word_lm.py  --data_path <PATH TO PTB DATASET> --model large

### Describe the problem
The final perplexity on test dataset is > 500, but I was expecting to be bellow 80 as described at the end of this page: https://www.tensorflow.org/tutorials/sequences/recurrent. And also as described on the paper this implementation is based on.

### Source code / logs
Nothing really useful here.",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-11-02 05:53:35,open,,,[],2018-11-03 00:20:25
456,tensorflow/models,models,5664,FilippoVannella,SSD MobineNetV2 structure,"I am confusing between SSD and mobilenet. As far as I know, both of them are neural networks. SSD provides localization while mobilenet provides classification. Thus the combination of SSD and mobilenet can produce the object detection. The default classification network of SSD is VGG-16. So, for SSD Mobilenet, VGG-16 is replaced with mobilenet. Are my statements correct?

Where can I get more information about SSD Mobilenet especially that one available on Tensorflow model zoo?",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-11-01 17:44:47,open,,,['stat:awaiting response'],2018-11-02 12:20:21
457,tensorflow/models,models,5663,asgill,[Deeplab v3+] Training model from scratch,"I want to train deeplab v3+ from scratch, without even using imagenet pretrained checkpoint. With this setting, I am not being able to get the model to converge. I tried changing learning rate, learning rate policy, optimization with Adam, but nothing seems to help. I would appreciate any help with this.
Thanks!",6,,[],2018-11-01 16:24:13,open,,,[],2019-04-03 03:47:24
458,tensorflow/models,models,5661,shengheng1018,"pyfunc_1 returns 3 values, but expects to see 1 values.","
- **What is the top-level directory of the model you are using**:
object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
pip install tensorflow-gpu
- **TensorFlow version (use command below)**:
1.11
- **Bazel version (if compiling from source)**:
none
- **CUDA/cuDNN version**:
CUDA9.0 cuDNN7.2
- **GPU model and memory**:
GeForce  GTX 1080Ti   11G
- **Exact command to reproduce**:
python models\research\object_detection\model_main.py --alsologtostderr --num_eval_steps=500 --num_train_steps=100000 --model_dir=training\ssd_main --pipeline_config_path=training\ssd_main\ssd_mobilenet_v2_coco.config

### Describe the problem
I use api and ssd_mobilenet_v2_coco.config to train my own dataset.The training program is executed normally,and the checkpint can be generated normally, but the error is reported when the evaluation program is executed.

### Source code / logs
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x000001975F672510>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From F:\tensorflow\models\research\object_detection\core\preprocessor.py:1205: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From F:\tensorflow\models\research\object_detection\builders\dataset_builder.py:146: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
2018-11-01 22:09:20.675703: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-11-01 22:09:21.216536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:07:00.0
totalMemory: 11.00GiB freeMemory: 9.10GiB
2018-11-01 22:09:21.222423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-01 22:09:22.091657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-01 22:09:22.094662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0
2018-11-01 22:09:22.096270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N
2018-11-01 22:09:22.098032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8795 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:07:00.0, compute capability: 6.1)
2018-11-01 22:19:48.414617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-01 22:19:48.416729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-01 22:19:48.419920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0
2018-11-01 22:19:48.421712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N
2018-11-01 22:19:48.423399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8795 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:07:00.0, compute capability: 6.1)
creating index...
index created!
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=7.91s).
Accumulating evaluation results...
DONE (t=0.36s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.061
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.163
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.021
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.083
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.056
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.107
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
Traceback (most recent call last):
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\client\session.py"", line 1292, in _do_call
    return fn(*args)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\client\session.py"", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\client\session.py"", line 1367, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: pyfunc_0 returns 3 values, but expects to see 1 values.
         [[{{node map/while/PyFunc}} = PyFunc[Tin=[DT_UINT8, DT_FLOAT, DT_INT64, DT_FLOAT], Tout=[DT_UINT8], _class=[""loc:@map/while/TensorArrayWrite/TensorArrayWriteV3""], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/while/TensorArrayReadV3, map/while/TensorArrayReadV3_1/_2353, map/while/TensorArrayReadV3_2, map/while/TensorArrayReadV3_3/_2355)]]
         [[{{node map/while/TensorArrayWrite/TensorArrayWriteV3/_2359}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1747_map/while/TensorArrayWrite/TensorArrayWriteV3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](^_cloopmap/while/NextIteration_2/_2188)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""models\research\object_detection\model_main.py"", line 103, in <module>
    tf.app.run()
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""models\research\object_detection\model_main.py"", line 99, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 610, in run
    return self.run_local()
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1181, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1215, in _train_model_default
    saving_listeners)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1409, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1148, in run
    run_metadata=run_metadata)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1239, in run
    raise six.reraise(*original_exc_info)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1224, in run
    return self._sess.run(*args, **kwargs)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1304, in run
    run_metadata=run_metadata))
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py"", line 581, in after_run
    if self._save(run_context.session, global_step):
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py"", line 606, in _save
    if l.after_save(session, step):
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 517, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 537, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 912, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 476, in evaluate
    return _evaluate()
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 469, in _evaluate
    output_dir=self.eval_dir(name))
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1528, in _evaluate_run
    config=self._session_config)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\evaluation.py"", line 212, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 783, in __exit__
    self._close_internal(exception_type)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 816, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py"", line 941, in end
    self._final_ops, feed_dict=self._final_ops_feed_dict)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\client\session.py"", line 887, in run
    run_metadata_ptr)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\client\session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\client\session.py"", line 1286, in _do_run
    run_metadata)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\client\session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: pyfunc_0 returns 3 values, but expects to see 1 values.
         [[{{node map/while/PyFunc}} = PyFunc[Tin=[DT_UINT8, DT_FLOAT, DT_INT64, DT_FLOAT], Tout=[DT_UINT8], _class=[""loc:@map/while/TensorArrayWrite/TensorArrayWriteV3""], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/while/TensorArrayReadV3, map/while/TensorArrayReadV3_1/_2353, map/while/TensorArrayReadV3_2, map/while/TensorArrayReadV3_3/_2355)]]
         [[{{node map/while/TensorArrayWrite/TensorArrayWriteV3/_2359}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1747_map/while/TensorArrayWrite/TensorArrayWriteV3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](^_cloopmap/while/NextIteration_2/_2188)]]

Caused by op 'map/while/PyFunc', defined at:
  File ""models\research\object_detection\model_main.py"", line 103, in <module>
    tf.app.run()
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""models\research\object_detection\model_main.py"", line 99, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 610, in run
    return self.run_local()
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1181, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1215, in _train_model_default
    saving_listeners)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1409, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1148, in run
    run_metadata=run_metadata)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1224, in run
    return self._sess.run(*args, **kwargs)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1304, in run
    run_metadata=run_metadata))
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py"", line 581, in after_run
    if self._save(run_context.session, global_step):
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py"", line 606, in _save
    if l.after_save(session, step):
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 517, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 537, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\training.py"", line 912, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 476, in evaluate
    return _evaluate()
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 462, in _evaluate
    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1422, in _evaluate_build_graph
    self._call_model_fn_eval(input_fn, self.config))
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1458, in _call_model_fn_eval
    features, labels, model_fn_lib.ModeKeys.EVAL, config)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1169, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""F:\tensorflow\models\research\object_detection\model_lib.py"", line 371, in model_fn
    use_normalized_coordinates=False))
  File ""F:\tensorflow\models\research\object_detection\utils\visualization_utils.py"", line 440, in draw_side_by_side_evaluation_image
    use_normalized_coordinates=use_normalized_coordinates)
  File ""F:\tensorflow\models\research\object_detection\utils\visualization_utils.py"", line 385, in draw_bounding_boxes_on_image_tensors
    images = tf.map_fn(draw_boxes, elems, dtype=tf.uint8, back_prop=False)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\ops\functional_ops.py"", line 460, in map_fn
    maximum_iterations=n)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3274, in while_loop
    return_same_structure)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2994, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2929, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3243, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(*lv))
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\ops\functional_ops.py"", line 449, in compute
    packed_fn_values = fn(packed_values)
  File ""F:\tensorflow\models\research\object_detection\utils\visualization_utils.py"", line 382, in draw_boxes
    tf.uint8)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 457, in py_func
    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 281, in _internal_py_func
    input=inp, token=token, Tout=Tout, name=name)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\ops\gen_script_ops.py"", line 132, in py_func
    ""PyFunc"", input=input, token=token, Tout=Tout, name=name)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\framework\ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""e:\Program Files\Anaconda3\envs\shengheng\lib\site-packages\tensorflow\python\framework\ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): pyfunc_0 returns 3 values, but expects to see 1 values.
         [[{{node map/while/PyFunc}} = PyFunc[Tin=[DT_UINT8, DT_FLOAT, DT_INT64, DT_FLOAT], Tout=[DT_UINT8], _class=[""loc:@map/while/TensorArrayWrite/TensorArrayWriteV3""], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/while/TensorArrayReadV3, map/while/TensorArrayReadV3_1/_2353, map/while/TensorArrayReadV3_2, map/while/TensorArrayReadV3_3/_2355)]]
         [[{{node map/while/TensorArrayWrite/TensorArrayWriteV3/_2359}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1747_map/while/TensorArrayWrite/TensorArrayWriteV3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](^_cloopmap/while/NextIteration_2/_2188)]]
",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-11-01 14:35:50,open,,,[],2018-11-02 12:19:45
459,tensorflow/models,models,5660,morimkb,Image segmentation with Deeplab model gives wrong result with Jetson TX2,"I have tested deeplab model for image segmentation on my pc and it gives a correct result but when I tranfered the model to Jetson Tx2, it did not work properly, the result is the image below from Tx2

https://i.stack.imgur.com/XfQi9.png
https://i.stack.imgur.com/cQLun.png


Tx2 information: 
Linux tegra-ubuntu 4.4.38-tegra, 16.04
Tensorflow-gpu 1.9.0 nvidia release 
Open CV 3.4.1-dev 
CUDA 9.0 
Jetpack 3.3

My system information: 
Linux Ubuntu 16.04
Tensorflow-gpu 1.11.0
Open CV 3.4.3 
CUDA 9.0

I also tested this project on github: https://github.com/GustavZ/realtime_segmenation But I recieved the same wrong result, I thought that it might be a vesrion conflict between tensorflow's versions but I could not find the tensorflow-gpu 1.11.0 nvidia release for Jetson Tx2. Did someone else have the same experience here with Deeplab models on Jetson Tx2? 
",4,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-11-01 12:49:01,open,,,[],2018-12-03 13:08:10
460,tensorflow/models,models,5654,1453042287,the model.ckpt generated by the ssd_lite is too big,"### System information
- **What is the top-level directory of the model you are using**:
object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
pypi tensorflow-gpu
- **TensorFlow version (use command below)**:
1.11
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA9.0 cuDNN7.3
- **GPU model and memory**:
TITAN V 11G
- **Exact command to reproduce**:
CUDA_VISIBLE_DEVICES=0 python object_detection/model_main.py --pipeline_config_path=project/ssdlite_mobilenet_v2_coco_2018_05_09/ssdlite.config --model_dir=project/ssdlite_result --num_train_steps=100000 --sample_1_of_n_eval_examples=1 --alsologtostderr

### Describe the problem
i use the api and the ssdlite_mobilenet_v2 pre-trained model to fine-tune my own dataset, but the size of the model.ckpt-200000.data-00000-of-00001 generated by the api is huge, almost 39M, it's weird cause the model.ckpt of the ssdlite in the model zoo is only 17M, why?

",2,"NamedUser(login=""wt-huang"")","[NamedUser(login=""wt-huang"")]",2018-11-01 01:56:15,open,,,[],2018-11-01 18:27:05
461,tensorflow/models,models,5651,FilippoVannella,Difference between SSD and Mobilenet,"I am confusing between SSD and mobilenet. As far as I know, both of them are neural networks. SSD provides localization while mobilenet provides classification. Thus the combination of SSD and mobilenet can produce the object detection. The default classification network of SSD is VGG-16. So, for SSD Mobilenet, VGG-16 is replaced with mobilenet. Are my statements correct?

Where can I get more information about SSD Mobilenet especially that one available on Tensorflow model zoo?",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-10-31 21:20:05,open,,,[],2018-12-07 10:11:54
462,tensorflow/models,models,5647,ismymajia,"[deeplab] export deeplab model, but only input one image, how to input batch images","
export deeplab model, but only input one image, how to input batch images ？？？




### System information
- **What is the top-level directory of the model you are using**:
master/research/deeplab

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
yes
- **TensorFlow version (use command below)**:
tensorflow 1.8
- **Bazel version (if compiling from source)**:


- **CUDA/cuDNN version**:
9.0

- **GPU model and memory**:
12G

- **Exact command to reproduce**:
",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-10-31 07:26:02,open,,,[],2018-11-10 09:07:05
463,tensorflow/models,models,5645,ltm920716,Maybe the mAP calculation is wrong,"Hello，we use the faster_rcnn_inception_v2_coco model to train our own data which have 7 classes，then we test on one image，the mAP is 90%.  Next， we use the saved frozen graph to show the detection result， 14 objects in the same image，only showing 9 detection boxes， and 3 are wrong classes in these 9 boxes . It could not be 90% mAP.  So we print all different objects‘ precision, recall and average precision. The average precision calculated according to corresponding p and r is no problem, so maye the problem is the p and r. For class 3, there are 3 objects in this image， and this class‘ average precision is 1.0， but in detection boxes， only showing one boxes. We set the eval and detection threshold are both 0.5.  I do not know if is my fault. So please help me, thanks!",4,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-10-31 02:07:33,open,,,[],2019-03-06 00:19:54
464,tensorflow/models,models,5643,GeneratorEX,What will happen if I set the batch size to 1? Train.py,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 8 
- **TensorFlow installed from (source or binary)**: I dont know. But I am sure I installed it using pip 
- **TensorFlow version (use command below)**: 1.9.0
- **Bazel version (if compiling from source)**: Did't use bazel
- **CUDA/cuDNN version**: Used tensorflow CPU instead
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Run the train.py command and edited the ssd_mobilenet_v1_pets configuration and set the batch size to 1 (due to cant start training because of insuficient system memory)

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-30 22:46:59,open,,,[],2018-11-01 12:18:30
465,tensorflow/models,models,5640,adithya-p,Failed to reproduce frozen inference graph as in models zoo,"### System information
- **What is the top-level directory of the model you are using**: tensorflow/models/tree/master/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0
- **Bazel version (if compiling from source)**: 0.17.2
- **CUDA/cuDNN version**: 9.2/7.1
- **GPU model and memory**: GeForce GTX 1050 Ti, 4GB
- **Exact command to reproduce**: Please check it in the problem description

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

**These are the exact steps I am following:**

- Training `ssd_inception_v2` model with the following command:
```
PIPELINE_CONFIG_PATH={path to pipeline config file}
MODEL_DIR={path to model directory}
NUM_TRAIN_STEPS=50000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr
```
-  Exporting the trained model for inference with the following command:
```
INPUT_TYPE=image_tensor
PIPELINE_CONFIG_PATH={path to pipeline config file}
TRAINED_CKPT_PREFIX={path to model.ckpt}
EXPORT_DIR={path to folder that will be used for export}
python object_detection/export_inference_graph.py \
    --input_type=${INPUT_TYPE} \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} \
    --output_directory=${EXPORT_DIR}
```
The above command generates the frozen graph format of the exported model. **The issue is - The [custom_ssd_inception.tar.gz](https://down.uploadfiles.io/get/uvg4m) is way different from what is given in the [ssd_inception_v2_coco_2018_01_28.tar.gz](http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz) file in model zoo of object detection when visualized using tensorboard.** 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Link to the config file: [ssd_inception_v2_coco.config ](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_inception_v2_coco.config)

When continued to convert the frozen graph to UFF using convert_to_uff.py, the output log files are as follows:
For the original model, given in model zoo: [original-output.log](http://txt.do/dw3z6) 
For custom model: [custom-output.log]( http://txt.do/dw3zj)

**Can someone help me identify the exact issue? Where am I going wrong?**",4,,[],2018-10-30 12:22:53,open,,,['type:bug/performance'],2019-04-05 20:45:40
466,tensorflow/models,models,5637,sunzhaoxing2016,imagenet_main.py running error,"I try to run imagenet_main.py in official/resnet/, but I got a error

![image](https://user-images.githubusercontent.com/22524061/47696134-506ae300-dc40-11e8-877f-cc48ce4f7ce9.png)

Could anyone help me to figure out why?

I  haven't changed the code, just git clone and ran the code by typing ""python imagenet_main.py --help"" in /tensorflow/models/official/resnet/. Now I notice the error may caused by ""\ufeff"" in line110 of /tensorflow/models/official/utils/flags/_base.py, however, still don't know how to fix it.

By the code run in a container, which is linux 16.04.4 LTS + CUDA 9.0 + cudnn 7.0 + tensorflow 1.8.0 + python 2.7.12.
",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-10-30 04:36:24,open,,,['stat:awaiting response'],2018-11-02 01:34:46
467,tensorflow/models,models,5636,mjhanphd,Transformer -> scheduled sampling not support?,Does the official transformer code not support the scheduled sampling?,1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-10-30 02:42:59,open,,,['stat:awaiting response'],2018-10-30 12:34:25
468,tensorflow/models,models,5633,dori-reichmann,"running object-detection on cloud: ""An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error""   ","I am trying to train object-detection following the tutuorial. When I run the same pipeline on a single machine, there are no problems.

The problem:
I constantly get the following Message: ""An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error""
The message show up from all replicas which lead the to restart and waster precision time.

Here is a patch of the logs (I have hours of them...)

> I  worker-replica-1 Start master session 75a97c7043cae868 with config: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } worker-replica-1 
> I  worker-replica-1 Graph was finalized. worker-replica-1 
> I  worker-replica-1 An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error worker-replica-1 
> I  worker-replica-4 Done running local_init_op. worker-replica-4 
> I  worker-replica-4 Running local_init_op. worker-replica-4 
> I  worker-replica-4 Start master session c68ba8523c26ae5c with config: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } worker-replica-4 
> I  worker-replica-4 Graph was finalized. worker-replica-4 
> I  worker-replica-4 An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error worker-replica-4 
> I  worker-replica-0 Done running local_init_op. worker-replica-0 
> I  worker-replica-0 Running local_init_op. worker-replica-0 
> I  worker-replica-0 Start master session 4a156c81c7f04920 with config: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } worker-replica-0 
> I  worker-replica-0 Graph was finalized. worker-replica-0 
> I  worker-replica-0 An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error worker-replica-0 
> I  worker-replica-0 loss = 1.4236093, step = 1084 (188.591 sec) worker-replica-0 
> I  worker-replica-4 Done running local_init_op. worker-replica-4 
> I  worker-replica-4 Running local_init_op. worker-replica-4 
> I  worker-replica-4 Start master session f8eab8afc462f35c with config: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } worker-replica-4 
> I  worker-replica-4 Graph was finalized. worker-replica-4 
> I  worker-replica-4 An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error worker-replica-4 
> I  worker-replica-0 Done running local_init_op. worker-replica-0 
> I  worker-replica-0 Running local_init_op. worker-replica-0 
> I  worker-replica-0 Start master session 41207ae796d00daf with config: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } worker-replica-0 
> I  worker-replica-0 Graph was finalized. worker-replica-0 
> I  worker-replica-0 An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error worker-replica-0 
> I  worker-replica-3 Done running local_init_op. worker-replica-3 
> I  worker-replica-3 Running local_init_op. worker-replica-3 
> I  worker-replica-3 Start master session bb761c1fbec37f9c with config: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } worker-replica-3 
> I  worker-replica-3 Graph was finalized. worker-replica-3 
> I  worker-replica-3 An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error worker-replica-3 
> I  worker-replica-2 loss = 1.054275, step = 1018 (172.128 sec) worker-replica-2 
> I  worker-replica-3 loss = 0.97805524, step = 987 (175.943 sec) worker-replica-3 
> I  worker-replica-4 loss = 0.6143055, step = 987 (177.687 sec) worker-replica-4 
> I  worker-replica-0 Done running local_init_op. worker-replica-0 
> I  worker-replica-0 Running local_init_op. worker-replica-0 
> I  worker-replica-0 Start master session e6c64f34a170eaf3 with config: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } worker-replica-0 
> I  worker-replica-1 loss = 0.65272343, step = 963 (159.242 sec) worker-replica-1 
> I  worker-replica-0 Graph was finalized. worker-replica-0 
> 


------------------------

### System information
-  What is the top-level directory of the model you are using: models/research/object-detection
-  Have I written custom code: no
- OS Platform and Distribution: N/A
-  Running on ML-engine (us-east1 and us-central1)
-  TensorFlow installed from: Conda
- Bazel version: N/A
- Tensorflow version = 1.9
- Python version = 2.,7
- **CUDA/cuDNN version**: ml-engine standard_gpu
- **GPU model and memory**: ml-engine standard_gpu
- **Exact command to reproduce**: Just following [](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_cloud.md)
",22,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-10-29 13:30:31,open,,,[],2019-03-13 12:27:40
469,tensorflow/models,models,5632,netanel-s,[Feature Request] Pre-trained RetinaNetLite-MobileNetV2 model,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**: N/A
------------------------

### Describe the problem
About a month ago you added support for FPN on top of MobileNetV2.
Could you please provide a pre-trained model of RetinaNetLite-MobileNetV2?
i.e. depth-wise separable FPN + depth-wise separable detection head comprised of two decoupled 4-layer detection towers + focal loss.

Thanks a lot in advance.",1,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-10-29 09:38:56,open,,,['type:feature'],2018-10-31 08:25:29
470,tensorflow/models,models,5625,Vildnex,Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint,"### System information
- **OS Platform**:Ubuntu 18.04
- **TensorFlow installed**:binary
- **TensorFlow version**:1.11.0
- **CUDA/cuDNN version**:9.0/7.2.1
- **GPU model and memory**: 1050Ti 4Gb

### Describe the problem
I've used **faster_rcnn_nas_coco.config** with **faster_rcnn_nas_coco_2018_01_28** as checkpoint but I'm getting this error message:

> NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
> 
> Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> 	 [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]


Here is the full log file:


> /usr/bin/python3.6 /home/vlad/MyProjects/models/research/object_detection/legacy/train.py --logtostderr --train_dir=./../training/ --pipeline_config_path=./../faster_rcnn_nas_coco.config --trained_checkpoint_prefix ./../training
> WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py:125: main (from __main__) is deprecated and will be removed in a future version.
> Instructions for updating:
> Use object_detection/model_main.py.
> W1027 18:35:55.097780 139648628430656 tf_logging.py:125] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py:125: main (from __main__) is deprecated and will be removed in a future version.
> Instructions for updating:
> Use object_detection/model_main.py.
> WARNING:tensorflow:From /home/vlad/MyProjects/models/research/object_detection/legacy/trainer.py:265: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
> Instructions for updating:
> Please switch to tf.train.create_global_step
> W1027 18:35:55.139720 139648628430656 tf_logging.py:125] From /home/vlad/MyProjects/models/research/object_detection/legacy/trainer.py:265: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
> Instructions for updating:
> Please switch to tf.train.create_global_step
> WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
> W1027 18:35:55.152857 139648628430656 tf_logging.py:125] num_readers has been reduced to 1 to match input file shards.
> WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:727: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
> Instructions for updating:
> To construct input pipelines, use the `tf.data` module.
> W1027 18:35:55.335119 139648628430656 tf_logging.py:125] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:727: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
> Instructions for updating:
> To construct input pipelines, use the `tf.data` module.
> WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:727: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
> Instructions for updating:
> To construct input pipelines, use the `tf.data` module.
> W1027 18:35:55.336304 139648628430656 tf_logging.py:125] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:727: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
> Instructions for updating:
> To construct input pipelines, use the `tf.data` module.
> 2018-10-27 18:35:55.396101: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
> 2018-10-27 18:35:55.488491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2018-10-27 18:35:55.488810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
> name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.455
> pciBusID: 0000:01:00.0
> totalMemory: 3.94GiB freeMemory: 3.29GiB
> 2018-10-27 18:35:55.488822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
> 2018-10-27 18:35:55.700871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2018-10-27 18:35:55.700897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
> 2018-10-27 18:35:55.700902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
> 2018-10-27 18:35:55.701018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/device:GPU:0 with 3004 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
> INFO:tensorflow:A GPU is available on the machine, consider using NCHW data format for increased speed on GPU.
> I1027 18:35:55.727262 139648628430656 tf_logging.py:115] A GPU is available on the machine, consider using NCHW data format for increased speed on GPU.
> INFO:tensorflow:Scale of 0 disables regularizer.
> I1027 18:36:03.899471 139648628430656 tf_logging.py:115] Scale of 0 disables regularizer.
> INFO:tensorflow:Scale of 0 disables regularizer.
> I1027 18:36:03.910351 139648628430656 tf_logging.py:115] Scale of 0 disables regularizer.
> INFO:tensorflow:depth of additional conv before box predictor: 0
> I1027 18:36:03.910492 139648628430656 tf_logging.py:115] depth of additional conv before box predictor: 0
> WARNING:tensorflow:From /home/vlad/MyProjects/models/research/object_detection/predictors/heads/box_head.py:93: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
> Instructions for updating:
> keep_dims is deprecated, use keepdims instead
> W1027 18:36:07.978588 139648628430656 tf_logging.py:125] From /home/vlad/MyProjects/models/research/object_detection/predictors/heads/box_head.py:93: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
> Instructions for updating:
> keep_dims is deprecated, use keepdims instead
> INFO:tensorflow:Scale of 0 disables regularizer.
> I1027 18:36:07.983829 139648628430656 tf_logging.py:115] Scale of 0 disables regularizer.
> INFO:tensorflow:Scale of 0 disables regularizer.
> I1027 18:36:07.999889 139648628430656 tf_logging.py:115] Scale of 0 disables regularizer.
> WARNING:tensorflow:From /home/vlad/MyProjects/models/research/object_detection/core/losses.py:340: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> 
> Future major versions of TensorFlow will allow gradients to flow
> into the labels input on backprop by default.
> 
> See `tf.nn.softmax_cross_entropy_with_logits_v2`.
> 
> W1027 18:36:08.309067 139648628430656 tf_logging.py:125] From /home/vlad/MyProjects/models/research/object_detection/core/losses.py:340: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> 
> Future major versions of TensorFlow will allow gradients to flow
> into the labels input on backprop by default.
> 
> See `tf.nn.softmax_cross_entropy_with_logits_v2`.
> 
> WARNING:tensorflow:From /home/vlad/MyProjects/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2108: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
> Instructions for updating:
> Please switch to tf.train.get_or_create_global_step
> W1027 18:36:28.577210 139648628430656 tf_logging.py:125] From /home/vlad/MyProjects/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2108: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
> Instructions for updating:
> Please switch to tf.train.get_or_create_global_step
> W1027 18:36:28.594722 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.594845 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.594899 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.594950 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.594995 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.595041 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.595082 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.595127 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595167 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595206 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595246 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595284 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.595322 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.595366 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.595405 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.595448 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595488 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595527 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595566 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595604 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.595642 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.595685 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.595724 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.595767 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595807 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595846 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595885 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.595923 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.595963 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.596006 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.596044 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.596088 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596127 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596165 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596203 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596240 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.596277 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.596318 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.596356 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.596399 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596437 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596476 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596514 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596552 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/final_path_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.596590 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/final_path_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.596632 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/path1_conv/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596671 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_0/path2_conv/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596709 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.596747 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.596784 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.596826 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.596876 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.596922 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.596960 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.597004 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597045 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597085 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597124 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597162 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.597200 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.597243 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.597281 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.597325 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597364 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597404 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597443 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597482 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.597520 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.597562 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.597600 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.597644 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597683 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597721 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597760 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597797 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.597835 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.597877 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.597914 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.597957 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.597996 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598035 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598073 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598111 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.598148 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.598191 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.598229 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.598273 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598312 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598351 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598390 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598429 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598466 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.598503 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_1/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.598547 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598586 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.598624 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.598667 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.598705 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.598747 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.598786 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.598829 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598869 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598908 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598948 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.598985 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.599024 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.599067 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.599105 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.599149 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599189 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599228 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599267 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599307 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.599345 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.599387 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.599425 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.599468 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599508 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599547 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599586 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599624 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.599662 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.599704 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.599742 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.599786 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599825 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599864 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599904 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.599942 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.599980 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.600022 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.600060 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.600103 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.600142 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.600180 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.600219 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.600257 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.600295 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.600333 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_10/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.600377 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.600415 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.600453 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.600496 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.600534 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.600576 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.600614 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.600657 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.600697 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.600736 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.600776 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.600815 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.600877 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.600941 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.600973 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.601011 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601044 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601077 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601110 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601142 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.601174 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.601210 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.601241 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.601279 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601312 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601345 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601378 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601410 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.601442 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.601478 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.601511 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.601549 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601582 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601615 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601648 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601680 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.601712 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.601749 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.601780 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.601817 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601850 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601883 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601916 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601948 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.601980 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.602012 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_11/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.602050 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602082 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.602113 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.602150 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.602182 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.602220 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.602252 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.602289 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602322 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602355 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602388 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602420 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.602452 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.602487 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.602519 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.602556 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602590 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602623 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602657 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602689 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.602720 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.602757 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.602789 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.602826 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602860 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602893 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602926 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.602958 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.602990 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.603026 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.603058 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.603095 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603127 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603160 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603193 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603224 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.603255 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.603291 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.603323 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.603363 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603396 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603429 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603462 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603494 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603526 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.603559 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_2/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.603595 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603627 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.603658 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.603695 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.603727 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.603763 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.603796 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.603833 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603866 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603900 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603934 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.603966 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.603997 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.604033 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.604065 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.604102 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604135 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604167 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604200 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604232 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.604263 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.604299 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.604331 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.604367 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604400 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604434 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604466 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604498 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.604530 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.604566 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.604598 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.604635 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604667 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604700 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604732 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604764 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.604796 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.604833 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.604899 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.604948 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.604982 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605015 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605048 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605081 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605113 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.605144 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_3/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.605182 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605214 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.605246 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.605282 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.605314 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.605351 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.605383 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.605421 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605455 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605488 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605522 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605554 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.605586 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.605622 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.605654 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.605691 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605725 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605758 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605791 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605823 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.605855 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.605892 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.605924 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.605962 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.605995 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606028 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606062 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606094 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.606126 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.606163 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.606196 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.606234 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606267 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606301 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606333 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606365 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.606397 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.606433 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.606466 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.606503 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606536 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606569 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606601 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606634 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606666 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.606697 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_4/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.606734 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.606766 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.606798 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.606834 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.606867 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.606903 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.606935 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.606972 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607005 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607039 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607072 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607104 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.607135 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.607171 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.607203 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.607240 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607274 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607306 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607339 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607371 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.607403 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.607440 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.607472 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.607510 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607542 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607576 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607608 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607640 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.607672 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.607709 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.607741 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.607778 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607811 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607844 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607877 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.607909 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.607941 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.607978 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.608010 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.608048 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608081 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608114 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608147 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608180 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608212 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.608244 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_5/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.608282 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608315 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.608347 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.608385 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.608417 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.608454 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.608487 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.608525 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608560 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608593 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608626 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608659 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.608691 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.608728 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.608760 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.608798 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608830 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608894 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608940 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.608977 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.609010 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.609047 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.609080 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.609117 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609149 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609183 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609216 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609248 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.609280 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.609316 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.609348 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.609385 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609418 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609452 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609485 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609517 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.609549 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.609586 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.609618 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.609655 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609688 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609721 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609754 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609786 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/final_path_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.609818 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/final_path_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.609854 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/path1_conv/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609888 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_6/path2_conv/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609920 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.609952 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.609983 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.610021 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.610053 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.610089 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.610121 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.610159 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610192 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610225 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610258 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610291 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.610322 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.610359 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.610391 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.610429 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610462 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610495 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610528 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610561 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.610592 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.610629 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.610662 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.610700 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610733 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610766 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610799 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.610831 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.610863 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.610899 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.610931 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.610968 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611006 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611040 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611073 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611104 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.611136 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.611173 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.611205 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.611242 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611274 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611307 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611340 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611373 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611405 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.611436 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_7/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.611475 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611507 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.611539 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.611576 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.611608 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.611650 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.611704 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.611743 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611787 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611831 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611865 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.611897 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.611940 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.611988 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.612020 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.612057 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612091 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612124 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612168 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612212 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.612254 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.612312 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.612346 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.612417 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612451 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612484 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612517 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612550 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.612582 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.612629 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.612682 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.612751 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612796 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612830 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612880 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.612915 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.612947 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.612984 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.613017 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.613055 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613088 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613121 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613154 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613188 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613231 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.613264 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_8/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.613322 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613366 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.613398 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.613435 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.613468 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.613505 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.613559 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.613619 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613687 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613736 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613770 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613803 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.613835 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.613872 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.613904 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.613941 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.613974 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614007 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614041 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614074 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.614106 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.614142 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.614175 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.614213 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614246 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614280 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614313 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614346 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.614378 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.614415 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.614448 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.614485 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614518 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614551 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614584 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614616 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.614648 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.614686 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.614718 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.614755 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614789 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614821 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614854 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614887 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.614919 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.614950 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_9/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.614989 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615022 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.615054 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.615090 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.615123 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.615159 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.615192 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.615230 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615263 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615296 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615329 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615361 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/right/bn_sep_7x7_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.615414 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/right/bn_sep_7x7_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.615451 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/right/bn_sep_7x7_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.615484 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/right/bn_sep_7x7_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.615521 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/right/separable_7x7_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615554 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/right/separable_7x7_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615586 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/right/separable_7x7_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615620 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_0/right/separable_7x7_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615653 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_1/right/bn_sep_7x7_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.615685 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_1/right/bn_sep_7x7_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.615722 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_1/right/bn_sep_7x7_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.615755 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_1/right/bn_sep_7x7_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.615792 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_1/right/separable_7x7_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615825 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_1/right/separable_7x7_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615859 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_1/right/separable_7x7_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615892 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_1/right/separable_7x7_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.615925 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_2/right/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.615957 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_2/right/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.615994 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_2/right/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.616027 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_2/right/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.616064 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_2/right/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616098 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_2/right/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616131 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_2/right/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616164 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_2/right/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616196 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.616228 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.616265 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.616297 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.616333 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616367 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616400 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616433 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_0/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616466 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616498 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.616531 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.616568 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.616600 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.616637 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.616669 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.616707 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616740 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616773 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616806 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.616838 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/right/bn_sep_7x7_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.616897 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/right/bn_sep_7x7_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.616935 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/right/bn_sep_7x7_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.616966 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/right/bn_sep_7x7_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.617004 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/right/separable_7x7_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617037 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/right/separable_7x7_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617070 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/right/separable_7x7_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617103 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_0/right/separable_7x7_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617136 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_1/right/bn_sep_7x7_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.617168 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_1/right/bn_sep_7x7_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.617205 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_1/right/bn_sep_7x7_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.617237 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_1/right/bn_sep_7x7_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.617275 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_1/right/separable_7x7_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617309 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_1/right/separable_7x7_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617342 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_1/right/separable_7x7_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617375 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_1/right/separable_7x7_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617408 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_2/right/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.617439 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_2/right/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.617477 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_2/right/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.617509 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_2/right/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.617547 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_2/right/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617580 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_2/right/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617613 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_2/right/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617647 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_2/right/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617679 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.617711 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.617748 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.617780 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.617818 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617851 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617885 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617918 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.617949 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/final_path_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.617981 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/final_path_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.618019 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/path1_conv/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618052 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/cell_stem_1/path2_conv/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618085 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/conv0/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618116 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/conv0_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.618148 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/conv0_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.618186 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618219 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.618251 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.618288 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.618320 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.618357 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.618391 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.618429 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618463 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618497 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618530 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618562 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/right/bn_sep_7x7_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.618594 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/right/bn_sep_7x7_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.618631 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/right/bn_sep_7x7_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.618663 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/right/bn_sep_7x7_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.618717 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/right/separable_7x7_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618752 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/right/separable_7x7_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618796 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/right/separable_7x7_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618850 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_0/right/separable_7x7_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.618902 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_1/right/bn_sep_7x7_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.618935 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_1/right/bn_sep_7x7_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.618972 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_1/right/bn_sep_7x7_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.619004 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_1/right/bn_sep_7x7_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.619041 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_1/right/separable_7x7_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619075 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_1/right/separable_7x7_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619107 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_1/right/separable_7x7_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619140 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_1/right/separable_7x7_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619172 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_2/right/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.619204 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_2/right/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.619241 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_2/right/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.619273 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_2/right/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.619311 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_2/right/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619344 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_2/right/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619377 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_2/right/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619410 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_2/right/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619443 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.619476 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.619513 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.619546 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.619583 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619616 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619649 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619682 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619716 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619748 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.619779 139648628430656 variables_helper.py:144] Variable [FirstStageFeatureExtractor/reduction_cell_0/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.619817 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.619850 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.619882 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.619919 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.619951 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.619988 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.620021 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.620059 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620093 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620126 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620160 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620192 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.620224 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.620262 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.620294 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.620332 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620365 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620398 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620432 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620464 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.620497 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.620534 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.620567 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.620605 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620639 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620672 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620706 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620738 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.620770 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.620807 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.620840 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.620883 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620917 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620950 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.620984 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621016 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.621048 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.621085 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.621118 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.621155 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621188 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621221 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621254 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621286 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/final_path_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.621318 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/final_path_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.621357 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/path1_conv/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621391 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_12/path2_conv/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621424 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621457 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.621489 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.621526 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.621559 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.621597 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.621630 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.621668 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621702 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621736 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621769 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621802 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.621834 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.621872 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.621904 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.621941 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.621974 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622009 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622042 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622073 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.622105 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.622142 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.622174 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.622211 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622245 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622278 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622311 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622344 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.622376 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.622413 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.622446 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.622483 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622517 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622550 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622583 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622616 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.622648 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.622685 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.622718 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.622756 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622790 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622823 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622856 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622890 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.622922 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.622954 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_13/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.622992 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623025 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.623057 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.623095 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.623127 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.623165 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.623197 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.623235 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623269 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623303 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623336 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623368 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.623400 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.623438 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.623470 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.623508 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623541 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623574 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623608 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623641 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.623673 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.623709 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.623742 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.623780 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623814 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623847 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623880 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.623912 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.623944 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.623981 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.624013 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.624051 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624084 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624117 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624150 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624183 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.624215 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.624251 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.624283 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.624321 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624354 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624387 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624420 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624452 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624485 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.624517 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_14/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.624555 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624588 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.624619 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.624657 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.624689 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.624727 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.624759 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.624797 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624830 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624881 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624925 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.624958 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.624990 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.625027 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.625060 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.625098 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625132 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625164 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625198 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625231 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.625263 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.625299 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.625331 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.625369 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625402 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625435 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625469 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625501 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.625533 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.625591 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.625624 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.625662 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625695 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625729 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625762 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625795 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.625827 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.625864 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.625896 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.625933 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625966 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.625999 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626032 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626065 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626097 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.626129 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_15/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.626167 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626199 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.626230 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.626268 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.626301 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.626338 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.626371 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.626409 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626443 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626477 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626510 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626543 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.626574 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.626612 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.626645 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.626683 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626716 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626749 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626783 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.626815 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.626847 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.626920 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.626955 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.626993 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627027 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627059 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627093 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627125 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.627158 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.627196 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.627228 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.627266 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627300 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627334 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627367 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627400 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.627431 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.627469 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.627501 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.627541 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627574 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627607 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627641 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627674 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627707 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.627739 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_16/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.627778 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.627810 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.627844 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.627882 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.627915 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.627952 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.627985 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.628023 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628057 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628091 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628124 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628157 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.628189 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.628227 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.628260 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.628298 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628331 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628365 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628399 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_0/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628431 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.628463 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.628500 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.628532 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.628571 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628605 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628638 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628672 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628704 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/right/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.628736 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/right/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.628773 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/right/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.628806 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/right/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.628844 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/right/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628883 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/right/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628916 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/right/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628949 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_1/right/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.628982 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.629014 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.629052 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.629084 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.629122 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629155 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629188 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629221 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629255 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629287 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.629319 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/cell_17/prev_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.629357 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629390 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/beginning_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.629422 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/beginning_bn/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.629460 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.629493 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.629530 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.629563 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.629601 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/left/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629634 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/left/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629667 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/left/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629701 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/left/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629733 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.629766 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.629802 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.629834 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.629872 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/right/separable_7x7_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629905 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/right/separable_7x7_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629938 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/right/separable_7x7_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.629972 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_0/right/separable_7x7_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630005 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.630037 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.630074 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.630107 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.630145 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_1/right/separable_7x7_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630177 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_1/right/separable_7x7_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630211 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_1/right/separable_7x7_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630244 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_1/right/separable_7x7_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630277 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.630309 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.630347 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.630379 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.630417 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_2/right/separable_5x5_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630450 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_2/right/separable_5x5_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630483 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_2/right/separable_5x5_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630516 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_2/right/separable_5x5_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630548 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.630581 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.630617 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.630650 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/gamma/Momentum] is not available in checkpoint
> W1027 18:36:28.630687 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_4/left/separable_3x3_1/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630720 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_4/left/separable_3x3_1/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630753 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_4/left/separable_3x3_2/depthwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630787 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/comb_iter_4/left/separable_3x3_2/pointwise_weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630820 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/prev_1x1/weights/Momentum] is not available in checkpoint
> W1027 18:36:28.630852 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/prev_bn/beta/Momentum] is not available in checkpoint
> W1027 18:36:28.630884 139648628430656 variables_helper.py:144] Variable [SecondStageFeatureExtractor/reduction_cell_1/prev_bn/gamma/Momentum] is not available in checkpoint
> WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:737: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
> Instructions for updating:
> Please switch to tf.train.MonitoredTrainingSession
> W1027 18:36:30.754613 139648628430656 tf_logging.py:125] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:737: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
> Instructions for updating:
> Please switch to tf.train.MonitoredTrainingSession
> 2018-10-27 18:36:34.324441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
> 2018-10-27 18:36:34.324479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2018-10-27 18:36:34.325855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
> 2018-10-27 18:36:34.326517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
> 2018-10-27 18:36:34.332915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3004 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
> INFO:tensorflow:Restoring parameters from ./../training/model.ckpt-0
> I1027 18:36:34.366144 139648628430656 tf_logging.py:115] Restoring parameters from ./../training/model.ckpt-0
> 2018-10-27 18:36:36.738256: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
> 
> Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> 	 [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
> 
> Caused by op 'save/RestoreV2', defined at:
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 184, in <module>
>     tf.app.run()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
>     return func(*args, **kwargs)
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 180, in main
>     graph_hook_fn=graph_rewriter_fn)
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/trainer.py"", line 376, in train
>     keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1094, in __init__
>     self.build()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1106, in build
>     self._build(self._filename, build_save=True, build_restore=True)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1143, in _build
>     build_save=build_save, build_restore=build_restore)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 787, in _build_internal
>     restore_sequentially, reshape)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps
>     restore_sequentially)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore
>     return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2
>     shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
>     op_def=op_def)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
>     self._traceback = tf_stack.extract_stack()
> 
> NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
> 
> Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> 	 [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
> 
> I1027 18:36:36.745308 139648628430656 tf_logging.py:115] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
> 
> Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> 	 [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
> 
> Caused by op 'save/RestoreV2', defined at:
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 184, in <module>
>     tf.app.run()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
>     return func(*args, **kwargs)
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 180, in main
>     graph_hook_fn=graph_rewriter_fn)
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/trainer.py"", line 376, in train
>     keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1094, in __init__
>     self.build()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1106, in build
>     self._build(self._filename, build_save=True, build_restore=True)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1143, in _build
>     build_save=build_save, build_restore=build_restore)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 787, in _build_internal
>     restore_sequentially, reshape)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps
>     restore_sequentially)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore
>     return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2
>     shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
>     op_def=op_def)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
>     self._traceback = tf_stack.extract_stack()
> 
> NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
> 
> Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> 	 [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
> 
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1292, in _do_call
>     return fn(*args)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1277, in _run_fn
>     options, feed_dict, fetch_list, target_list, run_metadata)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1367, in _call_tf_sessionrun
>     run_metadata)
> tensorflow.python.framework.errors_impl.NotFoundError: Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> 	 [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1538, in restore
>     {self.saver_def.filename_tensor_name: save_path})
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 887, in run
>     run_metadata_ptr)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1110, in _run
>     feed_dict_tensor, options, run_metadata)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
>     run_metadata)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1308, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.NotFoundError: Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> 	 [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
> 
> Caused by op 'save/RestoreV2', defined at:
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 184, in <module>
>     tf.app.run()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
>     return func(*args, **kwargs)
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 180, in main
>     graph_hook_fn=graph_rewriter_fn)
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/trainer.py"", line 376, in train
>     keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1094, in __init__
>     self.build()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1106, in build
>     self._build(self._filename, build_save=True, build_restore=True)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1143, in _build
>     build_save=build_save, build_restore=build_restore)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 787, in _build_internal
>     restore_sequentially, reshape)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps
>     restore_sequentially)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore
>     return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2
>     shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
>     op_def=op_def)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
>     self._traceback = tf_stack.extract_stack()
> 
> NotFoundError (see above for traceback): Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> 	 [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
> 
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1548, in restore
>     names_to_keys = object_graph_key_mapping(save_path)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1822, in object_graph_key_mapping
>     checkpointable.OBJECT_GRAPH_PROTO_KEY)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 359, in get_tensor
>     status)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py"", line 526, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 184, in <module>
>     tf.app.run()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
>     return func(*args, **kwargs)
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 180, in main
>     graph_hook_fn=graph_rewriter_fn)
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/trainer.py"", line 415, in train
>     saver=saver)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 748, in train
>     master, start_standard_services=False, config=session_config) as sess:
>   File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
>     return next(self.gen)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py"", line 1005, in managed_session
>     self.stop(close_summary_writer=close_summary_writer)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py"", line 833, in stop
>     ignore_live_threads=ignore_live_threads)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
>     six.reraise(*self._exc_info_to_raise)
>   File ""/usr/lib/python3/dist-packages/six.py"", line 693, in reraise
>     raise value
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py"", line 994, in managed_session
>     start_standard_services=start_standard_services)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py"", line 731, in prepare_or_wait_for_session
>     init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session
>     config=config)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 211, in _restore_checkpoint
>     saver.restore(sess, ckpt.model_checkpoint_path)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1554, in restore
>     err, ""a Variable name or other graph key that is missing"")
> tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
> 
> Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> 	 [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
> 
> Caused by op 'save/RestoreV2', defined at:
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 184, in <module>
>     tf.app.run()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
>     return func(*args, **kwargs)
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 180, in main
>     graph_hook_fn=graph_rewriter_fn)
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/trainer.py"", line 376, in train
>     keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1094, in __init__
>     self.build()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1106, in build
>     self._build(self._filename, build_save=True, build_restore=True)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1143, in _build
>     build_save=build_save, build_restore=build_restore)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 787, in _build_internal
>     restore_sequentially, reshape)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps
>     restore_sequentially)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore
>     return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2
>     shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
>     op_def=op_def)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
>     self._traceback = tf_stack.extract_stack()
> 
> NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
> 
> Key FirstStageFeatureExtractor/cell_0/1x1/weights not found in checkpoint
> 	 [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
> 
> ERROR:tensorflow:==================================
> Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
> <tf.Tensor 'init_ops/report_uninitialized_variables/boolean_mask/GatherV2:0' shape=(?,) dtype=string>
> If you want to mark it as used call its ""mark_used()"" method.
> It was originally created here:
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 184, in <module>
>     tf.app.run()  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
>     return func(*args, **kwargs)  File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 180, in main
>     graph_hook_fn=graph_rewriter_fn)  File ""/home/vlad/MyProjects/models/research/object_detection/legacy/trainer.py"", line 415, in train
>     saver=saver)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 791, in train
>     should_retry = True  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py"", line 189, in wrapped
>     return _add_should_use_warning(fn(*args, **kwargs))
> ==================================
> E1027 18:36:38.621998 139648628430656 tf_logging.py:105] ==================================
> Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
> <tf.Tensor 'init_ops/report_uninitialized_variables/boolean_mask/GatherV2:0' shape=(?,) dtype=string>
> If you want to mark it as used call its ""mark_used()"" method.
> It was originally created here:
>   File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 184, in <module>
>     tf.app.run()  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 306, in new_func
>     return func(*args, **kwargs)  File ""/home/vlad/MyProjects/models/research/object_detection/legacy/train.py"", line 180, in main
>     graph_hook_fn=graph_rewriter_fn)  File ""/home/vlad/MyProjects/models/research/object_detection/legacy/trainer.py"", line 415, in train
>     saver=saver)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 791, in train
>     should_retry = True  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py"", line 189, in wrapped
>     return _add_should_use_warning(fn(*args, **kwargs))
> ==================================

I also try this answers in order to fix my problem but it was useless:
[ans_1](https://github.com/tensorflow/models/issues/5230), [ans_2](https://github.com/tensorflow/tensorflow/issues/7244), [ans_3](https://github.com/tensorflow/models/issues/4783)
Process finished with exit code 1

I've also tried with ssd_mobilenet_v1 with out any luck(with config file and also checkpoint from this one)

Any idea?
",6,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-10-27 16:07:38,open,,,['stat:awaiting response'],2019-01-22 01:33:05
471,tensorflow/models,models,5624,van-tienhoang,Update README.md,Fix typo likeihood -> likelihood,2,,[],2018-10-27 14:52:11,open,,,['cla: no'],2018-10-27 14:54:54
472,tensorflow/models,models,5622,shizhiw,Add support 1. to keep cache data for reuse 2. to shard data for conv…,…enient consumption by multiple hosts etc.,0,,[],2018-10-26 21:05:07,open,,,['cla: yes'],2018-10-26 21:05:31
473,tensorflow/models,models,5621,asagot,SSD Large Image Support/Training,"Am getting no luck on StackOverflow so I am hoping you will be kind enough to answer a few questions for a noob.

Are there detailed instruction on how to train SSD with a larger image size? Specifically, I do not want to scale the input image down to 300x300 or 512x512 - I want to feed the network the larger image directly (~1024x1024)

Accuracy is not that big a deal at the moment as I am just trying to gauge performance.

So I am looking for:
* Changes need to be made and to what
* Anything else to look out for

Once again, am sorry to bring this up here but I am not getting much help from other resources.",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-26 18:37:13,open,,,[],2018-11-01 08:49:54
474,tensorflow/models,models,5620,adarvit,Training Keypoints Detection Model using Object Detection API,"
Mask-RCNN model can also be trained for keypoints detection and there is a keypoint_box_coder for the Mask RCNN model. However there is no documentation about preparing data and training a model for keypoints detection . Is there a plan to add keypoints detection to this API or are there any pointers to help add the implementation?
",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-10-26 15:02:13,open,,,['stat:awaiting response'],2018-10-27 12:21:08
475,tensorflow/models,models,5619,weidong8405347,code for searching for efficient multi-scale architectures for dense image prediction,"the deeplab code i just find the part of DPC but can not find the search part about random search and proxy task. 
So if i want to reproduce the paper's result that i should write the code by my own?",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-10-26 09:39:16,open,,,[],2018-10-27 12:20:59
476,tensorflow/models,models,5618,austinmw,Object Detection model_tpu_main.py No module named 'google.appengine',"
### System information
- **What is the top-level directory of the model you are using**:
`/models/research/object_detection/`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Debian GNU/Linux 9.5
- **TensorFlow installed from (source or binary)**:
Preinstalled (TPU)
- **TensorFlow version (use command below)**:
1.11
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
TPU v3-8
- **Exact command to reproduce**:
`python /home/austin/training/model_tpu_main.py \
	--gcp_project=<project_id> \
	--tpu_zone=us-central1-b \
	--tpu_name=<name> \
	--pipeline_config_path=/home/austin/training/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config \
	--model_dir=/home/<username>/training/models \
	--mode=train_and_eval \
	--train_batch_size=64 \
	--eval_training_data=True \
	--num_train_steps=50000 \
	--sample_1_of_n_eval_examples=1 \
	--sample_1_of_n_eval_on_train_examples=5 \
	--logtostderr=true`

### Describe the problem

I'm trying to run the `model_tpu_main.py` script, but it's failing with several import errors:
`ImportError: No module named 'google.appengine'`
`ImportError: No module named 'oauth2client.contrib.locked_file'`
`ImportError: No module named 'oauth2client.locked_file'`

Am I not supposed to run this TPU script locally while actually sshed into the TPU? I have previous experience running `model_main.py`, but this is my first attempt at trying a TPU and `model_tpu_main.py`

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""wt-huang"")","[NamedUser(login=""wt-huang"")]",2018-10-26 04:22:56,open,,,[],2018-11-06 01:30:16
477,tensorflow/models,models,5608,Kedron007,"fail to run train.py  I wanna run train.py base on ade20K, ,howerver it's show like this, i don't konw why? Did not show more, pause this does not move","python deeplab/train.py \
>     --logtostderr \
>     --training_number_of_steps=150000 \
>     --train_split=""train"" \
>     --model_variant=""xception_65"" \
>     --atrous_rates=6 \
>     --atrous_rates=12 \
>     --atrous_rates=18 \
>     --output_stride=16 \
>     --decoder_output_stride=4 \
>     --train_crop_size=513 \
>     --train_crop_size=513 \
>     --train_batch_size=4 \
>     --min_resize_value=513 \
>     --max_resize_value=513 \
>     --resize_factor=16 \
>     --dataset=""ade20k"" \
> --tf_initial_checkpoint='/home/nk/tensorflow/models/research/deeplab/datasets/deeplabv3_xception_ade20k_train/model.ckpt.index' \
> --train_logdir='/home/nk/deeplab/models/research/deeplab/datasets/ADE20K/train_on_trainval_set/train'  \
> --dataset_dir='/home/nk/deeplab/models/research/deeplab/datasets/ADE20K/tfrecord' 
INFO:tensorflow:Training on train set
INFO:tensorflow:Ignoring initialization; other checkpoint exists
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:737: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-10-25 14:00:53.986598: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-10-25 14:00:54.004305: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUDA_ERROR_UNKNOWN
2018-10-25 14:00:54.004337: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (s404): /proc/driver/nvidia/version does not exist
INFO:tensorflow:Restoring parameters from /home/nk/deeplab/models/research/deeplab/datasets/ADE20K/train_on_trainval_set/train/model.ckpt-0
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path /home/nk/deeplab/models/research/deeplab/datasets/ADE20K/train_on_trainval_set/train/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0

",4,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-10-25 06:10:04,open,,,[],2018-11-16 01:47:01
478,tensorflow/models,models,5607,imcarsonliao,can't run a3c_blogpost research model,"as title, looks like some file mssing? 
OSError: Unable to open file (unable to open file: name = './model_CartPole-v0.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-25 06:08:26,open,,,['stat:awaiting response'],2018-10-26 00:19:56
479,tensorflow/models,models,5606,billy19murahmi,Transformer -  replicating results with no. of steps instead of epochs,"### System information
- **What is the top-level directory of the model you are using**: models
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7.4
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.10.1
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A  training on CPU's
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

How is ""batches per epoch"" calculated in the Benchmarks Training Times mentioned in the document?
It is shown as 41365 for BIG transformer, but if you calculate using the code in the schedule.py, the results are different. 

Code segment that is used to calculate steps from epochs ,

total_num_tokens = NUM_EXAMPLES[mode] * self.max_length * num_epochs
return total_num_tokens // self.batch_size

if you choose the parameters from schedule.py and model_params.py,
NUM_EXAMPLES[train] = 4572160
max_length=256
batch_size=4096

Then the number of steps for 1 epoch would be 285760 and not  41365.
I understand the code segment I mentioned above is for TPU which uses static batch size, but I didn't find any code that did conversion for GPU/CPU. 
I'm trying to replicate results by mentioning the number of steps instead of number of epochs, any insights would be helpful.

Thanks 

",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta""), NamedUser(login=""k-w-w"")]",2018-10-25 04:06:48,open,,,[],2018-11-05 22:56:31
480,tensorflow/models,models,5594,msarfrazcss,Tensorflow model detecting wrong objects,"Hi guys, i am new in tensorflow, i trained a model with 900 images of shoes, i put 20%(180 images) into test forlder and 80%(720 images into train folder), but after training my trained model detecting other objects with shoes, below is the screen shot, anyone can help me please, where i am wrong....
I follow the link to train model, but i am training this model on MAC Machine, and using faster_rcnn_inception_v2_coco_2018_01_28 model to train my model: 
https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/tree/d1c5b59803543e48362c27c48d704d4b0d92d135

and more thing when i run python on this model on terminal webcam becomes very slow why?

Thanks in advance...

![screen shot 2018-10-24 at 10 18 35 am](https://user-images.githubusercontent.com/43664119/47407110-32e8d580-d777-11e8-97f7-6838bd8d2ff4.png)

![screen shot 2018-10-24 at 10 30 58 am](https://user-images.githubusercontent.com/43664119/47407283-03869880-d778-11e8-904e-45c0c6fa0c73.png)
",4,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-10-24 04:58:27,open,,,[],2018-10-30 00:18:58
481,tensorflow/models,models,5593,dont32,I using graph tranforms tool after export inference models to optimize_graph.pb . After load model have error :,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-24 02:43:38,open,,,[],2018-10-25 00:17:51
482,tensorflow/models,models,5588,gHtdoCAcHEtIn,Add optional parameter for non-RGB inputs,"Added `image_num_channels` for non-RGB input images 
* 1 channel: MNIST, grayscale images
* 4 channel inputs: RGB-D images
* other custom inputs",0,,[],2018-10-23 20:01:46,open,,,['cla: yes'],2018-10-23 20:01:48
483,tensorflow/models,models,5587,abdullahakmal,Tensorflow stucks in evaluation step after running local_init_op,"_OS: Ubuntu 18.0.41 LTS and 16.04
Tried on CPU and GPU both. Same error.
Tensorflow version 1.1, 1.5 and1.8 also.
Python 3.5 and 3.6.
Tensorflow was installed using the pip._

Command Prompt Snippet of the problem
-------------------------------------------------------------------------------------------------------------------


INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 512}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 4
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

WARNING:tensorflow:From /home/muhammad/Thesis/Ab/AutoViz/seq2seq/metrics/metric_specs.py:232: streaming_mean (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.mean
INFO:tensorflow:Starting evaluation at 2018-10-23-00:06:12
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /home/muhammad/Thesis/Ab/AutoViz/Model/model.ckpt-1406
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.

-------------------------------------------------------------------------------------------------------------------
I have left it running for more than a day but it remains stuck here. On GPU it shows that the process is running, but system doesn't seems like doing anything because of the processor usage.

Any help would be appreaciated a lot. 

Thank You.

",14,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-23 18:39:44,open,,,[],2019-03-27 07:41:04
484,tensorflow/models,models,5586,venkateshreddypala,Not detecting installation with py3.7,"my log here basically says that the tensorflow is already installed for 3.6 but I recently upgraded to 3.7 of python is there any fix to this issue or should I consider downgrading back to 3.6??



Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (1.8.0)
Requirement already satisfied: absl-py>=0.1.6 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (0.2.2)
Requirement already satisfied: astor>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (0.6.2)
Requirement already satisfied: tensorboard<1.9.0,>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.8.0)
Requirement already satisfied: gast>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: six>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.11.0)
Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.14.3)
Requirement already satisfied: wheel>=0.26 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (0.31.1)
Requirement already satisfied: grpcio>=1.8.6 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.12.0)
Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.1.0)
Requirement already satisfied: protobuf>=3.4.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (3.5.2.post1)
Requirement already satisfied: html5lib==0.9999999 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow) (0.9999999)
",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-10-23 18:13:31,open,,,['stat:awaiting response'],2018-10-24 00:15:27
485,tensorflow/models,models,5582,deercoder,InvalidArgumentError: tfrecord data type mismatch when training mask rcnn on PET dataset ,"### System information
- **What is the top-level directory of the model you are using**: models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.3.1611 (Core)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: cuda 8.0
- **GPU model and memory**: K80, 12GB
- **Exact command to reproduce**:  
```
PIPELINE_CONFIG_PATH=object_detection/samples/configs/mask_rcnn_resnet101_pets.config
MODEL_DIR=object_detection/data/mask_rcnn_resnet101_atrous_coco_2018_01_28
NUM_TRAIN_STEPS=150000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr
```

### Describe the problem
When I train the Pets dataset for mask-rcnn using pretrained mask-rcnn model from coco dataset, I find that the `object_detection/samples/configs/mask_rcnn_resnet101_pets.config` has a problem that will make the training failed, the previous solution is in [e45234](https://github.com/tensorflow/models/commit/e45234e32dbc485f74567f6c0297edc9c084677c), which adds the `mask_type: PNG_MASKS` in the config file, however, the newly code in master branch doesn't have such field that will cause the type mismatch, detailed log is as follows.

Addtionally, I find that in the config file the `tfrecord` is named `input_path: ""PATH_TO_BE_CONFIGURED/pet_fullbody_with_masks_train.record-?????-of-00010""`, however in its code `/object_detection/dataset_tools/create_pet_tf_record.py`, the file name is named `pets_fullbody_with_masks_train`, the minor difference between `pet` and `pets` will make the training interrupted with strange dimension mismatch error. I changed the config file to `pets` that will solve part of it.

After I added the changes in that previous commit, the problem disappear, I think the community should update the sample config file to avoid potential problems for beginners.



### Source code / logs
```
WARNING:tensorflow:From /data1/cliu/g-models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1930: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and w
ill be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /data1/cliu/g-models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1930: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and w
ill be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
2018-10-22 16:54:25.805897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:8b:00.0
totalMemory: 11.92GiB freeMemory: 11.44GiB
2018-10-22 16:54:25.805998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-10-22 16:54:26.189911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-22 16:54:26.190004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0
2018-10-22 16:54:26.190030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N
2018-10-22 16:54:26.190513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11093 MB memory) -> physical GPU (device: 0, name:
 Tesla K80, pci bus id: 0000:8b:00.0, compute capability: 3.7)
2018-10-22 16:55:06.921525: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: image/object/mask.  Data types don't match. Expected type: floa
t, Actual type: string
2018-10-22 16:55:06.921501: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: image/object/mask.  Data types don't match. Expected type: floa
t, Actual type: string
2018-10-22 16:55:06.921514: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: image/object/mask.  Data types don't match. Expected type: floa
t, Actual type: string
2018-10-22 16:55:06.922122: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: image/object/mask.  Data types don't match. Expected type: floa
t, Actual type: string
2018-10-22 16:55:06.922103: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: image/object/mask.  Data types don't match. Expected type: floa
t, Actual type: string
2018-10-22 16:55:06.922092: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: image/object/mask.  Data types don't match. Expected type: floa
t, Actual type: string
2018-10-22 16:55:06.922206: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: image/object/mask.  Data types don't match. Expected type: floa
t, Actual type: string
2018-10-22 16:55:06.923368: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: image/object/mask.  Data types don't match. Expected type: floa
t, Actual type: string
2018-10-22 16:55:06.923381: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: image/object/mask.  Data types don't match. Expected type: floa
t, Actual type: string
2018-10-22 16:55:06.924358: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: image/object/mask.  Data types don't match. Expected type: floa
t, Actual type: string
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 439, in train_and_evaluate
    executor.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 518, in run
    self.run_local()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 650, in run_local
    hooks=train_hooks)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 363, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 843, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 859, in _train_model_default
    saving_listeners)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1059, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 567, in run
    run_metadata=run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1043, in run
    run_metadata=run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1134, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1119, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1191, in run
    run_metadata=run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 971, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Key: image/object/mask.  Data types don't match. Expected type: float, Actual type: string
         [[Node: ParseSingleExample/ParseSingleExample = ParseSingleExample[Tdense=[DT_STRING, DT_STRING, DT_STRING, DT_INT64, DT_STRING, DT_STRING, DT_INT64], dense_keys=[""image/encoded"", ""image/filename"", ""imag
e/format"", ""image/height"", ""image/key/sha256"", ""image/source_id"", ""image/width""], dense_shapes=[[], [], [], [], [], [], []], num_sparse=14, sparse_keys=[""image/class/label"", ""image/class/text"", ""image/object/area
"", ""image/object/bbox/xmax"", ""image/object/bbox/xmin"", ""image/object/bbox/ymax"", ""image/object/bbox/ymin"", ""image/object/class/label"", ""image/object/class/text"", ""image/object/difficult"", ""image/object/group_of"",
 ""image/object/is_crowd"", ""image/object/mask"", ""image/object/weight""], sparse_types=[DT_INT64, DT_STRING, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_STRING, DT_INT64, DT_INT64, DT_INT64, DT_FL
OAT, DT_FLOAT]](Reshape, ParseSingleExample/Reshape, ParseSingleExample/Reshape_1, ParseSingleExample/Reshape_2, ParseSingleExample/Reshape_3, ParseSingleExample/Reshape_4, ParseSingleExample/Reshape_5, ParseSing
leExample/Reshape_6)]]
         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[1], [1,?,?,3], [1,3], [1,100], [1,100,4], [1,100,37], [1,100], [1,100,?,?], [1,100], [1,100], [1]], output_types=[DT_INT32, DT_FLOAT, DT_INT32, D
T_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_FLOAT, DT_BOOL, DT_FLOAT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
         [[Node: IteratorGetNext/_4831 = _HostRecv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_inca
rnation=1, tensor_name=""edge_52_IteratorGetNext"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
```",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-22 21:43:22,open,,,[],2019-02-20 09:00:44
486,tensorflow/models,models,5581,personableduck,Inference example code for imagenet_main.py traning,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: imagenet_main.py
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra, MacBook Pro, 3.1 GHz Intel Core i5, 8 GB 2133 MHz LPDDR3, Intel Iris Plus Graphics 650 1536 MB
- **TensorFlow installed from (source or binary)**: source, pip install tensorflow
- **TensorFlow version (use command below)**:  v1.11.0-rc2-4-gc19e29306c 1.11.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

Training:
python3.6 imagenet_main.py --data_dir=(data directory, tfrecords files) --model_dir=(ouput directory) --export_dir=(saved model directory) --train_epochs=100

Inference:
python3.6 classify_image.py --model_dir=""/Users/duckhahwang/Downloads/imagenet_test"" --image_file=""/Users/duckhahwang/Downloads/airplane/airplane/frame_62.jpg"" 

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Training:
python3.6 imagenet_main.py --data_dir=(data directory, tfrecords files) --model_dir=(ouput directory) --export_dir=(saved model directory) --train_epochs=100

Inference:
python3.6 classify_image.py --model_dir=""/Users/duckhahwang/Downloads/imagenet_test"" --image_file=""/Users/duckhahwang/Downloads/airplane/airplane/frame_62.jpg"" 

I trained a new model with other data that is not imagenet image data, new classes, and labels. This is scratch training. But my problem is that after scratch training I stuck by inference or predict new image with the trained model. 

I found a tutorial code:

https://www.tensorflow.org/tutorials/images/image_recognition

https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py

This code is that only let me know how to use the pre-trained model with ""classify_image_graph_def.pb"", 
First, I tried the same code and ""saved_model.pb"" that from export_dir but it is not working. 

Second, I tried freez_graph.py with checkpoint.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py
But, this method also not working.

Third, using tensorrt, this is quite close to what I want to use. but my MacPro not supports to Tensorrt GPU. 
https://github.com/tensorflow/models/tree/master/research/tensorrt

Could I get any sample or tutorial code for inference, predict test code for my trained model with imagenet_main.py? 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Error message from First, and Second way to predict new data with code.
saved_model.pb and freez_pragh.pb

Traceback (most recent call last):
  File ""classify_image.py"", line 264, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""classify_image.py"", line 230, in main
    run_inference_on_image(image)
  File ""classify_image.py"", line 192, in run_inference_on_image
    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3664, in get_tensor_by_name
    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3488, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3530, in _as_graph_element_locked
    ""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'softmax:0' refers to a Tensor which does not exist. The operation, 'softmax', does not exist in the graph.""


",2,"NamedUser(login=""seemuch"")","[NamedUser(login=""seemuch"")]",2018-10-22 18:38:31,open,,,[],2018-12-13 14:29:19
487,tensorflow/models,models,5580,fengbaozhiling111,clear the memory in tensorflow,"After a reinforcement learning algorithm is done in idle, I could not clear the memory. However, when I conduct these code in spyder, I could clear the memory by ""restart kernel"" in spyder but it will succeed only sometimes instead of everytime. So how can I clear these memory in idle. Is it possible?
If not, is there any other approaches to clear memory in tensorflow.
Thank you!",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-22 16:07:47,open,,,[],2018-10-23 18:39:38
488,tensorflow/models,models,5578,androuino,No Operation named [input] in the Graph,"Hi,
After I trained the **faster rcnn inception v2 pets** model and froze it. Now I'm trying to use the trained model with java using tensorflow lib and tensorflow gpu libraries but I'm having this error

> Exception in thread ""main"" java.lang.IllegalArgumentException: No Operation named [input] in the Graph
	at org.tensorflow.Session$Runner.operationByName(Session.java:372)
	at org.tensorflow.Session$Runner.parseOutput(Session.java:381)
	at org.tensorflow.Session$Runner.feed(Session.java:131)
	at edu.ml.tensorflow.ObjectDetector.executeYOLOGraph(ObjectDetector.java:91)
	at edu.ml.tensorflow.ObjectDetector.detect(ObjectDetector.java:49)
	at edu.ml.tensorflow.Main.main(Main.java:8)

Would like to ask what am I missing or did something wrong?
I have cuda 9.0 with tensorflow version 1.11. Thanks",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-10-22 07:29:37,open,,,['stat:awaiting response'],2018-10-23 00:15:19
489,tensorflow/models,models,5577,1453042287,what's the mean of the 'layer_15/expansion_output' ? ,"### System information
- **What is the top-level directory of the model you are using**:
object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
pypi
- **TensorFlow version (use command below)**:
tf 1.4
- **Bazel version (if compiling from source)**:

- **CUDA/cuDNN version**:
CUDA9.0 cuDNN7.3
- **GPU model and memory**:
TITAN V 11G
- **Exact command to reproduce**:
none

### Describe the problem
in the  /models/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
line 108 :
 feature_map_layout = {
        'from_layer': ['layer_15/expansion_output', 'layer_19', '', '', '', ''],
        'layer_depth': [-1, -1, 512, 256, 256, 128],
        'use_depthwise': self._use_depthwise,
        'use_explicit_padding': self._use_explicit_padding,
    }
what's the mean of the 'layer_15/expansion_output'? thanks a lot!

",6,"NamedUser(login=""wt-huang"")","[NamedUser(login=""wt-huang"")]",2018-10-21 09:33:56,open,,,[],2018-11-07 00:47:05
490,tensorflow/models,models,5575,Houd1ny,"Cityscapes class ""parking""","I`m wondering is 
[DeepLab models trained on Cityscapes](https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md) trained using ""parking"" class.
As according to official [dataset site](https://www.cityscapes-dataset.com/dataset-overview/) this class marked as  ""This label is not included in any evaluation and treated as void (or in the case of license plate as the vehicle mounted on).""
Where should I change config to include it in training if this class was not used?",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-10-20 14:20:48,open,,,['stat:awaiting response'],2018-10-23 00:15:14
491,tensorflow/models,models,5571,bapfeld,Fixes Python 3.x error and dependency deprecation warning in research/skip_thoughts,"Original code attempts to apply `str.decode('utf-8')`, which throws an error in python 3.x. This now uses a try/except block to catch the error and allow backwards compatibility.

It also fixes a deprecation error on a dependency.",1,,[],2018-10-19 14:55:22,open,,,['cla: yes'],2019-01-23 20:39:27
492,tensorflow/models,models,5568,395062753,change the shape of crop size when I train the model of deeplab,"tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape mismatch in tuple component 1. Expected [352,352,3], got [500,400,3]
	 [[Node: batch/padding_fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_INT64, DT_FLOAT, DT_STRING, DT_INT32, DT_UINT8, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](batch/padding_fifo_queue, Reshape_3/_845, add_2/_847, ParseSingleExample/ParseSingleExample:1, add_3/_849, batch/packed, Reshape_6/_851)]]


python3 ""${WORK_DIR}""/train.py \
 --logtostderr \
  --train_split=""trainval"" \
  --model_variant=""mobilenet_v2"" \
  --output_stride=16 \
  --train_crop_size=352 \
  --train_crop_size=352 \
  --train_batch_size=4 \
  --training_number_of_steps=""${NUM_ITERATIONS}"" \
  --fine_tune_batch_norm=true \
  --tf_initial_checkpoint=""${INIT_FOLDER}/${CKPT_NAME}/model.ckpt-30000"" \
  --train_logdir=""${TRAIN_LOGDIR}"" \
  --dataset_dir=""${PASCAL_DATASET}""

# Run evaluation. This performs eval over the full val split (1449 images) and
# will take a while.
# Using the provided checkpoint, one should expect mIOU=75.34%.
python3 ""${WORK_DIR}""/eval.py \
  --logtostderr \
  --eval_split=""val"" \
  --model_variant=""mobilenet_v2"" \
  --eval_crop_size=352 \
  --eval_crop_size=352 \
  --checkpoint_dir=""${TRAIN_LOGDIR}"" \
  --eval_logdir=""${EVAL_LOGDIR}"" \
  --dataset_dir=""${PASCAL_DATASET}"" \
  --max_number_of_evaluations=1



how can I change the shape of input_Image",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-10-19 02:36:11,open,,,['stat:awaiting response'],2018-10-19 13:33:24
493,tensorflow/models,models,5567,1453042287,something wrong when ssd mobilenetv2 training,"### System information
- **What is the top-level directory of the model you are using**:
object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
pypi tensorflow-gpu
- **TensorFlow version (use command below)**:
1.11
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA9.0 cuDNN7.3
- **GPU model and memory**:
TITAN V 11G
- **Exact command to reproduce**:
CUDA_VISIBLE_DEVICES=0   python  object_detection/model_main.py --pipeline_config_path=project/ssdlite_mobilenet_v2_coco_2018_05_09/ssdlite.config --model_dir=project/ssdlite_result --num_train_steps=100000 --sample_1_of_n_eval_examples=1 --alsologtostderr

### Describe the problem
ValueError: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(""arg0:0"", shape=(), dtype=float32, device=/device:CPU:0)'

### Source code / logs
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1181, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1215, in _train_model_default
    saving_listeners)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1409, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1148, in run
    run_metadata=run_metadata)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1239, in run
    raise six.reraise(*original_exc_info)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1224, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1304, in run
    run_metadata=run_metadata))
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 581, in after_run
    if self._save(run_context.session, global_step):
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 606, in _save
    if l.after_save(session, step):
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 517, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 537, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 912, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 476, in evaluate
    return _evaluate()
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 462, in _evaluate
    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1422, in _evaluate_build_graph
    self._call_model_fn_eval(input_fn, self.config))
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1455, in _call_model_fn_eval
    input_fn, model_fn_lib.ModeKeys.EVAL)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1049, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1136, in _call_input_fn
    return input_fn(**kwargs)
  File ""/home/chi_zhang/models/research/object_detection/inputs.py"", line 568, in _eval_input_fn
    transform_input_data_fn=transform_and_pad_input_data_fn)
  File ""/home/chi_zhang/models/research/object_detection/builders/dataset_builder.py"", line 134, in build
    config.input_path[:], input_reader_config)
  File ""/home/chi_zhang/models/research/object_detection/builders/dataset_builder.py"", line 80, in read_dataset
    sloppy=config.shuffle))
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1128, in apply
    dataset = transformation_func(self)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/interleave_ops.py"", line 87, in _apply_fn
    buffer_output_elements, prefetch_input_elements)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py"", line 128, in __init__
    cycle_length, block_length)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2312, in __init__
    super(InterleaveDataset, self).__init__(input_dataset, map_func)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2275, in __init__
    experimental_nested_dataset_support=True)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1473, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 479, in add_to_graph
    self._create_definition_if_needed()
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 335, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 344, in _create_definition_if_needed_impl
    self._capture_by_value, self._caller_device)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 865, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1411, in tf_data_structured_function_wrapper
    ret = func(*nested_args)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py"", line 194, in __init__
    filenames = ops.convert_to_tensor(filenames, dtype=dtypes.string)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1048, in convert_to_tensor
    as_ref=False)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1144, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/chi_zhang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 981, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(""arg0:0"", shape=(), dtype=float32, device=/device:CPU:0)'

",1,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-10-19 01:42:11,open,,,[],2018-10-30 00:18:53
494,tensorflow/models,models,5565,wolfshow,Why multi-GPU training not supported for object detection?,Is there any reason why multi-GPU training not supported?,12,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-10-18 14:45:42,open,,,[],2019-04-05 16:44:01
495,tensorflow/models,models,5564,1453042287,Update model_lib.py,"lead a bug ""TYPEERROR: CAN'T PICKLE DICTVALUES OBJECTS""",1,,[],2018-10-18 09:25:56,open,,,['cla: no'],2018-10-18 09:28:52
496,tensorflow/models,models,5563,Ashokcharu,"File ""generate_tfrecord.py"", line 110, in  tf.app.run()","When i execute this comment,

python generate_tfrecord.py --csv_input=images\train_labels.csv --image_dir=images\train --output_path=train.record

I'm getting this below error

Traceback (most recent call last):
  File ""generate_tfrecord.py"", line 110, in <module>
    tf.app.run()
  File ""C:\anaconda3\envs\tensorflowc\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""generate_tfrecord.py"", line 101, in main
    tf_example = create_tf_example(group, path)
  File ""generate_tfrecord.py"", line 56, in create_tf_example
    encoded_jpg = fid.read()
  File ""C:\anaconda3\envs\tensorflowc\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 125, in read
    self._preread_check()
  File ""C:\anaconda3\envs\tensorflowc\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 85, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""C:\anaconda3\envs\tensorflowc\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open: C:\tensorflowc\models\research\object_detection\images\train\tr1138a1a1_3_lar : The system cannot find the file specified.
; No such file or directory

I checked all online solutions for this error, nothing is resolved my error. Please help me to resolve this issue",4,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-10-18 09:02:39,open,,,[],2019-02-08 14:39:44
497,tensorflow/models,models,5562,AdrianaVR,"Export inference_graph Assign requires shapes of both tensors to match. lhs shape= [1,1,320,1280] rhs shape= [1,1,320,256]","

### System information
- **What is the top-level directory of the model you are using**:Object_Detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Modified ssd_mobilenet_v2_coco.config for my five class
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.5 LTS 64-bit
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.10.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:CUDA 8 / cuDNN 5.1
- **GPU model and memory**:gtx 980 4gb
- **Exact command to reproduce**:
python export_inference_graph.py     --input_type image_tensor     --pipeline_config_path training/ssd_mobilenet_v2_coco.config     --trained_checkpoint_prefix training/model.ckpt-200000     --output_directory inference_graph




### Describe the problem
After training the model ssd_mobilenet_v2_coco (following this tutorial https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html) to learn 5 new classes when wanting to export the model I get that error .

### Source code / logs
ARNING:tensorflow:From /home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py:280: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
WARNING:tensorflow:From /home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py:434: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.
Instructions for updating:
Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details
145 ops no flops stats due to incomplete shapes.
Parsing Inputs...
Incomplete shape.

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              0
-min_occurrence             0
-step                       -1
-order_by                   name
-account_type_regexes       _trainable_variables
-start_name_regexes         .*
-trim_name_regexes          .*BatchNorm.*
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     params
-output                     stdout:

==================Model Analysis Report======================
Incomplete shape.

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
param: Number of parameters (in the Variable).

Profile:
node name | # parameters
_TFProfRoot (--/4.64m params)
  BoxPredictor_0 (--/17.31k params)
    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)
      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)
      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)
    BoxPredictor_0/ClassPredictor (--/10.39k params)
      BoxPredictor_0/ClassPredictor/biases (18, 18/18 params)
      BoxPredictor_0/ClassPredictor/weights (1x1x576x18, 10.37k/10.37k params)
  BoxPredictor_1 (--/76.86k params)
    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)
      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)
      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)
    BoxPredictor_1/ClassPredictor (--/46.12k params)
      BoxPredictor_1/ClassPredictor/biases (36, 36/36 params)
      BoxPredictor_1/ClassPredictor/weights (1x1x1280x36, 46.08k/46.08k params)
  BoxPredictor_2 (--/30.78k params)
    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)
      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)
      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)
    BoxPredictor_2/ClassPredictor (--/18.47k params)
      BoxPredictor_2/ClassPredictor/biases (36, 36/36 params)
      BoxPredictor_2/ClassPredictor/weights (1x1x512x36, 18.43k/18.43k params)
  BoxPredictor_3 (--/15.42k params)
    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)
      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)
      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)
    BoxPredictor_3/ClassPredictor (--/9.25k params)
      BoxPredictor_3/ClassPredictor/biases (36, 36/36 params)
      BoxPredictor_3/ClassPredictor/weights (1x1x256x36, 9.22k/9.22k params)
  BoxPredictor_4 (--/15.42k params)
    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)
      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)
      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)
    BoxPredictor_4/ClassPredictor (--/9.25k params)
      BoxPredictor_4/ClassPredictor/biases (36, 36/36 params)
      BoxPredictor_4/ClassPredictor/weights (1x1x256x36, 9.22k/9.22k params)
  BoxPredictor_5 (--/7.74k params)
    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)
      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)
      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)
    BoxPredictor_5/ClassPredictor (--/4.64k params)
      BoxPredictor_5/ClassPredictor/biases (36, 36/36 params)
      BoxPredictor_5/ClassPredictor/weights (1x1x128x36, 4.61k/4.61k params)
  FeatureExtractor (--/4.48m params)
    FeatureExtractor/MobilenetV2 (--/4.48m params)
      FeatureExtractor/MobilenetV2/Conv (--/864 params)
        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)
      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)
        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)
      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)
        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)
          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)
        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)
          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)
      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)
        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)
          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)
        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)
          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)
        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)
          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)
      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)
        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)
          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)
        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)
          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)
        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)
          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)
      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)
        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)
          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)
        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)
          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)
        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)
          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)
      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)
        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)
          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)
        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)
          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)
        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)
          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)
      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)
        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)
          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)
        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)
          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)
        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)
          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)
      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)
        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)
          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)
        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)
          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)
        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)
          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)
      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)
        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)
          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)
        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)
          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)
        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)
          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)
      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)
        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)
          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)
        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)
          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)
        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)
          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)
      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)
        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)
          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)
        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)
          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)
        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)
          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)
      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)
        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)
          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)
        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)
          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)
        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)
          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)
      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)
        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)
          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)
        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)
          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)
        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)
          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)
      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)
        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)
          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)
        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)
          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)
        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)
          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)
      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)
        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)
          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)
        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)
          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)
        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)
          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)
      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)
        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)
          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)
        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)
          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)
        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)
          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)
      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)
        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)
          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)
        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)
          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)
        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)
          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)
      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)
        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)
          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)
        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)
          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)
        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)
          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)
          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)
      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)
        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)
      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)
        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)
      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)
        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)
      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)
        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)
      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/1.18m params)
        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (3x3x256x512, 1.18m/1.18m params)
      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/294.91k params)
        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)
      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/294.91k params)
        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)
      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/73.73k params)
        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)
        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (3x3x64x128, 73.73k/73.73k params)

======================End of Report==========================
145 ops no flops stats due to incomplete shapes.
Parsing Inputs...
Incomplete shape.

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================
Incomplete shape.

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/17.62k flops)
  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)
  MultipleGridAnchorGenerator/add_2 (2.17k/2.17k flops)
  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)
  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)
  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)
  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)
  MultipleGridAnchorGenerator/add_5 (1.20k/1.20k flops)
  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)
  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)
  MultipleGridAnchorGenerator/mul_29 (600/600 flops)
  MultipleGridAnchorGenerator/add_8 (300/300 flops)
  MultipleGridAnchorGenerator/mul_36 (300/300 flops)
  MultipleGridAnchorGenerator/sub_2 (300/300 flops)
  MultipleGridAnchorGenerator/mul_35 (300/300 flops)
  MultipleGridAnchorGenerator/mul_37 (150/150 flops)
  MultipleGridAnchorGenerator/mul_44 (108/108 flops)
  MultipleGridAnchorGenerator/sub_3 (108/108 flops)
  MultipleGridAnchorGenerator/mul_43 (108/108 flops)
  MultipleGridAnchorGenerator/add_11 (108/108 flops)
  MultipleGridAnchorGenerator/mul_45 (54/54 flops)
  MultipleGridAnchorGenerator/sub_4 (48/48 flops)
  MultipleGridAnchorGenerator/add_14 (48/48 flops)
  MultipleGridAnchorGenerator/mul_51 (48/48 flops)
  MultipleGridAnchorGenerator/mul_52 (48/48 flops)
  MultipleGridAnchorGenerator/mul_53 (24/24 flops)
  MultipleGridAnchorGenerator/add (19/19 flops)
  MultipleGridAnchorGenerator/mul_18 (19/19 flops)
  MultipleGridAnchorGenerator/mul_17 (19/19 flops)
  MultipleGridAnchorGenerator/add_1 (19/19 flops)
  MultipleGridAnchorGenerator/mul_59 (12/12 flops)
  MultipleGridAnchorGenerator/mul_60 (12/12 flops)
  MultipleGridAnchorGenerator/sub_5 (12/12 flops)
  MultipleGridAnchorGenerator/add_17 (12/12 flops)
  MultipleGridAnchorGenerator/add_4 (10/10 flops)
  MultipleGridAnchorGenerator/add_3 (10/10 flops)
  MultipleGridAnchorGenerator/mul_26 (10/10 flops)
  MultipleGridAnchorGenerator/mul_25 (10/10 flops)
  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)
  MultipleGridAnchorGenerator/mul_55 (6/6 flops)
  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)
  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)
  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)
  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)
  MultipleGridAnchorGenerator/mul_38 (6/6 flops)
  MultipleGridAnchorGenerator/mul_39 (6/6 flops)
  MultipleGridAnchorGenerator/mul_40 (6/6 flops)
  MultipleGridAnchorGenerator/mul_54 (6/6 flops)
  MultipleGridAnchorGenerator/mul_61 (6/6 flops)
  MultipleGridAnchorGenerator/mul_46 (6/6 flops)
  MultipleGridAnchorGenerator/mul_47 (6/6 flops)
  MultipleGridAnchorGenerator/mul_48 (6/6 flops)
  MultipleGridAnchorGenerator/mul_56 (6/6 flops)
  MultipleGridAnchorGenerator/mul_31 (6/6 flops)
  MultipleGridAnchorGenerator/mul_30 (6/6 flops)
  MultipleGridAnchorGenerator/mul_24 (6/6 flops)
  MultipleGridAnchorGenerator/mul_23 (6/6 flops)
  MultipleGridAnchorGenerator/mul_22 (6/6 flops)
  MultipleGridAnchorGenerator/mul_32 (6/6 flops)
  MultipleGridAnchorGenerator/mul_33 (5/5 flops)
  MultipleGridAnchorGenerator/mul_34 (5/5 flops)
  MultipleGridAnchorGenerator/add_7 (5/5 flops)
  MultipleGridAnchorGenerator/add_6 (5/5 flops)
  MultipleGridAnchorGenerator/add_9 (3/3 flops)
  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)
  MultipleGridAnchorGenerator/mul_14 (3/3 flops)
  MultipleGridAnchorGenerator/mul_15 (3/3 flops)
  MultipleGridAnchorGenerator/mul_16 (3/3 flops)
  MultipleGridAnchorGenerator/mul_42 (3/3 flops)
  MultipleGridAnchorGenerator/mul_41 (3/3 flops)
  MultipleGridAnchorGenerator/add_10 (3/3 flops)
  MultipleGridAnchorGenerator/add_12 (2/2 flops)
  MultipleGridAnchorGenerator/add_13 (2/2 flops)
  MultipleGridAnchorGenerator/mul_50 (2/2 flops)
  MultipleGridAnchorGenerator/mul_49 (2/2 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_5 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_4 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_3 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add (1/1 flops)
  Preprocessor/map/while/add_1 (1/1 flops)
  Preprocessor/map/while/add (1/1 flops)
  Preprocessor/map/while/Less_1 (1/1 flops)
  Preprocessor/map/while/Less (1/1 flops)
  Postprocessor/Decode/transpose_1/sub (1/1 flops)
  Postprocessor/Decode/transpose/sub (1/1 flops)
  Postprocessor/Decode/get_center_coordinates_and_sizes/transpose/sub (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add_1 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)
  MultipleGridAnchorGenerator/mul_6 (1/1 flops)
  MultipleGridAnchorGenerator/add_19 (1/1 flops)
  MultipleGridAnchorGenerator/add_20 (1/1 flops)
  MultipleGridAnchorGenerator/add_21 (1/1 flops)
  MultipleGridAnchorGenerator/add_22 (1/1 flops)
  MultipleGridAnchorGenerator/add_23 (1/1 flops)
  MultipleGridAnchorGenerator/mul_9 (1/1 flops)
  MultipleGridAnchorGenerator/mul_8 (1/1 flops)
  MultipleGridAnchorGenerator/mul_7 (1/1 flops)
  MultipleGridAnchorGenerator/mul_12 (1/1 flops)
  MultipleGridAnchorGenerator/add_18 (1/1 flops)
  MultipleGridAnchorGenerator/mul_5 (1/1 flops)
  MultipleGridAnchorGenerator/mul_58 (1/1 flops)
  MultipleGridAnchorGenerator/mul_57 (1/1 flops)
  MultipleGridAnchorGenerator/assert_equal/Equal (1/1 flops)
  MultipleGridAnchorGenerator/Minimum (1/1 flops)
  MultipleGridAnchorGenerator/mul (1/1 flops)
  MultipleGridAnchorGenerator/mul_1 (1/1 flops)
  MultipleGridAnchorGenerator/mul_10 (1/1 flops)
  MultipleGridAnchorGenerator/mul_11 (1/1 flops)
  MultipleGridAnchorGenerator/mul_2 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)
  MultipleGridAnchorGenerator/mul_3 (1/1 flops)
  MultipleGridAnchorGenerator/mul_4 (1/1 flops)
  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)
  MultipleGridAnchorGenerator/add_15 (1/1 flops)
  MultipleGridAnchorGenerator/add_16 (1/1 flops)
  MultipleGridAnchorGenerator/mul_13 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)
  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)
  MultipleGridAnchorGenerator/truediv (1/1 flops)

======================End of Report==========================
2018-10-17 23:14:24.221188: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1,1,320,1280] rhs shape= [1,1,320,256]
	 [[Node: save/Assign_33 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV2/Conv_1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](FeatureExtractor/MobilenetV2/Conv_1/weights, save/RestoreV2:33)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1725, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1,1,320,1280] rhs shape= [1,1,320,256]
	 [[Node: save/Assign_33 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV2/Conv_1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](FeatureExtractor/MobilenetV2/Conv_1/weights, save/RestoreV2:33)]]

Caused by op 'save/Assign_33', defined at:
  File ""export_inference_graph.py"", line 150, in <module>
    tf.app.run()
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""export_inference_graph.py"", line 146, in main
    write_inference_graph=FLAGS.write_inference_graph)
  File ""/home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py"", line 405, in export_inference_graph
    write_inference_graph=write_inference_graph)
  File ""/home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py"", line 334, in _export_inference_graph
    trained_checkpoint_prefix=checkpoint_to_use)
  File ""/home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py"", line 241, in write_graph_and_checkpoint
    tf.import_graph_def(inference_graph_def, name='')
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 234, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3289, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3289, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3180, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1,1,320,1280] rhs shape= [1,1,320,256]
	 [[Node: save/Assign_33 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV2/Conv_1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](FeatureExtractor/MobilenetV2/Conv_1/weights, save/RestoreV2:33)]]


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""export_inference_graph.py"", line 150, in <module>
    tf.app.run()
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""export_inference_graph.py"", line 146, in main
    write_inference_graph=FLAGS.write_inference_graph)
  File ""/home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py"", line 405, in export_inference_graph
    write_inference_graph=write_inference_graph)
  File ""/home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py"", line 334, in _export_inference_graph
    trained_checkpoint_prefix=checkpoint_to_use)
  File ""/home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py"", line 245, in write_graph_and_checkpoint
    saver.restore(sess, trained_checkpoint_prefix)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1759, in restore
    err, ""a mismatch between the current graph and the graph"")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Assign requires shapes of both tensors to match. lhs shape= [1,1,320,1280] rhs shape= [1,1,320,256]
	 [[Node: save/Assign_33 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV2/Conv_1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](FeatureExtractor/MobilenetV2/Conv_1/weights, save/RestoreV2:33)]]

Caused by op 'save/Assign_33', defined at:
  File ""export_inference_graph.py"", line 150, in <module>
    tf.app.run()
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""export_inference_graph.py"", line 146, in main
    write_inference_graph=FLAGS.write_inference_graph)
  File ""/home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py"", line 405, in export_inference_graph
    write_inference_graph=write_inference_graph)
  File ""/home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py"", line 334, in _export_inference_graph
    trained_checkpoint_prefix=checkpoint_to_use)
  File ""/home/ady/Documentos/TensorFlow/models/research/object_detection/exporter.py"", line 241, in write_graph_and_checkpoint
    tf.import_graph_def(inference_graph_def, name='')
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 234, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3289, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3289, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3180, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/home/ady/anaconda3/envs/TF/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Assign requires shapes of both tensors to match. lhs shape= [1,1,320,1280] rhs shape= [1,1,320,256]
	 [[Node: save/Assign_33 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV2/Conv_1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](FeatureExtractor/MobilenetV2/Conv_1/weights, save/RestoreV2:33)]]

",4,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-18 04:34:20,open,,,[],2019-04-05 05:48:45
498,tensorflow/models,models,5559,stoneyang,Where to know how to print loss for new loss function when using TF-Slim?,"Hi, there:

You guys did a great job, no doubt. 

But the documents seems severely unfriendly to new comer, at least like me myself.

The caption's my problem, how simple, but I just can't find where to learn it....

Could anyone shed some light? Thanks in advance!

I want to train a model using `tf-slim`, in a minimal modification manner, ie, I just add **a custom loss function** and train a multi-loss model. The function's adapted to slim, running, but I just cannot print the loss values of it.
",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-10-18 02:50:28,open,,,"['models: research', 'stat:awaiting response']",2019-02-01 22:16:48
499,tensorflow/models,models,5558,lovebabychen,How to train mask rcnn model,"when i want to train mask rcnn model  by using my own dataset ，i have a question that when i run model_main.py ，the program is stuck or it is very slow. I can not solve it ,",4,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-10-18 01:07:45,open,,,[],2018-10-30 02:07:52
500,tensorflow/models,models,5556,MarkGreeny,Where can one find a pretrained model for inception_resnet_v2.py? All models give same (key) error,"Hi, 

I am trying to get [this](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py) model to work. But all models I find, give the exact same error,

  > NotFoundError (see above for traceback): Key InceptionResnetV2/Conv2d_1a_3x3/biases not found in checkpoint

Any help greatly appreciated!

## Edit

Adding more detail.

This script is also described [here](https://www.guild.ai/models/slim/inception/#slim-inception-v2). At resources it lists [this](http://download.tensorflow.org/models/inception_v2_2016_08_28.tar.gz) checkpoint. But again, when I download try try to restore, I get the same key error.",2,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-10-17 17:49:59,open,,"NamedUser(login=""MarkGreeny"")",['stat:awaiting tensorflower'],2018-10-29 02:26:52
501,tensorflow/models,models,5554,vtrokhymenko,russia data,"hi,
anybody fitting this model in the russian news data?",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-10-17 10:27:30,open,,,[],2018-10-18 07:29:53
502,tensorflow/models,models,5553,netanel-s,[Object Detection][Feature Request] Option to restore (detection) weights from pre-trained model for existing classes when fine-tuning on *additional* classes,"### System information
- **What is the top-level directory of the model you are using**: Object Detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**:  Fine-tuning a model with additional classes (relative to pre-trained model).

### Describe the problem
Let's say we have a pre-trained model on COCO, and I want to fine-tune it to have **additional** classes. 
Now, since the number of classes has changed, then only the weights up to the last layer(s) are restored from the pre-trained model, and all the weights of the detection heads at the end are initialized. 
I (and others I talked to) would like to have an option where the weights of the existing classes would be restored too, and fine-tuning would then be of both the restored weights of the existing classes and the initialized weights of the additional classes.
It should be investigated whether it makes sense to restore **all** weights of existing classes, but it sure makes sense to do it for example to the bounding box regression weights.
""Finding out"" that the class set has **additional** classes relative to pre-trained model can be done by the label map.

Thanks in advance.",1,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc""), NamedUser(login=""yhliang2018"")]",2018-10-17 09:54:04,open,,,[],2019-03-06 00:19:50
503,tensorflow/models,models,5552,netanel-s,"[Object Detection (but not only)][Feature Request] Batch: Gradient Accumulation for large batch size, batch size scheduling","### System information
- **What is the top-level directory of the model you are using**: Object Detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**:  Not televant

### Describe the problem
I have a request for two features related to batch size. There are a few works which show large batch size is very effective in some computer vision tasks such as object classification and detection (e.g. [1](https://arxiv.org/pdf/1706.02677.pdf), [2](https://arxiv.org/pdf/1711.07240.pdf)). In cases where you don't have multiple-GPU distribution capability (because of lack of hardware or lack of sync capability between the GPUs), this is impossible.

1. Larger batch size that a single GPU can handle can be achieved by Gradient Accumulation, as explained [here](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) (Large batches on one or several GPU(s)).  This can also be useful in case of multiple-GPUs distribution, where it can simulate further distribution (e.g. 8 GPUs * 8 accumulation steps -> like 64 GPUs in means of batch size).

2. There's also a [paper by Google ](https://arxiv.org/pdf/1711.00489.pdf) which recommends increasing the batch size instead of decreasing the learning rate. 

It would be highly appreciated if you could add these two features.
Thank you very much in advance.
",4,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-17 08:35:08,open,,,[],2018-11-19 15:46:38
504,tensorflow/models,models,5550,dwSun,Can you provide detail version of cuda and cudnn tensorflow-gpu built with on pypi,I have to re-install the entire os to meet the version requirement of tensorflow-gpu.,1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-10-17 02:51:54,open,,,['stat:awaiting response'],2018-10-17 14:00:02
505,tensorflow/models,models,5549,DinLei,May be a bug -> the avg_cost in models/../Autoencoder.py,"Your cost is:  

> self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))

this is sum,

Then the function:

> def partial_fit(self, X):
        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X})
        return cost

in runner script, you use this function to calculate batch examples total cost,
but in a train loop each epoch:  

> avg_cost += cost / n_samples * batch_size

you multiply by batch_size,

I think ""batch_size"" should remove.",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-10-17 01:44:50,open,,,['stat:awaiting response'],2018-10-17 13:59:56
506,tensorflow/models,models,5548,bubba,Is CycleGAN missing a networks.py?,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
`models/research/gan/cyclegan`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Nope!
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS 10.14
- **TensorFlow installed from (source or binary)**:
Installed via pip
- **TensorFlow version (use command below)**:
v1.11.0-rc2-4-gc19e29306c 1.11.0
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
`python3 train.py`

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Running the `train.py` script in the CycleGAN example gives this error:
```
Traceback (most recent call last):
  File ""train.py"", line 26, in <module>
    import networks
ModuleNotFoundError: No module named 'networks'
```
And I noticed that unlike the other examples there's no `networks.py`. Is this missing?",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-10-16 21:10:15,open,,,[],2018-10-17 07:44:12
507,tensorflow/models,models,5547,bigboynaruto,Fixed typo in video_prediction model description,,3,,[],2018-10-16 21:05:25,open,,,['cla: yes'],2018-10-16 21:08:57
508,tensorflow/models,models,5543,Arpitjain250,Update README.md,,1,,[],2018-10-16 09:07:26,open,,,['cla: no'],2018-10-16 09:07:31
509,tensorflow/models,models,5541,personableduck,SavedModel to access ranked predictions on image classification.,"### System information
- **What is the top-level directory of the model you are using**: imagenet_main.py
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra, MacBook Pro, 3.1 GHz Intel Core i5, 8 GB 2133 MHz LPDDR3, Intel Iris Plus Graphics 650 1536 MB
- **TensorFlow installed from (source or binary)**: source, pip install tensorflow
- **TensorFlow version (use command below)**:  v1.11.0-rc2-4-gc19e29306c 1.11.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

from official.resnet.imagenet_preprocessing import _CHANNEL_MEANS

predict_fn = tf.contrib.predictor.from_saved_model(export_dir)
img_bgr = cv2.imread(image_file_path)

if img_bgr.shape != (224, 224, 3):
    img_resized = cv2.resize(img_bgr, (224, 224)).copy()  # resize image

img = img_resized[:, :, (2, 1, 0)].copy()  # convert to RGB
img = img - _CHANNEL_MEANS

batch_size = 64
img = np.stack([img] * batch_size, axis=0)

out = predict_fn({""input"": img})


### Describe the problem
SavedModel class requesting a feature that allows the SavedModel to access ranked predictions.

### Source code / logs
only one label prediction.

class top 5: 
[405 405 405 405 405]

probability:
[[9.0638358e-10 1.2027534e-08 1.6694778e-09 ... 1.2923358e-09
  5.7039379e-08 1.0450520e-08]
 [9.0638358e-10 1.2027534e-08 1.6694778e-09 ... 1.2923358e-09
  5.7039379e-08 1.0450520e-08]
 [9.0638358e-10 1.2027534e-08 1.6694778e-09 ... 1.2923358e-09
  5.7039379e-08 1.0450520e-08]
 [9.0638358e-10 1.2027534e-08 1.6694778e-09 ... 1.2923358e-09
  5.7039379e-08 1.0450520e-08]
 [9.0638358e-10 1.2027534e-08 1.6694778e-09 ... 1.2923358e-09
  5.7039379e-08 1.0450520e-08]]

print(out)

{'classes': array([405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405,
       405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405,
       405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405,
       405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405,
       405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405, 405]), 'probabilities': array([[9.0638358e-10, 1.2027534e-08, 1.6694778e-09, ..., 1.2923358e-09,
        5.7039379e-08, 1.0450520e-08],
       [9.0638358e-10, 1.2027534e-08, 1.6694778e-09, ..., 1.2923358e-09,
        5.7039379e-08, 1.0450520e-08],
       [9.0638358e-10, 1.2027534e-08, 1.6694778e-09, ..., 1.2923358e-09,
        5.7039379e-08, 1.0450520e-08],
       ...,
       [9.0638358e-10, 1.2027534e-08, 1.6694778e-09, ..., 1.2923358e-09,
        5.7039379e-08, 1.0450520e-08],
       [9.0638358e-10, 1.2027534e-08, 1.6694778e-09, ..., 1.2923358e-09,
        5.7039379e-08, 1.0450520e-08],
       [9.0638358e-10, 1.2027534e-08, 1.6694778e-09, ..., 1.2923358e-09,
        5.7039379e-08, 1.0450520e-08]], dtype=float32)}
",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-16 02:53:52,open,,,[],2018-10-29 16:53:41
510,tensorflow/models,models,5540,aaa-github,object_detection: Pre-trained weights don't result in the reported mAP for resnet50 fpn,"------------------------

### System information
- **What is the top-level directory of the model you are using**:
`research/object_detection`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
tf.VERSION = 1.9.0-rc0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
cuda 9.2
- **GPU model and memory**:
1080 GTX, 10 GB
- **Exact command to reproduce**:

```
checkpoint_dir=model_data/coco_ckpt/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/
pipeline_config_path=samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
python model_main.py \
    --checkpoint_dir=${checkpoint_dir} \
    --pipeline_config_path=${pipeline_config_path} \
    --run_once
```
The data was downloaded using the provided script and the tf records for COCO were generated using the provided config was changed to include this line:
    input_path: ""tf_record_data/coco_val.tfrecord-*-of-00010""


### Describe the problem

The result from running the code gives 34.4 mAP, which is 0.6 mAP less than the reported mAP of 35.2 [This is reported in the comments in the config file:

```
# Achieves 35.2 mAP on COCO14 minival dataset. Doubling the number of training
# steps to 50k gets 36.9 mAP
```

My understanding from the COCO website is taht COCO14 minival = COCO17 eval set, which is consistent with the script used for downloading the coco data in this repository. 

```
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.344
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.514
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.376
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.112
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.317
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.474
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.509
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.222
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.507
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.676
```

If you could help explain this discrepancy, that would be great!

A potentially related issue:

As far as I can tell, the COCO validation set has 5000 examples, not 8000 examples (e.g. the official coco website: http://cocodataset.org/#download, downloading the data myself).  The configs such as:
https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config#L187
have num_examples = 8000. Why is this?  [The evaluator throws out the repeat evaluations of the model, so it doesn't cause any major problems.]

As an aside, the config is misleading because if you do: input_path: ""PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010""
and just replace PATH_TO_BE_CONFIGURED, then you load only one shard (i.e. one tenth of the data). ",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-10-16 01:11:35,open,,,[],2018-10-18 16:49:44
511,tensorflow/models,models,5539,abderhasan,Training on own data without pre-trained checkpoints (i.e. training DeepLab from scratch),"Hello,

What changes should one make in order to train one own's data without the pre-trained checkpoints? I was able to train and visualize the results on my own data, but couldn't do that by training DeepLab from scratch without the pre-trained checkpoints.

Any ideas on how I can make such change?

Thanks.",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-10-15 23:27:41,open,,,['stat:awaiting response'],2018-11-08 10:55:26
512,tensorflow/models,models,5538,eaglep91,How to convert object detection output scores to probability over all the classes (but not the only confidence for the predicted class).,"I have a questions regarding the socres output, testing with tensorflow object detection API trained model.

I have no problem training and testing, but when trying to interpret the output in testing with the inference model, I have a question regarding the scores value which is assigned with each of the box prediction output (in my case I have maximun 300 boxes predicted, so there are 300 boxes and one label for each assigned). This is just one number showing how confident that the model is over the predicted class. But how am I able to access the overall probablity across all the classes for example if I have 10 classes, how can I have access to the socores(or convert the output socores) to the probability over all the classes, which might be really useful for the transfer learning and other related analysis after this.

Any related idea could be helpful and thanks!


What is the top-level directory of the model you are using: Object detection
Have I written custom code: No
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: system pip (not anaconda)
TensorFlow version: 1.8
Bazel version: N/A
CUDA/cuDNN version: 9.0/7.3
GPU model and memory: 1080Ti,12GB
Exact command to reproduce: train.py --logtostderr -train_dir=logdir --pipeline_config_path=pipeline.config",2,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-10-15 21:09:58,open,,,['stat:awaiting response'],2018-11-06 14:29:13
513,tensorflow/models,models,5535,Shadowkm,"InvalidArgumentError (see above for traceback): Incompatible shapes: [2,1917] vs. [8,1]","the train.py file was in the directory legacy, so I copied it into models/research/object-detection.

Not any custom code, I have just followed a tutorial and tried to replicate the same. The tutorial that I follow is: https://www.youtube.com/watch?v=kq2Gjv_pPe8

I run on windows 10 with Python 3.6 (Anaconda).

Installed tensorflow using the pip command.

Tensorflow version 1.9

Bazel version: Not sure about this as the tutorial didn't say anything about this.

CUDA/cuDNN: No 

GPU model and memory: I think I'm just using CPU as I'm training on only 20 images.

Exact command used: python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config


What happened? I am very urgent about this issue ",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-10-15 15:28:19,open,,,['stat:awaiting response'],2018-10-16 02:48:37
514,tensorflow/models,models,5533,leiup,Encode uint16 png with tfrecord,"### System information
What is the top-level directory of the model you are using: deeplab
Have I written custom code: no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.10.
Bazel version (if compiling from source): -
CUDA/cuDNN version: CUDA 9
GPU model and memory: 1080Ti, 12GB
Exact command to reproduce: Not televant

### Describe the problem
 In order to encode the uint16 png with tfrecord, I implemented the following codes. However, it outputs ""uint8"", how to modify it? 

`def _bytes_list_feature(values):
""""""Returns a TF-Feature of bytes.

Args:
values: A string.

Returns:
A TF-Feature.
""""""
  def norm2bytes(value):
    if isinstance(value, str) and six.PY3:
      print ""endcode""
      return value.encode()
    else:
      # print value
      return value

  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[norm2bytes(values)]))


def image_to_tfexample(depth_data):
  """"""Converts depth to tf example.

  Args:
    depth_data: string of depth data.

  Returns:
    tf example of depth.
  """"""
  return tf.train.Example(features=tf.train.Features(feature={
    'image/depth/encoded': (
      _bytes_list_feature(depth_data)),
    'image/depth/format': _bytes_list_feature(
      FLAGS.depth_format),
  }))`

The following Decode codes are used to specify how the TF-Examples are decoded. The decoder's dtype is uint8, while the dtype of source one is uint16. How to encode and decode uint16 image with tfrecord?

keys_to_features = {
  'image/depth/encoded': tf.FixedLenFeature(
      (), tf.string, default_value=''),
  'image/depth/format': tf.FixedLenFeature(
      (), tf.string, default_value='png'),
}
items_to_handlers = {
  'depth': tfexample_decoder.Image(
      image_key='image/depth/encoded',
      format_key='image/depth/format',
      channels=1),
}",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-10-15 13:56:02,open,,,['stat:awaiting response'],2018-10-17 09:30:10
515,tensorflow/models,models,5531,89douner,"Iteration, epoch, global step","System information
What is the top-level directory of the model you are using: /home/user
Have I written custom code: No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.10.1
Bazel version (if compiling from source): I don't use it
CUDA/cuDNN version: CUDA 9.0 / cuDNN: 7.1
GPU model and memory: Geforce GTX 1070
Exact command to reproduce: No

------------------------
![2018-10-15 17-06-52](https://user-images.githubusercontent.com/31752297/46937915-550a9580-d09d-11e8-87a4-e5fd751bb636.png)

Q1. Above image, global step means iteration??
Q2. In this model, if I have 175 training images and batch size is 24, then do I need 7 iterations? or 8 iterations ? for 1 epoch
Q3. there are num_step in model.config (ex:ssd_inception_v2_coco.config). num_step means global_step? (this question may relate to Q1)

",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-10-15 08:16:37,open,,,[],2018-10-15 21:22:14
516,tensorflow/models,models,5530,Pietromezza,ADD,,1,,[],2018-10-14 10:32:15,open,,,['cla: no'],2018-10-18 02:03:35
517,tensorflow/models,models,5528,ed-word,#5525 Fix predict.py read_and_process_light_curve(),"#5525
Updated predict.py to make seperate calls to `read_light_curve()` and `process_light_curve()` to eliminate **AttributeError: 'module' object has no attribute 'read_and_process_light_curve'**
",0,,[],2018-10-14 08:30:25,open,,,['cla: yes'],2018-10-14 08:30:27
518,tensorflow/models,models,5525,ed-word,AttributeError in astronet (predict.py),"The following error is machine/environment independent

While running astronet/astronet/predict.py, the following error is raised
**AttributeError: 'module' object has no attribute 'read_and_process_light_curve'**

The script does not work as line 105 in predict.py has not been updated according to the latest functions in astronet/astronet/data/preprocess.py
`  time, flux = preprocess.read_and_process_light_curve(FLAGS.kepler_id,
                                                       FLAGS.kepler_data_dir)`
prepocess.py does not contain `read_and_process_light_curve()`
It contains `read_light_curve()` and `process_light_curve()`

What is the top-level directory of the model you are using: astronet
Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: N/A
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: 
`bazel-bin/astronet/predict \
  --model=AstroCNNModel \
  --config_name=local_global \
  --model_dir=${MODEL_DIR} \
  --kepler_data_dir=${KEPLER_DATA_DIR} \
  --kepler_id=11442793 \
  --period=14.44912 \
  --t0=2.2 \
  --duration=0.11267 \
  --output_image_file=""${HOME}/astronet/kepler-90i.png""`

 
",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-14 07:22:42,open,,,[],2018-10-15 21:22:05
519,tensorflow/models,models,5524,CBCBCBCBCBCBCBCB,An error occurred when try to train ssd_inception_v2_coco,"System information
What is the top-level directory of the model you are using: /home/user/Work
Have I written custom code: No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.10.1
Bazel version (if compiling from source): I don't use it
CUDA/cuDNN version: CUDA 9.0 / cuDNN: 7.1
GPU model and memory: Geforce GTX 1070
Exact command to reproduce: No


#############################################################################
[Question]

An error occurred when try to train ssd_inception_v2_coco
from my error message, such that ""Restoring from checkpoint failed"" , I think the problem is loading the check point, so I checked my directory ""model/research/object_detection/training/ssd_inception_v2_coco_2018_01_28""
(I just unzip as soon as I download ""ssd_inception_v2_coco_2018_01_28.tar"" from tensorflow object detection model zoo.)

And here is file list in model/research/object_detection/training/ssd_inception_v2_coco_2018_01_28
checkpoint                      model.ckpt.index  
frozen_inference_graph.pb       model.ckpt.meta
model.ckpt.data-00000-of-00001  pipeline.config

and directory list
saved_model

I don't understand why ""Restoring from checkpoint failed"" is occured

I write my configuration in the comment below to read more easy

Thanks to read my question.

Here is my code and error message.

cb@cb-B150M-DS3H:~/Work/models/research/object_detection$ **python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco.config** 
WARNING:tensorflow:From /home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/platform/app.py:125: main (from __main__) is deprecated and will be removed in a future version. 
Instructions for updating: 
Use object_detection/model_main.py. 
WARNING:tensorflow:From /home/cb/Work/models/research/object_detection/legacy/trainer.py:265: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version. 
Instructions for updating: 
Please switch to tf.train.create_global_step 
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards. 
WARNING:tensorflow:From /home/cb/Work/models/research/object_detection/core/preprocessor.py:1205: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version. 
Instructions for updating: 
Use the `axis` argument instead 
INFO:tensorflow:depth of additional conv before box predictor: 0 
INFO:tensorflow:depth of additional conv before box predictor: 0 
INFO:tensorflow:depth of additional conv before box predictor: 0 
INFO:tensorflow:depth of additional conv before box predictor: 0 
INFO:tensorflow:depth of additional conv before box predictor: 0 
INFO:tensorflow:depth of additional conv before box predictor: 0 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_2_1x1_256/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_3_1x1_128/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint 
WARNING:root:Variable [FeatureExtractor/InceptionV2/Mixed_5c_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint 
WARNING:tensorflow:From /home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py:737: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version. 
Instructions for updating: 
Please switch to tf.train.MonitoredTrainingSession 
WARNING:tensorflow:From /home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py:737: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version. 
Instructions for updating: 
Please switch to tf.train.MonitoredTrainingSession 
2018-10-14 12:25:42.408365: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 
2018-10-14 12:25:42.515119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 
2018-10-14 12:25:42.515532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:  
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683 
pciBusID: 0000:01:00.0 
totalMemory: 7.93GiB freeMemory: 7.39GiB 
2018-10-14 12:25:42.515549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0 
2018-10-14 12:25:42.725347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix: 
2018-10-14 12:25:42.725384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0  
2018-10-14 12:25:42.725405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N  
2018-10-14 12:25:42.725579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7130 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1) 
INFO:tensorflow:Restoring parameters from training/model.ckpt-0 
INFO:tensorflow:Restoring parameters from training/model.ckpt-0 
2018-10-14 12:25:43.753944: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint 
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error: 
 
Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint 
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]] 
 
Caused by op 'save/RestoreV2', defined at: 
  File ""train.py"", line 184, in <module> 
    tf.app.run() 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run 
    _sys.exit(main(argv)) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 272, in new_func 
    return func(*args, **kwargs) 
  File ""train.py"", line 180, in main 
    graph_hook_fn=graph_rewriter_fn) 
  File ""/home/cb/Work/models/research/object_detection/legacy/trainer.py"", line 376, in train 
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1281, in __init__ 
    self.build() 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1293, in build 
    self._build(self._filename, build_save=True, build_restore=True) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1330, in _build 
    build_save=build_save, build_restore=build_restore) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 778, in _build_internal 
    restore_sequentially, reshape) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 397, in _AddRestoreOps 
    restore_sequentially) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 829, in bulk_restore 
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2 
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper 
    op_def=op_def) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func 
    return func(*args, **kwargs) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op 
    op_def=op_def) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__ 
    self._traceback = tf_stack.extract_stack() 
 
NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error: 
 
Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint 
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]] 
 
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error: 
 
Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint 
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]] 
 
Caused by op 'save/RestoreV2', defined at: 
  File ""train.py"", line 184, in <module> 
    tf.app.run() 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run 
    _sys.exit(main(argv)) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 272, in new_func 
    return func(*args, **kwargs) 
  File ""train.py"", line 180, in main 
    graph_hook_fn=graph_rewriter_fn) 
  File ""/home/cb/Work/models/research/object_detection/legacy/trainer.py"", line 376, in train 
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1281, in __init__ 
    self.build() 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1293, in build 
    self._build(self._filename, build_save=True, build_restore=True) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1330, in _build 
    build_save=build_save, build_restore=build_restore) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 778, in _build_internal 
    restore_sequentially, reshape) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 397, in _AddRestoreOps 
    restore_sequentially) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 829, in bulk_restore 
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2 
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper 
    op_def=op_def) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func 
    return func(*args, **kwargs) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op 
    op_def=op_def) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__ 
    self._traceback = tf_stack.extract_stack() 
 
NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error: 
 
Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint 
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]] 
 
Traceback (most recent call last): 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call 
    return fn(*args) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn 
    options, feed_dict, fetch_list, target_list, run_metadata) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun 
    run_metadata) 
tensorflow.python.framework.errors_impl.NotFoundError: Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint 
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]] 
 
During handling of the above exception, another exception occurred: 
 
Traceback (most recent call last): 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1725, in restore 
    {self.saver_def.filename_tensor_name: save_path}) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run 
    run_metadata_ptr) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run 
    feed_dict_tensor, options, run_metadata) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run 
    run_metadata) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call 
    raise type(e)(node_def, op, message) 
tensorflow.python.framework.errors_impl.NotFoundError: Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint 
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]] 
 
Caused by op 'save/RestoreV2', defined at: 
  File ""train.py"", line 184, in <module> 
    tf.app.run() 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run 
    _sys.exit(main(argv)) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 272, in new_func 
    return func(*args, **kwargs) 
  File ""train.py"", line 180, in main 
    graph_hook_fn=graph_rewriter_fn) 
  File ""/home/cb/Work/models/research/object_detection/legacy/trainer.py"", line 376, in train 
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1281, in __init__ 
    self.build() 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1293, in build 
    self._build(self._filename, build_save=True, build_restore=True) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1330, in _build 
    build_save=build_save, build_restore=build_restore) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 778, in _build_internal 
    restore_sequentially, reshape) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 397, in _AddRestoreOps 
    restore_sequentially) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 829, in bulk_restore 
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2 
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper 
    op_def=op_def) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func 
    return func(*args, **kwargs) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op 
    op_def=op_def) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__ 
    self._traceback = tf_stack.extract_stack() 
 
NotFoundError (see above for traceback): Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint 
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]] 
 
 
During handling of the above exception, another exception occurred: 
 
Traceback (most recent call last): 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1737, in restore 
    checkpointable.OBJECT_GRAPH_PROTO_KEY) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 351, in get_tensor 
    status) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__ 
    c_api.TF_GetCode(self.status.status)) 
tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint 
 
During handling of the above exception, another exception occurred: 
 
Traceback (most recent call last): 
  File ""train.py"", line 184, in <module> 
    tf.app.run() 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run 
    _sys.exit(main(argv)) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 272, in new_func 
    return func(*args, **kwargs) 
  File ""train.py"", line 180, in main 
    graph_hook_fn=graph_rewriter_fn) 
  File ""/home/cb/Work/models/research/object_detection/legacy/trainer.py"", line 415, in train 
    saver=saver) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 748, in train 
    master, start_standard_services=False, config=session_config) as sess: 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/contextlib.py"", line 81, in __enter__ 
    return next(self.gen) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 1005, in managed_session 
    self.stop(close_summary_writer=close_summary_writer) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 833, in stop 
    ignore_live_threads=ignore_live_threads) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join 
    six.reraise(*self._exc_info_to_raise) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/six.py"", line 693, in reraise 
    raise value 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 994, in managed_session 
    start_standard_services=start_standard_services) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 731, in prepare_or_wait_for_session 
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session 
    config=config) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 211, in _restore_checkpoint 
    saver.restore(sess, ckpt.model_checkpoint_path) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1743, in restore 
    err, ""a Variable name or other graph key that is missing"") 
tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error: 
 
Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint 
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]] 
 
Caused by op 'save/RestoreV2', defined at: 
  File ""train.py"", line 184, in <module> 
    tf.app.run() 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run 
    _sys.exit(main(argv)) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 272, in new_func 
    return func(*args, **kwargs) 
  File ""train.py"", line 180, in main 
    graph_hook_fn=graph_rewriter_fn) 
  File ""/home/cb/Work/models/research/object_detection/legacy/trainer.py"", line 376, in train 
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1281, in __init__ 
    self.build() 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1293, in build 
    self._build(self._filename, build_save=True, build_restore=True) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1330, in _build 
    build_save=build_save, build_restore=build_restore) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 778, in _build_internal 
    restore_sequentially, reshape) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 397, in _AddRestoreOps 
    restore_sequentially) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 829, in bulk_restore 
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2 
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper 
    op_def=op_def) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func 
    return func(*args, **kwargs) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op 
    op_def=op_def) 
  File ""/home/cb/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__ 
    self._traceback = tf_stack.extract_stack() 
 
NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error: 
 
Key BoxPredictor_0/BoxEncodingPredictor/biases not found in checkpoint 
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]] 
 
",5,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-10-14 07:15:55,open,,,[],2019-01-21 13:41:29
520,tensorflow/models,models,5520,MichaelX99,Object Detection Input Pipeline Design Question,"### System information
- models/research/object_detection
- No custom code
- Linux Ubuntu 16.04
- TensorFlow installed from N/A
- TensorFlow version N/A
- Bazel version N/A
- CUDA/cuDNN version N/A
- GPU N/A

#############################################

Hello,

I am curious as to why the input pipeline for the Object Detection API uses a combination of the Dataset API and the old PaddingFIFO Queue class.  Is there a reason why the PaddingFIFO Queue is being used instead of only the Dataset API besides the code was written a long time ago for the initial publication and has not been needed to be updated since?",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-10-13 20:26:20,open,,,"['models: research', 'stat:awaiting response']",2019-02-06 22:17:45
521,tensorflow/models,models,5519,junweima,mobilenet misspelled dictionary key word,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: slim.nets.mobilenet
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: gtx 1080 ti
- **Exact command to reproduce**: n/a

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

misspelled word `multiplier_transorm`, should it be `multiplier_transform`?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py#L113
",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-13 20:09:19,open,,,[],2018-10-14 07:22:32
522,tensorflow/models,models,5516,hammadullah125,LossTensor is inf or nan. : Tensor had NaN values,"Hey, I really don't understand what's going on? I have checked the solutions https://github.com/tensorflow/models/issues/1881#issue-241138655, https://github.com/tensorflow/models/issues/3688#issue-307468498, https://github.com/tensorflow/models/issues/1907#issue-241696033 and many many more but nothing is working for me. Any help should be appreciated. Thanks

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Used the official train script
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary, using ""pip install tensorflow-gpu""
- TensorFlow version (use command below): 1.6
- CUDA/cuDNN version: CUDA 9.1, cuDNN 7
- GPU model and memory: Quadro P2000 (4 Gb)
- Exact command to reproduce: python3 train.py --logtostderr --train_dir=/export/users/hull/Desktop/models-master/research/object_detection/training/ --pipeline_config_path=/export/users/hull/Desktop/models-master/research/object_detection/training/ssd_mobilenet_v1_pets.config

```
hull@ori:~/Desktop/models-master/research/object_detection/legacy$ python3 train.py --logtostderr --train_dir=/export/users/hull/Desktop/models-master/research/object_detection/training/ --pipeline_config_path=/export/users/hull/Desktop/models-master/research/object_detection/training/ssd_mobilenet_v1_pets.config

WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py:126: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
W1013 12:06:48.582990 140401173534464 tf_logging.py:126] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py:126: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/legacy/trainer.py:265: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
W1013 12:06:48.587885 140401173534464 tf_logging.py:126] From /usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/legacy/trainer.py:265: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1013 12:06:48.598516 140401173534464 tf_logging.py:126] num_readers has been reduced to 1 to match input file shards.
INFO:tensorflow:depth of additional conv before box predictor: 0
I1013 12:06:50.607872 140401173534464 tf_logging.py:116] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I1013 12:06:50.626033 140401173534464 tf_logging.py:116] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I1013 12:06:50.644225 140401173534464 tf_logging.py:116] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I1013 12:06:50.662438 140401173534464 tf_logging.py:116] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I1013 12:06:50.681107 140401173534464 tf_logging.py:116] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I1013 12:06:50.699337 140401173534464 tf_logging.py:116] depth of additional conv before box predictor: 0
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
W1013 12:06:55.176545 140401173534464 tf_logging.py:126] From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-10-13 12:06:55.974026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: 
name: Quadro P2000 major: 6 minor: 1 memoryClockRate(GHz): 1.4805
pciBusID: 0000:65:00.0
totalMemory: 4.94GiB freeMemory: 4.79GiB
2018-10-13 12:06:55.974057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-10-13 12:06:56.148137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4548 MB memory) -> physical GPU (device: 0, name: Quadro P2000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from /export/users/hull/Desktop/models-master/research/object_detection/ssd_mobilenet_v1_coco_11_06_2017/model.ckpt
I1013 12:06:57.485528 140401173534464 tf_logging.py:116] Restoring parameters from /export/users/hull/Desktop/models-master/research/object_detection/ssd_mobilenet_v1_coco_11_06_2017/model.ckpt
INFO:tensorflow:Running local_init_op.
I1013 12:06:57.669006 140401173534464 tf_logging.py:116] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I1013 12:06:57.758895 140401173534464 tf_logging.py:116] Done running local_init_op.
INFO:tensorflow:Starting Session.
I1013 12:07:01.883585 140401173534464 tf_logging.py:116] Starting Session.
INFO:tensorflow:Saving checkpoint to path /export/users/hull/Desktop/models-master/research/object_detection/training/model.ckpt
I1013 12:07:01.981616 140391835756288 tf_logging.py:116] Saving checkpoint to path /export/users/hull/Desktop/models-master/research/object_detection/training/model.ckpt
INFO:tensorflow:Starting Queues.
I1013 12:07:01.983547 140401173534464 tf_logging.py:116] Starting Queues.
INFO:tensorflow:global_step/sec: 0
I1013 12:07:03.104428 140391969974016 tf_logging.py:160] global_step/sec: 0
INFO:tensorflow:Recording summary at step 0.
I1013 12:07:03.785446 140391961581312 tf_logging.py:116] Recording summary at step 0.
INFO:tensorflow:global step 1: loss = 12.6485 (3.511 sec/step)
I1013 12:07:05.569523 140401173534464 tf_logging.py:116] global step 1: loss = 12.6485 (3.511 sec/step)
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](total_loss)]]

Caused by op 'CheckNumerics', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/legacy/trainer.py"", line 321, in train
    total_loss = tf.check_numerics(total_loss, 'LossTensor is inf or nan.')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 498, in check_numerics
    ""CheckNumerics"", tensor=tensor, message=message, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](total_loss)]]

I1013 12:07:05.709065 140401173534464 tf_logging.py:116] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](total_loss)]]

Caused by op 'CheckNumerics', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/legacy/trainer.py"", line 321, in train
    total_loss = tf.check_numerics(total_loss, 'LossTensor is inf or nan.')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 498, in check_numerics
    ""CheckNumerics"", tensor=tensor, message=message, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](total_loss)]]

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1361, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1340, in _run_fn
    target_list, status, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](total_loss)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/legacy/trainer.py"", line 415, in train
    saver=saver)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 768, in train
    sess, train_op, global_step, train_step_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_step
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](total_loss)]]

Caused by op 'CheckNumerics', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/legacy/trainer.py"", line 321, in train
    total_loss = tf.check_numerics(total_loss, 'LossTensor is inf or nan.')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 498, in check_numerics
    ""CheckNumerics"", tensor=tensor, message=message, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](total_loss)]]
```",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-10-13 10:24:01,open,,,['stat:awaiting response'],2018-10-13 19:44:54
523,tensorflow/models,models,5515,ATS-Official,Error while using mnist dataset -Negative dimension  size caused by subtracting 3 from 1 for 'AttentionOcr_v1/conv_tower_fn/INCE/Inc eptionV3/Conv2d_4a_3x3/Conv2D' (op: 'Conv2D')  ,"Trying to test the model on the mnist dataset and encountered this error.

Tensorflow Version: 1.10 
OS: Windows 10
Installation: Anaconda (Python 3.6)


python train.py --checkpoint_inception=./datasets/inception_v3.ckpt --data
set_name=newtextdataset

INFO 2018-10-13 05:41:51.000365: train.py: 167 Use already existing training dir
ectory /tmp/attention_ocr/train
INFO 2018-10-13 05:41:51.000366: fsns.py: 130 Using MYDATASET dataset split_name
=train dataset_dir=D://***//tensorflow_models//models-master//research//
attention_ocr//python//datasets//data//fsns
DEBUG 2018-10-13 05:41:52.000233: model.py: 354 images: Tensor(""shuffle_batch:0""
, shape=(32, 100, 48, 3), dtype=float32)
DEBUG 2018-10-13 05:41:52.000237: model.py: 359 Views=4 single view: Tensor(""Att
entionOcr_v1/split:0"", shape=(32, 100, 12, 3), dtype=float32)
DEBUG 2018-10-13 05:41:52.000238: model.py: 200 Using final_endpoint=Mixed_5d
Traceback (most recent call last):
  File ""D:\ProgramFiles\Anaconda\envs\TFCore\lib\site-packages\tensorflow\python
\framework\ops.py"", line 1576, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension
 size caused by subtracting 3 from 1 for 'AttentionOcr_v1/conv_tower_fn/INCE/Inc
eptionV3/Conv2d_4a_3x3/Conv2D' (op: 'Conv2D') with input shapes: [32,23,1,80], [
3,3,80,192].",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-13 09:53:44,open,,,[],2018-10-15 21:21:56
524,tensorflow/models,models,5511,ilyamironov,Streamlining rdp_accountant.,,0,,[],2018-10-13 00:26:07,open,,,['cla: yes'],2018-10-15 17:35:05
525,tensorflow/models,models,5510,cvdeep,How to save one-hot encoded lables in deeplab v3+?,"
### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: u 16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.9
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: titanX
- **Exact command to reproduce**:


### Describe the problem


I need the one-hot encoded labels for each image in train and Val sets. I found the labels are converted to one-hot before computing loss, but how to save one-hot encoded labels for all the images in the same order that deeplab uses; so each image should have a one-hot encoded matrix of hxwx19 
",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-10-12 16:52:33,open,,,[],2018-10-16 20:56:31
526,tensorflow/models,models,5509,EdwardVincentMa,why .pb file can not be load by tf1.8 c++ on Win10,"I used object detection generate pb file, but it can be load by tf1.9 with python, but can not be loaded by tf1.8-python with 'NonMaxSuppressionV3' error. The .pb file also cannot be loaded by tf1.8 C++ API on Win10. ",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-12 05:28:00,open,,,['stat:awaiting response'],2018-10-12 13:36:54
527,tensorflow/models,models,5508,EdwardVincentMa,Which version of the object detection is available for old tf version,"I used object detection generated the .pb file, but can not be loaded by tf1.8, but can be used（loaded） by tf1.9 or later version .
 If I used old version tf，there were errors can not be fixed.",1,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-10-12 04:52:28,open,,,['stat:awaiting response'],2018-11-07 17:10:02
528,tensorflow/models,models,5471,cvdeep,input and logits tesnor names,"how to get input and logits (one layer before last activation) tensors names of the following pre-trained model?

http://download.tensorflow.org/models/deeplab_cityscapes_xception71_trainfine_2018_09_08.tar.gz",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-10-11 01:39:31,open,,,['stat:awaiting response'],2018-10-11 13:26:00
529,tensorflow/models,models,5467,smitshilu,Update model_lib.py,Added support for python3. Was falling because of list() issue.,0,,[],2018-10-10 16:18:17,open,,,['cla: yes'],2018-10-10 16:18:20
530,tensorflow/models,models,5466,NazariAmin,absl.flags._exceptions.IllegalFlagValueError: flag --train_logdir=None: Flag --train_logdir must be specified.,"Nazari:deeplab Amin$ python train.py \ --logtostderr \ --training_number_of_steps=10 \ --train_split=""train"" \ --model_variant=""xception_65"" \ --atrous_rates=6 \ --atrous_rates=12 \ --atrous_rates=18 \ --output_stride=16 \ --decoder_output_stride=4 \ --train_crop_size=513 \ --train_crop_size=513 \ --train_batch_size=1 \ --dataset=""pascal_voc_seg"" \ --tf_initial_checkpoint=""datasets/pascal_voc_seg/init_models/deeplabv3_pascal_train_aug/model.ckpt"" \ --train_logdir=""datasets/pascal_voc_seg/exp/train_on_trainval_set/train/"" \ --dataset_dir=""./datasets/pascal_voc_seg/tfrecord/""
Traceback (most recent call last):
  File ""/Users/macbook/anaconda/lib/python3.5/site-packages/absl/flags/_flagvalues.py"", line 527, in _assert_validators
    validator.verify(self)
  File ""/Users/macbook/anaconda/lib/python3.5/site-packages/absl/flags/_validators.py"", line 81, in verify
    raise _exceptions.ValidationError(self.message)
absl.flags._exceptions.ValidationError: Flag --train_logdir must be specified.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 394, in <module>
    tf.app.run()
  File ""/Users/macbook/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 119, in run
    argv = flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)
  File ""/Users/macbook/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/flags.py"", line 112, in __call__
    return self.__dict__['__wrapped'].__call__(*args, **kwargs)
  File ""/Users/macbook/anaconda/lib/python3.5/site-packages/absl/flags/_flagvalues.py"", line 635, in __call__
    self._assert_all_validators()
  File ""/Users/macbook/anaconda/lib/python3.5/site-packages/absl/flags/_flagvalues.py"", line 509, in _assert_all_validators
    self._assert_validators(all_validators)
  File ""/Users/macbook/anaconda/lib/python3.5/site-packages/absl/flags/_flagvalues.py"", line 530, in _assert_validators
    raise _exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))
absl.flags._exceptions.IllegalFlagValueError: flag --train_logdir=None: Flag --train_logdir must be specified.

",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-10-10 10:53:37,open,,,[],2018-10-14 19:21:26
531,tensorflow/models,models,5463,wujsy,deep_speech: can this used for training chinese model?,"Hi, could I use models/research/deep_speech to train a chinese model now?
",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-10-10 08:16:11,open,,,[],2018-10-11 07:30:49
532,tensorflow/models,models,5460,under94,logits and labels must be broadcastable traceback,"Hello, 
I'm new in tensorflow. 
I'm trying the resnet_v2_101 but i get an exception and i can't solve it.
My images are 488x488x3 my batchs size and num classes are 2.

So, this is my code:

 
```
num_classes = 2
net_input = tf.placeholder(tf.float32,shape=[None,None,None,3])
    net_output = tf.placeholder(tf.float32,shape=[None,None,None,num_classes]) 

   #build network
    network,_ = resnet_v2.resnet_v2_101(tf.zeros([2,488,488,3]),num_classes , is_training=True, scope='resnet_v2_101')

sess.run(tf.global_variables_initializer())


        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=network, labels=net_output))

        opt = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.995).minimize(loss, var_list=[var for var in tf.trainable_variables()])

        saver=tf.train.Saver(max_to_keep=1000)
        sess.run(tf.global_variables_initializer())

        # Do the training here
        for epoch in range(args.epoch_start_i, args.num_epochs):

            current_losses = []

            # Equivalent to shuffling
            id_list = np.random.permutation( len(train_input_names) )

            num_iters = int(np.floor(len(id_list) / args.batch_size))

            #for i in range(num_iters):
            description_train = '[i]Train Epoch {:>2}/{}'.format(epoch + 1, args.num_epochs)
            for i in tqdm(range(num_iters), desc=description_train, unit='batch'):

                input_image_batch = []
                output_image_batch = []

                # Collect a batch of images
                for j in range(args.batch_size):
                    index = i*args.batch_size + j
                    id = id_list[index]
                    input_image = utils.load_image(train_input_names[id])
                    output_image = utils.load_image(train_output_names[id])

                    with tf.device('/cpu:0'):
                        input_image, output_image = data_augmentation(input_image, output_image)

                        # Prep the data. Make sure the labels are in one-hot format
                        input_image = np.float32(input_image) / 255.0
                        output_image = np.float32(helpers.one_hot_it(label=output_image, label_values=label_values))

                        input_image_batch.append(np.expand_dims(input_image, axis=0))
                        output_image_batch.append(np.expand_dims(output_image, axis=0))

                    input_image_batch = np.squeeze(np.stack(input_image_batch, axis=1))
                    output_image_batch = np.squeeze(np.stack(output_image_batch, axis=1))

                # Do the training
                _,current=sess.run([opt,loss],feed_dict={net_input:input_image_batch,net_output:output_image_batch})

```

I get this traceback:
`InvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[512,2048] labels_size=[100352,2]`

How can i solve it?

### System information
- **What is the top-level directory of the model you are using**: https://github.com/tensorflow/models/blob/master/research/slim/nets/resnet_v2.py
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.10.0
- **Bazel version (if compiling from source)**: /
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: NVIDIA Tesla V100
- **Exact command to reproduce**:",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-09 15:47:59,open,,,[],2018-10-10 19:42:36
533,tensorflow/models,models,5459,xiaogangLi,Object detection API：How to save the optimal model?,I used old train.py and eval.py to finetune a pretrained detection model. How to save the optimal model during training? Has the API already provided early stopping algorithm to save the optimal model?,2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-10-09 07:36:14,open,,,['stat:awaiting response'],2018-10-30 16:09:30
534,tensorflow/models,models,5457,sjain-stanford,"VGG-16 Top-1 accuracy evaluates to 70.894%, not 71.5% as reported on TF-slim.","### System information
- **What is the top-level directory of the model you are using**: https://github.com/tensorflow/models/blob/master/research/slim/nets/vgg.py
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: inux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.10
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.0
- **GPU model and memory**: Titan V, 12GB
- **Exact command to reproduce**: 

### Describe the problem
I ran VGG-16 validation directly from TF-slim and the accuracies were top-1=70.894%, top-5 = 89.848%. However the numbers reported on TF-slim's [README](https://github.com/tensorflow/models/blob/master/research/slim/README.md) are top-1=71.5%, top-5 = 89.8%.

Here's what I did:
1. Cloned [tf-slim](https://github.com/tensorflow/models/tree/master/research/slim). 
2. Downloaded and processed Imagenet dataset to TF-records to `/data/imagenet/tf-records/`
3. Downloaded VGG-16 [ckpt](http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz) to `/tmp/vgg_16.ckpt`
4. Ran `eval_image_classifier.py` as follows:
```
python eval_image_classifier.py \
  --checkpoint_path /tmp/vgg_16.ckpt \
  --dataset_dir /data/imagenet/tf-records/ \
  --dataset_name imagenet \
  --dataset_split_name validation \
  --model_name vgg_16 \
  --labels_offset=1
```

Here's the output:
```
INFO:tensorflow:Restoring parameters from /tmp/vgg_16.ckpt
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [50/500]
INFO:tensorflow:Evaluation [100/500]
INFO:tensorflow:Evaluation [150/500]
INFO:tensorflow:Evaluation [200/500]
INFO:tensorflow:Evaluation [250/500]
INFO:tensorflow:Evaluation [300/500]
INFO:tensorflow:Evaluation [350/500]
INFO:tensorflow:Evaluation [400/500]
INFO:tensorflow:Evaluation [450/500]
INFO:tensorflow:Evaluation [500/500]
eval/Accuracy[0.70894]eval/Recall_5[0.89848]

INFO:tensorflow:Finished evaluation at 2018-10-08-22:26:23
```",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-10-08 22:43:35,open,,,[],2018-10-24 16:39:18
535,tensorflow/models,models,5453,ryanwcyin,python3 compatibility issue,Fixed python3 compatibility issue #4780,7,,[],2018-10-08 14:02:27,open,,,['cla: yes'],2018-10-22 06:56:44
536,tensorflow/models,models,5452,abigeer,Train.py handles pipeline file error when using object_detection,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:tensorflow/models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows10
- **TensorFlow installed from (source or binary)**:no
- **TensorFlow version (use command below)**:1.8.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:on CPU
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When I used the target detection api, this error occurred. I generated the data according to the official tutorial and configured the pipeline file. When I run train.py, the following code reports an error.
Configs = config_util.get_configs_from_pipeline_file(
         FLAGS.pipeline_config_path)
The follow-up error is in the \research\object_detection\utils\config_util.py directory, the 97th line of code error: text_format.Merge(proto_str, pipeline_config), this is a headache I hope to help me solve, thank you!

### Source code / logs
WARNING:tensorflow:From D:\develop\Python3.6.4\lib\site-packages\tensorflow\python\platform\app.py:126: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
W1008 17:21:25.471641  3112 tf_logging.py:126] From D:\develop\Python3.6.4\lib\site-packages\tensorflow\python\platform\app.py:126: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
Traceback (most recent call last):
  File ""D:\workspace\eclipsejee\TensorFlow-model\research\object_detection\legacy\train.py"", line 198, in <module>
    tf.app.run()
  File ""D:\develop\Python3.6.4\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""D:\develop\Python3.6.4\lib\site-packages\tensorflow\python\util\deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""D:\workspace\eclipsejee\TensorFlow-model\research\object_detection\legacy\train.py"", line 107, in main
    FLAGS.pipeline_config_path)
  File ""D:\workspace\eclipsejee\TensorFlow-model\research\object_detection\utils\config_util.py"", line 97, in get_configs_from_pipeline_file
    text_format.Merge(proto_str, pipeline_config)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 888, in _MergeScalarField
    value = tokenizer.ConsumeString()
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 1251, in ConsumeString
    the_bytes = self.ConsumeByteString()
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 1266, in ConsumeByteString
    the_list = [self._ConsumeSingleByteString()]
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_format.py"", line 1291, in _ConsumeSingleByteString
    result = text_encoding.CUnescape(text[1:-1])
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_encoding.py"", line 103, in CUnescape
    result = ''.join(_cescape_highbit_to_str[ord(c)] for c in result)
  File ""D:\develop\Python3.6.4\lib\site-packages\google\protobuf\text_encoding.py"", line 103, in <genexpr>
    result = ''.join(_cescape_highbit_to_str[ord(c)] for c in result)
IndexError: list index out of range
",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-10-08 09:41:24,open,,,[],2019-02-02 04:59:24
537,tensorflow/models,models,5451,wanglidog,"tensorflow object_detection API trouble google.protobuf.text_format.ParseError: 158:3 : Message type ""object_detection.protos.TrainConfig"" has no field named ""load_all_detection_checkpoint_vars"".","When I was using train.py , I got this error. How to solve? 

google.protobuf.text_format.ParseError: 158:3 : Message type ""object_detection.protos.TrainConfig"" has no field named ""load_all_detection_checkpoint_vars"".

",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-07 15:27:51,open,,,[],2018-11-05 15:43:47
538,tensorflow/models,models,5450,TopKech,[Object detection] Removed num_eval_steps kwarg from estimator.evaluate() call in model_main.py,"When using the run_once flag ` estimator.evaluate(input_fn, num_eval_steps=None, checkpoint_path=tf.train.latest_checkpoint(FLAGS.checkpoint_dir))` is called, but [tf.estimator.Estimator.evaluate()](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate) has no num_eval_steps kwarg. This leads to ""TypeError: evaluate() got an unexpected keyword argument 'num_eval_steps'"". Fixed by removing the argument.",3,,[],2018-10-07 15:16:52,open,,,['cla: yes'],2018-10-07 17:32:31
539,tensorflow/models,models,5448,yuelimv,There is no num-workers parameter for cifar10_main.py,"We are trying to play with the distributed training using the cifar10_estimator example:

https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator

In the README.md, it requires the master to execute the following command:

python cifar10_main.py --data-dir=gs://path/cifar-10-data \
                       --job-dir=gs://path/model_dir/ \
                       --num-gpus=4 \
                       --train-steps=40000 \
                       --sync \
                       --num-workers=2

However, there is no such parameter called num-workers supported by cifar10_main.py...

Did I miss anything here? Thanks!
",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-10-07 09:35:36,open,,,[],2018-10-09 19:30:17
540,tensorflow/models,models,5436,johnnylu305,Fix missing variables between .ckpt file and resnet_v1/resnet_v2,"There are missing variables between [.ckpt file](https://github.com/tensorflow/models/tree/master/research/slim)  you give and resnet. Therefore, error occurred when restoring variables with setting ignore_missing_vars=False. Warning occurred when restoring variables with setting ignore_missing_vars=True.

# Resnet with ignore_missing_vars=False
```
import tensorflow as tf
import tensorflow.contrib.slim.nets as nets
import tensorflow.contrib.slim as slim
import os

# use single GPU
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""

# define placeholder 
xp = tf.placeholder(tf.float32, shape = (None, None, None, 3))

nets.resnet_v1.resnet_v1_101(xp, num_classes = 1000, is_training = True)

with tf.Session() as sess:
    # initial weight
    init = tf.global_variables_initializer()
    sess.run(init)
    # load weight
    variables_to_restore = slim.get_variables_to_restore()
    init_assign_op, init_feed_dict = slim.assign_from_checkpoint('./models/resnet_v1_101.ckpt', 
                                                                 variables_to_restore, 
                                                                 ignore_missing_vars=False)
    sess.run(init_assign_op, feed_dict=init_feed_dict)
```
# Error
Traceback (most recent call last):
  File ""test.py"", line 23, in <module>
    ignore_missing_vars=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 671, in assign_from_checkpoint
    raise ValueError(log_str)
ValueError: Checkpoint is missing variable [resnet_v1_101/block3/unit_22/bottleneck_v1/conv1/biases]

# Resnet with ignore_missing_vars=True
```
import tensorflow as tf
import tensorflow.contrib.slim.nets as nets
import tensorflow.contrib.slim as slim
import os

# use single GPU
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""

# define placeholder 
xp = tf.placeholder(tf.float32, shape = (None, None, None, 3))

nets.resnet_v1.resnet_v1_101(xp, num_classes = 1000, is_training = True)

with tf.Session() as sess:
    # initial weight
    init = tf.global_variables_initializer()
    sess.run(init)
    # load weight
    variables_to_restore = slim.get_variables_to_restore()
    init_assign_op, init_feed_dict = slim.assign_from_checkpoint('./models/resnet_v1_101.ckpt', 
                                                                 variables_to_restore, 
                                                                 ignore_missing_vars=True)
    sess.run(init_assign_op, feed_dict=init_feed_dict)
```
# Warning

WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_22/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_6/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_20/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_20/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block4/unit_3/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_14/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_11/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_20/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_1/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_2/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_6/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_9/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_7/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_4/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_11/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_7/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_2/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block4/unit_2/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_16/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block4/unit_1/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_14/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block1/unit_2/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_12/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_1/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_2/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_21/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_9/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_13/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_1/bottleneck_v1/shortcut/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_8/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_17/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_3/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_3/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_2/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_13/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_10/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_10/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_5/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_7/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_23/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block4/unit_3/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block1/unit_3/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_3/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_21/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_4/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_2/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_8/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_11/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_19/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_4/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block1/unit_3/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_12/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block1/unit_3/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_15/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block4/unit_3/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_1/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_3/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_21/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_18/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_23/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_19/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_22/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block1/unit_1/bottleneck_v1/shortcut/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block1/unit_1/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_16/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_16/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block1/unit_2/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_3/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_3/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_4/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_9/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_17/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_2/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block1/unit_1/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block4/unit_1/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_14/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_10/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block1/unit_2/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_6/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_23/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_8/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_13/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_15/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block4/unit_1/bottleneck_v1/shortcut/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_19/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_4/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_17/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_12/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block4/unit_2/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_1/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_18/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block2/unit_1/bottleneck_v1/shortcut/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_15/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_18/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block1/unit_1/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_22/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_5/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_5/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_1/bottleneck_v1/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_1/bottleneck_v1/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block3/unit_4/bottleneck_v1/conv3/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v1_101/block4/unit_2/bottleneck_v1/conv3/biases]

# Resnet with ignore_missing_vars=False
```
import tensorflow as tf
import tensorflow.contrib.slim.nets as nets
import tensorflow.contrib.slim as slim
import os

# use single GPU
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""

# define placeholder 
xp = tf.placeholder(tf.float32, shape = (None, None, None, 3))

nets.resnet_v2.resnet_v2_101(xp, num_classes = 1001, is_training = True)

with tf.Session() as sess:
    # initial weight
    init = tf.global_variables_initializer()
    sess.run(init)
    # load weight
    variables_to_restore = slim.get_variables_to_restore()
    init_assign_op, init_feed_dict = slim.assign_from_checkpoint('./models/resnet_v2_101.ckpt', 
                                                                 variables_to_restore, 
                                                                 ignore_missing_vars=False)
    sess.run(init_assign_op, feed_dict=init_feed_dict)
```
# Error
Traceback (most recent call last):
  File ""test.py"", line 22, in <module>
    ignore_missing_vars=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 671, in assign_from_checkpoint
    raise ValueError(log_str)
ValueError: Checkpoint is missing variable [resnet_v2_101/block3/unit_20/bottleneck_v2/conv2/biases]

# Resnet with ignore_missing_vars=True
```
import tensorflow as tf
import tensorflow.contrib.slim.nets as nets
import tensorflow.contrib.slim as slim
import os

# use single GPU
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""

# define placeholder 
xp = tf.placeholder(tf.float32, shape = (None, None, None, 3))

nets.resnet_v2.resnet_v2_101(xp, num_classes = 1001, is_training = True)

with tf.Session() as sess:
    # initial weight
    init = tf.global_variables_initializer()
    sess.run(init)
    # load weight
    variables_to_restore = slim.get_variables_to_restore()
    init_assign_op, init_feed_dict = slim.assign_from_checkpoint('./models/resnet_v2_101.ckpt', 
                                                                 variables_to_restore, 
                                                                 ignore_missing_vars=True)
    sess.run(init_assign_op, feed_dict=init_feed_dict)

```
# Warning
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_20/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_7/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_19/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block1/unit_1/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block1/unit_3/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block2/unit_2/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_1/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_15/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_10/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_13/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_12/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block4/unit_3/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block4/unit_3/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block2/unit_4/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_17/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block1/unit_2/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_9/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_21/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block2/unit_1/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_21/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_4/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block2/unit_3/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_6/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_22/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_15/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_8/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_8/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_14/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_4/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block4/unit_1/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_5/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block4/unit_2/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_9/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block4/unit_2/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block1/unit_1/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_17/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_22/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_2/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_20/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_6/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_19/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_14/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_1/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block2/unit_4/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block1/unit_2/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block2/unit_2/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_2/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block2/unit_3/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_11/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block4/unit_1/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_12/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_3/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block1/unit_3/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_11/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_3/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_23/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_18/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_13/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_5/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_16/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_16/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_18/bottleneck_v2/conv2/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_23/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_7/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block3/unit_10/bottleneck_v2/conv1/biases]
WARNING:tensorflow:Checkpoint is missing variable [resnet_v2_101/block2/unit_1/bottleneck_v2/conv1/biases]


",4,,[],2018-10-04 10:09:47,open,,,['cla: yes'],2018-10-09 14:25:02
541,tensorflow/models,models,5435,nisseb,Update train.proto,"keep_checkpoint_every_n_hour (uint32 -> float)
Interpreted as a float in the API (saver.py).
https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/training/saver.py#L699

Lets us save checkpoints more often than every hour",3,,[],2018-10-04 06:52:55,open,,,['cla: yes'],2018-10-30 16:40:49
542,tensorflow/models,models,5433,samridhsharma, update for kaggle api,,2,,[],2018-10-03 19:08:36,open,,,['cla: no'],2018-10-04 15:41:47
543,tensorflow/models,models,5432,aanagn,model_main.py freezes on execution,"When I try to run model_main.py, it generates the following errors, and then freezes.
https://pastebin.com/0f8mvDWS

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- ** this is the output of ls -R **: https://pastebin.com/6yDdX3tM
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 18.04LTS
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.11.0-0-gc19e29306c 1.11.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
Cuda compilation tools, release 9.0, V9.0.176
cuDNN 7.2.1
- **GPU model and memory**:
03:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)
04:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)
RAM 64GB
- **Exact command to reproduce**:
python3 model_main.py --logtostderr --train_dir=/home/tanasi/Documents/models/research/object_detection/training/ --pipeline_config_path=/home/tanasi/Documents/models/research/object_detection/training/ssd_mobilenet_v1_pets.config 


Thanks in advance!


",6,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-03 11:53:42,open,,,[],2019-03-22 20:29:53
544,tensorflow/models,models,5427,IgorFobia,vid2depth bazel error: no org_bzip_bzip2 package,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: vid2depth
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4 LTS 
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0
- **Bazel version (if compiling from source)**: 0.17.2
- **CUDA/cuDNN version**: CUDA Version 8.0.61
- **GPU model and memory**: NVIDIA-SMI 384.130 
- **Exact command to reproduce**: bazel build ops:pcl_demo

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

When I run the command `bazel build ops:pcl_demo`, which is required for running `vid2depth`, I get the following error:
```
ERROR: Analysis of target '//ops:pcl_demo' failed; build aborted: no such package '@org_bzip_bzip2//': Error downloading [http://www.bzip.org/1.0.6/bzip2-1.0.6.tar.gz] to /home/fabio/.cache/bazel/_bazel_fabio/0d97ea14302a2780e97a4c982df93810/external/org_bzip_bzip2/bzip2-1.0.6.tar.gz: Checksum was 04ccd15864975864b819c059a1442cbf8d3dd29ab81290661035512d9992037a but wanted a2848f34fcd5d6cf47def00461fcb528a0484d8edef8208d6d2e2909dc61d9cd
INFO: Elapsed time: 1.014s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```",1,"NamedUser(login=""rezama"")","[NamedUser(login=""rezama"")]",2018-10-02 17:43:53,open,,,['stat:awaiting owner'],2018-10-29 02:06:40
545,tensorflow/models,models,5426,89douner,SSD_inception_V2 paper architecture,"System information
What is the top-level directory of the model you are using: /home/user
Have I written custom code: No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.10.1
Bazel version (if compiling from source): I don't use it
CUDA/cuDNN version: CUDA 9.0 / cuDNN: 7.1
GPU model and memory: Geforce GTX 1070
Exact command to reproduce: No

[Question]
Paper: Speed/accuracy trade-offs for modern convolutional object detectors

The paper says that ""We use Mixed 4c and Mixed 5c, appending four additional convolutional layers with
decaying resolution with depths 512, 256, 256, 128 respectively."" But, I didn't understand the architecture exactly. This means the order of Mixed 4c -> Mixed 5c -> 512 -> 256 -> 256 -> 128 ?? I also referred code, but still didn't. Just, I know that SSD_inception_V2 feature extractor structure ('Conv2d_1a_7x7', 'MaxPool_2a_3x3', 'Conv2d_2b_1x1', 'Conv2d_2c_3x3', 'MaxPool_3a_3x3', 'Mixed_3b', 'Mixed_3c', 'Mixed_4a', 'Mixed_4b', 'Mixed_4c', 'Mixed_4d', 'Mixed_4e', 'Mixed_5a', 'Mixed_5b', 'Mixed_5c').

https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v2.py

[Reference code site]
Keyword: from_layer

https://github.com/tensorflow/models/blob/1f484095c0981e2a62403b16256cb877749dfe94/research/object_detection/models/feature_map_generators.py
https://github.com/tensorflow/models/blob/master/research/object_detection/models/ssd_inception_v2_feature_extractor.py






### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",7,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-10-02 16:06:59,open,,,[],2018-10-17 16:44:21
546,tensorflow/models,models,5425,mhusseinsh,Feature Maps/Vectors visualization/extraction from frozen inference graph - Object Detection APIs,"### System information
- **What is the top-level directory of the model you are using**: _**models/research/object_detection/**_
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: _**YES, a simple script to load the frozen inference graph and predict**_
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: _**Open SuSE 12**_
- **TensorFlow installed from (source or binary)**: _**source**_
- **TensorFlow version (use command below)**: _**1.5.0**_
- **Bazel version (if compiling from source)**: _**N/A**_
- **CUDA/cuDNN version**: _**Cuda compilation tools, release 8.0, V8.0.61**_
- **GPU model and memory**:  _**Tesla P100-PCIE**_
- **Exact command to reproduce**: _**N/A**_

Is there a way to extract/visualize the feature maps/vectors after activation layers from the frozen graph model ?

**EDIT**
I managed to get the computational graph from the frozen model, and accordingly know which tensor is where exactly

So in my case for example, I am dealing with the faster-RCNN
![capsadsadasdture](https://user-images.githubusercontent.com/33177438/46363271-d52f0500-c672-11e8-9452-b27167985f6a.PNG)
so with `get_tensor_by_name('Squeeze_1:0')` I can run a session and get the output tensor of `Squeeze_1`
which has a shape of `Tensor(""Squeeze_1:0"", shape=(300, 90, 4), dtype=float32)`
Is there a way to visualize this activation ? taking into consideration that it is a 4 channel ?

I am looking into an output more or less like this 


![activations](https://user-images.githubusercontent.com/33177438/46363284-dbbd7c80-c672-11e8-85fe-90ceb173f731.png)
",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-10-02 15:24:44,open,,,[],2018-11-21 06:21:47
547,tensorflow/models,models,5423,anirudhkm,"InvalidArgumentError (see above for traceback): Incompatible shapes: [2,1917] vs. [3,1]","I'm trying to develop a custom object detection model using online tutorials, and when I run the train.py script, I land up in the error _InvalidArgumentError (see above for traceback): Incompatible shapes: [2,1917] vs. [3,1]_.

Please, help in understanding the error and steps to fix it.",32,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-02 06:18:30,open,,,[],2019-01-25 04:10:49
548,tensorflow/models,models,5421,waltermaldonado,Multiple GPU in model_main.py (since there is no more train.py),"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: V9.0.176
- **GPU model and memory**: 2x Tesla P100 16Gb
- **Exact command to reproduce**: NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

------------------------
Greetings,

I would like to know how to proceed to use both of my GPUs for training a Faster R-CNN with NASNet-A featurization model with the model_main.py file included in the object_detection tools now that train.py is gone. If it is not possible, I would like to request this feature or a workaround to make it work.

Thanks in advance.
",31,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-10-02 02:04:24,open,,,[],2019-04-07 06:44:00
549,tensorflow/models,models,5416,subhamvs,tf.profiler provides incompatible shape for ssd mobilenet V2 object detection model,"
### System information
- **What is the top-level directory of the model you are using**: ssd_mobilenet_v2
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Using pip
- **TensorFlow version (use command below)**: 1.10.0
- **Bazel version (if compiling from source)**: No
- **CUDA/cuDNN version**: 9.0/7.1
- **GPU model and memory**: Geforce GTX 1080 Ti / 11171 MB

### Describe the problem
I am facing ""incompatible shape"" issue while profiling tensorflow objection detection model. 
Node FeatureExtractor/MobilenetV2/Conv/Conv2D incompatible shapes: Shapes (?, ?, ?, 32) and (1, 32, 256, 256) are not compatible.
Node FeatureExtractor/MobilenetV2/Conv/Relu6 incompatible shapes: Shapes (?, ?, ?, 32) and (1, 32, 256, 256) are not compatible.
Node FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise incompatible shapes: Shapes (?, ?, ?, 32) and (1, 32, 256, 256) are not compatible.
Node FeatureExtractor/MobilenetV2/expanded_conv/depthwise/Relu6 incompatible shapes: Shapes (?, ?, ?, 32) and (1, 32, 256, 256) are not compatible.
Node FeatureExtractor/MobilenetV2/expanded_conv/project/Conv2D incompatible shapes: Shapes (?, ?, ?, 16) and (1, 16, 256, 256) are not compatible.
...
Basically, I want to get flip flops of the model.
How to fix the issue? Is there any other way to get flops of the model?
",2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-10-01 08:29:01,open,,,['stat:awaiting tensorflower'],2018-10-29 02:04:27
550,tensorflow/models,models,5415,omg777,Hyper-parameters for reproducing results for Domain adaptation.,"
### Describe the problem
Can you provide the hyper-parameters for reproducing the results for the DANN?
In 'models/research/domain_adaptation/domain_separation/models.py' 
there are DANN models such as dann_mnist, dann_svhn, dann_gtsrb.
but there is no information about hyper-parameters like learning_rate, weight_decay.",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-10-01 07:40:48,open,,,['stat:awaiting response'],2018-10-01 19:51:09
551,tensorflow/models,models,5414,mytrjp,class-specific augmentation in API,"### System information
- What is the top-level directory of the model you are using: research/object_detection/
- Have I written custom code : No
- System Information
- OS Platform and Distribution:Ubuntu 16.04
- TensorFlow installed from: Binary
- TensorFlow version:1.4.0
- Bazel version:N/A
- CUDA/cuDNN version:9.0, V9.0.176
- GPU model and memory:GeForce GTX 1080 Ti/PCIe/SSE2
- Exact command to reproduce: N/A

### Describe the problem

I have an imbalanced dataset and having very low accuracy on limited classes.
I know there are many standard ways like downsampling or upsampling to handle it. So I found data augmentation options in preprocessor.py. 

My question is that how to augment class-specific examples only, not the entire dataset.  
I can not see any configuration setting in the API, though. 

Could you help me with that?",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-10-01 06:15:48,open,,,[],2018-10-01 13:30:35
552,tensorflow/models,models,5412,afcruzs,Parse method of StringParser is broken in Python 3,"### System information
- **What is the top-level directory of the model you are using**: models\research\object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**:  1.10.0
- **CUDA/cuDNN version**: 9.0/64_7
- **Bazel version (if compiling from source)**: none
- **GPU model and memory**: gtx 1050
- **Exact command to reproduce**: [using python 3] 
 python offline_eval_map_corloc.py --eval_dir=path/to/eval_dir \
        --eval_config_path=path/to/evaluation/configuration/file \
        --input_config_path=path/to/input/configuration/file

### Describe the problem

The parse method of [StringParser](https://github.com/tensorflow/models/blob/38385b0a0260609192423e371ce8a30a9f5571ba/research/object_detection/metrics/tf_example_parser.py#L40) is loading a list of bytes like this: `tf_example.features.feature[self.field_name].bytes_list.value`, and then is making a join over that list. In python 3 ''.join(..) requires the iterable to have string elements only, as this is a bytes list it fails. 

A simple fix to make this works in python 3 is to add the 'b' character before the join statement.

### Source code / logs
This is the exact method which breaks in python 3:
```
def parse(self, tf_example):
    return """".join(tf_example.features.feature[self.field_name]
                   .bytes_list.value) if tf_example.features.feature[
                       self.field_name].HasField(""bytes_list"") else None
```
",0,"NamedUser(login=""tensorflowbutler"")","[NamedUser(login=""tensorflowbutler"")]",2018-10-01 00:44:48,open,,,['stat:awaiting tensorflower'],2018-10-29 02:54:00
553,tensorflow/models,models,5410,ltrottier,Empty list returned by tf.global_variables() on tensorflow object detector checkpoints,"I would like this question answered, because I have the same problem:

https://stackoverflow.com/questions/51708245/empty-list-returned-by-tf-global-variables-on-tensorflow-object-detector-check",4,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-10-01 00:06:24,open,,,[],2018-10-19 06:52:11
554,tensorflow/models,models,5407,s-gupta,Fix Instructions for installing swiftshader.,,0,,[],2018-09-30 15:42:36,open,,,['cla: yes'],2018-09-30 15:42:38
555,tensorflow/models,models,5405,xiyacao,cannot make SwiftShader with the checkout version,"If I checkout 91da6b00584afd7dcaed66da88e2b617429b3950, the folder pnacl-subzero is empty and the cmake .. will fail. 
@s-gupta ",5,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-09-30 05:26:11,open,,,['stat:community support'],2018-10-13 06:21:22
556,tensorflow/models,models,5403,pcgreat,ptb_word_lm.py no longer working for multi-gpu with tf 1.11,"when running with multi-gpu, ptb_word_lm.py will throw an exception when OptimizeGraph(). I tried from tf 1.8-1.11, and this bug can be reproduced from all versions.

```
Traceback (most recent call last):
  File ""ptb_word_lm.py"", line 527, in <module>
    tf.app.run()
  File ""/home/chao/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""ptb_word_lm.py"", line 498, in main
    util.auto_parallel(metagraph, m)
  File ""/home/chao/Project/models/tutorials/rnn/ptb/util.py"", line 96, in auto_parallel
    optimized_graph = tf_optimizer.OptimizeGraph(rewriter_config, metagraph)
  File ""/home/chao/anaconda3/lib/python3.6/site-packages/tensorflow/python/grappler/tf_optimizer.py"", line 39, in OptimizeGraph
    verbose, graph_id, status)
  File ""/home/chao/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 526, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Non-existent input AutoParallel-Replica-0/AutoParallel-Div-Const for node AutoParallel-Replica-0/AutoParallel-Div-AutoParallel-Replica-0/Train/Model/GradientDescent/update_Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/bias/ApplyGradientDescent
```

### System information
- **What is the top-level directory of the model you are using**: tutorials
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: GTX 1080 Ti 11G
- **Exact command to reproduce**:  python ptb_word_lm.py --data_path=$HOME/ProjData/ptb/ --model=small --num_gpus 3

",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-30 04:30:08,open,,,[],2019-02-15 03:48:24
557,tensorflow/models,models,5400,TerryBryant,Can't restore your resnet50_v2 pretrained model,"### System information
- What is the top-level directory of the model you are using: https://github.com/tensorflow/models
- Have I written custom code: yes
- OS Platform and Distribution: ubuntu 14.04
- TensorFlow installed from: nvidia-docker, [latest tensorflow gpu](https://hub.docker.com/r/tensorflow/tensorflow/)
- Bazel version: N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: NVIDIA GTX 1050Ti, 4G
- Exact command to reproduce: just run the script in python interpreter


### Problem Description
I'm trying to use your pretrained resnet50 v2 model, download from here:
[http://download.tensorflow.org/models/official/20180601_resnet_v2_fp16_imagenet_checkpoint.tar.gz](url)
But when I tried to load the model, it just go wrong


### Source code
My code is quite simple, here it is:
```
import tensorflow as tf

with tf.Session() as sess:
    saver = tf.train.import_meta_graph('20180601_resnet_v2_imagenet_checkpoint/model.ckpt-62500.meta')
    saver.restore(sess, '20180601_resnet_v2_imagenet_checkpoint/model.ckpt-62500')
```
### Errors
ValueError: NodeDef missing attr 'output_types' from Op<name=FunctionBufferingResource; signature=string_arg:string, target_device:string -> resource:resource; attr=shared_name:string; attr=container:string; attr=f:func; attr=buffer_size:int; attr=output_types:list(type); is_stateful=true>; NodeDef: {{node FunctionBufferingResource}} = FunctionBufferingResource[_output_shapes=[<unknown>], buffer_size=1, container="""", f=_prefetch_fn_0CSL7hFXlzE[], shared_name="""", _device=""/device:GPU:0""](IteratorToStringHandle, IteratorGetDevice)
",4,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-29 06:48:49,open,,,[],2019-03-12 08:52:20
558,tensorflow/models,models,5399,waterson,Add utilities and documentation for preparing training data.,,0,,[],2018-09-29 01:43:30,open,,,['cla: yes'],2018-09-29 17:50:10
559,tensorflow/models,models,5396,t27,Adding from_detection_checkpoint flag in the sample config for oid,"This is to ensure consistency with the other checkpoints of the faster_rcnn_inception_resnet_v2_atrous configs (coco, pets etc)
This was pointed out in #3562",0,,[],2018-09-28 10:23:57,open,,,['cla: yes'],2018-09-28 10:23:59
560,tensorflow/models,models,5391,ZhenyF,"Error using SSD-mobilenet-v1-coco: Incompatible shapes: [3,1917] vs. [16,1]","
### System information
- **What is the top-level directory of the model you are using**:tensorflow/models
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.9
- **CUDA/cuDNN version**: CUDA8.0
- **GPU model and memory**:GTX 1080TI 11G


### Describe the problem
I tried train a facial detector using object detection API. The dataset is WIDER Face dataset and the conversion code (to tfrecord) is from [here](https://github.com/iitzco/widerface-to-tfrecord). The pretrained model is **ssd_mobilenet_v1_coco** and the config file is **ssd_mobilenet_v1_focal_loss_pets**. I didn't do any modification except setting the number of class  to 1, imcreasing the batch size from 25 to 32 and changing the address of label map, checkpoint, and tfrecords. The initialisation process looks fine but I got an error after that:

`I0927 11:59:49.965208  7440 tf_logging.py:115] Restoring parameters from C:/Users/mhb14150/Documents/TF_object_api/My_model/check_point/ssd_mobilenet_v1_coco/model.ckpt
INFO:tensorflow:Running local_init_op.
I0927 11:59:50.130070  7440 tf_logging.py:115] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I0927 11:59:50.387650  7440 tf_logging.py:115] Done running local_init_op.
INFO:tensorflow:Starting Session.
I0927 11:59:57.981596  7440 tf_logging.py:115] Starting Session.
INFO:tensorflow:Saving checkpoint to path C:/Users/mhb14150/Documents/TF_object_api/My_model/mode_trained\model.ckpt
I0927 11:59:58.153400 12356 tf_logging.py:115] Saving checkpoint to path C:/Users/mhb14150/Documents/TF_object_api/My_model/mode_trained\model.ckpt
INFO:tensorflow:Starting Queues.
I0927 11:59:58.169021  7440 tf_logging.py:115] Starting Queues.
INFO:tensorflow:global_step/sec: 0
I0927 12:00:09.622732  8096 tf_logging.py:159] global_step/sec: 0
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Incompatible shapes: [3,1917] vs. [16,1]
         [[Node: Loss/Match_25/cond/mul_4 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Loss/Match_25/cond/one_hot/_4475, Loss/Match_25/cond/Cast_2)]]
         [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/BatchNorm/AssignMovingAvg_1/mul/_3975 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_5635_...gAvg_1/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'Loss/Match_25/cond/mul_4', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\util\deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\legacy\trainer.py"", line 290, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\legacy\trainer.py"", line 205, in _create_losses
    losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\meta_architectures\ssd_meta_arch.py"", line 680, in loss
    keypoints, weights)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\meta_architectures\ssd_meta_arch.py"", line 853, in _assign_targets
    groundtruth_weights_list)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\core\target_assigner.py"", line 483, in batch_assign_targets
    anchors, gt_boxes, gt_class_targets, unmatched_class_label, gt_weights)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\core\target_assigner.py"", line 182, in assign
    valid_rows=tf.greater(groundtruth_weights, 0))
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\core\matcher.py"", line 241, in match
    return Match(self._match(similarity_matrix, valid_rows),
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\matchers\argmax_matcher.py"", line 194, in _match
    _match_when_rows_are_non_empty, _match_when_rows_are_empty)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\util\deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2040, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 1890, in BuildCondBranch
    original_result = fn()
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\matchers\argmax_matcher.py"", line 175, in _match_when_rows_are_non_empty
    tf.cast(tf.expand_dims(valid_rows, axis=-1), dtype=tf.float32))
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 847, in binary_op_wrapper
    return func(x, y, name=name)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1091, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 5066, in mul
    ""Mul"", x=x, y=y, name=name)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Incompatible shapes: [3,1917] vs. [16,1]
         [[Node: Loss/Match_25/cond/mul_4 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Loss/Match_25/cond/one_hot/_4475, Loss/Match_25/cond/Cast_2)]]
         [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/BatchNorm/AssignMovingAvg_1/mul/_3975 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_5635_...gAvg_1/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

I0927 12:00:12.403529  7440 tf_logging.py:115] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Incompatible shapes: [3,1917] vs. [16,1]
         [[Node: Loss/Match_25/cond/mul_4 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Loss/Match_25/cond/one_hot/_4475, Loss/Match_25/cond/Cast_2)]]
         [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/BatchNorm/AssignMovingAvg_1/mul/_3975 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_5635_...gAvg_1/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'Loss/Match_25/cond/mul_4', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\util\deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\legacy\trainer.py"", line 290, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\legacy\trainer.py"", line 205, in _create_losses
    losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\meta_architectures\ssd_meta_arch.py"", line 680, in loss
    keypoints, weights)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\meta_architectures\ssd_meta_arch.py"", line 853, in _assign_targets
    groundtruth_weights_list)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\core\target_assigner.py"", line 483, in batch_assign_targets
    anchors, gt_boxes, gt_class_targets, unmatched_class_label, gt_weights)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\core\target_assigner.py"", line 182, in assign
    valid_rows=tf.greater(groundtruth_weights, 0))
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\core\matcher.py"", line 241, in match
    return Match(self._match(similarity_matrix, valid_rows),
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\matchers\argmax_matcher.py"", line 194, in _match
    _match_when_rows_are_non_empty, _match_when_rows_are_empty)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\util\deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2040, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 1890, in BuildCondBranch
    original_result = fn()
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\matchers\argmax_matcher.py"", line 175, in _match_when_rows_are_non_empty
    tf.cast(tf.expand_dims(valid_rows, axis=-1), dtype=tf.float32))
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 847, in binary_op_wrapper
    return func(x, y, name=name)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1091, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 5066, in mul
    ""Mul"", x=x, y=y, name=name)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Incompatible shapes: [3,1917] vs. [16,1]
         [[Node: Loss/Match_25/cond/mul_4 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Loss/Match_25/cond/one_hot/_4475, Loss/Match_25/cond/Cast_2)]]
         [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/BatchNorm/AssignMovingAvg_1/mul/_3975 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_5635_...gAvg_1/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Traceback (most recent call last):
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [3,1917] vs. [16,1]
         [[Node: Loss/Match_25/cond/mul_4 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Loss/Match_25/cond/one_hot/_4475, Loss/Match_25/cond/Cast_2)]]
         [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/BatchNorm/AssignMovingAvg_1/mul/_3975 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_5635_...gAvg_1/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\util\deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\legacy\trainer.py"", line 415, in train
    saver=saver)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py"", line 770, in train
    sess, train_op, global_step, train_step_kwargs)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py"", line 487, in train_step
    run_metadata=run_metadata)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [3,1917] vs. [16,1]
         [[Node: Loss/Match_25/cond/mul_4 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Loss/Match_25/cond/one_hot/_4475, Loss/Match_25/cond/Cast_2)]]
         [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/BatchNorm/AssignMovingAvg_1/mul/_3975 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_5635_...gAvg_1/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'Loss/Match_25/cond/mul_4', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\util\deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\legacy\trainer.py"", line 290, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\legacy\trainer.py"", line 205, in _create_losses
    losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\meta_architectures\ssd_meta_arch.py"", line 680, in loss
    keypoints, weights)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\meta_architectures\ssd_meta_arch.py"", line 853, in _assign_targets
    groundtruth_weights_list)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\core\target_assigner.py"", line 483, in batch_assign_targets
    anchors, gt_boxes, gt_class_targets, unmatched_class_label, gt_weights)
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\core\target_assigner.py"", line 182, in assign
    valid_rows=tf.greater(groundtruth_weights, 0))
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\core\matcher.py"", line 241, in match
    return Match(self._match(similarity_matrix, valid_rows),
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\matchers\argmax_matcher.py"", line 194, in _match
    _match_when_rows_are_non_empty, _match_when_rows_are_empty)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\util\deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2040, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 1890, in BuildCondBranch
    original_result = fn()
  File ""C:\Users\mhb14150\Documents\TF_object_api\models\research\object_detection\matchers\argmax_matcher.py"", line 175, in _match_when_rows_are_non_empty
    tf.cast(tf.expand_dims(valid_rows, axis=-1), dtype=tf.float32))
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 847, in binary_op_wrapper
    return func(x, y, name=name)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1091, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 5066, in mul
    ""Mul"", x=x, y=y, name=name)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\Users\mhb14150\Desktop\winpython\WinPython-64bit-3.5.3.1Qt5\python-3.5.3.amd64\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Incompatible shapes: [3,1917] vs. [16,1]
         [[Node: Loss/Match_25/cond/mul_4 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Loss/Match_25/cond/one_hot/_4475, Loss/Match_25/cond/Cast_2)]]
         [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/BatchNorm/AssignMovingAvg_1/mul/_3975 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_5635_...gAvg_1/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]`

Can someone help me with this? Thanks!
",25,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-09-27 12:16:47,open,,,[],2018-12-02 02:08:04
561,tensorflow/models,models,5389,nimaous,beam search symbols_to_logits_fn is not correct,"
------------------------

### System information
- **What is the top-level directory of the model you are using**:
models/official/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Centos 7
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
tensorflow-gpu           1.10.1  
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
cuda-9.1
- **GPU model and memory**:
GeForce GTX 1080
- **Exact command to reproduce**:


### Describe the problem
In beam search symbols_to_logits_fn, it always use the last decoded symbol as the input of the decoder
`decoder_input = ids[:, -1:]`
but, It must be the complete decoded sequence previous to the currenct index of decoding like 
`decoder_input = ids[:, : i ]`  
Indeed in decoding (predicting mode)  we need to refer to the output of encoder and all previously decoded sequence not only the last decoded symbol. 

",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-27 09:11:29,open,,,[],2018-09-27 19:52:35
562,tensorflow/models,models,5388,netanel-s,[Object Detection][Bug] keep_aspect_ratio_resizer doesn't work well with pad_to_max_dimension,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**: training and evaluating ssdlite_mobilenet_v2_coco_2018_05_09 using object_detection/model_main.py.

### Describe the problem
For a while now I tried to train a model using keep_aspect_ratio_resizer without any luck. I believe I found that the reason is that this resizer simply doesn't work well.
Here's a keep_aspect_ratio_resizer configuration I've been used:
```
      keep_aspect_ratio_resizer {
        min_dimension: 480
        max_dimension: 640
        pad_to_max_dimension: true
      }
```
The way I expected it to work is to resize the image such that the longer axis will have length 640 while keeping the original aspect ratio, and then the image is padded (right-bottom) in order to achieve 640x640. Moreover, the bounding boxes should be resized in the same manner.
BTW, the reason I use the padding is to have batch_size>1. Otherwise, multiple images which don't have the same dimensions won't fit together to a single tensor.

Here you can see visualization of GT with **keep_aspect_ratio_resizer** and **pad_to_max_dimension** (note how the aspect ratio is not kept, and that the BBs have the original shape and location):
![image](https://user-images.githubusercontent.com/38940293/46131974-9ef21f80-c245-11e8-84d8-2d010278e40c.png)
![image](https://user-images.githubusercontent.com/38940293/46132022-bb8e5780-c245-11e8-9dbc-816708f1c7b4.png)
and here the same images with fixed_shape_resizer:
![image](https://user-images.githubusercontent.com/38940293/46132047-cba63700-c245-11e8-924e-3744b588e832.png)
![image](https://user-images.githubusercontent.com/38940293/46132055-d1038180-c245-11e8-8ef1-ac14866fa3f4.png)

Thanks in advance,",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-09-27 08:11:20,open,,,[],2019-01-31 04:19:30
563,tensorflow/models,models,5386,data-steve,how to get this repo isn't stated here,"other README.md docs in tensorflow have the git clone requirement declared.

the way the documentation is set up to point PYTHON PATH to the models module path sounds like the models module was already on my machine , esp since the github repo has it as part of tensorflow master repo",1,,[],2018-09-26 18:19:31,open,,,['cla: no'],2018-09-26 18:19:34
564,tensorflow/models,models,5385,prasanth-ntu,Problem in retraining ssd_mobilenet on my own dataset using TensorFlow object detection API,"I have followed the tutorial (https://pythonprogramming.net/training-custom-objects-tensorflow-object-detection-api-tutorial) to retrain 'ssd_mobilenet_v1_coco' on my own dataset. I am trying to use atleast 1 or more classes of my own dataset. I converted my dataset with annotated bounding boxes to the desired TFRecord format without errors and changed the .config file as well. I also used dataset created by other user from github (https://github.com/datitran/raccoon_dataset). 
___
My server environment details:
GPU: 4 x GeForce GTX 1080 Ti 
Ubuntu 16.04.4 LTS
TensorFlow GPU Version: 1.9.0
Running in Virtual environment
CUDA Version 9.0.176
___
**I then executed the cmd below to retrain the 'ssd_mobilenet_v1_coco' on my own dataset.**:
`python3 legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config`
___
What I observe is that the the training takes few random number of steps and stops throwing me an error messsage. I have attached a portion of training info below:

`INFO:tensorflow:Restoring parameters from ssd_mobilenet_v1_coco_11_06_2017/model.ckpt
I0927 01:01:09.224665 139707445737216 tf_logging.py:115] Restoring parameters from ssd_mobilenet_v1_coco_11_06_2017/model.ckpt
INFO:tensorflow:Running local_init_op.
I0927 01:01:09.527872 139707445737216 tf_logging.py:115] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I0927 01:01:09.858946 139707445737216 tf_logging.py:115] Done running local_init_op.
INFO:tensorflow:Starting Session.
I0927 01:01:18.386830 139707445737216 tf_logging.py:115] Starting Session.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
I0927 01:01:18.674590 139650089871104 tf_logging.py:115] Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Starting Queues.
I0927 01:01:18.680459 139707445737216 tf_logging.py:115] Starting Queues.
INFO:tensorflow:global_step/sec: 0
I0927 01:01:25.934769 139650014369536 tf_logging.py:159] global_step/sec: 0
INFO:tensorflow:Recording summary at step 0.
I0927 01:01:35.952289 139650081478400 tf_logging.py:115] Recording summary at step 0.
INFO:tensorflow:global step 1: loss = 13.3887 (17.959 sec/step)
I0927 01:01:36.882163 139707445737216 tf_logging.py:115] global step 1: loss = 13.3887 (17.959 sec/step)
INFO:tensorflow:global step 2: loss = 12.4938 (0.381 sec/step)
I0927 01:01:37.541383 139707445737216 tf_logging.py:115] global step 2: loss = 12.4938 (0.381 sec/step)
INFO:tensorflow:global step 3: loss = 11.3832 (0.425 sec/step)
I0927 01:01:37.968488 139707445737216 tf_logging.py:115] global step 3: loss = 11.3832 (0.425 sec/step)
INFO:tensorflow:global step 4: loss = 10.6289 (0.469 sec/step)
I0927 01:01:38.439345 139707445737216 tf_logging.py:115] global step 4: loss = 10.6289 (0.469 sec/step)
INFO:tensorflow:global step 5: loss = 10.0338 (0.400 sec/step)
I0927 01:01:38.841859 139707445737216 tf_logging.py:115] global step 5: loss = 10.0338 [(0.400](sec/step)`
...
`INFO:tensorflow:global step 16: loss = 6.6211 (0.420 sec/step)
I0927 01:22:40.314219 140712953894656 tf_logging.py:115] global step 16: loss = 6.6211 (0.420 sec/step)

INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Incompatible shapes: [2,1917] vs. [3,1]
	 [[Node: Loss/Match_22/cond/mul_4 = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Loss/Match_22/cond/one_hot, Loss/Match_22/cond/Cast_2)]]
	 [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/BatchNorm/AssignMovingAvg_1/mul/_3803 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_6517_...gAvg_1/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]`



Incompatible shapes: [2,1917] vs. [3,1] changes for different dataset and keeps changing to different number now and then. Also, the program randomly stops and different number of steps.

However, when I use the 'macncheese' dataset given in the tutorial website, I am able to run beyond 10000 steps without any issues. I have checked many other posts and none of them are helping me to resolve the issue. 

___

I have visited the following links, but still couldn't resolve the issue:
[1] https://github.com/dennybritz/chatbot-retrieval/issues/15
[2] https://github.com/tensorflow/models/issues/1760
[3] https://github.com/balancap/SSD-Tensorflow/issues/88
___

**This is my config:**
`
model {
  ssd {
    num_classes: 1
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.03
              mean: 0.0
            }
          }
          batch_norm {
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v1'
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.9997,
          epsilon: 0.001,
        }
      }
    }
    loss {
      classification_loss {
        weighted_sigmoid {
          anchorwise_output: true
        }
      }
      localization_loss {
        weighted_smooth_l1 {
          anchorwise_output: true
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 0
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  batch_size: 24
  optimizer {
    rms_prop_optimizer: {
      learning_rate: {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.004
          decay_steps: 800720
          decay_factor: 0.95
        }
      }
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""ssd_mobilenet_v1_coco_11_06_2017/model.ckpt""
  from_detection_checkpoint: true
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""data/train.record""
  }
  label_map_path: ""data/object-detection.pbtxt""
}

eval_config: {
  num_examples: 40
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""data/test.record""
  }
  label_map_path: ""training/object-detection.pbtxt""
  shuffle: false
  num_readers: 1
}
`


**This is my object-detection.pbtxt**
item {
  id: 1
  name: 'raccoon'
}
",12,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-09-26 17:41:22,open,,,[],2018-11-03 10:37:02
565,tensorflow/models,models,5384,tianye2017resnet,Problems encountered in training resnet_v2 from scratch with your code on slim,"System information
•What is the top-level directory of the model you are using: slim
•Have I written custom code: no
•OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
•TensorFlow installed from (source or binary): binary
•TensorFlow version (use command below): 1.9.0.
•Bazel version (if compiling from source): -
•CUDA/cuDNN version: CUDA 9
•GPU model and memory: Tesla V100, 16GB
•Exact command to reproduce: training and evaluating /home/linkface/sunyi/models/research/slim/nets/resnet_v2.py.

Describe the problem

I have recently run your code to train the resnet_v2_50 and resnet_v2_101 model from scratch which was downloaded from slim.I have tried to set the batch size to 32,64,128 and 256 and set the image size to 224 and 299.All other parameters remain unchanged from the code.However,both the top1 accuracy and the top5 accuracy are 2 or 3 points lower than your results.
I am wondering if you could kindly send me your settings to achieve the results and the necessary information about it.
Thanks.
",1,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-09-26 12:33:15,open,,,[],2018-10-15 07:45:41
566,tensorflow/models,models,5383,netanel-s,[Object Detection][Bug/Feature Request] Leave evaluation time out of training time,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**: training and evaluating ssdlite_mobilenet_v2_coco_2018_05_09 using object_detection/model_main.py.

### Describe the problem
When the model is evaluated, the time that the ""training clock"" isn't stopping, so it includes the time that evaluation takes.
Therefore, the training steps which the evaluation takes place in have longer time (see step=3400 below for example), and the value of global_step/sec is not relevant as can be seen in Tensorboard:
![image](https://user-images.githubusercontent.com/38940293/46076919-b83c9280-c197-11e8-88fb-584843eb1004.png)

```
INFO:tensorflow:Starting evaluation at 2018-09-26-11:07:54
INFO:tensorflow:Starting evaluation at 2018-09-26-11:07:54
...
INFO:tensorflow:Evaluation [<500-5000>/5000]
...
<COCO_metrics>
INFO:tensorflow:Finished evaluation at 2018-09-26-11:14:11
INFO:tensorflow:Finished evaluation at 2018-09-26-11:14:11
INFO:tensorflow:Saving dict for global step 3389: <evaluation_dict>
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3389: <model_path>
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3389: <model_path>
INFO:tensorflow:global_step/sec: 0.206893
INFO:tensorflow:global_step/sec: 0.206893
INFO:tensorflow:loss = 9.355345, step = 3400 (483.343 sec)
INFO:tensorflow:loss = 9.355345, step = 3400 (483.343 sec)
INFO:tensorflow:global_step/sec: 1.01595
INFO:tensorflow:global_step/sec: 1.01595
INFO:tensorflow:loss = 9.42148, step = 3500 (98.430 sec)
INFO:tensorflow:loss = 9.42148, step = 3500 (98.430 sec)
INFO:tensorflow:global_step/sec: 1.01467
INFO:tensorflow:global_step/sec: 1.01467
INFO:tensorflow:loss = 9.942209, step = 3600 (98.554 sec)
INFO:tensorflow:loss = 9.942209, step = 3600 (98.554 sec)
```

I think that you might want to keep both exclusive training time (which will be used in particular for golbal_step/sec) and exclusive evaluation time (since this value is important as well), and their sum will give the total time.

Thanks in advance.",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-26 11:33:21,open,,,[],2018-09-26 19:45:31
567,tensorflow/models,models,5382,ernstgoyer,faster rcnn on cloud tpu,"I'm trying to run faster rcnn resnet inception v2 on a cloud TPU instance, I know that there is no config in the object detection API that is compatible with TPUs, but I am getting always the following error:

Compilation failure: Detected unsupported operations when trying to compile graph _functionalize_body_20[] on XLA_TPU_JIT:Where, RandomShuffle, Where, RandomShuffle, Where, RandomShuffle, Where, RandomShuffle, Where, RandomShuffle, Where, RandomShuffle, Where, RandomShuffle, Where, RandomShuffle, Where, RandomShuffle, Where, RandomShuffle, Where, Where, RandomShuffle, Where, RandomShuffle, Where, Where, RandomShuffle, Where, RandomShuffle, Where, Where, RandomShuffle, Where, RandomShuffle, Where, Where, CropAndResize, CropAndResizeGradImage, CropAndResizeGradBoxes

so I was wondering if there is a way to change these operations to be compatible with TPU

TensorFlow installed from: preinstalled on cloud instance
TensorFlow version: tf 1.9
GPU model and memory: Google Cloud TPU Instance
CUDA/cuDNN version: no
Exact command to reproduce: model_tpu_main.py .....",2,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-09-26 10:46:04,open,,,['stat:awaiting response'],2019-01-31 19:12:32
568,tensorflow/models,models,5381,netanel-s,"[Feature Request] Object Detection - ""Fixing"" loss graphs on Tensorboard","### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**: training and evaluating ssdlite_mobilenet_v2_coco_2018_05_09 using object_detection/model_main.py.

### Describe the problem
In previous versions of the OD API, Tensorboard used to show total loss breakdown to classification and localization on both train and eval, and the corresponding loss graphs were on top of each other (on the same plot).
Now there's only breakdown for eval, not for train, and the total loss of the train and eval are shown on two different graphs (loss, loss_1). In fact, the train total loss graph is shown twice (loss_1, loss_2) - Or is there a difference I'm not aware of?

Thanks in advance.",1,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-09-26 07:26:55,open,,,[],2019-03-15 21:28:54
569,tensorflow/models,models,5378,hiepph,attention_ocr: A script to export SavedModel for Tensorflow Serving,Solution to this issue: https://github.com/tensorflow/models/issues/5330,3,,[],2018-09-25 16:41:30,open,,,['cla: yes'],2018-09-25 16:46:18
570,tensorflow/models,models,5375,rohitsaluja22,visualizing attention maps and saliency maps,"What is the top-level directory of the model you are using: attention_ocr/python
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): pip install --upgrade tensorflow-gpu
TensorFlow version (use command below): 1.4.1.
Bazel version: N/A
CUDA/cuDNN version: cuda/8.0 cudnn/7.1.2
GPU model and memory: 8 x Tesla K80
Exact command to reproduce: CUDA_VISIBLE_DEVICES=6,7 python train.py --batch_size=32

Hi, have your shared or is it possible to share the code to visualize attention maps and saliency maps as shown in the paper?

Thanks in advance.",0,"NamedUser(login=""alexgorban"")","[NamedUser(login=""alexgorban"")]",2018-09-25 10:44:07,open,,,[],2018-09-25 17:05:07
571,tensorflow/models,models,5371,nikitaverbis,"[Object Detection] TypeError: `name` must be string, given: 0","### System information
- **What is the top-level directory of the model you are using**: ~/tf_1_10_src/tensorflow/tensorflow/models
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: 1.10.1
- **Bazel version (if compiling from source)**: 0.15.2
- **CUDA/cuDNN version**: CUDA 9.0/cuDNN 7.3
- **GPU model and memory**: GTX TitanXp
- **Exact command to reproduce**: `cd ~/tf_1_10_src/tensorflow/tensorflow/models/research; ~/train_object_detection_v1.sh`

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs

Script train_object_detection_v1.sh source:
```
!/bin/bash
echo ""Object detection script v.1""
echo ""Check execution path""
if [[ ""$PWD"" = ""$TFMODELPATH/research"" ]]
then
  echo ""Current working directory is correct.""
  
  PROJECT_DIR=/media/nikita/LinuxBD4Tb/SSD_PROJECT
  PIPELINE_CONFIG_PATH=$PROJECT_DIR/ssd_inception_v2_coco_nik.config
  MODEL_DIR=$PROJECT_DIR/models/model
  NUM_TRAIN_STEPS=4000000
  SAMPLE_1_OF_N_EVAL_EXAMPLES=1
  
  echo ""PIPELINE_CONFIG_PATH=$PIPELINE_CONFIG_PATH""
  echo ""MODEL_DIR=$MODEL_DIR""
  echo ""NUM_TRAIN_STEPS=$NUM_TRAIN_STEPS""
  echo ""SAMPLE_1_OF_N_EVAL_EXAMPLES=$SAMPLE_1_OF_N_EVAL_EXAMPLES""
  echo ""-----------------""
  echo ""Start object_detection/model_main.py""
  python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsologtostderr 
else
  echo ""Current working directory must be 'tensorflow/models/research/'.""
  echo ""Correct path is '$TFMODELPATH/research'""
  echo ""Exit.""  
fi
```
Dir struct:

```
./SSD_PROJECT/
├── data
│   ├── mscoco_label_map.pbtxt
│   ├── mscoco_train.record
│   └── mscoco_val.record
├── models
│   └── model
│       ├── eval
│       ├── pipeline.config
│       └── train
└── ssd_inception_v2_coco_nik.config
```
File ssd_inception_v2_coco_nik.config:
```
model {
  ssd {
    num_classes: 90
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
        reduce_boxes_in_lowest_layer: true
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 3
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.03
              mean: 0.0
            }
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_inception_v2'
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.9997,
          epsilon: 0.001,
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    loss {
      classification_loss {
        weighted_sigmoid {
        }
      }
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 0
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  batch_size: 32
  optimizer {
    rms_prop_optimizer: {
      learning_rate: {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.004
          decay_steps: 800720
          decay_factor: 0.95
        }
      }
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""/home/nikita/tf_1_10_src/pretrained_InceptionV2_ImageNet_CLS2012/inception_v2.ckpt""
  from_detection_checkpoint: false
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  #num_steps: 300000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""/media/nikita/LinuxBD4Tb/SSD_PROJECT/data/mscoco_train.record""
  }
  label_map_path: ""/media/nikita/LinuxBD4Tb/SSD_PROJECT/data/mscoco_label_map.pbtxt""
}

eval_config: {
  num_examples: 5000
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""/media/nikita/LinuxBD4Tb/SSD_PROJECT/data/mscoco_val.record""
  }
  label_map_path: ""/media/nikita/LinuxBD4Tb/SSD_PROJECT/data/mscoco_label_map.pbtxt""
  shuffle: false
  num_readers: 1
}
```


Error log:
```
nikita@ubuntulinux:~/tf_1_10_src/tensorflow/tensorflow/models/research$ ~/train_object_detection_v1.sh 
Object detection script v.1
Check execution path
Current working directory is correct.
PIPELINE_CONFIG_PATH=/media/nikita/LinuxBD4Tb/SSD_PROJECT/ssd_inception_v2_coco_nik.config
MODEL_DIR=/media/nikita/LinuxBD4Tb/SSD_PROJECT/models/model
NUM_TRAIN_STEPS=4000000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
-----------------
Start object_detection/model_main.py
/home/nikita/tf_1_10_src/tensorflow/tensorflow/models/research/object_detection/utils/visualization_utils.py:27: UserWarning: 
This call to matplotlib.use() has no effect because the backend has already
been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

The backend was *originally* set to u'TkAgg' by the following code:
  File ""object_detection/model_main.py"", line 26, in <module>
    from object_detection import model_lib
  File ""/home/nikita/tf_1_10_src/tensorflow/tensorflow/models/research/object_detection/model_lib.py"", line 27, in <module>
    from object_detection import eval_util
  File ""/home/nikita/tf_1_10_src/tensorflow/tensorflow/models/research/object_detection/eval_util.py"", line 27, in <module>
    from object_detection.metrics import coco_evaluation
  File ""/home/nikita/tf_1_10_src/tensorflow/tensorflow/models/research/object_detection/metrics/coco_evaluation.py"", line 20, in <module>
    from object_detection.metrics import coco_tools
  File ""/home/nikita/tf_1_10_src/tensorflow/tensorflow/models/research/object_detection/metrics/coco_tools.py"", line 47, in <module>
    from pycocotools import coco
  File ""/home/nikita/tf_1_10_src/tensorflow/tensorflow/models/research/pycocotools/coco.py"", line 49, in <module>
    import matplotlib.pyplot as plt
  File ""/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py"", line 69, in <module>
    from matplotlib.backends import pylab_setup
  File ""/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py"", line 14, in <module>
    line for line in traceback.format_stack()


  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
W0924 15:58:51.491380 140112981780224 tf_logging.py:125] Forced number of epochs for all eval validations to be 1.
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
W0924 15:58:51.491605 140112981780224 tf_logging.py:125] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f6e49985c80>) includes params argument, but params are not passed to Estimator.
W0924 15:58:51.491858 140112981780224 tf_logging.py:125] Estimator's model_fn (<function model_fn at 0x7f6e49985c80>) includes params argument, but params are not passed to Estimator.
1) exporter_name=Servo_0; eval_spec_name=0(type <type 'int'>)
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 102, in main
    eval_on_train_data=False)
  File ""/home/nikita/tf_1_10_src/tensorflow/tensorflow/models/research/object_detection/model_lib.py"", line 659, in create_train_and_eval_specs
    exporters=exporter))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 237, in __new__
    raise TypeError('`name` must be string, given: {}'.format(name))
TypeError: `name` must be string, given: 0
```

Debug of file object_detection/model_lib.py shows:
File modifications:
```
eval_specs = [] #line 646
  cntr = 0 # add counter
  for eval_spec_name, eval_input_fn in zip(eval_spec_names, eval_input_fns):
    cntr += 1 # add counter inc
    exporter_name = '{}_{}'.format(final_exporter_name, eval_spec_name)
    print(""{}) exporter_name={}; eval_spec_name={}(type {})"".format(cntr, exporter_name, eval_spec_name, type(eval_spec_name))) # add debuging variables
    exporter = tf.estimator.FinalExporter(
        name=exporter_name, serving_input_receiver_fn=predict_input_fn)
    eval_specs.append(
        tf.estimator.EvalSpec(
            name=eval_spec_name),
            input_fn=eval_input_fn,
            steps=None,
            exporters=exporter))
```
Result:
```
...
1) exporter_name=Servo_0; eval_spec_name=0(type <type 'int'>)
...
```
Possible solution (IMHO):
Change in object_detection/model_lib.py:
```
eval_specs.append(
        tf.estimator.EvalSpec(
            name=eval_spec_name),
            input_fn=eval_input_fn,
            steps=None,
            exporters=exporter))
```
to:
```
eval_specs.append(
        tf.estimator.EvalSpec(
            name=str(eval_spec_name),
            input_fn=eval_input_fn,
            steps=None,
            exporters=exporter))
```

P.S. Can't understand why in file object_detection/model_lib.py at line 644 list of integers generated:
```
if eval_spec_names is None: # line 643
  eval_spec_names = range(len(eval_input_fns)) # creates integers. Line 644
```
",4,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-09-24 14:27:21,open,,,[],2018-09-27 12:48:30
572,tensorflow/models,models,5370,kristijanbartol,Simple fix for ICP implementation issues.,"As mentioned in the code line comment and as pointed out in the [paper](https://arxiv.org/abs/1802.05522), ICP is not differentiable. ICP algorithm can be wrapped in a TF graph using tf.py_func (simple example of py_func [gist](https://gist.github.com/kristijanbartol/1b7b7c5d431415284217bbf63ca25c66)). This way, we can provide gradients manually, i.e., override them in general. 

However, there is more to this... An important problem arises when we want to apply gradients for ego motion matrix, which has some special properties that are not allowed to be destroyed by improper gradient update. Therefore, gradients tried to be applied only for translation x-axis.

The project was broken in case of training using the provided code because of broken bazel build. 

It turns out there is a correct way to approximate gradients without overriding gradients and even using ICP and fighting bazel (as we are not interested in rotation; with translation it's trivial to obtain residuals). The code is also missing principal mask, but it can be added in another pull request.

As for this PR, it needs to be tested. ATM I don't have KITTI dataset locally to try it out, so this is **not ready for merge**. I wanted to point this trivial fix out.",2,,[],2018-09-24 13:22:01,open,,,['cla: no'],2018-09-25 10:50:10
573,tensorflow/models,models,5369,rootkitchao,NameError: name 'unicode' is not defined on object_detection_evaluation.py,"### System information
- **What is the top-level directory of the model you are using**:tensorflow/models/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:MS Windows 10 Pro 64bit Build 17134
- **TensorFlow installed from (source or binary)**:binary（tensorflow-gpu)
- **TensorFlow version (use command below)**:v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Bazel version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0/7.1
- **GPU model and memory**:NVIDIA Geforce GTX1080TI 11GB
- **Exact command to reproduce**:

Hello I have found a bug when I was trying to evaluate a model I trained myself.When i use the command python legacy/eval.py --logtostderr --checkpoint_dir= --pipeline_config_path= --eval_dir=.An error has occurred:File ""\tensorflow-models\research\object_detection\utils\object_detection_evaluation.py"", line 307, in evaluate
    category_name = unicode(category_name, 'utf-8')
NameError: name 'unicode' is not defined

unicode() is note work on python 3.6,it should be str().I hope some people can fix this.
thanks.

",5,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-09-24 04:04:13,open,,,['stat:awaiting tensorflower'],2019-02-05 21:44:05
574,tensorflow/models,models,5368,Handsome-Jun,deeplab,"When I was visualizing the results with the vis.py file, I found that there were files in the segmentation_results folder, but there were no files under the raw_segmentation_results folder. I want to ask what is going on?
",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-24 03:09:01,open,,,['stat:awaiting response'],2018-09-27 17:56:19
575,tensorflow/models,models,5364,dythebs,The code and the paper have different structures,"### System information
- **What is the top-level directory of the model you are using**: models/research/slim/nets/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**:N/A
- **Bazel version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:N/A


### Describe the problem
I read the [code](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v1.py) and find `end_point = 'MaxPool_5a_2x2'` in line 214, however I see it is `3x3+2(S)` in the [paper](https://arxiv.org/pdf/1409.4842v1.pdf) (Figure 3)
",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-09-23 11:54:34,open,,,['stat:awaiting response'],2018-09-24 00:53:03
576,tensorflow/models,models,5362,kmenita,Unable to train my own object detector in local mac ,"Hi All, 

I am trying to build my object detector for my own data set and run locally on my Mac. I have followed the documentation. When I try to run the ""Running the Training Job"" I getting error message. Please advice 

Steps followed: 

https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md

Installation - Completed and working as expected. 
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md 

Preparing Inputs - Completed and working as expected. 
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/preparing_inputs.md

Configuring the Object Detection Training Pipeline - completed. 
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md


When I run the final command for running the train job , I am getting the error. 

CODE: 

python object_detection/model_main.py \ --pipeline_config_path=“/Users/kmenita/Downloads/venky_project/models/ObjectDetecor/train/ssd_mobilenet_v1_pets.config ” \ --model_dir=“/Users/kmenita/Downloads/venky_project/models/ObjectDetecor/train” \ --num_train_steps=5000 \ -- sample_1_of_n_eval_examples=1 \ --alsologtostderr

Error: 

ImportError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of 'python' with 'pythonw'. See 'Working with Matplotlib on OSX' in the Matplotlib FAQ for more information.

Please advice
",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-23 02:51:27,open,,,['stat:awaiting response'],2018-09-23 13:11:10
577,tensorflow/models,models,5360,moinkhan3012," DuplicateFlagError: The flag 'master' is defined twice. First from E:/project/models/research/object_detection/train.py, Second from E:/project/models/research/object_detection/train.py.  Description from first occurrence: Name of the TensorFlow master to use.","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-22 16:07:14,open,,,[],2019-02-20 06:48:27
578,tensorflow/models,models,5357,SoumiDas,Using the trained models on some other dataset,"Hi,
I wanted to know whether these existing trained models can be used on our own dataset and not on CITYSCAPES, ADE20K or PASCAL_VOC ?

Thanks!",1,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-09-22 06:22:02,open,,,['stat:awaiting response'],2018-09-22 21:26:36
579,tensorflow/models,models,5356,tispratik,np.array(data) takes too long... can we replace it with something else?,"https://github.com/tensorflow/models/blob/1f484095c0981e2a62403b16256cb877749dfe94/research/object_detection/object_detection_tutorial.ipynb?short_path=ddbe0dc#L296

In my benchmark, it see that it takes anywhere between 900 and 1200 milliseconds on a laptop with 16GiB RAM and 2.6 GHz Intel Core i7 processor. This is important as i am evaluating images from a live video stream from opencv and it is very slow.",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-22 04:46:09,open,,,[],2018-09-22 13:24:15
580,tensorflow/models,models,5355,feynmanliang,Fix BSD compatibility of '\n' in sed create_pycocotools_package.sh,"See https://stackoverflow.com/questions/46082397/insert-newline-n-using-sed

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/tensorflow/models/5355)
<!-- Reviewable:end -->
",0,,[],2018-09-22 01:37:04,open,,,['cla: yes'],2018-09-22 01:37:45
581,tensorflow/models,models,5349,feynmanliang,Nits in using_your_own_dataset.md," * I might be misunderstanding (e.g. if the docs are compiled), but the [anchor on github](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md#conversion-script-outline) is currently broken
 * Tutorials modify `PYTHONPATH` to resolve module imports directly out of `models/research`; the rest of the world doesn't have a `google3` namespace ;)

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/tensorflow/models/5349)
<!-- Reviewable:end -->
",1,,[],2018-09-21 14:15:00,open,,,['cla: yes'],2018-09-22 16:25:51
582,tensorflow/models,models,5348,SheptunovaAA,No messages like 'INFO:tensorflow:global step .. loss...' while training model,"Hi all!

I'm stuck while trying various examples of object-detection. I try 'coco' net with my own images (and I have only 30 pictures now to train model), also I try this example with [(Faster-RCNN-Inception-V2 model.](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10) model and it's examples images and config. But No one of this models doesn't train! First example [(train)](https://becominghuman.ai/tensorflow-object-detection-api-tutorial-training-and-evaluating-custom-object-detector-ed2594afcf73) I run and wait more than 10 hours! And It's not stop.. Second example also has this problems.

All of this examples I'm running on macOs on MacBook Pro 2017 and I'm always getting only this in logs:

```
WARNING:tensorflow:Ignoring detection with image id 1241141149 since it was previously added

WARNING:tensorflow:Ignoring ground truth with image id 558212937 since it was previously added

WARNING:tensorflow:Ignoring detection with image id 558212937 since it was previously added

WARNING:tensorflow:Ignoring ground truth with image id 1493033516 since it was previously added

WARNING:tensorflow:Ignoring detection with image id 1493033516 since it was previously added

creating index...

index created!

creating index...

index created!

Running per image evaluation...

Evaluate annotation type *bbox*

DONE (t=0.11s).

Accumulating evaluation results...

DONE (t=0.12s).

 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.130

 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.325

 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.113

 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000

 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000

 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.130

 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170

 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.344

 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.421

 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.421
```

I have NO logs like this

```
INFO:tensorflow:global step 11788: loss = 0.6717 (0.398 sec/step)
INFO:tensorflow:global step 11789: loss = 0.5310 (0.436 sec/step)
```

I try to find something about this problem - but no information was found. 
I try to change config values such 'num_examples' (to make it equals of images size), 'batch_size' param - but no one help me.

Can someone tell me why I've got this messages instead of ""normal"" log with loss steps ?",27,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-09-21 10:21:24,open,,,['stat:awaiting response'],2019-02-15 05:17:04
583,tensorflow/models,models,5346,heweijia02,How to train Nasnet on multiple servers,"Tensorflow version :1.10.1
model: Nasnet_large

I have 8 servers, each with 8 GPUs, and I have started training Nasnet_large on one of them. The command used is:
 python train_image_classifier.py  --train_dir=${TRAIN_DIR}  --dataset_dir=${DATASET_DIR} --dataset_name=imagenet --dataset_split_name=train --model_name=nasnet_large --checkpoint_path=${CHECKPOINT_PATH}  --checkpoint_exclude_scopes=final_layer  trainable_scopes=final_layer --batch_size=8 --num_clones=8 --max_number_of_steps=100 --worker_replicas=1 --num_ps_tasks=0 --log_every_n_steps=100


Currently I need to train Nasnet_large on 8 servers (64 GPUs), but I don't know what command to use? How can I get them to run? Github does not describe how to run on multiple servers?",3,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-21 02:23:02,open,,,[],2018-09-25 13:21:22
584,tensorflow/models,models,5344,mpjlu,The dcgan model is unstable with default gradient_penalty_weight = 1 ,"When I test https://github.com/tensorflow/models/blob/master/research/gan/cifar/train.py with the default value.
I found the model is very unstable,  and it is not convergent. 
One of the reason is the model is DCGAN, which is using BN for discriminator, and the default gan_loss using gradient_penalty_weight = 1.0. 
According to the paper: https://arxiv.org/pdf/1704.00028.pdf, it is better not to use BN for gradient_penalty.
My experiment also shows the model is better with gradient_penalty_weight = 0 in this case.

I propose to change   
https://github.com/tensorflow/models/blob/master/research/gan/cifar/train.py:115
`    gan_loss = tfgan.gan_loss(gan_model,
                              gradient_penalty_weight=1.0,
                              add_summaries=True)`
to
    `gan_loss = tfgan.gan_loss(gan_model,
                              gradient_penalty_weight=0,
                              add_summaries=True)`

### System information
- **What is the top-level directory of the model you are using**:
tensorflow/models/blob/master/research/gan
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: EL7
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.8
- **Bazel version (if compiling from source)**:0.15
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:p100
- **Exact command to reproduce**: python train.py

@joel-shor ",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-09-20 12:46:34,open,,,['stat:awaiting response'],2018-09-21 04:54:05
585,tensorflow/models,models,5343,ltung-cit,Training job stuck in ML Engine,"### System information
- **What is the top-level directory of the model you are using**: tensorflow/models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: command issued from Linux Ubuntu 16.04 to run job at Google ML Engine.
- **TensorFlow installed from (source or binary)**: Installed by Google ML Engine.
- **TensorFlow version (use command below)**: 1.8
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: no CUDA
- **GPU model and memory**: no GPU
- **Exact command to reproduce**: `gcloud ml-engine jobs submit training ${JOB_ID}  --runtime-version 1.8 --job-dir=gs://${DATASET_BUCKET}/mini-dataset/trained-models/${JOB_ID}/staging --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,dist/pycocotools-2.0.tar.gz --module-name object_detection.model_main --region us-central1 --config object_detection/samples/cloud/cloud.yml -- --model_dir=gs://${DATASET_BUCKET}/mini-dataset/trained-models/${JOB_ID} --pipeline_config_path=gs://${DATASET_BUCKET}/mini-dataset/plate/v0/ssd_mobilenet_v1_sas.config
`

### Describe the problem
I created an object detection training job to run at Google ML Engine with a quite simple dataset with the command above.

Training configs are defined in the file `ssd_mobilenet_v1_sas.config` (attached as ssd_mobilenet_v1_sas.txt).
Initially I ran a training job with 20000 steps and the job completed successfully in about an hour, then I ran the same command with different JOB_ID and the same config.

Looking at the ML Engine job log, master replica and workers both show that step 20000 was reached in about an hour and the checkpoint saved to GCS. However, the ML Engine job itself didn't finish and the parameter server replica remained active for more than 2 days, incurring ML unit charges.
After 2 days with the job active doing nothing, the parameter server replica logged the message: `Signal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.`
The tensorboard also shows no training progress after reaching 20000 steps.

After contacting with Google Cloud support team, their investigation pointed that there's a problem with the Tensorflow Object Detection code and suggested to file an issue here.

### Source code / logs
Attached images show logs from master, parameter server and worker replicas, as well as tensorboad:

![master_0](https://user-images.githubusercontent.com/11962395/45818243-83bc6880-bcb7-11e8-8413-48470fe17464.png)
![ps_0](https://user-images.githubusercontent.com/11962395/45818244-83bc6880-bcb7-11e8-8a13-39c0e9d265f3.png)
![ps_1](https://user-images.githubusercontent.com/11962395/45818245-83bc6880-bcb7-11e8-8079-4330d1849975.png)
![ps_2](https://user-images.githubusercontent.com/11962395/45818246-8454ff00-bcb7-11e8-8eb1-c9d6ce1b51ac.png)
![tensorboard](https://user-images.githubusercontent.com/11962395/45818247-8454ff00-bcb7-11e8-849a-d37ecb336db5.png)
![worker_0](https://user-images.githubusercontent.com/11962395/45818248-84ed9580-bcb7-11e8-9f66-a1984389a322.png)
![worker_1](https://user-images.githubusercontent.com/11962395/45818249-84ed9580-bcb7-11e8-8021-fc1bfaa22994.png)
![worker_2](https://user-images.githubusercontent.com/11962395/45818252-85862c00-bcb7-11e8-9ac4-53c7d959756f.png)
![worker_3](https://user-images.githubusercontent.com/11962395/45818253-85862c00-bcb7-11e8-9a45-7e93007cf6d8.png)
![worker_4](https://user-images.githubusercontent.com/11962395/45818255-861ec280-bcb7-11e8-858f-f8de296ab5db.png)
[ssd_mobilenet_v1_sas.txt](https://github.com/tensorflow/models/files/2401269/ssd_mobilenet_v1_sas.txt)


",3,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-09-20 12:36:49,open,,,[],2018-11-24 20:12:27
586,tensorflow/models,models,5342,drorsimon,reproduce deeplab results on cityscapes,"Hello,
I'm trying to reproduce the results published on the cityscapes dataset using deeplab model.
I am using a batch size of 12, with a learning_rate of 1e-2 and learning the batch_norm params set to True., but i can't go pass 77% after 90k iterations.
Is there anything not listed in the paper I should be aware of? Any other non-default configuration I should be aware of?

Also, how exactly did you ""fine-tune"" the network on the trainval_coarse set?
What was the learning_rate? How many iterations? Did you also finetune the batch_norm params?

Thanks for the help!

### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Tesla-V100
- **Exact command to reproduce**: 
Code used for training:
python deeplab/train.py \
--logtostderr \
--training_number_of_steps=90000 \
--train_split=""train"" \
--model_variant=""xception_65"" \
--atrous_rates=6 \
--atrous_rates=12 \
--atrous_rates=18 \
--output_stride=16 \
--decoder_output_stride=4 \
--train_crop_size=769 \
--train_crop_size=769 \
--train_batch_size=12 \
--dataset=""cityscapes"" \
--tf_initial_checkpoint=""../pretrained/xception_65/model.ckpt"" \
--train_logdir=""deeplab/datasets/cityscapes/exp/train_on_train_set"" \
--dataset_dir=""deeplab/datasets/cityscapes/tfrecord""\
--num_clones=4 \
--base_learning_rate=0.01 \
--fine_tune_batch_norm=True

Command used for evaluation:
python deeplab/eval.py \
    --logtostderr \
    --eval_split=""val"" \
    --model_variant=""xception_65"" \
    --atrous_rates=6 \
    --atrous_rates=12 \
    --atrous_rates=18 \
    --output_stride=16 \
    --decoder_output_stride=4 \
    --eval_crop_size=1025 \
    --eval_crop_size=2049 \
    --dataset=""cityscapes"" \
    --colormap_type=""cityscapes"" \
    --checkpoint_dir=""deeplab/datasets/cityscapes_coarse/exp/train_on_my_train_set"" \
    --tf_initial_checkpoint=""../pretrained/xception_65/model.ckpt""\
    --eval_logdir=""deeplab/datasets/cityscapes_coarse/exp/eval_on_my_train_set"" \
    --dataset_dir=""deeplab/datasets/cityscapes/tfrecord""
",4,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-09-20 09:38:52,open,,,['stat:awaiting tensorflower'],2019-02-20 19:12:31
587,tensorflow/models,models,5341,aejaex,Detecting+Recognizing Text as an Image,"Tensorflow: Version 1.9.0
Python: 2.7.15
OS: Mac OSX High Sierra

I trained a Fast RCNN model using the object detection API to detect specific regions of text in an unstructured document. Basically, our institute receives over 2000 assignments every week, and my goal is to extract ""Student Number"" or ""Student ID"" from each of these assignments and save them to database. The position of said region within the document can be absolutely random.

I trained the Fast RCNN-Coco model using about ~200 images to detect ""Student Number"" within the document, using 1024x1024 input images where ""Student Number"" had been labeled using LabelImg in the Pascal format and all the xml files converted into tfRecords. I trained for 2000 steps and achieved a loss of lesser than 0.100.

However, my frozen model doesn't detect anything - it fails even on images it was trained on. 

Is this an inherent problem when training a model to detect text as an image? Or is it because documents are not natural images and their features vary from natural(Scene) images? Is this problem not solvable using transfer learning based upon one of the popular, conventional models (Fast RCNN)? If so, how would I go about solving this problem?
",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-20 08:54:08,open,,,[],2018-09-23 19:46:16
588,tensorflow/models,models,5340,SimoneNigro,Added 'rb' as a parameter for FastGFile,"If you do not add 'rb', python will think you're opening a text file, and the program would fail with an UnicodeDecodeError",3,,[],2018-09-20 08:45:56,open,,,['cla: yes'],2018-09-20 09:45:47
589,tensorflow/models,models,5338,mab85,Feature extraction,"I want to extract features for own dataset, so i trained Xception model from scratch to get pretrained model belong to my dataset and i saved the model (checkpoint, .ckpt, .meta). In these following two methods i can't pass my saved model because they only reads .h5py files
`tf.keras.applications.Xception(
    include_top=False,
    weights='imagenet',
   input_tensor=None,
   input_shape=(150, 150, 3),
    pooling=None,
   classes=1000
)
`
OR `conv_base = Xception(weights='imagenet',
include_top=False,
input_shape=(150, 150, 3))`
",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-09-20 06:50:41,open,,,['stat:awaiting response'],2018-09-21 02:37:26
590,tensorflow/models,models,5336,mking1011,Model detects only 1 class,"### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I want to recognize my fist and palm.

So I follow the Tensorflow Object Dection API

But I could see my model recognizing only one palm

I adjusted the number of images in the palm and fist to be the same.

I also modified the label file, generate_tfrecord, ssd_mobilenet_v1_pets.

I want to train the model to detect 2 classes, but after training it recognize only 1 class

I would appreciate it if someone could help.


I will upload the file I modified.
[Desktop.zip](https://github.com/tensorflow/models/files/2399404/Desktop.zip)




### System information
- **What is the top-level directory of the model you are using**: models-master/research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.1
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.1
- **GPU model and memory**: GeForce GTX 1080 Ti / 64GB
- **Exact command to reproduce**: python legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config

",2,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-09-20 02:45:04,open,,,[],2018-11-06 05:11:16
591,tensorflow/models,models,5332,Bedrettin-Cetinkaya,Perspective Transformer Nets shapenet_all training error,"### System information
- **What is the top-level directory of the model you are using**: ptn_directory
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  empty __init__.py file added under ptn_directory and nets directory.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: via pip install
- **TensorFlow version (use command below)**:1.4.1
- **Bazel version (if compiling from source)**: Not installed
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**: Tesla k80, 16 GB
- **Exact command to reproduce**:python train_ptn.py

### Describe the problem
When i changed dataset from shapenet_chair to shapenet_all ( shapenet_chair runs without any error),it gives following error after creating 0.jpg: 

### Source code / logs
2018-09-19 17:00:53.382598: W tensorflow/core/framework/op_kernel.cc:1198] Unknown: exceptions.IndexError: arrays used as indices must be of integer (or boolean) type
Traceback (most recent call last):
  File ""/truba/home/bcetinkaya/ptn/ptn_orig/train_ptn.py"", line 231, in <module>
    app.run()
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""/truba/home/bcetinkaya/ptn/ptn_orig/train_ptn.py"", line 227, in main
    save_interval_secs=FLAGS.save_interval_secs)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 782, in train
    ignore_live_threads=ignore_live_threads)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 826, in stop
    ignore_live_threads=ignore_live_threads)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 387, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 295, in stop_on_exception
    yield
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 492, in run
    self.run_loop()
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 1028, in run_loop
    self._sv.global_step])
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1344, in _do_run
    options, run_metadata)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: exceptions.IndexError: arrays used as indices must be of integer (or boolean) type
         [[Node: PyFunc = PyFunc[Tin=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT], Tout=[DT_UINT8], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](mul_3, ResizeNearestNeighbor, ResizeNearestNeighbor_1, global_step/read, data_loading_shapenet_all/val/batching_queues/shapenet_all/val:1, decoder_1/Conv3d_transpose_2/Sigmoid/_523)]]

Caused by op u'PyFunc', defined at:
  File ""/truba/home/bcetinkaya/ptn/ptn_orig/train_ptn.py"", line 231, in <module>
    app.run()
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""/truba/home/bcetinkaya/ptn/ptn_orig/train_ptn.py"", line 202, in main
    output_voxels=val_outputs['voxels_1'])
  File ""/truba/home/bcetinkaya/ptn/ptn_orig/model_ptn.py"", line 147, in write_disk_grid
    ], [tf.uint8], 'write_grid')[0]
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/ops/script_ops.py"", line 300, in py_func
    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/ops/script_ops.py"", line 209, in _internal_py_func
    input=inp, token=token, Tout=Tout, name=name)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_script_ops.py"", line 93, in _py_func
    ""PyFunc"", input=input, token=token, Tout=Tout, name=name)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/truba/home/bcetinkaya/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

UnknownError (see above for traceback): exceptions.IndexError: arrays used as indices must be of integer (or boolean) type
         [[Node: PyFunc = PyFunc[Tin=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT], Tout=[DT_UINT8], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](mul_3, ResizeNearestNeighbor, ResizeNearestNeighbor_1, global_step/read, data_loading_shapenet_all/val/batching_queues/shapenet_all/val:1, decoder_1/Conv3d_transpose_2/Sigmoid/_523)]]


",5,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-19 16:49:36,open,,,[],2019-01-06 22:05:07
592,tensorflow/models,models,5331,lczazu,ImportError: No module named '_pywrap_tensorflow_internal',"## I use python in different directories, it also doesn't work

+ platform: macOS High Sierra 10.13.6
+ TensorFlow installed from : anaconda
+ TensorFlow version: 1.8 or 1.6 ( I forget )
+ python version: 3.6 
+ Exact command to reproduce: import tensorflow as tf

Source code / logs:
```python
Traceback (most recent call last):
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/chengzhilin/Desktop/\u673a\u5668\u5b66\u4e60/tensorflow\u673a\u5668\u5b66\u4e60/1.py"", line 1, in <module>
    import tensorflow as tf
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/chengzhilin/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
````

## I use python in different directories, it also doesn't work
",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-09-19 10:21:29,open,,,['stat:awaiting response'],2018-09-25 20:39:32
593,tensorflow/models,models,5329,xm1112,【deeplab】InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [320] rhs shape= [2048],"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-19 06:45:03,open,,,[],2018-09-25 19:09:03
594,tensorflow/models,models,5328,wenouyang,[Object-detection] the error message of \src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_driver.cc:1108] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED ::,"When training the Google object-detection model, the training process was broken with the following error message, what can be the underlying reason?

```
INFO:tensorflow:global step 16171: loss = 2.1875 (1.656 sec/step)
INFO:tensorflow:global step 16171: loss = 2.1875 (1.656 sec/step)
INFO:tensorflow:global step 16172: loss = 2.5936 (1.560 sec/step)
INFO:tensorflow:global step 16172: loss = 2.5936 (1.560 sec/step)
2018-09-18 15:21:34.702531: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_driver.cc:1108] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED ::
2018-09-18 15:21:34.702567: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED
2018-09-18 15:21:34.712913: F T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_event_mgr.cc:206] Unexpected Event status: 1


```",3,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-09-19 04:26:22,open,,,"['models: research', 'stat:awaiting response']",2019-02-06 21:41:52
595,tensorflow/models,models,5327,andypang,"""Predict house prices: regression"" tutorial has racist content","https://www.tensorflow.org/tutorials/keras/basic_regression

<img width=""652"" alt=""screen shot 2018-09-18 at 5 58 48 pm"" src=""https://user-images.githubusercontent.com/8040481/45718914-b443bb80-bb6c-11e8-8512-fd0c4732f2d6.png"">

This is not right.",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-09-18 22:08:09,open,,,['stat:awaiting response'],2018-09-19 08:05:52
596,tensorflow/models,models,5325,Dingzixiang,A problem about the NASNet: used_hiddenstates,"Hello,  there is a question about the nasnet. In the Fig.4 of  ""Learning Transferable Architectures for Scalable Image Recognition"", the node 2-6 is been concatenated for normal cell obviously, but the class of NasNetANormalCell in nasnet_utils.py, why ""used_hiddenstates = [1, 0, 0, 0, 0, 0, 0]""? Similarly, in  the class of NasNetAReductionCell, why ""used_hiddenstates = [1, 1, 1, 0, 0, 0, 0]""?
Look forward to your reply!",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-18 10:48:23,open,,,['stat:awaiting response'],2018-09-18 19:49:07
597,tensorflow/models,models,5323,Art31,"Training from checkpoint issue: lots of ""not found"" and ""root variable not available"" errors","
### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Stock example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro 64 bits
- **TensorFlow installed from (source or binary)**: binary - ""pip install --ignore-installed --upgrade tensorflow"" with a new env in anaconda
- **GPU model and memory**: 16Gb RAM and AMD Ryzen 7 1700 (CPU)
- **Tensorflow version**: 1.10.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CPU version
- **GPU model and memory**: 16Gb RAM and AMD Ryzen 7 1700 (CPU)
- **Exact command to reproduce**:  python train.py --logtostderr --train_dir=ssdlite_mobilenet_v2_coco_2018_05_09/ --pipeline_config_path=ssdlite_mobilenet_v2_coco_2018_05_09/ssdlite.config

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. (I am not 100% sure whether this is a bug or just a wrong setup, but I have trained networks using tensorflow before.)

I have downloaded the pretrained network (ssdlite_mobilenet_v2_coco) and used the default checkpoint from: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md 

After properly creating the train and test.record files from my image dataset and setting the configuration file, I executed the following command:

python train.py --logtostderr --train_dir=ssdlite_mobilenet_v2_coco_2018_05_09/ --pipeline_config_path=ssdlite_mobilenet_v2_coco_2018_05_09/ssdlite.config

But a lot of ""not found"" and ""root variable not available"" errors popped up, halting the process. The detailed log is attached below.

I don't know if it helps much to mention, but I have this same repo in my google drive and tried to execute the command using google codelab, but I encountered the same errors. I've attached the error log, the config file I used, the result from the environment capture script and the print of my training directory.

Also guys I really appreciate the help given, please if there is anything I can improve in my description I will be happy to do it, since I am depending on this project to finish my bachelors 🙏  

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Error log after issuing command:
[error_log.txt](https://github.com/tensorflow/models/files/2390634/error_log.txt)

Environment capture script result:
[tf_env.txt](https://github.com/tensorflow/models/files/2390635/tf_env.txt)

Config file:
[ssdlite.txt](https://github.com/tensorflow/models/files/2390655/ssdlite.txt)

Print of training directory content:
![training_folder](https://user-images.githubusercontent.com/34986200/45657092-3a092d80-babf-11e8-87d4-1b660b9797e8.JPG)
",4,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-09-18 00:45:24,open,,,[],2018-09-21 01:11:32
598,tensorflow/models,models,5322,atalreja,Spelling fix for keypointnet in README.md,,3,,[],2018-09-17 23:21:05,open,,,['cla: yes'],2018-09-17 23:24:51
599,tensorflow/models,models,5319,francoisCat,Unale to run  local-test.sh demo ...  Resource exhausted ,"------------------------
### System information
- **What is the top-level directory of the model you are using**:
deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\
ubuntu
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.5.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.2 / 7.1
- **GPU model and memory**:
GeForce GTX 1080 
- **Exact command to reproduce:
/research/deeplab$ sh local_test.sh

Hello,

fist of all thank you very much for helping me with this problem. I m trying to test/evaluate deeplab V3+ on my computer. However when I run the local-test.sh demo/evaluation script I encountered the following error which I quite don't understand:

2018-09-17 16:15:15.408809: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats: 
Limit:                  7382571418
InUse:                  7211576320
MaxInUse:               7266935808
NumAllocs:                    3473
MaxAllocSize:            934797312

2018-09-17 16:15:15.408861: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ****************************************************************************************************
2018-09-17 16:15:15.408877: W tensorflow/core/framework/op_kernel.cc:1198] Resource exhausted: OOM when allocating tensor with shape[4,129,129,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
INFO:tensorflow:Error reported to Coordinator: OOM when allocating tensor with shape[4,129,129,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
\

Is it due to some limitation of my graphic card or did I mess a step during the installation of tensor flow and when installing deeplab lib requirements.


Thank you again for your support and help.

Francois",2,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-09-17 14:26:49,open,,,['stat:awaiting tensorflower'],2018-09-22 00:15:51
600,tensorflow/models,models,5317,woolfel,consider not using pycocotools since it doesn't work on windows,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

_== cat /etc/issue ===============================================
MINGW64_NT-10.0 Beast 2.6.0(0.304/5/3) 2016-09-09 09:46 x86_64 Msys

== are we in docker =============================================
No

== compiler =====================================================
bash: c++: command not found

== uname -a =====================================================
MINGW64_NT-10.0 Beast 2.6.0(0.304/5/3) 2016-09-09 09:46 x86_64 Msys

== check pips ===================================================
numpy              1.14.2   
protobuf           3.6.1    
tensorflow         1.7.0    
tensorflow-gpu     1.10.0   
tensorflow-hub     0.1.1    

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = b'v1.10.0-rc1-19-g656e7a2b34'
tf.COMPILER_VERSION = b'v1.10.0-rc1-19-g656e7a2b34'
Sanity check: array([1])_

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
the current model_main.py script for object detection uses pycoco and pycocotools. There is a known bug in pycocotools and the maintainers of that project have no plans to support windows. This means anyone using windows has to use the legacy.trainer.py instead. On MacOS model_main.py works fine. Until pycocotools maintainers decide to support windows, I suggest not using pycocotools or at least let users know it only works on Mac and Linux.

### Source code / logs


The exact issue with pycocotools can be found at this link. https://github.com/cocodataset/cocoapi/issues/169
The error when you try to run pip install pycocotools on windows.

  Running setup.py clean for pycocotools
Failed to build pycocotools
Installing collected packages: pycocotools
  Running setup.py install for pycocotools ... error
    Complete output from command d:\python\python36\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\P
ETERL~1\\AppData\\Local\\Temp\\pip-install-qwoxf3_t\\pycocotools\\setup.py';f=getattr(tokenize, 'open', open)(__file__);
code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\PETERL~1\
AppData\Local\Temp\pip-record-nkxviuzj\install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.6
    creating build\lib.win-amd64-3.6\pycocotools
    copying pycocotools\coco.py -> build\lib.win-amd64-3.6\pycocotools
    copying pycocotools\cocoeval.py -> build\lib.win-amd64-3.6\pycocotools
    copying pycocotools\mask.py -> build\lib.win-amd64-3.6\pycocotools
    copying pycocotools\__init__.py -> build\lib.win-amd64-3.6\pycocotools
    running build_ext
    building 'pycocotools._mask' extension
    creating build\temp.win-amd64-3.6
    creating build\temp.win-amd64-3.6\Release
    creating build\temp.win-amd64-3.6\Release\pycocotools
    creating build\temp.win-amd64-3.6\Release\common
    C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\bin\HostX86\x64\cl.exe /c /n
ologo /Ox /W3 /GL /DNDEBUG /MD -Id:\python\python36\lib\site-packages\numpy\core\include -Icommon -Id:\python\python36\i
nclude -Id:\python\python36\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14
.26428\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\includ
e"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\1
0.0.17134.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared"" ""-IC:\Program Files (x86)\Windo
ws Kits\10\include\10.0.17134.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt"" ""-IC:\Program
Files (x86)\Windows Kits\10\include\10.0.17134.0\cppwinrt"" /Tcpycocotools/_mask.c /Fobuild\temp.win-amd64-3.6\Release\py
cocotools/_mask.obj -Wno-cpp -Wno-unused-function -std=c99
    cl : Command line error D8021 : invalid numeric argument '/Wno-cpp'
    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.14.26428\\bin
\\HostX86\\x64\\cl.exe' failed with exit status 2

    ----------------------------------------",5,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-09-17 10:55:49,open,,,[],2019-02-11 22:55:01
601,tensorflow/models,models,5316,tdf1995,"object detection api,how to display the global step and total loss during training","
",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-09-17 09:17:35,open,,,"['models: research', 'stat:awaiting response']",2019-02-06 22:19:46
602,tensorflow/models,models,5315,yimintsai,[Object detection] Mismatch between pretrained model and configs for ssd_mobilenet_v2_coco,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: from source
- **TensorFlow version (use command below)**: 1.9.0
- **Bazel version (if compiling from source)**: 0.10.1
- **CUDA/cuDNN version**: CUDA 9.0/cuDNN 7
- **GPU model and memory**: 1080Ti
- **Exact command to reproduce**: 
training by setting pipeline config with 
fine_tune_checkpoint: ""ssd_mobilenet_v2_coco_2018_03_29/model.ckpt""
fine_tune_checkpoint_type: ""detection""


### Describe the problem
From object detection model zoo, the pretrained ssd_mobilenet_v2_coco (**ssd_mobilenet_v2_coco_2018_03_29**) has different network structure as that described in samples/configs/ssd_mobilenet_v2_coco.config. 
The pretrained **ssd_mobilenet_v2_coco_2018_03_29/pipeline.config** uses **depthwise conv** for feature extractor, which is similar to ssdlite. However, **the samples/configs/ssd_mobilenet_v2_coco.config** uses standard conv2d for feature extractor. This mismatch leads to incompatible shape while using ssd_mobilenet_v2_coco_2018_03_29 as the initial finetune ckpt.

### Source code / logs
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights] is available in checkpoint, but has an incompatible shape with model variable.
",10,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-17 07:25:15,open,,,[],2019-03-12 15:22:02
603,tensorflow/models,models,5311,amirjamez,Low Inference accuracy after training alexnet_v2 using tf.slim - Misses a proper preprocessing script,"System information:
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 14.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): r1.8
Python version: 2.7
Bazel version (if compiling from source): bazel release 0.13.0
GCC/Compiler version (if compiling from source): 4.8
CUDA/cuDNN version: 8/7
GPU model and memory: NVIDIA-1060


I have trained `alexnet_v2` available in `tf.slim` by manually making a preprocessing file based on #423 suggestion. However, after 200K step the accuracy is almost 0:

```
2018-09-15 13:15:04.595764: I tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_4[0.004]
2018-09-15 13:15:04.595803: I tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0.001]
2018-09-15 13:15:04.596105: I tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_5[0.005]
2018-09-15 13:15:04.596198: I tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_3[0.003]
```

Here is the preprocessing file I manually put in `preprocessing/alexnet_preprocessing.py`:

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

_PADDING = 4
slim = tf.contrib.slim

def preprocess_image(image, output_height, output_width, is_training=False):
  resized_image = tf.image.resize_image_with_crop_or_pad(image, output_width, output_height)
  tf.summary.image('image', tf.expand_dims(image, 0))
  return tf.image.per_image_standardization(resized_image)
```

Does it need some missing optimization functions? Thanks! @snnn @yifeif  @abes975 @sguada @jmchen-g",12,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-09-14 22:57:48,open,,"NamedUser(login=""amirjamez"")",['type:feature'],2019-01-09 16:59:49
604,tensorflow/models,models,5310,diamondcm,R1.5,,1,,[],2018-09-14 03:29:34,open,,,['cla: no'],2018-09-14 03:29:38
605,tensorflow/models,models,5304,pksubbarao,"tflogging.py reports ""TypeError: not all arguments converted during string formatting"" during transformer evaluation.","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: ~/models/official/transformer
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Rhel 7.5
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.2.88-1 / 7.0.4
- **GPU model and memory**: 4xTesla P100, 16GB
- **Exact command to reproduce**: python transformer_main.py --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --vocab_file=$VOCAB_FILE --param_set=$PARAM_SET --bleu_source=test_data/newstest2014.en --bleu_ref=test_data/newstest2014.de

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

After training for an epoch (or # of steps), during evaluation, tf.logging.py throws error  - ""TypeError: not all arguments converted during string formatting"". Due to this, not able to get bleu score from evaluation.

I0913 12:57:17.766772 70366521413712 tf_logging.py:116] Writing to file /tmp/tmp6MU5MO
Traceback (most recent call last):
  File ""/home/prashant/anaconda2/lib/python2.7/logging/__init__.py"", line 861, in emit
    msg = self.format(record)
  File ""/home/prashant/anaconda2/lib/python2.7/logging/__init__.py"", line 734, in format
    return fmt.format(record)
  File ""/home/prashant/anaconda2/lib/python2.7/site-packages/absl/logging/__init__.py"", line 818, in format
    return prefix + super(PythonFormatter, self).format(record)
  File ""/home/prashant/anaconda2/lib/python2.7/logging/__init__.py"", line 465, in format
    record.message = record.getMessage()
  File ""/home/prashant/anaconda2/lib/python2.7/logging/__init__.py"", line 329, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Logged from file tf_logging.py, line 116
Traceback (most recent call last):
  File ""/home/prashant/anaconda2/lib/python2.7/logging/__init__.py"", line 861, in emit
    msg = self.format(record)
  File ""/home/prashant/anaconda2/lib/python2.7/logging/__init__.py"", line 734, in format
    return fmt.format(record)
  File ""/home/prashant/anaconda2/lib/python2.7/site-packages/absl/logging/__init__.py"", line 818, in format
    return prefix + super(PythonFormatter, self).format(record)
  File ""/home/prashant/anaconda2/lib/python2.7/logging/__init__.py"", line 465, in format
    record.message = record.getMessage()
  File ""/home/prashant/anaconda2/lib/python2.7/logging/__init__.py"", line 329, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Logged from file tf_logging.py, line 116
I0913 12:57:24.777055 70366521413712 tf_logging.py:116] Benchmark metric: {'name': 'bleu_uncased', 'timestamp': '2018-09-13T17:57:24.777002Z', 'value': 19.70687061548233, 'extras': [], 'unit': None, 'global_step': 312000}
I0913 12:57:24.777210 70366521413712 tf_logging.py:116] Benchmark metric: {'name': 'bleu_cased', 'timestamp': '2018-09-13T17:57:24.777191Z', 'value': 19.149336218833923, 'extras': [], 'unit': None, 'global_step': 312000}
",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-13 18:19:31,open,,,[],2018-09-25 20:50:30
606,tensorflow/models,models,5303,bleqdyce,[Feature Request] A clear way to control freqency of saving checkpoint and evaluation,"### System information
- **What is the top-level directory of the model you are using**: research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Docker on Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: In docker, tensorflow/tensorflow:1.10.0-devel-gpu
- **TensorFlow version (use command below)**: 1.10.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**:  V9.0.176 / 7
- **GPU model and memory**:  GeForce GTX 1060 
- **Exact command to reproduce**: N/A

### Describe the problem
The **eval_interval_secs** in the eval.proto doesn't work in estimator-based training and I found out that it will works if you pass eval_interval_secs to EvalSpec.throttle_secs. Therefore I send a PR #5144. 

However, https://github.com/tensorflow/tensorflow/commit/3edb609926f2521c726737fc1efeae1572dc6581#diff-bc4a1638bbcd88997adf5e723b8609c7 has been merged in TensorFlow 1.10 and it change the way to customize the frequency of  saving checkpoint. 

For now, if you want to change the frequency of saving checkpoint, **RunConfig.save_checkpoints_secs** and **RunConfig.save_checkpoints_steps** is much prefered according to [estimator/training.py#L672](https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/estimator/training.py#L672), and **EvalSpec.throttle_secs** does not define the frequency of  saving checkpoint anymore. It only define the minimum time interval of evaluation. For more detail, I've done some summary in https://github.com/tensorflow/models/issues/5139#issuecomment-418963839

Currently, we don't have a proper way to customize the behavior of saving checkpoint and evaluation in estimator-based training.  I think the ability to configure the behavior of saving checkpoint and evaluation is pretty important because I've suffered from the memory leak #5139 for a while. By changing the frequency of evaluation, I can prevent my training process from being killed while working.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-09-13 09:19:57,open,,,['type:feature'],2018-09-14 07:16:13
607,tensorflow/models,models,5302,mehditlili,graph.pbtxt missing in maskrcnn models,"I want to compile those models using tfcompile but I need the graph proto file for that. Is there a reason why it was not uploaded for maskrcnn?

https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-09-13 08:42:50,open,,,[],2018-09-14 19:30:24
608,tensorflow/models,models,5300,Akhtar303nu,Converting TF Model(frozen_graph) to TensorRT Engine,"import keras
import keras.backend as K
import tensorflow as tf
import uff

output_names = ['predictions/Softmax']
frozen_graph_filename = 'frozen_inference_graph.pb'
sess = K.get_session()

# freeze graph and remove training nodes
graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_names)
graph_def = tf.graph_util.remove_training_nodes(graph_def)

# write frozen graph to file
with open(frozen_graph_filename, 'wb') as f:
f.write(graph_def.SerializeToString())
f.close()

# convert frozen graph to uff
uff_model = uff.from_tensorflow_frozen_model(frozen_graph_filename, output_names)
G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)
parser = uffparser.create_uff_parser()
parser.register_input(""Placeholder"", (1,28,28), 0)
parser.register_output(""fc2/Relu"")
engine = trt.utils.uff_to_trt_engine(G_LOGGER, uff_model, parser, 1, 1 << 20)
parser.destroy()
runtime = trt.infer.create_infer_runtime(G_LOGGER)
context = engine.create_execution_context()
output = np.empty(10, dtype = np.float32)

# Alocate device memory
d_input = cuda.mem_alloc(1 * img.nbytes)
d_output = cuda.mem_alloc(1 * output.nbytes)

bindings = [int(d_input), int(d_output)]
stream = cuda.Stream()
# Transfer input data to device
cuda.memcpy_htod_async(d_input, img, stream)
# Execute model
context.enqueue(1, bindings, stream.handle, None)
# Transfer predictions back
cuda.memcpy_dtoh_async(output, d_output, stream)
# Syncronize threads
stream.synchronize()
print(""Test Case: "" + str(label))
print (""Prediction: "" + str(np.argmax(output)))
trt.utils.write_engine_to_file(""./tf_mnist.engine"", engine.serialize())



list of pakages
Linux:16
Cuda:9.0
tensorRt:4
Python 3.5
Gpu:Gtx 1080
Tensorflow:1.10.1


Error

Traceback (most recent call last):
File ""bbb.py"", line 11, in <module>
graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_names)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/graph_util_impl.py"", line 232, in convert_variables_to_constants
inference_graph = extract_sub_graph(input_graph_def, output_node_names)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/graph_util_impl.py"", line 174, in extract_sub_graph
_assert_nodes_are_present(name_to_node, dest_nodes)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/graph_util_impl.py"", line 133, in _assert_nodes_are_present
assert d in name_to_node, ""%s is not in graph"" % d
AssertionError: predictions/Softmax is not in graph




OR please suggest me any code which convert tensorflow frozen graph(frozen_inference_graph.pb) to trt engine for object detection task",8,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-09-13 06:26:53,open,,,[],2018-11-10 15:53:27
609,tensorflow/models,models,5298,FreestylePocker,object detection api toco model conversion problem,"### System information
- **What is the top-level directory of the model you are using**: research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.0
- **Bazel version (if compiling from source)**: 0.16.1
- **CUDA/cuDNN version**: 9.2/7.2
- **GPU model and memory**: GeForce GTX 970 4G
- **Exact command to reproduce**: 
```
bazel run --config=opt tensorflow/contrib/lite/toco:toco -- \
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,640,640,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops
```

### Describe the problem
i am trying to create a fully quantized tflite model for inference

while trained this model from scratch with custom dataset there was a problem related to https://github.com/tensorflow/models/issues/5139 but i used a workaround to increase eval delay and restarted process few times so this problem was just slowed down training process

finnaly model was trained and works fine with .pb file created by  `export_inference_graph.py`

to create tflite file i followed this instructions https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md
```
object_detection/export_tflite_ssd_graph.py \
--pipeline_config_path=$CONFIG_FILE \
--trained_checkpoint_prefix=$CHECKPOINT_PATH \
--output_directory=$OUTPUT_DIR \
--add_postprocessing_op=true
```
exported tflite_graph.pb without a problem

but when converting it to the tflite with toco it crashes:
```
bazel run --config=opt tensorflow/contrib/lite/toco:toco -- \
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,640,640,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops
```

results in
`tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:116] Check failed: dim_x == dim_y (256 vs. 24)Dimensions must match `


### Source code / logs
**toco log:**
```
2018-09-13 05:07:46.154129: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: TFLite_Detection_PostProcess                                                                                                                                      
2018-09-13 05:07:46.361651: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1992 operators, 2969 arrays (0 quantized)                                                                                                       
2018-09-13 05:07:46.429271: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1992 operators, 2969 arrays (0 quantized)                                                                                             
2018-09-13 05:07:50.926104: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:116] Check failed: dim_x == dim_y (256 vs. 24)Dimensions must match                                                                                                             
Emergency stop (memory stack is flushed to disk)
```

**export_tflite_ssd_graph.py log:**
```
2018-09-13 05:03:51.397799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero                                                               
2018-09-13 05:03:51.398192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:                                                                                                                                                                      
name: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.367                                                                                                                                                                                                                       
pciBusID: 0000:04:00.0                                                                                                                                                                                                                                                                    
totalMemory: 3.95GiB freeMemory: 3.88GiB                                                                                                                                                                                                                                                  
2018-09-13 05:03:51.398208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0                                                                                                                                                                        
2018-09-13 05:03:51.637059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:                                                                                                                                       
2018-09-13 05:03:51.637107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0                                                                                                                                                                                                
2018-09-13 05:03:51.637115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N                                                                                                                                                                                                
2018-09-13 05:03:51.637295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3607 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:04:00.0, compute capability: 5.2)   
2018-09-13 05:03:55.605093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0                                                                                                                                                                        
2018-09-13 05:03:55.605145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:                                                                                                                                       
2018-09-13 05:03:55.605162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0                                                                                                                                                                                                
2018-09-13 05:03:55.605168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N                                                                                                                                                                                                
2018-09-13 05:03:55.605283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3607 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:04:00.0, compute capability: 5.2)   
2018-09-13 05:03:57.218987: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes
```

**config:**
```
# SSD with Resnet 50 v1 FPN feature extractor, shared box predictor and focal
model {
  ssd {
    inplace_batchnorm_update: true
    freeze_batchnorm: false
    num_classes: 2
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    encode_background_as_zeros: true
    anchor_generator {
      multiscale_anchor_generator {
        min_level: 3
        max_level: 7
        anchor_scale: 4.0
        aspect_ratios: [1.0, 2.0, 0.5]
        scales_per_octave: 2
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 640
        width: 640
      }
    }
    box_predictor {
      weight_shared_convolutional_box_predictor {
        depth: 256
        class_prediction_bias_init: -4.6
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            random_normal_initializer {
              stddev: 0.01
              mean: 0.0
            }
          }
          batch_norm {
            scale: true,
            center: true,
            train: true,
            decay: 0.97,
            epsilon: 0.001,
          }
        }
        num_layers_before_predictor: 4
        kernel_size: 3
      }
    }
    feature_extractor {
      type: 'ssd_resnet50_v1_fpn'
      fpn {
        min_level: 3
        max_level: 7
      }
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          scale: true,
          center: true,
          decay: 0.97,
          epsilon: 0.001,
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    loss {
      classification_loss {
        weighted_sigmoid_focal {
          alpha: 0.25
          gamma: 2.0
        }
      }
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    normalize_loc_loss_by_codesize: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  batch_size: 1
  sync_replicas: true
  startup_delay_steps: 0
  replicas_to_aggregate: 1
  num_steps: 400000
  data_augmentation_options {
    random_rgb_to_gray {
      probability: 0.75
    }
  }
  data_augmentation_options {
    random_adjust_brightness {
    }
  }
  data_augmentation_options {
    random_adjust_contrast {
    }
  }
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: .005
          total_steps: 400000
          warmup_learning_rate: .0001
          warmup_steps: 1000
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}

train_input_reader: {
  tf_record_input_reader {
    input_path: train.record
  }
  label_map_path: label_map.pbtxt
}

eval_config: {
  use_moving_averages: false
  num_examples: 213
  metrics_set: ""coco_detection_metrics""
  eval_interval_secs: 300
  max_evals: 100
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: eval.record
  }
  label_map_path: label_map.pbtxt
  shuffle: false
  num_readers: 1
}

graph_rewriter {
  quantization {
    delay: 0
    activation_bits: 8
    weight_bits: 8
  }
}
```
",25,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel""), NamedUser(login=""achowdhery"")]",2018-09-12 21:22:52,open,,,['comp:lite'],2019-03-15 01:50:15
610,tensorflow/models,models,5297,mking1011,Does it work with TF 1.7.1 or TF 1.4?,"### Describe the problem

I worked under the following conditions.

But I want to work with another version of Tensorflow

Does it work with TF 1.7.1 or TF 1.4?


### System information
- **What is the top-level directory of the model you are using**: models-master/research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.1
- **GPU model and memory**: GeForce GTX 1080 Ti / 64GB
- **Exact command to reproduce**: python legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config


### Source code / logs
NA
",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-12 12:49:41,open,,,[],2018-09-17 05:09:34
611,tensorflow/models,models,5296,jonbakerfish,Tensorflow Object Detection API - SSD Continuously Increasing RAM Usage during Training,"### System information
- **What is the top-level directory of the model you are using**:  models/research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.1
- **GPU model and memory**: GeForce GTX 1080 Ti / 12GB
- **Exact command to reproduce**: python object_detection/model_main.py --pipeline_config_path=${PIPELINE_CONFIG_PATH}  --model_dir=${MODEL_DIR} --alsologtostderr

### Describe the problem
I use the object detection API to train different models (ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync, ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync, faster_rcnn_resnet50_coco) on my own dataset. The faster rcnn can run without problem, but the ssd models will continuously increasing RAM usage during training and finally got OOM. There is [a similar problem on SO](https://stackoverflow.com/questions/50516424/tensorflow-object-detection-api-continuously-increasing-ram-usage-during-train).

### Source code / logs
NA
",17,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-09-12 09:48:11,open,,,[],2019-03-25 20:53:36
612,tensorflow/models,models,5295,blancyin,How to export wide&deep model in numpy ?,"How to export wide&deep model in numpy ?
Or some format human can read.
I am a beginner of TensorFlow，
Thanks. ",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-09-12 09:45:27,open,,,['stat:awaiting response'],2018-09-12 19:41:09
613,tensorflow/models,models,5294,cjr0106,any one know how to build owner mobilenetv2-ssd model for object_detection,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-09-12 08:40:25,open,,,[],2018-09-12 19:40:46
614,tensorflow/models,models,5288,OtakuSenpai,Please create the Java version of word2vec tutorial!! Plus improve docs,"### System information
- **What is the top-level directory of the model you are using**: None, I can't make a demo even
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Void Linux, latest
- **TensorFlow installed from (source or binary)**: Tried the source as well as binary
- **TensorFlow version (use command below)**: 1.10.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**:

### Describe the problem
 I want to create a natural language parser, one that emits grammerly correct words from a dataset.
I tried the repos for tensorflow in java but they were only for images. Also I tried the tutorials on word2vec but they were in python. Also documentation such as
 Session sess = new Session.....
 sess.runner().feed() 
This feed() function requires some operation parameters, which aren't stated in the docs. 
Thank you.",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-11 22:33:49,open,,,[],2018-09-12 07:32:52
615,tensorflow/models,models,5287,yaeldekel,License in Mobilenet models tar file is missing,"In the tar files in models/research/object_detection/g3doc/detection_model_zoo.md there is no license file. Can these models be redistributed? I found a license file in the models/ folder, but it was unclear to me whether it applies to the object detection models as well.
Thanks!",4,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-09-11 20:28:25,open,,,[],2018-09-21 23:17:30
616,tensorflow/models,models,5281,DehuaTang,(deeplabV3 plus)  the function of get_branch_logits ?,"1.System information

the top-level directory of the model **: /home/yswang/models3/research/deeplab
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): tensorflow-gpu 1.8.0
Bazel version (if compiling from source): None.
CUDA/cuDNN version: 9 7.
GPU model and memory: Titan xp / 24GB / 2 GPU
Exact command to reproduce:
for the first 30k iterations :
NUM_ITERATIONS=30000
CUDA_VISIBLE_DEVICES=2,3 python ""${WORK_DIR}""/train.py
--logtostderr
--train_split=""trainaug""
--model_variant=""mobilenet_v2""
--atrous_rates=6
--atrous_rates=12
--atrous_rates=18
--output_stride=16
--decoder_output_stride=4
--train_crop_size=512
--train_crop_size=512
--num_clones=2
--train_batch_size=32
--dataset=""pascal_voc_seg""
--training_number_of_steps=""${NUM_ITERATIONS}""
--fine_tune_batch_norm=True
--tf_initial_checkpoint=""${INIT_FOLDER}/mobilenet_v2/mobilenet_v2_1.0_224.ckpt""
--train_logdir=""${TRAIN_LOGDIR}/gpu_test/20""
--dataset_dir=""${PASCAL_DATASET}""

Describe the problem
I trained the deeplabv3+ models with the mobilenet_v2_1.0_224 versions.
I noticed the code in **model.py**, the function **def get_branch_logits**  used **so many 1*1 conv2d**
to form the final logits . 
            
        branch_logits.append(
            slim.conv2d(
                features,
                num_classes,
                kernel_size=kernel_size,
                rate=rate,
                activation_fn=None,
                normalizer_fn=None,
                scope=scope))

      return tf.add_n(branch_logits)

Can you explain it to me?
I think **only one 1*1 conv2d** can form the final logits . 
**In my experiment，using only one 1*1 conv2d can get better performance than the code.**
How about you ? Is there any difference for the performance in your experiment ? 
Waiting for your replay !",3,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-11 02:43:25,open,,,[],2018-09-17 07:20:36
617,tensorflow/models,models,5279,shinstra,"Mobilenet width multiplier misleading, updated documentation request.","While trying to understand how the width multiplier parameter is used I noticed that the number of channels expected in each convloutional block doesn't match what is shown in the graphs for multipliers 1.3 and 1.4 (for example round(1.4 * 32) = 45 but the graph says 48). In the code there is an operation `_make_divisible` in conv_blocks.py that rounds the multiplier up to the nearest value divisible by 8. This effectively makes 1.3 equivalent to 1.25 and 1.4 equivalent to 1.5. 

This is a bit misleading - particularly as the naming conventions use the values of 1.3 and 1.4. It may be helpful if this rounding was more visibly documented and naming conventions of the checkpoints updated to reflect it.       

### System information
- **What is the top-level directory of the model you are using**: 
[tensorflow/models/research/slim/nets/mobilenet](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.6.0 (but cross-referencing with git source at commit 0a16112 11Sept2018)
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0.61
- **GPU model and memory**: GTX 1080 Ti
- **Exact command to reproduce**: N/A",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-09-10 23:20:35,open,,,['stat:awaiting response'],2018-09-11 22:07:03
618,tensorflow/models,models,5275,jadhm,Problem with export_inference_graph() ,"system information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
TensorFlow version (use command below): tensorflow-gpu 1.9.0 
GPU model and memory:Quadro M1000M , 4 Gb
Bazel version: NA
CUDA/cuDNN version:NA
Exact command to reproduce: NA
Mobile device : NA


```
python export_inference_graph.py \
    --input_type image_tensor \
    --pipeline_config_path training/ssd_mobilenet_v1_coco.config \
    --trained_checkpoint_prefix train/model.ckpt-266\
    --output_directory learned_model
```

**### Describe the problem**

I am trying to train a ssd_mobilenet_v1_coco.config  and after I finished training I got the last model.ckpt-xxx and then I wanted to use  `export_inference_graph.py ` but I am getting this error : 
**TypeError: export_inference_graph() got an unexpected keyword argument 'input_shape'**

and in details this is the error : 
python export_inference_graph.py     --input_type image_tensor0     --pipeline_config_path training/ssd_mobilenet_v1_coco.config     --trained_checkpoint_prefix train/model.ckpt-0    --output_directory learned_model 
/home/jad/.local/lib/python2.7/site-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._conv import register_converters as _register_converters
/home/jad/.local/lib/python2.7/site-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z
/home/jad/.local/lib/python2.7/site-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/lil.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _csparsetools
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._tools import csgraph_to_dense, csgraph_from_dense,\
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:167: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._traversal import breadth_first_order, depth_first_order, \
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:169: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._min_spanning_tree import minimum_spanning_tree
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:170: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching, \
/home/jad/.local/lib/python2.7/site-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._solve_toeplitz import levinson
/home/jad/.local/lib/python2.7/site-packages/scipy/linalg/__init__.py:207: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._decomp_update import *
/home/jad/.local/lib/python2.7/site-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._ufuncs import *
/home/jad/.local/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm
/home/jad/.local/lib/python2.7/site-packages/scipy/interpolate/_bsplines.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _bspl
/home/jad/.local/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .ckdtree import *
/home/jad/.local/lib/python2.7/site-packages/scipy/spatial/__init__.py:96: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .qhull import *
/home/jad/.local/lib/python2.7/site-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _voronoi
/home/jad/.local/lib/python2.7/site-packages/scipy/spatial/distance.py:122: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _hausdorff
/home/jad/.local/lib/python2.7/site-packages/scipy/ndimage/measurements.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _ni_label
/home/jad/.local/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime
/home/jad/.local/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import (hashtable as _hashtable,
/home/jad/.local/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import algos, lib
/home/jad/.local/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import hashing, tslib
/home/jad/.local/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import (lib, index as libindex, tslib as libts,
/home/jad/.local/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  import pandas._libs.tslibs.offsets as liboffsets
/home/jad/.local/lib/python2.7/site-packages/pandas/core/ops.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import algos as libalgos, ops as libops
/home/jad/.local/lib/python2.7/site-packages/pandas/core/indexes/interval.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs.interval import (
/home/jad/.local/lib/python2.7/site-packages/pandas/core/internals.py:14: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import internals as libinternals
/home/jad/.local/lib/python2.7/site-packages/pandas/core/sparse/array.py:33: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  import pandas._libs.sparse as splib
/home/jad/.local/lib/python2.7/site-packages/pandas/core/window.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  import pandas._libs.window as _window
/home/jad/.local/lib/python2.7/site-packages/pandas/core/groupby/groupby.py:68: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import (lib, reduction,
/home/jad/.local/lib/python2.7/site-packages/pandas/core/reshape/reshape.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import algos as _algos, reshape as _reshape
/home/jad/.local/lib/python2.7/site-packages/pandas/io/parsers.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  import pandas._libs.parsers as parsers
/home/jad/.local/lib/python2.7/site-packages/pandas/io/pytables.py:50: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import algos, lib, writers as libwriters
/home/jad/.local/lib/python2.7/site-packages/sklearn/utils/__init__.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .murmurhash import murmurhash3_32
/home/jad/.local/lib/python2.7/site-packages/scipy/optimize/_trlib/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._trlib import TRLIBQuadraticSubproblem
/home/jad/.local/lib/python2.7/site-packages/scipy/optimize/_numdiff.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._group_columns import group_dense, group_sparse
/home/jad/.local/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _stats
/home/jad/.local/lib/python2.7/site-packages/sklearn/utils/extmath.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._logistic_sigmoid import _log_logistic_sigmoid
/home/jad/.local/lib/python2.7/site-packages/sklearn/utils/extmath.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .sparsefuncs_fast import csr_row_norms
/home/jad/.local/lib/python2.7/site-packages/sklearn/metrics/cluster/supervised.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .expected_mutual_info_fast import expected_mutual_information
/home/jad/.local/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan
Traceback (most recent call last):
  File ""export_inference_graph.py"", line 150, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""export_inference_graph.py"", line 146, in main
    write_inference_graph=FLAGS.write_inference_graph)
TypeError: export_inference_graph() got an unexpected keyword argument 'input_shape'

And I tried other models and also updated the object_detection but still getting the same error. Can anyone help ? 

",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-10 08:28:02,open,,,['stat:awaiting response'],2018-09-10 19:33:38
619,tensorflow/models,models,5274,DehuaTang,(deeplabV3 plus) the function of get_branch_logits,"def get_branch_logits(features,
                      num_classes,
                      atrous_rates=None,
                      aspp_with_batch_norm=False,
                      kernel_size=1,
                      weight_decay=0.0001,
                      reuse=None,
                      scope_suffix=''):
 
      for i, rate in enumerate(atrous_rates):
        scope = scope_suffix
        if i:
          scope += '_%d' % i

        branch_logits.append(
            slim.conv2d(
                features,
                num_classes,
                kernel_size=kernel_size,
                rate=rate,
                activation_fn=None,
                normalizer_fn=None,
                scope=scope))

      return tf.add_n(branch_logits)


Thanks for your work!
Can you tell me why we should use **so many 1*1 conv2d** to get the final logits ? When the kernel_size=1,the rate means nothing ! 
I think we can use **the only one** 1*1 conv2d to get the final logits . Is there any difference for the performance?
Waiting for your replay !",1,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-09-10 07:07:17,open,,,['stat:awaiting response'],2018-09-10 19:44:13
620,tensorflow/models,models,5273,kaelzhang,Module 'tensorflow' has no 'LegacySyncReplicasOptimizer' member and other import issues,"### System information
- **What is the top-level directory of the model you are using**: `research/attention_ocr`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

```
System Version: macOS 10.13.6 (17G65)
Kernel Version: Darwin 17.7.0
Boot Volume: HD
Boot Mode: Normal
User Name: Ying Zhang (kael)
Secure Virtual Memory: Enabled
System Integrity Protection: Enabled
Time since boot: 4 days 18:25
```

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: tensorflow-gpu@1.1.0 / tensorflow@1.10.1(CPU)
- **Python version**: 2.7.15
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```sh
cd models/research/attention_ocr/python
conda create --name tensor python=2.7
source activate tensor
pip install --upgrade pip
pip install --upgrade tensorflow-gpu
python train.py
```

### Describe the problem

```
Module 'tensorflow' has no 'LegacySyncReplicasOptimizer' member
tf: tensorflow
```

The error occurs at https://github.com/tensorflow/models/blob/8e4a1e2e0a7450f0457b820a589960e03c20060d/research/attention_ocr/python/train.py#L125

### Source code / logs

And if use tensorflow cpu instead, the error remains. And there are tons of other import errors.",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-10 06:02:29,open,,,['stat:awaiting response'],2018-09-12 01:53:53
621,tensorflow/models,models,5272,RobertLexis,A bug of argmax_matcher.ArgMaxMatcher when force_match_for_each_row = True,"### Describe the problem
In method `object_detection.core.target_assigner.TargetAssigner.assign`, when `match_quality_matrix` has one or many zero rows (which can happen due to `object_detection.inputs.pad_input_data_to_static_shapes`), in the line 180 `match = self._matcher.match(match_quality_matrix, **params)` where `self._matcher` is `argmax_matcher.ArgMaxMatcher` with `force_match_for_each_row = True`, `match` will  give an unexpected match.This is because the `matchers.argmax_atcher.ArgMaxMatcher._match` 
```
if self._force_match_for_each_row:
        similarity_matrix_shape = shape_utils.combined_static_and_dynamic_shape(
            similarity_matrix)
        force_match_column_ids = tf.argmax(similarity_matrix, 1,
                                           output_type=tf.int32)
        force_match_column_indicators = tf.one_hot(
            force_match_column_ids, depth=similarity_matrix_shape[1])
        force_match_row_ids = tf.argmax(force_match_column_indicators, 0,
                                        output_type=tf.int32)
        force_match_column_mask = tf.cast(
            tf.reduce_max(force_match_column_indicators, 0), tf.bool)
        final_matches = tf.where(force_match_column_mask,
                                 force_match_row_ids, matches)
        return final_matches
``` 
is implemented unproperly.
### Source code / logs
```
import numpy as np
import tensorflow as tf
from object_detection.matchers import argmax_matcher

match_quality_matrix = np.zeros((4, 10))
match_quality_matrix[0][1] = 0.9
match_quality_matrix[1][9] = 0.7
match_quality_matrix_tensor = tf.constant(match_quality_matrix, dtype=tf.float32)
matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.7,
                                           unmatched_threshold=0.3,
                                           force_match_for_each_row=True,
                                           use_matmul_gather=False)

match = matcher.match(match_quality_matrix_tensor)
sess = tf.InteractiveSession()
sess.run(match.match_results)
```
The expected result is:
```
[ -1,  0, -1, -1, -1, -1, -1, -1, -1,  1]
```
BUT the code gives:
```
[ 2,  0, -1, -1, -1, -1, -1, -1, -1,  1]
```",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-09-10 02:56:02,open,,,['stat:awaiting response'],2018-09-10 13:17:50
622,tensorflow/models,models,5270,yashkotadia,Attention OCR: Visualise Attention Heatmap (Feature Request),"Hi, is there an easy way to plot the Attention Heatmaps? Visualising the heatmaps would help in analyzing the results of the attention mechanism.

Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from N/A
TensorFlow version: N/A
Bazel version: N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-09 19:57:02,open,,,['stat:awaiting response'],2018-09-11 07:38:09
623,tensorflow/models,models,5267,magick2,Tensorflow GPU not using GPU,"
[object_detection_tutorial2.zip](https://github.com/tensorflow/models/files/2363376/object_detection_tutorial2.zip)

Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information
What is the top-level directory of the model you are using:
Object detection
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, just to insert a video instead of using the webcam
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 64 bits (Last version)
TensorFlow installed from (source or binary):
Binary
TensorFlow version (use command below):
v1.8.0-0-g93bc2e2072' 1.8.0
Bazel version (if compiling from source):
N/A
CUDA/cuDNN version:
CUDA: cuda_9.0.176
cuDNN: cudnn-9.0
GPU model and memory:
MSI Geforce GTX 1070 8gb
Exact command to reproduce:
N/A
You can collect some of this information using our environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

Describe the problem
Long ago I come with this problem, and tried everything, reinstall, format the PC and again reinstall but my software runs with CPU only, just use one (9% of GPU)
I added these sentences:
       device_name = tf.test.gpu_device_name ()

       print ('Found GPU at: {}'. format (device_name))
And it clearly says that it uses GPU = ""Found GPU at: / device: GPU: 0""
I can not get my programs to work with GPU and this takes too long, since the video is less than 30 fps

Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
The code I use is in a .zip file
PS: I'm sorry for my English

",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-08 14:26:39,open,,,[],2018-09-09 02:16:09
624,tensorflow/models,models,5261,akku1506,Resnet-101 trainable despite setting is_training flag to False,"Hello,

I was extracting the pre-trained features of Resnet-101 and then finetune a small net from the features I get from Resnet-101.

I followed the API and wrote the below lines of code to extract-2048 dimesnioanll features:

 with slim.arg_scope(resnet_v1.resnet_arg_scope()):
            _, end_points = resnet_v1.resnet_v1_101(images, None, is_training=False)

But when, i do a simple check of getting all the trainable variables in my code **by calling tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES), I found all the parameters of Resnet-101 trainable**.

Further, I also inspected the code and found that maybe there should be a global scope defined for slim.conv2d, slim.batch_norm which should be set to False in resnet_utils, if is_training=False to extract pre-trained features. 

This also means that the entire network is getting trained, which I don't expect if I am setting is_training=False.

What is the top-level directory of the model you are using: models/research/slim/nets
Have I written custom code: No
OS Platform and Distribution :Ubuntu 16.04 LTS
TensorFlow installed from: Anaconda and this library from (https://github.com/tensorflow/models.git)
TensorFlow version: 1.9
Reproduce the issue by: 

with slim.arg_scope(resnet_v1.resnet_arg_scope()):
            _, end_points = resnet_v1.resnet_v1_101(images, None, is_training=False)
and then printing all trainable variable name from tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
",3,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-09-06 23:58:23,open,,,[],2018-09-22 07:25:50
625,tensorflow/models,models,5259,Akarshk,Request for Inception v3 pretrained model,"I was wondering if I can get pre trained inception v3 model as it is not available in model zoo.
Please do reply
",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-09-06 12:49:43,open,,,[],2018-09-11 13:28:45
626,tensorflow/models,models,5257,Sukanya-Kudi,Loading Resnet 50 pretrained model gives error,"### System information
- **What is the top-level directory of the model you are using**:  https://github.com/tensorflow/models/tree/master/official/resnet
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows
- **TensorFlow installed from (source or binary)**: anaconda
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

path='path to .pb file'
detection_graph=tf.Graph()
with detection_graph.as_default():
     od_graph_def=tf.GraphDef()
     with tf.gfile.GFile(path,'rb') as fid:
             serialized_graph=fid.read()
             od_graph_def.ParseFromString(serialized_graph)
             tf.import_graph_def(od_graph_def,name='')
sess = tf.Session(graph=detection_graph) 

The whole ""20180601_resnet_v2_imagenet_savedmodel.tar"" is 90.6 MB, but the pb is just 703KB.
The biggest file is : variables.data-00000-of-00001 of 97.7 MB.

The error thrown is :

---------------------------------------------------------------------------
DecodeError                               Traceback (most recent call last)
<ipython-input-12-23bb7554821d> in <module>()
      6      with tf.gfile.GFile(path,'rb') as fid:
      7              serialized_graph=fid.read()
----> 8              od_graph_def.ParseFromString(serialized_graph)
      9              tf.import_graph_def(od_graph_def,name='')
     10 sess = tf.Session(graph=detection_graph)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\message.py in ParseFromString(self, serialized)
    183     """"""
    184     self.Clear()
--> 185     self.MergeFromString(serialized)
    186 
    187   def SerializeToString(self, **kwargs):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py in MergeFromString(self, serialized)
   1081     length = len(serialized)
   1082     try:
-> 1083       if self._InternalParse(serialized, 0, length) != length:
   1084         # The only reason _InternalParse would return early is if it
   1085         # encountered an end-group tag.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py in InternalParse(self, buffer, pos, end)
   1118         pos = new_pos
   1119       else:
-> 1120         pos = field_decoder(buffer, new_pos, end, self, field_dict)
   1121         if field_desc:
   1122           self._UpdateOneofState(field_desc)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\decoder.py in DecodeField(buffer, pos, end, message, field_dict)
    631         raise _DecodeError('Truncated message.')
    632       # Read sub-message.
--> 633       if value._InternalParse(buffer, pos, new_pos) != new_pos:
    634         # The only reason _InternalParse would return early is if it encountered
    635         # an end-group tag.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py in InternalParse(self, buffer, pos, end)
   1118         pos = new_pos
   1119       else:
-> 1120         pos = field_decoder(buffer, new_pos, end, self, field_dict)
   1121         if field_desc:
   1122           self._UpdateOneofState(field_desc)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\decoder.py in DecodeRepeatedField(buffer, pos, end, message, field_dict)
    610           raise _DecodeError('Truncated message.')
    611         # Read sub-message.
--> 612         if value.add()._InternalParse(buffer, pos, new_pos) != new_pos:
    613           # The only reason _InternalParse would return early is if it
    614           # encountered an end-group tag.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py in InternalParse(self, buffer, pos, end)
   1118         pos = new_pos
   1119       else:
-> 1120         pos = field_decoder(buffer, new_pos, end, self, field_dict)
   1121         if field_desc:
   1122           self._UpdateOneofState(field_desc)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\decoder.py in DecodeMap(buffer, pos, end, message, field_dict)
    741       # Read sub-message.
    742       submsg.Clear()
--> 743       if submsg._InternalParse(buffer, pos, new_pos) != new_pos:
    744         # The only reason _InternalParse would return early is if it
    745         # encountered an end-group tag.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py in InternalParse(self, buffer, pos, end)
   1107       if field_decoder is None:
   1108         value_start_pos = new_pos
-> 1109         new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)
   1110         if new_pos == -1:
   1111           return pos

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\decoder.py in SkipField(buffer, pos, end, tag_bytes)
    848     # The wire type is always in the first byte since varints are little-endian.
    849     wire_type = ord(tag_bytes[0:1]) & wiretype_mask
--> 850     return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
    851 
    852   return SkipField

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\decoder.py in _SkipGroup(buffer, pos, end)
    797   while 1:
    798     (tag_bytes, pos) = ReadTag(buffer, pos)
--> 799     new_pos = SkipField(buffer, pos, end, tag_bytes)
    800     if new_pos == -1:
    801       return pos

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\decoder.py in SkipField(buffer, pos, end, tag_bytes)
    848     # The wire type is always in the first byte since varints are little-endian.
    849     wire_type = ord(tag_bytes[0:1]) & wiretype_mask
--> 850     return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
    851 
    852   return SkipField

~\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\decoder.py in _SkipFixed32(buffer, pos, end)
    812   pos += 4
    813   if pos > end:
--> 814     raise _DecodeError('Truncated message.')
    815   return pos
    816 

DecodeError: Truncated message.

What is the problem here.
Also is TF-Slim deprecated ? 
Why are there no pretrained models in TF itself. ",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-06 08:51:13,open,,,[],2019-04-08 05:20:11
627,tensorflow/models,models,5249,tensorbuffer,[feature request] freeze classification weights when training for detection,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I have trained mobilenet model which has a pretty good mAP, would like to train mobilenet-ssd model on top of that. I understand that I can load it as a fine-tune checkpoint and set from_detection_checkpoint to false. However the result is not good. Looks like when start training I need to freeze mobilenet weights during the first few epoch and then unfreeze them. However I don't see such an entry in the config file.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-05 15:54:55,open,,,[],2019-01-08 14:37:43
628,tensorflow/models,models,5246,volkerstampa,object_detection: Allow to provide RunConfig parameters on command line of model_main,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.5 LTS (Xenial Xerus)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.10.1-0-g4dcfddc5d1', '1.10.1')
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: cuda-9.0
- **GPU model and memory**: Tesla K80 major: 3 minor: 7 / 11.17GiB
- **Exact command to reproduce**: - (This is a feature request)

### Describe the problem

An Estimator run is configured with a `RunConfig` object, however when starting `model_main` parameters of `RunConfig` can not be defined to overwrite defaults. That is why I propose to add command line parameters to `model_main` that allow to define `RunConfig` parameters. I see two options here:

1. Add specific command line parameters for specific `RunConfig` parameters, e.g. `--save_checkpoints_steps`, `--save_checkpoints_secs`, ...
1. Add a generic command line parameter for `RunConfig` parameters similar to the already existing `--hparams_overrides`. E.g. `--runconfig_overrides save_checkpoints_steps=100,keep_checkpoint_max=10`

Option 2 should result in less maintenance when `RunConfig` is changed. OTOH it would only allow to overwrite _primitive_ parameters, but no structured parameters like `session_config`. With Option 1 changes in `RunConfig` might need to be reflected in code. OTOH it would for example allow to define command line switches for `session_config` parameters.

I would personally vote for Option 2 and if there is interest I could also open a PR for this.
",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-09-05 09:03:58,open,,,"['stat:awaiting owner', 'type:feature']",2019-01-21 06:35:32
629,tensorflow/models,models,5245,sumsuddin,changed the prediction label data type from int64 to int32,"As the number of classes (which is generally less than 200) should perfectly fit in `int32`, there is no need to use an `int64` data type for prediction labels. 

Also, this change should fix this [issue](https://github.com/tensorflow/models/issues/4278) while using the model in **tflite** or `toco`.

Didn't use `uint8` instead of `int32`, because **argmax** expects only `int32` or `int64` datatype.",0,,[],2018-09-05 07:07:17,open,,,['cla: yes'],2018-09-05 07:07:19
630,tensorflow/models,models,5244,SdustZhangzhen,How to evaluate my vis result quantitatively？,"------------------------

### System information
**What is the top-level directory of the model you are using:~/models/research/deeplab
**OS Platform and Distribution : Linux Ubuntu 16.04
**TensorFlow version (use command below):1.6.0
**CUDA/cuDNN version: CUDA 9.0, cuDNN 7.0
**GPU model and memory: GeForce GTX 1080, 8G

After finishing my train and vis, I use the tool of tensorboard to inspect  those jobs. But how can evalute my vis result quantitatively by evaluating indicator of Precision, Recall and F1. By writing programs or through special tools?",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-09-05 02:01:55,open,,,['stat:awaiting response'],2018-09-05 13:32:20
631,tensorflow/models,models,5241,blross,Flush eval summary writer after metrics are written,"At the moment, when the model dir is set to an S3 prefix, metrics logging is flaky (the eval dir checkpoints show up late or never). We can fix this issue by flushing the `summary_writer` after each evaluation.",0,,[],2018-09-04 18:49:23,open,,,['cla: yes'],2018-10-27 15:18:30
632,tensorflow/models,models,5239,q740052959,absl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --model_dir before flags were parsed.,"when I run python research/object_detection/model_main.py

Traceback (most recent call last):
File ""research/object_detection/model_main.py"", line 103, in 
tf.app.run()
File ""/home/hd/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""research/object_detection/model_main.py"", line 57, in main
config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir)
File ""/home/hd/.local/lib/python3.5/site-packages/absl/flags/_flagvalues.py"", line 488, in getattr
raise _exceptions.UnparsedFlagAccessError(error_message)
absl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --model_dir before flags were parsed.

What is the top-level directory of the model you are using: models/
Have I written custom code: No
OS Platform and Distribution: Windows7
TensorFlow installed from: Official Website
TensorFlow version: 1.4.0
Bazel version: 0.13.0-homebrew
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-04 14:30:06,open,,,[],2018-09-10 21:29:04
633,tensorflow/models,models,5237,Sukanya-Kudi,SSD prediction,"Is there a way to know from which feature map an object is being detected in the SSD architecture.
Can this be done using the Session run on the pre-trained model.
PS: I am using MobileNet_v2 SSD for predictions, and I can also extract the image feature maps during Session Run, but I am interested in visualizing/extracting features  of an object from the feature map that  returns the highest confidence.",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-04 01:45:47,open,,,[],2018-09-20 10:00:37
634,tensorflow/models,models,5233,jadhm,TypeError: export_inference_graph() takes at most 5 arguments (6 given) ,"System information

    Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
    TensorFlow installed from (source or binary):
    TensorFlow version (use command below):('v1.9.0-0-g25c197e023', '1.9.0')
    GPU model and memory:Quadro M1000M , 4 Gb
    Bazel version: NA
    CUDA/cuDNN version:NA
    Exact command to reproduce: NA
    Mobile device : NA


Hi,

I have trained a faster cnn inception model for particular object and after I finished training I was trying to use the function export_inference_graph()  to export the model like this : 
python export_inference_graph.py \
    --input_type image_tensor \
    --pipeline_config_path training/faster_rcnn_inception_v2_coco.config \
    --trained_checkpoint_prefix training/model.ckpt-17135 \
    --output_directory learned_model

But I am getting this error can anyone help and if it didn't work can I create the model manually ? : 


python export_inference_graph.py  --input_type image_tensor     --pipeline_config_path training/faster_rcnn_inception_v2_coco.config     --trained_checkpoint_prefix training/model.ckpt-0     --output_directory learned_model
/home/jad/.local/lib/python2.7/site-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._conv import register_converters as _register_converters
/home/jad/.local/lib/python2.7/site-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z
/home/jad/.local/lib/python2.7/site-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/lil.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _csparsetools
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._tools import csgraph_to_dense, csgraph_from_dense,\
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:167: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._traversal import breadth_first_order, depth_first_order, \
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:169: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._min_spanning_tree import minimum_spanning_tree
/home/jad/.local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:170: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching, \
/home/jad/.local/lib/python2.7/site-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._solve_toeplitz import levinson
/home/jad/.local/lib/python2.7/site-packages/scipy/linalg/__init__.py:207: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._decomp_update import *
/home/jad/.local/lib/python2.7/site-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._ufuncs import *
/home/jad/.local/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm
/home/jad/.local/lib/python2.7/site-packages/scipy/interpolate/_bsplines.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _bspl
/home/jad/.local/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .ckdtree import *
/home/jad/.local/lib/python2.7/site-packages/scipy/spatial/__init__.py:96: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .qhull import *
/home/jad/.local/lib/python2.7/site-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _voronoi
/home/jad/.local/lib/python2.7/site-packages/scipy/spatial/distance.py:122: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _hausdorff
/home/jad/.local/lib/python2.7/site-packages/scipy/ndimage/measurements.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _ni_label
/home/jad/.local/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime
/home/jad/.local/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import (hashtable as _hashtable,
/home/jad/.local/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import algos, lib
/home/jad/.local/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import hashing, tslib
/home/jad/.local/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import (lib, index as libindex, tslib as libts,
/home/jad/.local/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  import pandas._libs.tslibs.offsets as liboffsets
/home/jad/.local/lib/python2.7/site-packages/pandas/core/ops.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import algos as libalgos, ops as libops
/home/jad/.local/lib/python2.7/site-packages/pandas/core/indexes/interval.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs.interval import (
/home/jad/.local/lib/python2.7/site-packages/pandas/core/internals.py:14: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import internals as libinternals
/home/jad/.local/lib/python2.7/site-packages/pandas/core/sparse/array.py:33: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  import pandas._libs.sparse as splib
/home/jad/.local/lib/python2.7/site-packages/pandas/core/window.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  import pandas._libs.window as _window
/home/jad/.local/lib/python2.7/site-packages/pandas/core/groupby/groupby.py:68: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import (lib, reduction,
/home/jad/.local/lib/python2.7/site-packages/pandas/core/reshape/reshape.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import algos as _algos, reshape as _reshape
/home/jad/.local/lib/python2.7/site-packages/pandas/io/parsers.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  import pandas._libs.parsers as parsers
/home/jad/.local/lib/python2.7/site-packages/pandas/io/pytables.py:50: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from pandas._libs import algos, lib, writers as libwriters
/home/jad/.local/lib/python2.7/site-packages/sklearn/utils/__init__.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .murmurhash import murmurhash3_32
/home/jad/.local/lib/python2.7/site-packages/scipy/optimize/_trlib/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._trlib import TRLIBQuadraticSubproblem
/home/jad/.local/lib/python2.7/site-packages/scipy/optimize/_numdiff.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._group_columns import group_dense, group_sparse
/home/jad/.local/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from . import _stats
/home/jad/.local/lib/python2.7/site-packages/sklearn/utils/extmath.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from ._logistic_sigmoid import _log_logistic_sigmoid
/home/jad/.local/lib/python2.7/site-packages/sklearn/utils/extmath.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .sparsefuncs_fast import csr_row_norms
/home/jad/.local/lib/python2.7/site-packages/sklearn/metrics/cluster/supervised.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .expected_mutual_info_fast import expected_mutual_information
/home/jad/.local/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan
Traceback (most recent call last):
  File ""export_inference_graph.py"", line 150, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""export_inference_graph.py"", line 146, in main
    FLAGS.write_inference_graph)
**TypeError: export_inference_graph() takes at most 5 arguments (6 given)**
",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-09-03 11:34:42,open,,,['stat:awaiting response'],2018-09-03 18:53:20
635,tensorflow/models,models,5231,neouyghur,slim inception preprocessing wrapper function miss central_fraction arguments,"Since it is a very obvious bug, I didn't provide my system information.

### Describe the problem
Can't fully use the  preprocess_for_eval function in models/research/slim/preprocessing/inception_preprocessing.py  by the wrapper function preprocess_image. Specifically, we can't adjust the central crop property by this wrapper function, because of missing the 'central_fraction' parameter.

###
```
Traceback (most recent call last):
  File ""extract_features.py"", line 136, in <module>
    main(parser.parse_args())
  File ""extract_features.py"", line 132, in main
    extract_features(args)
  File ""extract_features.py"", line 69, in extract_features
    pro_image = image_preprocessing_fn(image, eval_image_size, eval_image_size, central_fraction=None, scope=None)
  File ""~/models/research/slim/preprocessing/preprocessing_factory.py"", line 78, in preprocessing_fn
    image, output_height, output_width, is_training=is_training, **kwargs)
TypeError: preprocess_image() got an unexpected keyword argument 'central_fraction'

```
",1,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-09-03 06:09:52,open,,,['stat:awaiting response'],2018-09-04 18:09:27
636,tensorflow/models,models,5230,pranali3215,Not able to convert checkpoint models to Tensorflow graph proto,"**System Information:**
**OS Platform and Distribution:** Windows 10 
**Tensorflow version:** 1.10.0 
**TensorFlow installed from:** Installed through pip.
**Bazel version:** As I installed through pip and not binary, I cannot check the bazel version (?)
**GPU model and memory:** I do not have a GPU on my system. 
**CUDA/cUDNN version:** Not being used

I created my own dataset of ~100 images and 1 class and ran the **legacy/train.py** script for the training.
I got the below checkpoint files:
model.ckpt-649.data-00000-of-00001
model.ckpt-649.index
model.ckpt-649.meta

However, I get the below error when I run the export_inference_graph.py file.
No custom code was written.

**Exact commands to reproduce the error:**
python .\object_detection\export_inference_graph.py --input_type image_tensor --pipeline_config_path .\object_detection\training\ssd_mobilenet_v1_pets.config --trained_checkpoint_prefix .\object_detection\training\model.ckpt-649.index --output_directory .\object_detection\trained-inference-graphs

Top level directory (as mentioned above already): object_detection

I ultimately want to use the .pb file to evaluate the model using the object_detection_tutorial notebook provided.

Logs:
Traceback (most recent call last):
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\client\session.py"", line 1278, in _do_call
    return fn(*args)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\client\session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\client\session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Tensor name ""BoxPredictor_0/BoxEncodingPredictor/biases"" not found in checkpoint fil
es .\object_detection\training\model.ckpt-649.index
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT
_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2
/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\training\saver.py"", line 1725, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\client\session.py"", line 877, in run
    run_metadata_ptr)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\client\session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\client\session.py"", line 1272, in _do_run
    run_metadata)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\client\session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Tensor name ""BoxPredictor_0/BoxEncodingPredictor/biases"" not found in checkpoint fil
es .\object_detection\training\model.ckpt-649.index
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT
_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2
/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File "".\object_detection\export_inference_graph.py"", line 153, in <module>
    tf.app.run()
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File "".\object_detection\export_inference_graph.py"", line 149, in main
    write_inference_graph=FLAGS.write_inference_graph)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\models\research\object_detection\exporter.py"", line 405, in export_inference_graph
    write_inference_graph=write_inference_graph)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\models\research\object_detection\exporter.py"", line 334, in _export_inference_graph
    trained_checkpoint_prefix=checkpoint_to_use)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\models\research\object_detection\exporter.py"", line 241, in write_graph_and_checkpoint
    tf.import_graph_def(inference_graph_def, name='')
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\util\deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\importer.py"", line 234, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\ops.py"", line 3289, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\ops.py"", line 3289, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\ops.py"", line 3180, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

NotFoundError (see above for traceback): Tensor name ""BoxPredictor_0/BoxEncodingPredictor/biases"" not found in checkpoint files .\object_de
tection\training\model.ckpt-649.index
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT
_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2
/shape_and_slices)]]


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\training\saver.py"", line 1737, in restore
    checkpointable.OBJECT_GRAPH_PROTO_KEY)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 348, in get_tensor
    status)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint file

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "".\object_detection\export_inference_graph.py"", line 153, in <module>
    tf.app.run()
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File "".\object_detection\export_inference_graph.py"", line 149, in main
    write_inference_graph=FLAGS.write_inference_graph)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\models\research\object_detection\exporter.py"", line 405, in export_inference_graph
    write_inference_graph=write_inference_graph)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\models\research\object_detection\exporter.py"", line 334, in _export_inference_graph
    trained_checkpoint_prefix=checkpoint_to_use)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\models\research\object_detection\exporter.py"", line 245, in write_graph_and_checkpoint
    saver.restore(sess, trained_checkpoint_prefix)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\training\saver.py"", line 1743, in restore
    err, ""a Variable name or other graph key that is missing"")
tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or othe
r graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Origin
al error:

Tensor name ""BoxPredictor_0/BoxEncodingPredictor/biases"" not found in checkpoint files .\object_detection\training\model.ckpt-649.index
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT
_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2
/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File "".\object_detection\export_inference_graph.py"", line 153, in <module>
    tf.app.run()
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File "".\object_detection\export_inference_graph.py"", line 149, in main
    write_inference_graph=FLAGS.write_inference_graph)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\models\research\object_detection\exporter.py"", line 405, in export_inference_graph
    write_inference_graph=write_inference_graph)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\models\research\object_detection\exporter.py"", line 334, in _export_inference_graph
    trained_checkpoint_prefix=checkpoint_to_use)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\models\research\object_detection\exporter.py"", line 241, in write_graph_and_checkpoint
    tf.import_graph_def(inference_graph_def, name='')
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\util\deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\importer.py"", line 234, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\ops.py"", line 3289, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\ops.py"", line 3289, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\ops.py"", line 3180, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""C:\Anaconda3\Lib\site-packages\tensorflow\python\framework\ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key th
at is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Tensor name ""BoxPredictor_0/BoxEncodingPredictor/biases"" not found in checkpoint files .\object_detection\training\model.ckpt-649.index
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT
_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2
/shape_and_slices)]]

",8,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-09-03 06:00:20,open,,,[],2019-01-11 18:16:15
637,tensorflow/models,models,5228,danieljimeneznz,Changed import order for object_detection model_lib.py to allow for headless,Moved object_detection vis_utils import in model_lib.py above eval_utils as matplotlib import has to be set to Agg to allow for headless. (current import order meant that matplotlib was not set correctly for headless).,0,,[],2018-09-03 04:53:03,open,,,['cla: yes'],2018-09-03 04:53:06
638,tensorflow/models,models,5224,vaibhav2ghadge,How to get parsey mcparseface all tree not just final output tree,after running demo.sh we just got final output tree. i want all tree that mcparseface created and elemented which file we need to edit,1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-09-02 10:39:05,open,,,['stat:awaiting response'],2018-09-02 18:48:50
639,tensorflow/models,models,5222,cjr0106,the problem of json.encoder.FLOAT_REPR not exist in python3.x,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-09-01 07:02:45,open,,,[],2018-09-01 18:56:38
640,tensorflow/models,models,5221,jrbtaylor,KeyError: 'IteratorGetDevice' importing official pre-trained Resnet meta graph files,"### System information
- **What is the top-level directory of the model you are using**: official
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Titan X
- **Exact command to reproduce**: see below

### Describe the problem
The meta files included with the [pre-trained Resnets](https://github.com/tensorflow/models/tree/master/official/resnet#pre-trained-model) can't be imported using tf.train.import_meta_graph. 

Here's the code: 
```
meta = '/home/jason.taylor/Downloads/20180601_resnet_v1_imagenet_checkpoint/model.ckpt-257706.meta'
graph = tf.train.import_meta_graph(meta)
```

Here's the error:
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/jason.taylor/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1955, in import_meta_graph
>     **kwargs)
>   File ""/home/jason.taylor/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py"", line 743, in import_scoped_meta_graph
>     producer_op_list=producer_op_list)
>   File ""/home/jason.taylor/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
>     return func(*args, **kwargs)
>   File ""/home/jason.taylor/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 460, in import_graph_def
>     _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
>   File ""/home/jason.taylor/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 227, in _RemoveDefaultAttrs
>     op_def = op_dict[node.op]
> KeyError: 'IteratorGetDevice'
",3,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-08-31 21:55:56,open,,,[],2018-09-20 19:53:22
641,tensorflow/models,models,5220,fdkssdks,How to Store Coordinates only once?,"Hello, I am interested to know if there's a way to store the coordinates of the detected object only once?

So, when the detection is running it keeps on detecting the object and keeps on appending the coordinate of the same objects again and again(I am using CSV file to write the coordinates). Though there's a slight delta in the coordinates that doesn't matter for now since the object is stationary.

This is what I've now.

		for box in y_pred_thresh[0]:
			# Transform the predicted bounding boxes for the 300x300 image to the original image dimensions.
			xmin = box[2] * 300 / img_width
			ymin = box[3] * 300 / img_height
			xmax = box[4] * 300 / img_width
			ymax = box[5] * 300 / img_height
			cv2.rectangle(smallFrame, (int(xmin),int(ymin)),(int(xmax),int(ymax)),(0,250,0), thickness=1,lineType=8)
			xcenter=((xmin+xmax)/2) #For center X
			ycenter=((ymin+ymax)/2) #For center Y
			csv.write(round(xcenter)+ ' , ' + round(ycenter))
			csv.write('\n')
			csv.close()
			cv2.waitKey(0)
		return (y_pred_thresh)			
Any help is appreciated.
Thanks

    * Operating System Ubuntu 16.04
    * Which commit of this repository you're on: Latest
    * Keras version: 2.2.0
    * TensorFlow version: 1.9.0
If someone knows what kind of coordinates do we get? Is it pixel coordinate or something else?
How can I convert it to Robot Coordinate that is in X Y Z?",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-31 19:40:30,open,,,['stat:awaiting response'],2018-09-01 06:52:57
642,tensorflow/models,models,5219,AakashKumarNain,Very slow training and OOM error ,"### System information
- **What is the top-level directory of the model you are using**: Object Detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:16.04
- **TensorFlow installed from (source or binary)**:Binary
- **TensorFlow version (use command below)**:1.8
- **Bazel version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:M60
- **Exact command to reproduce**:N/A


### Describe the problem
I was trying to use the `ssd_resnet_50_fpn_coco ` and `faster_rcnn_resnet101_coco` models. With the following command:

```
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --num_eval_steps=${NUM_EVAL_STEPS} \
    --alsologtostderr
```
I have no idea how to make sure that `training` and `validation` run on different GPUs. I have two GPUs with 8Gb of memory on each. As soon as I start training, it starts to throw OOM warnings and memory, that too when my batch size is only 8. 

Plus, for a batch size of 8, I can see that my GPU usage is at 100% but the training is very very slow,. For each iteration it takes around 43sec on an average. 

IMHO, the earlier API version was much more cleaner and simpler to use as this one. It feels like I am starting to learn about this API from scratch again. ",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-31 13:41:50,open,,,[],2018-12-06 12:14:25
643,tensorflow/models,models,5218,cjr0106,The problem of coco_evaluation_test.py,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-08-31 12:17:53,open,,,['stat:awaiting tensorflower'],2018-09-19 21:46:36
644,tensorflow/models,models,5217,normandra,[TFlite/Object Detection] TFlite Model outputting the same confidence value for every detected object ,"### System information
- **What is the top-level directory of the model you are using**:
ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync_2018_07_18

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04

- **TensorFlow installed from (source or binary)**:
source

- **TensorFlow version (use command below)**:
1.8

- **Bazel version (if compiling from source)**:
0.16

- **CUDA/cuDNN version**:
n/a

- **GPU model and memory**:
n/a

- **Exact command to reproduce**:

```
object_detection/export_tflite_ssd_graph.py \
--pipeline_config_path=$CONFIG_FILE \
--trained_checkpoint_prefix=$CHECKPOINT_PATH \
--output_directory=$OUTPUT_DIR \
--add_postprocessing_op=true
```

```
bazel run --config=opt tensorflow/contrib/lite/toco:toco -- \
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops
```

### Describe the problem
Converted Model is outputting correct predictions but the confidence probability for every prediction is 89% (rounded) this value is exactly the same to the last decimal point. This makes it a little bit difficult to work with the outputted results. I've trained a different model with the same dataset (ssdlite_mobilenet_v2) with no such problem. The difference there is that the model is unquantized. Is this a behavior that is expected?

### Source code / logs
",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-08-31 12:07:46,open,,,[],2018-08-31 19:48:36
645,tensorflow/models,models,5216,ghost,deeplabv3+ multi gpu,"What is the top-level directory of the model you are using
--models-master/research/deeplab

Have I written custom code
--No, I just use the deeplabv3+ code

OS Platform and Distribution
--Ubuntu

TensorFlow installed from

TensorFlow version
--1.10
Bazel version

CUDA/cuDNN version
--9.0

GPU model and memory
--Nvidia DGX, 16


Exact command to reproduce
--I wrote it below

Hi, I tried to fine tune pre-trained model which is provided by tensorflow using 2 gpu 
So I set flags like below

 python train.py --logtostderr 
--train_split='train' 
--model_variant='xception_65' 
--atrous_rates=12 
--atrous_rates=24 
--atrous_rates=36 
--output_stride=8 
--decoder_output_stride=4 
--train_crop_size=513 
--train_crop_size=513 
--train_batch_size=2 
--training_number_of_steps=20000 
--fine_tune_batch_norm=false 
--tf_initial_checkpoint=""/storage/models-master/research/deeplab/datasets/pascal_voc_seg/init_models/deeplabv3_pascal_train_aug/model.ckpt"" --train_logdir=""/storage/models-master/research/deeplab/datasets/pascal_voc_seg/exp/train_on_train_set/train/0831_2"" 
--dataset_dir=""/storage/models-master/research/deeplab/datasets/pascal_voc_seg/tfrecord""  
--image_pyramid=0.5 
--image_pyramid=0.25 
--image_pyramid=1.75 
**--num_replicas=1 
--num_clones=2
--num_ps_tasks=1**

But I got an error message like below
**Cannot assign a device for operation 'parallel_read/filenames/Greater': Operation was explicitly assigned to /job:worker/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0, /job:localhost/replica:0/task:0/device:GPU:1 ]. Make sure the device specification refers to a valid device.
         [[Node: parallel_read/filenames/Greater = Greater[T=DT_INT32, _device=""/job:worker/device:CPU:0""](parallel_read/filenames/Size, parallel_read/filenames/Greater/y)]]**

It seems operations have been assigned  '/job:worker/device:CPU:0' but my setting is 2 gpus,  how can I this problem?
",4,"NamedUser(login=""YknZhu"")","[NamedUser(login=""YknZhu"")]",2018-08-31 09:40:09,open,,,"['models: research', 'stat:awaiting response']",2019-02-01 22:29:46
646,tensorflow/models,models,5215,xyou365,Key resnet_model/batch_normalization/beta/Momentum not found in checkpoint,"### System information
- **What is the top-level directory of the model you are using**: tensorflow/models/official
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Bazel version (if compiling from source)**: Not installed
- **CUDA/cuDNN version**: cuda-9.0/libcudart.7
- **GPU model and memory**: Nvidia 1080 Ti, 11GBx1
- **Exact command to reproduce**: 
`python3 ./official/resnet/cub_main.py  --data_dir=/media/4TDisk/yc/slim_cub3/datasets-dir/cub --model_dir=resnet-cub-model --num_gpus=1 --batch_size=120 --train_epochs=30 --epochs_between_evals=10 --pretrained_model_checkpoint_path=/media/4TDisk/r_official/resnet_v2_imagenet_checkpoint --fine_tune
`
then
`python3 ./official/resnet/cub_main.py  --data_dir=/media/4TDisk/yc/slim_cub3/datasets-dir/cub --model_dir=resnet-cub-model --num_gpus=1 --batch_size=120 --train_epochs=90 --epochs_between_evals=10`

### Describe the problem
I have used the `--fine_tune` to fine-tune the fc layer as in part transfer learning. But in order to better learning, I want to continually fine-tune all layers for some epochs. The problem is when I run the code as above, there are errors as,
`NotFoundError (see above for traceback): Key resnet_model/batch_normalization/beta/Momentum not found in checkpoint
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_475 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_716_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
`
It means fine-tuned model cannot be used to subsequent 'all layers fine-tune' because some variable are not successfully saved to checkpoints.

Any solutions? Need help.


",3,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-31 04:18:53,open,,,[],2018-11-16 12:57:35
647,tensorflow/models,models,5214,chrisrapson,from deeplab import common gives NameError: name 'python' is not defined,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow
### System information
- **What is the top-level directory of the model you are using**:
models/research/deeplab/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04.1
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
1.10.1
- **Bazel version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
n/a (running on CPU as test)
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:
`python -u research/deeplab/common_test.py`

or

```
python
>>>>from deeplab import common
```

### Describe the problem
Trying to import common from deeplab gives the following error:

```
>>> from deeplab import common
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/path/to/research/deeplab/common.py"", line 22, in <module>
    import tensorflow as tf
  File ""/path/to/research/deeplab/tensorflow/__init__.py"", line 615, in <module>
    del python
NameError: name 'python' is not defined
```
I originally thought it was a problem with tensorflow being reloaded, as explained in this issue
https://github.com/tensorflow/tensorflow/issues/15741
But the error persists even after restarting the computer.

I am using this version of the models repo:
```
commit 23b5b4227dfa1b23d7c21f0dfaf0951b16671f43 (HEAD -> master, origin/master, origin/HEAD)
Author: Aman Gupta <4409685+aman2930@users.noreply.github.com>
Date:   Thu Aug 30 10:51:38 2018 -0700

    Bypassing Export model step, if training on TPU's. As this need inference to be supported on TPU's. Remove this check once inference is supported. (#5209)
```


### Source code / logs
n/a
",4,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-08-31 03:49:28,open,,,[],2018-11-07 18:54:34
648,tensorflow/models,models,5210,samclearman,Op type not registered 'NonMaxSuppressionV3' deploying model in Cloud ML,New issue for https://github.com/tensorflow/models/issues/5171,4,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-08-30 18:11:29,open,,,['type:build/install'],2018-12-03 21:24:16
649,tensorflow/models,models,5208,volkerstampa,object_detection: Continuous evaluation in model_main picks up the wrong checkpoint-dir,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.5 LTS (Xenial Xerus)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.10.1-0-g4dcfddc5d1', '1.10.1')
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: cuda-9.0
- **GPU model and memory**:  Tesla K80 major: 3 minor: 7 / 11.17GiB 
- **Exact command to reproduce**:
  ```
  python -m object_detection.model_main \
      --logtostderr \
      --pipeline_config_path ""models/dev_pipeline.config"" \
      --model_dir ""models/dev"" \
      --checkpoint_dir ""models/train/"" 
  ```
### Describe the problem
I am trying to start a continuous evaluation with `model_main.py` in a similar way as it could be started with `eval.py` (that was moved the the legacy folder). By providing the _train_-`model_dir` as `checkpoint_dir` it starts in evaluation mode as expected. Without `--run_once` it start continuous evaluation with:
```
      model_lib.continuous_eval(estimator, FLAGS.model_dir, input_fn,
                                eval_steps, train_steps, name)
```
This makes it use the _dev_-`model_dir` as `checkpoint_dir`, i.e. it waits for new checkpoints in `models/dev`. I believe this should read:
```
      model_lib.continuous_eval(estimator, FLAGS.checkpoint_dir, input_fn,
                                eval_steps, train_steps, name)
```
So that it uses the _train_-`model_dir` (`models/train`) as `checkpoint_dir` as this is where new checkpoints are created during training.


### Source code / logs
See above.",1,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""pkulzc"")]",2018-08-30 13:16:54,open,,,[],2018-09-01 06:52:38
650,tensorflow/models,models,5204,jillelajitta,warnings during retraining the model,"Hi,

I'm getting following warnings when I'm to training my model, I am using SSD. Please someone help me.

Thank you






WARNING:tensorflow:Ignoring ground truth with image id 2132974076 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 2132974076 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 483474429 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 483474429 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 1042541771 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1042541771 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 374107777 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 374107777 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 851704672 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 851704672 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 1267094741 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1267094741 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 674017641 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 674017641 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 2011324514 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 2011324514 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 655823972 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 655823972 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 1069886348 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1069886348 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 432647899 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 432647899 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 192947873 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 192947873 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 1881834530 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1881834530 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 309074883 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 309074883 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 515532141 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 515532141 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 1804293832 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1804293832 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 1933803716 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1933803716 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 144546819 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 144546819 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 1887856555 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1887856555 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 1334524773 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1334524773 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 642435942 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 642435942 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 2089332883 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 2089332883 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 963719283 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 963719283 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 788508605 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 788508605 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 1557220932 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1557220932 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 2013002728 since it was previously added
",6,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-30 01:10:35,open,,,[],2019-01-18 03:56:57
651,tensorflow/models,models,5202,jillelajitta,slow inference,"Hi

I trained my 450 custom classes on SSD+Googlenet using pre trained coco weights. When I tested just with coco weights I'm getting 30fps, when I tested my trained weights(coco pre trained+450 classes), I'm getting 6-7 fps. Does adding more classes decrease speed of the inference. Please help me out.
I'm using NVIDIA GTX1080TI, LINUX.
Thanks
",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-08-29 22:30:41,open,,,['stat:awaiting response'],2018-08-31 15:29:49
652,tensorflow/models,models,5201,jillelajitta,Why GPU loads every time when training with model_main.py?,"**Hi I am training SSD+Googlenet,my gpu loads for every iteration. Could someone help me?
Below are the messages I'm getting.**




totalMemory: 10.91GiB freeMemory: 9.82GiB
2018-08-29 13:36:29.582988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-29 13:36:29.714503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 13:36:29.714528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-08-29 13:36:29.714532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-08-29 13:36:29.714691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9492 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-08-29 13:47:16.037951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-29 13:47:16.037996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 13:47:16.038002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-08-29 13:47:16.038007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-08-29 13:47:16.038115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9492 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
creating index...
index created!
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=108.24s).
Accumulating evaluation results...
DONE (t=20.21s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.038
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.039
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.039
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.007
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.042
2018-08-29 14:07:06.136712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-29 14:07:06.136756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 14:07:06.136761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-08-29 14:07:06.136765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-08-29 14:07:06.136863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9492 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
creating index...
index created!
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=93.26s).
Accumulating evaluation results...
DONE (t=24.50s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.038
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.039
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.039
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.007
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.041
2018-08-29 14:26:31.902258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-29 14:26:31.902304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-29 14:26:31.902325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-08-29 14:26:31.902329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-08-29 14:26:31.902445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9492 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
creating index...
index created!
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=89.85s).
Accumulating evaluation results...
DONE (t=23.94s).
",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-29 21:49:15,open,,,['stat:awaiting response'],2018-11-29 08:39:21
653,tensorflow/models,models,5198,zeyademam,Bug residual bottleneck unit implementation in slim. ,"### System information
- **What is the top-level directory of the model you are using**: N/A
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

------------------------
### Problem Description
In the documentation of the `bottleneck()` function here: https://github.com/tensorflow/models/blob/master/research/slim/nets/resnet_v2.py

It says the bottleneck unit is a direct adaptation of figure 1)(b) here: https://arxiv.org/pdf/1603.05027.pdf

However, there are differences, for instance, the code has only one batch norm layer whereas the figure has two. ",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-28 22:34:54,open,,,[],2018-10-22 20:08:46
654,tensorflow/models,models,5196,Buevara,"Whether it is asynchronous or synchronous update, the effect of using estimator distributed is not as good as the stand-alone version.","Try to use estimator distributed, using 16 workers 8workers 4workers to train the faster rcnn object detection task, the results are very poor, I would like to ask if anyone has encountered this problem.

",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-08-28 08:33:54,open,,,['stat:awaiting response'],2018-08-31 15:29:33
655,tensorflow/models,models,5195,SdustZhangzhen,Deeplab V3+ error： convert my own datasets from jpg and png to tfrecord.,"
**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using:~/models/research/deeplab
- **OS Platform and Distribution : Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below):1.6.0
- **CUDA/cuDNN version: CUDA 9.0, cuDNN 7.0
- **GPU model and memory: GeForce GTX 1080, 8G
- **Exact command to reproduce**:

When I convert my own datasets from jpg and png to tfrecord, using the command as follow:

python build_voc2012_data.py  --image_folder=""/home/zhang/Deeplab/models/research/deeplab/datasets/pascal_voc_seg/VOCdevkit/VOC2012/JPEGImages""   --semantic_segmentation_folder=""/home/zhang/Deeplab/models/research/deeplab/datasets/pascal_voc_seg/VOCdevkit/VOC2012/SegmentationClass""   --list_folder=""/home/zhang/Deeplab/models/research/deeplab/datasets/pascal_voc_seg/VOCdevkit/VOC2012/ImageSets/Segmentation""   --image_format=""jpg""   --output_dir=""/home/zhang/Deeplab/models/research/deeplab/datasets/pascal_voc_seg/tfrecord""

But Error as fllow:

>> Converting image 666/2662 shard 0
>> Converting image 1332/2662 shard 1
>> Converting image 1998/2662 shard 2
>> Converting image 2662/2662 shard 3Traceback (most recent call last):
  File ""build_voc2012_data.py"", line 141, in <module>
    tf.app.run()
  File ""/home/zhang/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""build_voc2012_data.py"", line 137, in main
    _convert_dataset(dataset_split)
  File ""build_voc2012_data.py"", line 116, in _convert_dataset
    image_data = tf.gfile.FastGFile(image_filename, 'rb').read()
  File ""/home/zhang/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 120, in read
    self._preread_check()
  File ""/home/zhang/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 80, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""/home/zhang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: /home/zhang/Deeplab/models/research/deeplab/datasets/pascal_voc_seg/VOCdevkit/VOC2012/JPEGImages/.jpg; No such file or directory

In the last error cue is ""No such file or directory"", but all my files and directory are set as require.
where is wrong? Thanks.

",8,,[],2018-08-28 02:42:59,open,,"NamedUser(login=""SdustZhangzhen"")",[],2018-08-30 16:51:03
656,tensorflow/models,models,5193,Mr-zihao,cifar10_multi_gpu_train.py  can't use the multiGPUs,"I used cifar10_multi_gpu_train.py to test how to train models with multi-gpu，when I ran it,  I found the Volatile GPU-Utils of gpu:6 and gpu:7 were still 0. Why? Please help me, thanks!
![image](https://user-images.githubusercontent.com/34296901/44666712-942c4b00-aa4b-11e8-81e5-21ba26997446.png)
I added this line in the code.
![image](https://user-images.githubusercontent.com/34296901/44666788-bc1bae80-aa4b-11e8-9549-e70ab5234203.png)

",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-27 14:52:26,open,,,['stat:awaiting response'],2018-10-19 23:13:25
657,tensorflow/models,models,5191,ShiyuZhangUST,How can I perform a multi-label training?,"I want to train a model with 2 layers' output. For example, a person stand in front of a building, so that pixel belongs both to person and building. In my test, I add 2 softmax cross entropy for each layer. But the training result turned to be 2 layers with same color lump. The first layer's output is all class 1 and the second layer's output is all background.

Is the way I add cross entropy right?

    tf.losses.softmax_cross_entropy(
      one_hot_labels_layer1,
      tf.reshape(logits_layer1, shape = [-1, num_classes_layer1]),
      weights = mask_layer1,
      scope = '%s_%s' % (loss_scope, 'layer1'))

    tf.losses.softmax_cross_entropy(
      one_hot_labels_layer2,
      tf.reshape(logits_layer2, shape = [-1, num_classes_layer2]),
      weights = mask_layer2,
      scope = '%s_%s' % (loss_scope, 'layer2'))",1,"NamedUser(login=""ymodak"")","[NamedUser(login=""ymodak"")]",2018-08-27 02:16:31,open,,,['type:support'],2018-09-21 17:53:39
658,tensorflow/models,models,5174,EhsanVahab,Image compression time delay,"I've used image compression model to compress and store my images. the result is fantastic when I use iteration 15 but it takes a long time.
my OS specification is :
Windows 10 x64 , 16GB RAM , CPU Corei7 -6700HQ 2.6 GHz
I've seen when I use iteration 15 all the iteration results are performed and stored in results array.
In order to reduce time delay, I wanted to know is it possible to perform only iteration 15, without others [0,1,2...14]?
Or is there any way to reduce time delay and the output.npz file size?
",5,"NamedUser(login=""nmjohn"")","[NamedUser(login=""nmjohn"")]",2018-08-23 12:08:42,open,,,['stat:awaiting owner'],2018-11-12 00:14:08
659,tensorflow/models,models,5173,lianshushu,"if the model in rescaled image to (-1, 1) or (0 ,1) or just (0,255) in models/tutorials/image/imagenet/classify_image.py","if the model in rescaled image to (-1, 1) or (0 ,1) or just (0,255) in models/tutorials/image/imagenet/classify_image.py
model name is classify_image_graph_def.pb",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-23 11:07:06,open,,,['stat:awaiting response'],2018-08-27 10:38:49
660,tensorflow/models,models,5172,netanel-s,How to visualize results and export COCO json when evaluating using the new OD API?,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**: evaluating pretrained ssdlite_mobilenet_v2_coco_2018_05_09 using object_detection/model_main.py when changing configuration of the eval to have visualization_export_dir and export_path.

### Describe the problem
Since moving to the new OD API , I don't see how to visualize results and export detections to COCO json file. I tried using visualization_export_dir  and export_path in the eval configuration, but these happen using eval_util.visualize_detection_results in deprecated evalutor.py, and are not being used in the new OD API.",1,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""pkulzc"")]",2018-08-23 09:56:33,open,,,['stat:awaiting tensorflower'],2018-11-08 04:42:58
661,tensorflow/models,models,5168,Kayee6,Cannot build icp.so,"Background of my problem:
```
What is the top-level directory of the model you are using: vid2depth
Have I written custom code: NO
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: binary
TensorFlow version:('v1.5.0-0-g37aa430d84', '1.5.0')
Bazel version:0.16.1
CUDA/cuDNN version: CUDA 9.0 and CUDNN 6.0 or 7.0.x (cannot remember..)
GPU model and memory: 1080
Exact command to reproduce: bazel build ops:pcl_demo
```
I installed Bazel as the official site recommended, and run the command as above. And here is the error:

```
ERROR: /home/kj/.cache/bazel/_bazel_kj/6133cd459ec400cb06d5eb6c3cd88b0e/external/com_github_pointcloudlibrary_pcl/BUILD.bazel:329:1: no such package '@boost//': Error downloading [https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz] to /home/kj/.cache/bazel/_bazel_kj/6133cd459ec400cb06d5eb6c3cd88b0e/external/boost/boost_1_66_0.tar.gz: GET returned 403 Forbidden and referenced by '@com_github_pointcloudlibrary_pcl//:registration'
ERROR: Analysis of target '//ops:pcl_demo' failed; build aborted: no such package '@boost//': Error downloading [https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz] to /home/kj/.cache/bazel/_bazel_kj/6133cd459ec400cb06d5eb6c3cd88b0e/external/boost/boost_1_66_0.tar.gz: GET returned 403 Forbidden
INFO: Elapsed time: 14.759s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```
I am not familiar with the system, and I checked similar problems posted. But it seems not match to my problem. Some errors happen when I run `bazel build ops:icp_op.so` as well:
```
/home/kj/.cache/bazel/_bazel_kj/6133cd459ec400cb06d5eb6c3cd88b0e/external/jpeg/BUILD:126:12: Illegal ambiguous match on configurable attribute ""deps"" in @jpeg//:jpeg:
@jpeg//:k8
@jpeg//:armeabi-v7a
Multiple matches are not allowed unless one is unambiguously more specialized.
INFO: Elapsed time: 0.173s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```

I can see someone said he already built the `icp.so` successfully, but now I cannot even get the `.so` file. Please help!",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-22 23:04:27,open,,,[],2018-11-19 08:47:40
662,tensorflow/models,models,5165,harshilpatel312,Loss increases suddenly after training the model nicely for ~30 mins ,"### System information
- **What is the top-level directory of the model you are using**: models/research/object-detection
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.8.0    
- **GPU model and memory**: GeForce GTX 980 Ti
- **CUDA/cuDNN version**: N/A
- **Bazel version**: N/A
- **Have I written custom code**: N/A
- **Exact command to reproduce**: python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet101_coco.config --num_clones=2 --ps_tasks=1

### Describe the problem
I gathered labelled data for object detection using my own camera, trained the model, ran predictions, everything works okay. Then I decided to supplement the data with labelled data from Open Images Dataset: cleaned up data, added zero padding to resize it to 1920x1080, and trained on it. The loss decreased steadily, as expected, for ~30 mins, after which it suddenly increases and the model never converges after that (see attached TotalLoss log plots).

Could someone tell me what's wrong? I'm not sure if it is a bug or if I'm doing anything wrong.


**ZOOMED IN :**

![Zoomed In](https://user-images.githubusercontent.com/25410696/44469230-0557a280-a5f5-11e8-88e7-f42a4874f096.jpg)

**ZOOMED OUT :**

![Zoomed Out](https://user-images.githubusercontent.com/25410696/44469258-12749180-a5f5-11e8-8def-2e69a6b80f61.jpg)
   ",20,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-08-22 14:24:23,open,,,['stat:awaiting owner'],2018-09-19 17:48:26
663,tensorflow/models,models,5164,Ordgod,Update the URL of one script,The URL of the script for downloading and converting ImageNet data to TFRecord format has changed. I update the URL.,3,,[],2018-08-22 11:12:26,open,,,['cla: yes'],2018-08-22 11:15:04
664,tensorflow/models,models,5163,ucasiggcas,How to save the model?,"Dear,
https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py
how to save the model ?
Thx",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-08-22 07:00:36,open,,,[],2018-08-22 19:36:21
665,tensorflow/models,models,5162,mjjdick,"Message type ""object_detection.protos.SsdFeatureExtractor"" has no field named ""fpn"".","Hi, all:
    I just follow the install guide of `object detection`. everything is OK but when I testing the installation, the error is like follow:
```
.........E.E..EE..
======================================================================
ERROR: test_create_ssd_inception_v2_model_from_config (__main__.ModelBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 152, in test_create_ssd_inception_v2_model_from_config
    text_format.Merge(model_text_proto, model_proto)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 730, in _MergeField
    (message_descriptor.full_name, name))
ParseError: 64:9 : Message type ""object_detection.protos.Ssd"" has no field named ""use_expected_classification_loss_under_sampling"".

======================================================================
ERROR: test_create_ssd_mobilenet_v1_fpn_model_from_config (__main__.ModelBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 554, in test_create_ssd_mobilenet_v1_fpn_model_from_config
    text_format.Merge(model_text_proto, model_proto)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 730, in _MergeField
    (message_descriptor.full_name, name))
ParseError: 7:11 : Message type ""object_detection.protos.SsdFeatureExtractor"" has no field named ""fpn"".

======================================================================
ERROR: test_create_ssd_mobilenet_v2_model_from_config (__main__.ModelBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 707, in test_create_ssd_mobilenet_v2_model_from_config
    text_format.Merge(model_text_proto, model_proto)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 730, in _MergeField
    (message_descriptor.full_name, name))
ParseError: 64:9 : Message type ""object_detection.protos.Ssd"" has no field named ""weight_regression_loss_by_score"".

======================================================================
ERROR: test_create_ssd_resnet_v1_fpn_model_from_config (__main__.ModelBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 316, in test_create_ssd_resnet_v1_fpn_model_from_config
    text_format.Merge(model_text_proto, model_proto)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 730, in _MergeField
    (message_descriptor.full_name, name))
ParseError: 5:11 : Message type ""object_detection.protos.SsdFeatureExtractor"" has no field named ""fpn"".

----------------------------------------------------------------------
Ran 18 tests in 0.057s

FAILED (errors=4)

```

I searched on the net about these errors,but found nothing. so, anyone could help me? thx very much!!",8,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-08-22 01:21:14,open,,,[],2018-12-24 16:43:27
666,tensorflow/models,models,5160,jillelajitta,config file for SSD,"where can I find inceptionV3&V4 config file for SSD for coco.

Thank you.",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-21 22:02:15,open,,,['stat:awaiting response'],2018-08-22 06:54:39
667,tensorflow/models,models,5158,Aksei,Version1,Unicode decode error--   Issue  #5016,5,,[],2018-08-21 15:44:58,open,,,['cla: no'],2018-08-22 21:24:18
668,tensorflow/models,models,5156,pinakinathc,StringIO is not supported in python3,"putting `import StrinIO` in try-except block with `except ImportError` ensures that the code runs in python2 and python3

Author: Pinaki Nath Chowdhury <pinakinathc@gmail.com>",0,,[],2018-08-21 14:44:25,open,,,['cla: yes'],2018-08-21 14:44:28
669,tensorflow/models,models,5150,jackyLens,Audioset: Coverting the vggish_model.ckpt to *.pb file,"I am going to do a transfer learning based on Vggish model. 
However, the model file vggish_model.ckpt is about 280M.  Then I use the freeze_graph to produce pb file.
After doing this, I find that the pb file is nearly same to ckpt file.
Is it Normal or Something wrong with my code?
here is my code to convert ckpt to pb ：

with tf.Graph().as_default(), tf.Session() as sess:
    vggish_slim.define_vggish_slim()
    with tf.Graph().as_default():
        vggish_slim.define_vggish_slim(training=False)
        vggish_var_names = [v.name for v in tf.global_variables()]

    # Get the list of all currently existing variables that match
    # the list of variable names we just computed.
    vggish_vars = [v for v in tf.global_variables() if v.name in vggish_var_names]

    # Use a Saver to restore just the variables selected above.
    saver = tf.train.Saver(vggish_vars, name='vggish_load_pretrained',write_version=1)
    saver.restore(sess, checkpoint_path)
    tf.train.write_graph(sess.graph_def, 'model', 'model.pbtxt')
    freeze_graph.freeze_graph('model/model.pbtxt', '', False, checkpoint_path, 'vggish/embedding', 'save/restore_all',
                              'save/Const:0', 'model/frozen_model.pb', False, """")

",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-21 08:13:17,open,,,['stat:awaiting response'],2018-10-25 12:19:51
670,tensorflow/models,models,5149,gsaibro,[deeplab] pre-training as Auto-Encoder,"Hi everyone,

I have a question/feature request. Given that some of the databases available for semantic segmentation don't have a lot of images, is it possible to pre-training deeplab Xception as an Auto-Encoder? What modifications should I do in the code?

My idea is to train a checkpoint of deeplab Xception in a huge dataset as an Auto-Encoder raw image to raw image and then use this chekpoint to train the semantic segmentation model.

Thanks for your support.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
Deep Lab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, just the basic to train on others datasets
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.8
- **Bazel version (if compiling from source)**:
No idea
- **CUDA/cuDNN version**:
7.4
- **GPU model and memory**:
2 GTX 1080 TI 11GB
- **Exact command to reproduce**:
....
",0,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-21 08:12:47,open,,,[],2018-08-21 19:13:07
671,tensorflow/models,models,5148,shuiqingliu,fix: fine_tune_batch_norm not match the train_batch_size," We should set  fine_tune_batch_norm to false while the  train_batch_size is 4  to avoid the OOM of  the limited resource at hand.The details can be found in the file of train.py:
>  Set to True if one wants to fine-tune the batch norm parameters in DeepLabv3.
    Set to False and use small batch size to save GPU memory.
    flags.DEFINE_boolean('fine_tune_batch_norm', False,
                     'Fine tune the batch norm parameters or not.')
",0,,[],2018-08-21 06:37:42,open,,,['cla: yes'],2018-08-21 06:37:44
672,tensorflow/models,models,5147,RobotIntelligence,Transpose op only supports 1D-4D input arrays,"When I run tflite object detection on android demo, I got this problem:

java.lang.RuntimeException: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/contrib/lite/kernels/transpose.cc Transpose op only supports 1D-4D input arrays.Node number 148 (TRANSPOSE) failed to prepare.

Thanks ",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-08-21 03:24:43,open,,,['stat:awaiting response'],2019-03-21 08:22:22
673,tensorflow/models,models,5146,sumsuddin,Added support for specifying label weights for loss calculation,"This pull request adds support for specifying class weights as below,

```
_CUSTOM_SEG_INFORMATION = DatasetDescriptor(
    splits_to_sizes={
        'train': 8643,
        'trainval': 10803,
        'val': 2160,
    },
    num_classes=5,
    ignore_label=255,
    label_weights=[1.0, 5.0, 30.0, 10.0, 5.0],
)
```

Please have a look and let me know if any change required.",0,,[],2018-08-20 22:00:54,open,,,['cla: yes'],2018-08-20 22:01:49
674,tensorflow/models,models,5144,bleqdyce,Fix eval_config field - eval_interval_secs not working,"I think eval_interval_secs is originally designed to be passed as one of EvalSpec parameter - throttle_secs, which defines the frequency of saving checkpoint and evaluation.
Because setting eval_interval_secs is not working, there is no other way to change the frequency of evaluation.
Therefore, I simply parse eval_interval_secs from eval config and pass it to EvalSpec as a parameters to complete this functionality.",2,,[],2018-08-20 16:32:13,open,,,['cla: yes'],2019-02-10 05:15:45
675,tensorflow/models,models,5138,xunzhaogancao,google.protobuf.text_format.ParseError: 6:21 : 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape,"My environment: anaconda 4.2.0, python3.5.5, win10 64bit, tensorflow-gpu 1.10.0
When I test object_detection, It happens:
(C:\tensorflow\tensorflow) C:\tensorflow\tensorflow\models\research>python object_detection/builders/input_reader_builder_test.py
2018-08-20 09:08:53.105492: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-08-20 09:08:54.106143: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1405] Found device 0 with properties:
name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.3415
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 4.97GiB
2018-08-20 09:08:54.117708: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-20 09:08:55.612782: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-20 09:08:55.618360: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971]      0
2018-08-20 09:08:55.622696: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:984] 0:   N
2018-08-20 09:08:55.626112: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1843 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
E2018-08-20 09:08:55.773250: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-20 09:08:55.777761: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-20 09:08:55.782797: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971]      0
2018-08-20 09:08:55.785656: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:984] 0:   N
2018-08-20 09:08:55.788341: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1843 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
E..
======================================================================
ERROR: test_build_tf_record_input_reader (__main__.InputReaderBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1291, in _ConsumeSingleByteString
    result = text_encoding.CUnescape(text[1:-1])
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_encoding.py"", line 105, in CUnescape
    .decode('unicode_escape')
UnicodeDecodeError: 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/builders/input_reader_builder_test.py"", line 79, in test_build_tf_record_input_reader
    text_format.Merge(input_reader_text_proto, input_reader_proto)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 888, in _MergeScalarField
    value = tokenizer.ConsumeString()
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1251, in ConsumeString
    the_bytes = self.ConsumeByteString()
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1266, in ConsumeByteString
    the_list = [self._ConsumeSingleByteString()]
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1293, in _ConsumeSingleByteString
    raise self.ParseError(str(e))
google.protobuf.text_format.ParseError: 5:21 : 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

======================================================================
ERROR: test_build_tf_record_input_reader_and_load_instance_masks (__main__.InputReaderBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1291, in _ConsumeSingleByteString
    result = text_encoding.CUnescape(text[1:-1])
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_encoding.py"", line 105, in CUnescape
    .decode('unicode_escape')
UnicodeDecodeError: 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/builders/input_reader_builder_test.py"", line 111, in test_build_tf_record_input_reader_and_load_instance_masks
    text_format.Merge(input_reader_text_proto, input_reader_proto)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 888, in _MergeScalarField
    value = tokenizer.ConsumeString()
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1251, in ConsumeString
    the_bytes = self.ConsumeByteString()
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1266, in ConsumeByteString
    the_list = [self._ConsumeSingleByteString()]
  File ""C:\tensorflow\tensorflow\lib\site-packages\google\protobuf\text_format.py"", line 1293, in _ConsumeSingleByteString
    raise self.ParseError(str(e))
google.protobuf.text_format.ParseError: 6:21 : 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

----------------------------------------------------------------------
Ran 4 tests in 2.721s

FAILED (errors=2)",5,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-08-20 01:21:03,open,,,['stat:awaiting response'],2019-03-13 13:38:54
676,tensorflow/models,models,5136,kuilef,models not works on TF 1.8.0 because of tf.contrib.data module has been deprecated,"### System information
- **What is the top-level directory of the model you are using**: d:\Documents\GitHub\models 
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bit
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: 1080 Ti
- **Exact command to reproduce**: cd models/official/mnist  python mnist.py

### Describe the problem
running mnist.py script gives an error
AttributeError: module 'tensorflow.contrib.data' has no attribute 'TFRecordDataset'
which can be solved changing strings 70 and 77 as written in https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/data/README.md
Same thing with other models.

### Source code / logs
not working code:
```
  dataset = tf.contrib.data.TFRecordDataset([tfrecords_file])

  # For training, repeat the dataset forever
  if mode == tf.estimator.ModeKeys.TRAIN:
    dataset = dataset.repeat()

  # Map the parser over dataset, and batch results by up to batch_size
  dataset = dataset.map(parser, num_threads=1, output_buffer_size=batch_size)
```
working code:
```
  dataset = tf.data.TFRecordDataset([tfrecords_file])

  # For training, repeat the dataset forever
  if mode == tf.estimator.ModeKeys.TRAIN:
    dataset = dataset.repeat()

  # Map the parser over dataset, and batch results by up to batch_size
  dataset = dataset.map(parser, num_parallel_calls=1).prefetch(batch_size)
```",0,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-19 10:29:15,open,,,"['models: official', 'stat:awaiting tensorflower']",2019-02-01 21:51:16
677,tensorflow/models,models,5133,fortunedove,How to creat a own voc-2012dataset quickly?,"Hello!
I need a application on windows for creating dataset easily? I can't find a suitable software.
Anyone can provide? Thank you!",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-08-18 11:28:58,open,,,['stat:awaiting response'],2018-08-18 18:55:51
678,tensorflow/models,models,5132,lizaigaoge550,add_n ValueError,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-18 11:04:40,open,,,['stat:awaiting response'],2018-08-21 19:52:42
679,tensorflow/models,models,5129,arlcurten,Slim: Data Lost or Disorderly When Reading Data from tfrecord Files,"### System information
- **What is the top-level directory of the model you are using**: Research/Slim
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
 Yes, we tested the problem with few parameters changed, a tfPrint() to display data, and some tfrecord file with batch of same lable (please see my situation below)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: 0.15.0
- **CUDA/cuDNN version**: None (CPU used)
- **GPU model and memory**: None (CPU used)
- **Exact command to reproduce**: use fine-tuning_inception-v3_on_flowers shell script to run the example but need to generate tfrecord file with serial tags

### Describe the problem
Our datasets are from videos so we hope they can be read in their original order. We have 200 videos and we extracted 32 frames from each of them, tagging them with the same tag number for the same video. After disable the shuffle function, we still found that the order is changed or some data are missing as below when doing training. We used `tf.Print()` to print out the tags after `tf.train.batch()`. It appeared like:
`labels=[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]
....
labels=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]
....
labels=[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2]
....`
It didn't happen in each data read, just randomly decrease a few tags sometimes. When we applied the same method to evaluation, it was totally fine to show the result we want, like:
`labels=[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
labels=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
labels=[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]`
A bunch of 32 tags was read together, no data missing or disorderly.
I am not sure if it is a bug or not? Also, how can we make sure our data is read in order and without missing?
Thanks

### Source code / logs
in `train_image_classifier.py`
##############################################################
Create a dataset provider that loads data from the dataset
##############################################################
`with tf.device(deploy_config.inputs_device()):
  provider = slim.dataset_data_provider.DatasetDataProvider(
      dataset,
      num_readers=FLAGS.num_readers,
      shuffle=_data_shuffle,
      common_queue_capacity=20 * FLAGS.batch_size,
      common_queue_min=10 * FLAGS.batch_size)
  [image, label] = provider.get(['image', 'label'])
  label -= FLAGS.labels_offset
  train_image_size = FLAGS.train_image_size or network_fn.default_image_size

  image = image_preprocessing_fn(image, train_image_size, train_image_size)

  images, labels = tf.train.batch(
      [image, label],
      batch_size=FLAGS.batch_size,
      num_threads=FLAGS.num_preprocessing_threads,
      capacity=5 * FLAGS.batch_size)

  labels = tf.Print(labels,[labels], ""....labels="",summarize=32)

  labels = slim.one_hot_encoding(
      labels, dataset.num_classes - FLAGS.labels_offset)
  batch_queue = slim.prefetch_queue.prefetch_queue(
      [images, labels], capacity=2 * deploy_config.num_clones) `

Only `tf.Print()` is added in it.

We added `shuffle=false` and set `num_readers=1` in the script, trying to let the data_provider() read data in sequence without parallel jobs. For tf.train.batch(), `num_preprocessing_threads=1` is also set in script.",1,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-08-17 21:20:58,open,,,[],2018-08-27 16:34:55
680,tensorflow/models,models,5127,peterjliu,Update README.md,Add a note pointing to pointer-generator network.,0,"NamedUser(login=""peterjliu"")","[NamedUser(login=""peterjliu"")]",2018-08-17 17:21:11,open,,,['cla: yes'],2018-08-17 18:45:02
681,tensorflow/models,models,5125,abhishek202206,Poor AP and Detection results using faster R-CNN_inception_resnetV2_oid model,"


### System information
- **What is the top-level directory of the model you are using**:object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 18.04
- **TensorFlow installed from (source or binary)**:Source
- **TensorFlow version (use command below)**:1.10
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:Titax Xp
- **Exact command to reproduce**:


### Describe the problem
I have followed this article for using object detection on custom dataset https://blog.algorithmia.com/deep-dive-into-object-detection-with-open-images-using-tensorflow/. I have used google open images dataset and prepared the custom dataset as stated in the link. I used the model check point for inception_v2 model trained on OID. I trained the model for more than 142000 steps and the loss value also reduced substantially but the AP score for both classes have been extremely poor. 


![](https://screenshotscdn.firefoxusercontent.com/images/3158d6b2-7406-43ea-b4cc-35c1d186054b.png)

The config file is shown below.
model {
  faster_rcnn {
    num_classes: 2
    image_resizer {
      fixed_shape_resizer {
        height: 600
        width: 1024
      }
}
    feature_extractor {
      type: 'faster_rcnn_inception_resnet_v2'
      first_stage_features_stride: 16
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 8
        width_stride: 8
      }
    }
    first_stage_atrous_rate: 2
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.001
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 17
    maxpool_kernel_size: 1
    maxpool_stride: 1
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
        use_dropout: false
        dropout_keep_probability: 1.0
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.300000011921
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 1
  optimizer {
   adam_optimizer: {
        learning_rate {
            exponential_decay_learning_rate: {initial_learning_rate:0.00001}
            }
        }
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""object_detection/faster_rcnn_resnet101_coco/model.ckpt""
  from_detection_checkpoint: true
  # Note: The below line limits the training process to 800K steps, which we
  # empirically found to be sufficient enough to train the Open Images dataset.
  # This effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  #num_steps: 8000000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
   input_path: ""object_detection/data/train_window_door.record""
  }
  label_map_path: ""object_detection/data/label_map_train_door_windows.pbtxt""
}

eval_config: {
  #metrics_set: ""open_images_metrics""
  num_examples: 544
  metrics_set: ""open_images_V2_detection_metrics""
  #batch_queue_capacity: 50
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""object_detection/data/validate_window_door.record""
  }
  label_map_path: ""object_detection/data/label_map_train_door_windows.pbtxt""
  shuffle: true
  num_readers: 1
}

Please have a look at the output snapshot. Every time I run the eval.py I get the same image no matter how many epoch the model runs. If you have a look the model shows poor detection even after so many epochs.  
![](https://screenshotscdn.firefoxusercontent.com/images/e47303fa-afe2-4fa7-a2dd-cdb544dcb82b.png)

Any help would be much appreciated.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

",4,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-17 16:20:01,open,,,[],2018-09-10 12:50:32
682,tensorflow/models,models,5124,chowkamlee81,how to run deeplabv3+ on google cloud?,"Currently iam working on deeplabv3+ on google cloud.

Some scripts are there to upload slim to google cloud. But how to upload deeplab directory and make it package on google cloud. Kindly help

how to run deeplabv3+ on google cloud?",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-17 12:41:47,open,,,['stat:awaiting response'],2018-08-17 19:15:44
683,tensorflow/models,models,5121,noname2048,official.mnist.dataset data download error when using window10,"https://github.com/tensorflow/models/blob/b64f67d4632b82aa881e5c9cc164f9b1423f9e22/official/mnist/dataset.py#L77

I just download and run this code at my computer, got message like
```
Traceback (most recent call last):
  File ""C:/Users/csw9507/Desktop/PycharmProjects/tesorflow/1.py"", line 96, in <module>
    train('./MNIST2')
  File ""C:/Users/csw9507/Desktop/PycharmProjects/tesorflow/1.py"", line 91, in train
    return dataset(directory, 'train-images-idx3-ubyte', 'train-labels-idx1-ubyte')
  File ""C:/Users/csw9507/Desktop/PycharmProjects/tesorflow/1.py"", line 65, in dataset
    images_file = download(directory, images_file)
  File ""C:/Users/csw9507/Desktop/PycharmProjects/tesorflow/1.py"", line 58, in download
    os.remove(zipped_filepath)
PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\Users\\csw9507\\AppData\\Local\\Temp\\tmporbxe4ii.gz'
```

This problem occurs when run this code at Window10, not in mac.
Although it causes error, file will normally downloaded (but just first case)
MNIST Dataset need to download 4 files, so I need to run code 4 times with error.

I think this problem relate to window mystery.

I solve temporarily insert code like
```python
import sys
if 'win' not in sys.platform:
    os.remove(zipped_filepath)
else:
    print('%s will remain, delete menually later' % zipped_filepath)
```

Tensorflow installed using pip3, 1.9.0
Windows 10 Education, 1803, OS build 17134
Python 3.6.5 (with Microsoft Visual Studio 2017)
CUDA 9.0/cuDNN 7.2
GPU Geforce 840M, 2GB
",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-08-17 07:31:06,open,,,"['models: official', 'stat:awaiting tensorflower']",2019-02-01 21:52:33
684,tensorflow/models,models,5120,qingzew,how to do the same data augmentation in a batch,"I am using object detection api, like this: `python model_main.py`, with `tf.estimator` function,
the thing that I want to do is all images in a batch have the same augmentation, for example, if batch_size = 2,  after `random_rotation90()` is done, all images in the batch must be rotated with a same angle 90

I found that [PreprocessorCache](https://github.com/tensorflow/models/blob/fd7b6887fb294e90356d5664724083d1f61671ef/research/object_detection/core/preprocessor_cache.py) can do this, but I do not know how does it work with `tf.estimator`, 
or with [augment_input_data](https://github.com/tensorflow/models/blob/master/research/object_detection/inputs.py#L235) function.",3,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-08-17 07:23:20,open,,,[],2018-08-23 10:08:50
685,tensorflow/models,models,5119,aj96,How to freeze weights of deeplabv3+ for fine-tuning? ,I know that normally you are suppose to filter out variables you don't want to train to a particular scope as per this stackoverflow: https://stackoverflow.com/questions/35298326/freeze-some-variables-scopes-in-tensorflow-stop-gradient-vs-passing-variables But I do not see where to do this in the deeplab folder as they are using tf.slim to do all of the training. ,2,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-08-17 02:36:32,open,,,['stat:awaiting tensorflower'],2018-08-31 12:23:11
686,tensorflow/models,models,5117,yswang0522,(deeplabv3+) (mobilenetv2) training mobilenet_v2 on deeplabv3+ ,"**1.System information**

the top-level directory of the model **: /home/yswang/models3/research/deeplab
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): tensorflow-gpu 1.6.0
Bazel version (if compiling from source): None.
CUDA/cuDNN version: None.
GPU model and memory: Titan xp / 12GB / 8 GPU
Exact command to reproduce:
for the first 30k iterations :
NUM_ITERATIONS=30000
CUDA_VISIBLE_DEVICES=4,5,6,7 python ""${WORK_DIR}""/train.py 
--logtostderr 
--train_split=""trainaug"" 
--model_variant=""mobilenet_v2"" 
--atrous_rates=6 
--atrous_rates=12 
--atrous_rates=18 
--output_stride=16 
--decoder_output_stride=4 
--train_crop_size=513 
--train_crop_size=513 
--num_clones=4 
--train_batch_size=16 
--dataset=""pascal_voc_seg"" 
--training_number_of_steps=""${NUM_ITERATIONS}"" 
--fine_tune_batch_norm=True 
--tf_initial_checkpoint=""${INIT_FOLDER}/mobilenet_v2/mobilenet_v2_1.0_224.ckpt"" 
--train_logdir=""${TRAIN_LOGDIR}/gpu_test/20"" 
--dataset_dir=""${PASCAL_DATASET}""

**Describe the problem**
I trained the deeplabv3+ models with the mobilenet_v2_1.0_224 versions. 
I want to achieve the mIOU=75.7% in the table 7 of mobilenetv2 paper, but I got just 70.89% for 30k iterations. 

I followed the same training protocol as in deeplabv3 paper. firstly, train with following parameters(poly, initial lr=0.007, crop size = 513 x 513 , OS=16, fine-tuning batch=True, augmentation during training. ) with 4 GPU. and it performs 73.49% mIOU. 

The table7 in the mobilenetv2 paper describes that 75% mIOU without decoder. However, I achieved just 70.89% with decoder ( without MS COCO pretrained models. ) Does anyone tell me how to reproduce mIOU=75.7%?",6,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-08-17 01:43:42,open,,,"['models: research', 'stat:awaiting response']",2019-02-01 22:20:42
687,tensorflow/models,models,5116,yswang0522,"(deeplabv3+) (deeplabv3) reproduce ResNet mIOU 78.85%, but I got just 74.94% ","
**1.System information**
-  the top-level directory of the model **: /home/yswang/models3/research/deeplab
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 
- **TensorFlow installed from (source or binary)**: pip 
- **TensorFlow version (use command below)**: tensorflow-gpu 1.6.0 
- **Bazel version (if compiling from source)**: None. 
- **CUDA/cuDNN version**: None. 
- **GPU model and memory**: Titan xp / 12GB / 8 GPU 
- **Exact command to reproduce**:
for the first 30k iterations : 
NUM_ITERATIONS=30000
CUDA_VISIBLE_DEVICES=4,5,6,7 python ""${WORK_DIR}""/train.py \
  --logtostderr \
  --train_split=""trainaug"" \
  --model_variant=""resnet_v1_101_beta"" \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=16 \
  --decoder_output_stride=4 \
  --train_crop_size=513 \
  --train_crop_size=513 \
  --num_clones=4 \
  --train_batch_size=16 \
  --dataset=""pascal_voc_seg"" \
  --training_number_of_steps=""${NUM_ITERATIONS}"" \
  --fine_tune_batch_norm=True \
  --tf_initial_checkpoint=""${INIT_FOLDER}/resnet_v1_101/model.ckpt"" \
  --train_logdir=""${TRAIN_LOGDIR}/gpu_test/8_1"" \
  --dataset_dir=""${PASCAL_DATASET}""

for the rest 60k iterations : 
NUM_ITERATIONS=60000
CUDA_VISIBLE_DEVICES=4,5,6,7 python ""${WORK_DIR}""/train.py \
  --logtostderr \
  --train_split=""train"" \
  --model_variant=""resnet_v1_101_beta"" \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=8 \
  --decoder_output_stride=4 \
  --train_crop_size=513 \
  --train_crop_size=513 \
  --num_clones=4 \
  --train_batch_size=8 \
  --dataset=""pascal_voc_seg"" \
  --training_number_of_steps=""${NUM_ITERATIONS}"" \
  --fine_tune_batch_norm=False \
  --tf_initial_checkpoint=""${INIT_FOLDER}/resnet_v1_101/model.ckpt"" \
  --train_logdir=""${TRAIN_LOGDIR}/gpu_test/8_1"" \
  --dataset_dir=""${PASCAL_DATASET}""


### Describe the problem
I followed the same training protocol as in deeplabv3 paper. firstly, train with following parameters(poly, initial lr=0.007, crop size = 513 x 513 , OS=16, fine-tuning batch=True,  augmentation during training. ) with 4 GPU.  and it performs **73.49%** mIOU. after that, I keep to follow training protocol in deeplabv3(Rethinking ..) freeze BN parameters(False) and OS=8, train on the official PASCAL VOC 2012 trainaug set for another 30k iterations and smaller base learning rate = 0.001. But It performs 74.94% for the  PASCAL VOC 2012 val set. I used eval OS =16 for both settings. 
The table3 in the deeplabv3+ paper describes that 78.85% for train OS,eval OS, Decoder settings. But I got just 74.94%, not 78.85%. Does anyone tell me how to reproduce mIOU=78.85%? 


",3,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-17 01:34:37,open,,,"['models: research', 'stat:awaiting response']",2019-02-06 21:41:38
688,tensorflow/models,models,5114,zhiweige,Accuracy of the RNN for quickdraw Classification,"Hello,
  Thanks for sharing the work on quickdraw classification.
  I have tried to train the network using the default settings in the train_model.py (I changed the --steps=1000000), and I got the accuracy about 55%.
  I wonder whether I missed something during the training? Thanks.",5,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-17 00:32:58,open,,,['stat:awaiting owner'],2018-10-01 13:04:47
689,tensorflow/models,models,5112,szm2015,A more comprehensive documentation for training a model locally (using model_main.py),"Hello everyone,

I have been using the train.py code for training various object detection models. That code was pretty straightforward and the outputs were quite self-explanatory (the code outputted loss values). Now, as far as I have understood, in the new release of Object Detection API (July 13), train.py has moved to legacy and for training a model locally, one must use [model_main.py](https://github.com/tensorflow/models/blob/master/research/object_detection/model_main.py). Unfortunately, I can't really understand the training process in this code. Here are my problems (in training an SSD model on a custom dataset):

1- The code performs evaluation no matter what! I tried to set the parameters [max_evals](https://github.com/tensorflow/models/blob/b64f67d4632b82aa881e5c9cc164f9b1423f9e22/research/object_detection/protos/eval.proto#L17) and [eval_interval_secs](https://github.com/tensorflow/models/blob/b64f67d4632b82aa881e5c9cc164f9b1423f9e22/research/object_detection/protos/eval.proto#L14), but they don't seem to be used at all. So, to get around this, I made a small dataset, with only 6 images, to make the evaluation time as low as possible! I really don't want the code to perform any evaluation at all.

2- The are no ""Loss/classification_loss"", ""Loss/localization_loss"" or ""Loss/total_loss"" available in tensorboard for the training process, though they are there for evaluation:

![screenshot_2018-08-16 tensorboard](https://user-images.githubusercontent.com/16157023/44212390-825fc380-a180-11e8-94d2-d5b77c20cdd0.png)

The only thing I see for training is loss1 and loss2 which are exactly the same:

![screenshot_2018-08-16 tensorboard 1](https://user-images.githubusercontent.com/16157023/44226077-59512a00-a1a4-11e8-865b-7637d88c167b.png)

3- During the training process a huge number of lines such as these are displayed before every evaluation:

```
WARNING:tensorflow:Ignoring ground truth with image id 427235694 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 427235694 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 968669325 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 968669325 since it was previously added
WARNING:tensorflow:Ignoring ground truth with image id 1298836725 since it was previously added
WARNING:tensorflow:Ignoring detection with image id 1298836725 since it was previously added
```
I really have no idea what could these mean!

------------------------

### System information
- **What is the top-level directory of the model you are using**: models-master/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: From source
- **TensorFlow version (use command below)**: 1.10.0
- **Bazel version (if compiling from source)**: 0.16.0
- **CUDA/cuDNN version**: 9.2/7.1
- **GPU model and memory**: Geforce GTX 1070 8GB
- **Exact command to reproduce**: 
```
# From the tensorflow/models/research/ directory
PIPELINE_CONFIG_PATH={path to pipeline config file}
MODEL_DIR={path to model directory}
NUM_TRAIN_STEPS=50000
NUM_EVAL_STEPS=2000
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --num_eval_steps=${NUM_EVAL_STEPS} \
    --alsologtostderr
```
### Describe the problem
Well, I pretty much described the problem above. ",12,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-08-16 18:03:36,open,,,['stat:awaiting owner'],2019-03-20 22:53:04
690,tensorflow/models,models,5109,arijun,AttributeError: 'TrainConfig' object has no attribute 'update_trainable_variables' when running object detection,"I'm trying to run very basic object detection locally on my CPU, based on the tutorial (just using different images)

Just fetched the latest to solve an (I assume) unrelated problem, and got the following traceback:

```
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/home/ari/miniconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/ari/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 451, in train_and_evaluate
    return executor.run()
  File ""/home/ari/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 590, in run
    return self.run_local()
  File ""/home/ari/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 691, in run_local
    saving_listeners=saving_listeners)
  File ""/home/ari/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ari/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1145, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/ari/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1170, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/ari/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1133, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/ari/models/research/object_detection/model_lib.py"", line 320, in model_fn
    if train_config.update_trainable_variables else None)
AttributeError: 'TrainConfig' object has no attribute 'update_trainable_variables'
```
I tried doing a `pip install --upgrade` of tensorflow, which moved me from 1.9  to 1.10, it didn't help.

I assume it's expecting some modification to the pipeline configuration file which I haven't made. But since it's not in the sample .config files, and since there is no documentation for those files as far as I can tell, I'm not sure how to fix it.

As a band-aid I just reverted the offending code in model_lib.py, which got me past that error.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04 on Windows
- **TensorFlow installed from (source or binary)**:
Pip install
- **TensorFlow version (use command below)**:
v1.10.0-0-g656e7a2b34 1.10.0
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
```
~/models/research$ python object_detection/model_main.py     --pipeline_config_path=${PIPELINE_CONFIG_PATH}     --model_dir=${MODEL_DIR}     --num_train_steps=${NUM_TRAIN_STEPS}     --num_eval_steps=${NUM_EVAL_STEPS}     --alsologtostderr
You can collect some of this information using our environment capture script:
```",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-16 12:58:36,open,,,[],2018-08-17 06:51:07
691,tensorflow/models,models,5108,guillaume-michel,Fix NasNet used hiddenstates to match https://arxiv.org/abs/1707.07012,"I think there is a bug in NasNet normal and reduction cell: used_hiddenstates does not match the authors article. The current code introduces additional skip connections from prev prev cell to concat for normal cell and from second block to concat for reduction cell that are not present in the paper. This claim is easily verified by observing the graph of the provided checkpoint in tensorboard.

There are 3 possibilities:
- The schemas in the paper are wrong and should be fixed. Minor changes in the text should describe the motivations for these additional connections.
- The code is not correct and the present request can be used to fix it.
- I am missing something and it would be helpful if someone could clarify for me.

In case the code is wrong, provided checkpoints should be fixed.",5,,[],2018-08-16 11:03:54,open,,,['cla: no'],2018-08-20 10:27:06
692,tensorflow/models,models,5103,xueleihan,tensorflow/models/official/keras_application_models/benchmark_main.py,"when running tensorflow/models/official/keras_application_models/benchmark_main.py
when running benchmark_main.py model=inceptionv3
training.py"", line 108, in _standardize_input_data
    'Found: ' + str(data)[:200] + '...')
TypeError: Error when checking model input: data should be a Numpy array, or list/dict of Numpy arrays. Found: <RepeatDataset shapes: ((32, 224, 224, 3), (32, 1000)), types: 
",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-08-16 02:52:08,open,,,['stat:awaiting response'],2018-09-19 11:20:28
693,tensorflow/models,models,5099,jillelajitta,How to integrate tensorrt with tensorflow+ssd+mobilenet?,"Hi,
I am running tensorflow+ssd+mobilenet model on jetson tx2. I want to utilize tensorrt. Could you please tell me how to integrate tensorrt and boost fps.

Thank you.",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-08-15 19:25:29,open,,,['stat:awaiting response'],2018-08-16 07:02:52
694,tensorflow/models,models,5096,giacomobartoli,Update trainer.py - fix CUDNN_STATUS_INTERNAL_ERROR,"Some user still prefers to run concurrently legacy/train.py and legacy/eval.py instead of /model_main.py. What happens is that both files cannot be executed at the same time because the first one saturates the GPU memory. With just these 3 loc it is possible to avoid this problem.
At this [link](https://gist.github.com/giacomobartoli/694986a2d9a0d89bd9a92d58a3c1743f) you can find a gist of this reported error.",3,,[],2018-08-15 10:01:17,open,,,['cla: yes'],2018-08-15 10:24:01
695,tensorflow/models,models,5095,Jiewen2017,keras for object_detection,"------------------------

### System information
- **What is the top-level directory of the model you are using**: research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Bazel version (if compiling from source)**: No
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: 9.0
- **Exact command to reproduce**: No

Hi, all,

I reproduced a network architecture use keras these days. when i decide to use the object_detection API to make the network become a detection model, I failed. the API use the slim.

can you give some example to use keras for object_detection API? 

thanks a lot!

",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-08-15 09:32:35,open,,,[],2018-08-15 19:26:30
696,tensorflow/models,models,5091,gzmkl,[INTEL MKL] Upstream inter-/intra-op settings for slim models,"In this PR , we added following parameters to improve the training and evaluation
out-of-box experiences on CPU and make it easier for users to control the parallelism 
on CPU for slim models: 
 1. inter_op_parallelism_threads
 2. intra_op_parallelism_threads
",1,,[],2018-08-14 19:06:04,open,,,['cla: yes'],2018-08-28 19:03:34
697,tensorflow/models,models,5090,szm2015,Exact settings to train the provided SSD models on COCO dataset,"### System information
- **What is the top-level directory of the model you are using**:
tensorflow/models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **TensorFlow installed from (source or binary)**:
From source
- **TensorFlow version (use command below)**:
1.9.0
- **Bazel version (if compiling from source)**:
0.15
- **CUDA/cuDNN version**:
9.2/7.1
- **GPU model and memory**:
GTX Geforce 1070
- **Exact command to reproduce**:

### Describe the problem
I was wondering if there is any document, explanation, anything! to guide through training the provided [ssd configs](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs) on COCO dataset. I believe that the provided config files are configured for fine-tuning the [pre-trained weights on COCO](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models) on another dataset. But, I want to know how to fine-tune a SSD model, say ssd_inception_v2, from a pre-trained model on ImageNet on COCO dataset. I have plenty of experience in fine-tuning a COCO pre-train model on another dataset ([UA-DETRAC](http://detrac-db.rit.albany.edu/)) and they all have been successful, I have also fine-tuned models trained on ImageNet on the same dataset and these kinds of models also work. However, I'm not able to train the provided configs on COCO dataset to create the reported results. I have examined with quite a number of training hyper-parameters and all of them have failed (by failing I mean getting an AP of 0!).

",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-08-14 17:58:26,open,,,[],2019-01-03 15:58:59
698,tensorflow/models,models,5089,RichardLiee,object_detection/export_tflite_ssd_graph.py failed [ssdlite_mobilenet_v2_coco_2018_05_09],"

### System information
- **What is the top-level directory of the model you are using**: research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9.0
- **Bazel version (if compiling from source)**:no
- **CUDA/cuDNN version**:7.0
- **GPU model and memory**:9.0
- **Exact command to reproduce**: NA

I export grapy which ssdlite_mobilenet_v2_coco, fine tune the pretrain model from model zoo ssdlite_mobilenet_v2_coco,
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
here is my config script:


<<<<<<<<<<<<<<
CONFIG_FILE=/home/vip/TisanBrain/sample/objd/models/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config
CHECKPOINT_PATH=/home/vip/fastData/objd/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt-50000.index                                                                                                         
OUTPUT_DIR=/home/vip/fastData/objd/ssdlite_mobilenet_v2_coco_2018_05_09/

python object_detection/export_tflite_ssd_graph.py \
    --pipeline_config_path=${CONFIG_FILE} \
    --trained_checkpoint_prefix=${CHECKPOINT_PATH} \
    --output_directory=$OUTPUT_DIR \
    --add_postprocessing_op=true
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<,

here is error log:

2018-08-14 19:19:11.789173: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2018-08-14 19:19:12.615432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:17:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-08-14 19:19:12.735887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:65:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-08-14 19:19:12.736823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1
2018-08-14 19:19:13.210824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-14 19:19:13.210870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 
2018-08-14 19:19:13.210878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y 
2018-08-14 19:19:13.210883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N 
2018-08-14 19:19:13.214277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10411 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)
2018-08-14 19:19:13.317100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10410 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
2018-08-14 19:19:14.348039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1
2018-08-14 19:19:14.348136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-14 19:19:14.348144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 
2018-08-14 19:19:14.348151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y 
2018-08-14 19:19:14.348156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N 
2018-08-14 19:19:14.348320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10411 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)
2018-08-14 19:19:14.348404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10410 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Tensor name ""BoxPredictor_0/BoxEncodingPredictor/biases"" not found in checkpoint files /home/vip/fastData/objd/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt-50000.index
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_301 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_306_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/export_tflite_ssd_graph.py"", line 137, in <module>
    tf.app.run(main)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/export_tflite_ssd_graph.py"", line 133, in main
    FLAGS.max_classes_per_detection)
  File ""/home/vip/TisanBrain/sample/objd/models/research/object_detection/export_tflite_ssd_graph_lib.py"", line 262, in export_tflite_graph
    initializer_nodes='')
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/tools/freeze_graph.py"", line 104, in freeze_graph_with_def_protos
    saver.restore(sess, input_checkpoint)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1768, in restore
    six.reraise(exception_type, exception_value, exception_traceback)
  File ""/usr/lib/python3/dist-packages/six.py"", line 686, in reraise
    raise value
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1752, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Tensor name ""BoxPredictor_0/BoxEncodingPredictor/biases"" not found in checkpoint files /home/vip/fastData/objd/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt-50000.index
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_301 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_306_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op 'save/RestoreV2', defined at:
  File ""object_detection/export_tflite_ssd_graph.py"", line 137, in <module>
    tf.app.run(main)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/export_tflite_ssd_graph.py"", line 133, in main
    FLAGS.max_classes_per_detection)
  File ""/home/vip/TisanBrain/sample/objd/models/research/object_detection/export_tflite_ssd_graph_lib.py"", line 248, in export_tflite_graph
    saver = tf.train.Saver(**saver_kwargs)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1284, in __init__
    self.build()
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1296, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1333, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal
    restore_sequentially, reshape)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 400, in _AddRestoreOps
    restore_sequentially)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 832, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/home/vip/tensorflow/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Tensor name ""BoxPredictor_0/BoxEncodingPredictor/biases"" not found in checkpoint files /home/vip/fastData/objd/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt-50000.index
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_301 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_306_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
",18,"NamedUser(login=""achowdhery"")","[NamedUser(login=""achowdhery""), NamedUser(login=""pkulzc"")]",2018-08-14 11:22:20,open,,"NamedUser(login=""RichardLiee"")",[],2018-10-09 07:32:26
699,tensorflow/models,models,5088,piotrekm7,"Object detection low performance after Feb 10, 2018 commit","When running refactored object_detection_tutorial.ipynb modified to camera stream i have very low frame rate, like 0.5 frame per second.  Older script works fine, above 30 frames per second easily.
Seems like script doesn't use gpu, memory is allocated, but gpu usage is around 1%.
Tested on gtx 960m, 980ti , 1080ti. Same  behaviour.
Tensorflow gpu 1.6 or 1.8",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-14 09:18:20,open,,,[],2018-08-16 20:27:26
700,tensorflow/models,models,5080,deepkshikha,Tensorrt compilation issue,"Below is the last few lines when I run tensorrt.py with all parameters



Running INT8 graph
2018-08-13 13:42:29.249291: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 4
2018-08-13 13:42:30.004405: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:419] MULTIPLE tensorrt candidate conversion: 2
2018-08-13 13:42:30.022164: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3090] Max batch size= 128 max workspace size= 24292802
2018-08-13 13:42:30.022202: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3104] finished op preparation
2018-08-13 13:42:30.022227: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3112] OK
2018-08-13 13:42:30.022236: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3113] finished op building
2018-08-13 13:42:30.198919: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3090] Max batch size= 128 max workspace size= 2123190784
2018-08-13 13:42:30.199123: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3104] finished op preparation
2018-08-13 13:42:30.199394: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3112] OK
2018-08-13 13:42:30.199408: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3113] finished op building
INFO:tensorflow:Starting execution
2018-08-13 13:42:34.610332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3
2018-08-13 13:42:34.610409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-13 13:42:34.610421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3
2018-08-13 13:42:34.610430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y
2018-08-13 13:42:34.610436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y
2018-08-13 13:42:34.610443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y
2018-08-13 13:42:34.610449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N
2018-08-13 13:42:34.611918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 16248 MB memory) -> physical GPU (device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0)
2018-08-13 13:42:34.612048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 16249 MB memory) -> physical GPU (device: 1, name: Tesla V100-DGXS-32GB, pci bus id: 0000:08:00.0, compute capability: 7.0)
2018-08-13 13:42:34.612138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 16249 MB memory) -> physical GPU (device: 2, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0e:00.0, compute capability: 7.0)
2018-08-13 13:42:34.612221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 16249 MB memory) -> physical GPU (device: 3, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0)
INFO:tensorflow:Starting Warmup cycle
Segmentation fault (core dumped)


I am using Tesla V100

and 

Cuda compilation tools, release 9.0, V9.0.176
Tensorflow version : 1.8

Please suggest

",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-08-13 13:51:13,open,,,[],2018-08-16 01:09:57
701,tensorflow/models,models,5079,chokobole,Fix typo,,3,,[],2018-08-13 11:55:01,open,,,['cla: yes'],2018-08-13 11:57:32
702,tensorflow/models,models,5076,YijinLiu,Could only save 5 checkpoints w/ model_main.py,"### System information
- **What is the top-level directory of the model you are using**:
research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.10
- **Bazel version (if compiling from source)**:
0.15.2
- **CUDA/cuDNN version**:
9.2/7.1.4.18
- **GPU model and memory**:
GTX 960/4G
- **Exact command to reproduce**:
python tf_models/research/object_detection/model_main.py --alsologtostderr \
            --pipeline_config_path=ssdlite_mobilenet_v2_coco.config --model_dir=XXX
### Describe the problem
Looks like the new model_main.py hardcodes many parameters. It simply ignores keep_checkpoint_every_n_hours. In another [bug](https://github.com/tensorflow/models/issues/5067), it only exports one image to tensorboard, ignoring
num_visualizations.",14,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-08-13 06:39:06,open,,"NamedUser(login=""YijinLiu"")",[],2019-02-12 09:56:28
703,tensorflow/models,models,5075,wuchichung,model from train.py is not the same as model_main train,"I am using train.py to train my own dataset. The model make sense. However, when use model_main.py train. The model is totally off. I want to know what is the possible error ? I think one of the problem may be from the tf record file. Is there any additional information added into the tf record required ?",4,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-13 02:46:26,open,,,['stat:awaiting response'],2019-01-27 13:58:08
704,tensorflow/models,models,5074,Jinksi,Fix typo,,3,,[],2018-08-13 02:39:30,open,,,['cla: yes'],2018-08-13 02:42:41
705,tensorflow/models,models,5064,npow,fixes for `ptb_word_lm.py`,Fixes #3382 and #2709,6,,[],2018-08-11 00:03:59,open,,,['cla: yes'],2018-08-17 15:38:45
706,tensorflow/models,models,5057,dcyoung,TF v1.10 issues with repeated evaluation during estimator based training,"### System information
- **What is the top-level directory of the model you are using**:
`models/tree/master/research/object_detection`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu `16.04`, Ubuntu `18.04` and tensorflow's own docker container `latest-gpu-py3`
- **TensorFlow installed from (source or binary)**:
binary, or included on `latest-gpu-py3` docker image
- **TensorFlow version (use command below)**:
working -> `v1.9`, broken -> `v1.10`
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
`7.0`
- **GPU model and memory**:
`1080Ti`
- **Exact command to reproduce**:
```bash
export CUDA_DEVICE_ORDER=""PCI_BUS_ID""
export CUDA_VISIBLE_DEVICES=""0""

# From the tensorflow/models/research/ directory
PIPELINE_CONFIG_PATH={path to pipeline config file}
MODEL_DIR={path to model directory}
NUM_TRAIN_STEPS=200000
NUM_EVAL_STEPS=8000
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --num_eval_steps=${NUM_EVAL_STEPS} \
    --alsologtostderr
```

### Describe the problem
Using tensorflow v1.9, estimator based training properly interleaved training and evaluation.

But, using tensorflow v1.10, and attempting to train a mask rcnn based network (perhaps other, but I haven't tried) results in repeated evaluations with no training in between. 

More specifically, on v1.10, it appears the model trains until the first evaluation, and then proceeds to repeatedly evaluate. 
For example: 
```
-> first ~1400 steps are training... visible in tensorboard
-> eval at step 1401 visible in tensorboard
-> eval at step 1402 visible in tensorboard
-> eval at step 1403 visible in tensorboard
....
```",0,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-08-10 16:17:06,open,,,[],2018-08-15 14:32:23
707,tensorflow/models,models,5056,aseelam,TypeError: non_max_suppression() got an unexpected keyword argument 'score_threshold',"### System information
- **What is the top-level directory of the model you are using**:
/models/research/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.10.0,1.8.0
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**:9.0,7.0
- **GPU model and memory**:Nvidia Quadro P3000
- **Exact command to reproduce**:
python object_detection/model_main.py     --pipeline_config_path=${PIPELINE_CONFIG_PATH}     --model_dir=${MODEL_DIR}     --num_train_steps=${NUM_TRAIN_STEPS}     --num_eval_steps=${NUM_EVAL_STEPS}     --alsologtostderr

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

TypeError: non_max_suppression() got an unexpected keyword argument 'score_threshold' when running TF object detection API. Looks like the recent commit by @pkulzc 59f7e80 has some bug.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 439, in train_and_evaluate
    executor.run()
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 518, in run
    self.run_local()
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 650, in run_local
    hooks=train_hooks)
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 363, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 843, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 856, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 831, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/anikethseelam/HAVAL/Summer2018/repos/models/research/object_detection/model_lib.py"", line 252, in model_fn
    preprocessed_images, features[fields.InputDataFields.true_image_shape])
  File ""/home/anikethseelam/HAVAL/Summer2018/repos/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 680, in predict
    self._anchors.get(), image_shape, true_image_shapes))
  File ""/home/anikethseelam/HAVAL/Summer2018/repos/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 767, in _predict_second_stage
    anchors, image_shape_2d, true_image_shapes)
  File ""/home/anikethseelam/HAVAL/Summer2018/repos/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1234, in _postprocess_rpn
    clip_window=clip_window)
  File ""/home/anikethseelam/HAVAL/Summer2018/repos/models/research/object_detection/core/post_processing.py"", line 402, in batch_multiclass_non_max_suppression
    parallel_iterations=parallel_iterations)
  File ""/home/anikethseelam/HAVAL/Summer2018/repos/models/research/object_detection/utils/shape_utils.py"", line 228, in static_or_dynamic_map_fn
    return tf.map_fn(fn, elems, dtype, parallel_iterations, back_prop)
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/functional_ops.py"", line 423, in map_fn
    swap_memory=swap_memory)
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3224, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2956, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2893, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/home/anikethseelam/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/functional_ops.py"", line 413, in compute
    packed_fn_values = fn(packed_values)
  File ""/home/anikethseelam/HAVAL/Summer2018/repos/models/research/object_detection/core/post_processing.py"", line 378, in _single_image_nms_fn
    additional_fields=per_image_additional_fields)
  File ""/home/anikethseelam/HAVAL/Summer2018/repos/models/research/object_detection/core/post_processing.py"", line 150, in multiclass_non_max_suppression
    score_threshold=score_thresh)
TypeError: non_max_suppression() got an unexpected keyword argument 'score_threshold'
",18,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-08-10 15:33:15,open,,,[],2019-03-25 20:30:53
708,tensorflow/models,models,5055,KosukeArase,README for Domain Separation Network,"Thank you for open sourcing your great work! However, the code have some problems.
It depends on a binary file that does not work on different architectures (ex. MacOS).
I also found problem in creating TFRecords, and  I fixed them.

",0,,[],2018-08-10 12:43:13,open,,,['cla: yes'],2018-08-14 05:33:45
709,tensorflow/models,models,5054,dichotomies,TensorFlow Slim: Inception V2 Confusion,"On https://github.com/tensorflow/models/tree/master/research/slim, `Inception V2` refers to [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167).

This reference seems contrary to the definition of Inception V2 in the paper [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567.pdf), because it is introduced there.

The paper you refer to is about BN-Inception if I'm not mistaken.",3,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-08-10 11:37:50,open,,,"['stat:contributions welcome', 'type:docs']",2018-08-21 09:43:21
710,tensorflow/models,models,5053,seth-johnson-sp,object_detection training silently exits without doing anything,"------------------------

### System information
- **What is the top-level directory of the model you are using**:
MODEL_DIR=/home/seth/tensorflow/BurmesePython/ssd_inception_v2_coco_2018_01_28
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: running bundled script object_detection/model_main.py
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
# more tf_env.txt 

== cat /etc/issue ===============================================
Linux ubuntu 4.15.0-29-generic #31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2
018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ubuntu 4.15.0-29-generic #31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2
018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                         1.14.5                
protobuf                      3.6.0                 
tensorflow                    1.10.0                

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = v1.10.0-0-g656e7a2b34
tf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./env.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

- **TensorFlow installed from (source or binary)**: installed using pip3
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**:n/a
- **GPU model and memory**:n/a
- **Exact command to reproduce**:

MODEL_DIR=/home/seth/tensorflow/BurmesePython/ssd_inception_v2_coco_2018_01_28
PIPELINE_CONFIG_PATH=/home/seth/tensorflow/BurmesePython/ssd_inception_v2_coco.config
NUM_TRAIN_STEPS=200000
NUM_EVAL_STEPS=2000
python3 object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --num_eval_steps=${NUM_EVAL_STEPS} \
    --alsologtostderr

### Describe the problem

Trying to train my model using the tf records I've created.  Seems like processing never begins, but the pipeline.config file is written to the models directory as the timestamp is updated. Modifying my model config file to include broken references to tf record files does not yield an error, so it seems that the training process doesn't ever begin. 

I have modified the model_main.py file to enable logging: tf.logging.set_verbosity(tf.logging.DEBUG)

Running tensorboard in parallel also never observes any activity.

### Source code / logs

```

INFO:tensorflow:Maybe overwriting eval_steps: 2000
INFO:tensorflow:Maybe overwriting train_steps: 200000
INFO:tensorflow:Maybe overwriting retain_original_images_in_eval: True
INFO:tensorflow:Maybe overwriting load_pretrained: True
INFO:tensorflow:Ignoring config override key: load_pretrained
INFO:tensorflow:create_estimator_and_inputs: use_tpu False
INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3c53771ba8>, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tf_random_seed': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_log_step_count_steps': 100, '_save_summary_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_save_checkpoints_secs': 600, '_device_fn': None, '_master': '', '_train_distribute': None, '_is_chief': True, '_task_id': 0, '_model_dir': '/home/seth/tensorflow/BurmesePython/ssd_inception_v2_coco_2018_01_28', '_session_config': None, '_service': None}
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f3c53712b70>) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:Writing pipeline config file to /home/seth/tensorflow/BurmesePython/ssd_inception_v2_coco_2018_01_28/pipeline.config
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.
INFO:tensorflow:Skipping training since max_steps has already saved.


```",8,,[],2018-08-10 09:50:05,open,,,[],2018-11-25 08:30:08
711,tensorflow/models,models,5052,as32608,"models/research/object_detection/core/post_processing.py recent commit breaks the code, with error message unknown keyword argument ""score_threshold"".","models/research/object_detection/core/post_processing.py recent commit breaks the code, with error message unknown keyword argument ""score_threshold"".

Previous commits used to work fine.",3,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-08-10 05:02:50,open,,,['stat:awaiting owner'],2018-08-17 21:34:02
712,tensorflow/models,models,5051,Programmerwyl,Where does the protoc generated configuration file save( data augmentation),"- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.8
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:3G
- **Exact command to reproduce**:

How do ssd_mobilenet_v1_pets. Config and preprocessor. Proto and preprocessor_pb2.py connect  data augmentation

Did preprocessor_pb2.py save the image preprocessing configuration file locally during training, where did it stay

thanks!",2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-08-10 02:30:56,open,,,[],2018-08-20 01:35:04
713,tensorflow/models,models,5049,accraze,Update VariationalAutoencoder.py,"Added `axis=1` for reconstruction error calculation. This will avoid taking the sum of cost so that the training objective will no longer be batch-size dependent.

Fixes #2243",2,,[],2018-08-10 00:29:03,open,,,['cla: yes'],2018-08-22 02:14:48
714,tensorflow/models,models,5042,SubWayss,Training my own data set is worse than the pre training model,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-09 08:57:37,open,,,[],2018-10-15 23:31:47
715,tensorflow/models,models,5041,dandiao190,cannot import name 'faster_rcnn_inception_v2_feature_extractor',"hi,
I download the package then follow the installation steps(https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md).
When I did the last one and tested the installation in the direction of tensorflow/models/research/, running the following command:
python object_detection/builders/model_builder_test.py

Traceback (most recent call last):
File ""object_detection/builders/model_builder_test.py"", line 26, in
from object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2
ImportError: cannot import name 'faster_rcnn_inception_v2_feature_extractor'

Hope your help, thanks.",6,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-08-09 08:27:55,open,,,[],2018-08-15 03:21:09
716,tensorflow/models,models,5039,TheFlashover,Bounding boxes of different shapes,"Is it possible to train and to draw bounding boxes not only rectangular shape, but triangular or trapezoidal shape using tensorflow/object-detection?
And is it possible to draw a line drawn at a certain angle instead of bounding box?

Example
![eraser_or_not_eraser](https://user-images.githubusercontent.com/35437615/43886522-eeeec102-9bf6-11e8-9ed5-682b0e197b9b.jpg)

What is the top-level directory of the model you are using: NA
Have I written custom code: NA
OS Platform and Distribution: Win 10
TensorFlow installed from: NA
TensorFlow version: 1.6
Bazel version: NA
CUDA/cuDNN version: NA
GPU model and memory: Nvidia 750 Ti, 2Gb
Exact command to reproduce: NA
",2,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-08-09 08:09:59,open,,,"['stat:awaiting owner', 'stat:awaiting response']",2018-08-17 21:38:58
717,tensorflow/models,models,5038,LouisDLK,Im2txt inference with frozen_graph.pb,"Hi everyone,

### Describe the problem
I want to use a graph (.pb)  (a frozen or optimized one) on im2txt for inference. Is there a way to load the model from the .pb? It will be a helpfull feature to add for embedded device.

### Idea
In inference_wrapper_base (https://github.com/tensorflow/models/blob/master/research/im2txt/im2txt/inference_utils/inference_wrapper_base.py) there is "" build_graph_from proto "" but a SaverDef is needed.

In advance, thanks!


What is the top-level directory of the model you are using : https://github.com/tensorflow/models/tree/master/research/im2txt

Have I written custom code : N/A
OS Platform and Distribution : N/A
TensorFlow installed from : N/A
TensorFlow version : N/A
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory :N/A
Exact command to reproduce : N/A",1,"NamedUser(login=""cshallue"")","[NamedUser(login=""cshallue"")]",2018-08-09 06:48:48,open,,,['stat:awaiting response'],2018-08-17 17:45:19
718,tensorflow/models,models,5037,ghost,Warning when using TOCO to convert SSD Quantized MobileNet from model zoo,"### System information
- **What is the top-level directory of the model you are using**: ssd_mobilenet_v1_0.75_depth_coco and ssd_mobilenet_v1_quantized_coco models taken from model zoo: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: ('v1.9.0-0-g25c197e023', '1.9.0')
- **Bazel version (if compiling from source)**: 0.15.2
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `~/Documents/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=tflite_graph.pb --output_file=detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor **--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'** --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops`

### Describe the problem
When running TOCO on the SSD quantized models taken from the model zoo a warning is given about a conversion made that may result in low inference accuracy. ~~I can confirm that the inference accuracy is poor when running the networking using TFLite on Android.~~ **EDIT:** I'm now getting decent results. It turns out that the COCO labels skip some numbers, which I was not accounting for in an enum definition. However I would still like to know if this warning is cause for concern.

I also found a download link for a pre-converted model here: https://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip ~~- this also gives poor inference results.~~

**ADDITIONAL SUGGESTION:** It would be nice if the model zoo also provided the tflite conversions so that end users would not have to run TOCO. This should be fairly easy to do and would be very useful.

### Source code / logs
Relevant TOCO Output line:
`2018-08-08 22:34:40.146364: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:88] Constant array anchors lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.`

Full TOCO Output for 0.75 model:
```
2018-08-08 22:34:39.530307: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: TFLite_Detection_PostProcess
2018-08-08 22:34:39.560136: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 900 operators, 1355 arrays (0 quantized)
2018-08-08 22:34:39.596850: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 900 operators, 1355 arrays (0 quantized)
2018-08-08 22:34:40.076775: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 112 operators, 224 arrays (1 quantized)
2018-08-08 22:34:40.079683: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 112 operators, 224 arrays (1 quantized)
2018-08-08 22:34:40.081171: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 65 operators, 177 arrays (1 quantized)
2018-08-08 22:34:40.082159: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 65 operators, 177 arrays (1 quantized)
2018-08-08 22:34:40.133134: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 1: 71 operators, 183 arrays (151 quantized)
2018-08-08 22:34:40.137098: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 2: 71 operators, 183 arrays (155 quantized)
2018-08-08 22:34:40.140418: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 3: 66 operators, 178 arrays (157 quantized)
2018-08-08 22:34:40.143706: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 4: 66 operators, 178 arrays (158 quantized)
2018-08-08 22:34:40.146364: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:88] Constant array anchors lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-08-08 22:34:40.147048: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 5: 64 operators, 176 arrays (159 quantized)
2018-08-08 22:34:40.150294: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before shuffling of FC weights: 64 operators, 176 arrays (159 quantized)
2018-08-08 22:34:40.151181: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 2160064 bytes, theoretical optimal value: 1620032 bytes.
2018-08-08 22:34:40.151447: I tensorflow/contrib/lite/toco/toco_tooling.cc:392] Estimated count of arithmetic ops: 1.47434 billion (note that a multiply-add is counted as 2 ops).
```

Full TOCO Output for 1.0 model:
```
2018-08-08 22:50:14.665584: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: TFLite_Detection_PostProcess
2018-08-08 22:50:14.692447: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 900 operators, 1355 arrays (0 quantized)
2018-08-08 22:50:14.721198: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 900 operators, 1355 arrays (0 quantized)
2018-08-08 22:50:15.609686: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 112 operators, 224 arrays (1 quantized)
2018-08-08 22:50:15.616612: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 112 operators, 224 arrays (1 quantized)
2018-08-08 22:50:15.618133: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 65 operators, 177 arrays (1 quantized)
2018-08-08 22:50:15.619248: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 65 operators, 177 arrays (1 quantized)
2018-08-08 22:50:15.710603: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 1: 71 operators, 183 arrays (151 quantized)
2018-08-08 22:50:15.717264: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 2: 71 operators, 183 arrays (155 quantized)
2018-08-08 22:50:15.723055: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 3: 66 operators, 178 arrays (157 quantized)
2018-08-08 22:50:15.728960: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 4: 66 operators, 178 arrays (158 quantized)
2018-08-08 22:50:15.733798: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:88] Constant array anchors lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-08-08 22:50:15.734546: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 5: 64 operators, 176 arrays (159 quantized)
2018-08-08 22:50:15.740297: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before shuffling of FC weights: 64 operators, 176 arrays (159 quantized)
2018-08-08 22:50:15.741305: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 2880000 bytes, theoretical optimal value: 2160000 bytes.
2018-08-08 22:50:15.741685: I tensorflow/contrib/lite/toco/toco_tooling.cc:392] Estimated count of arithmetic ops: 2.49483 billion (note that a multiply-add is counted as 2 ops).
```",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-09 03:04:35,open,,,[],2018-11-30 07:38:03
719,tensorflow/models,models,5034,sheetaldhar,Issue running train.py,"Hi,
I am running the object detection model locally and on EC2 Deep Learning AMI. Please help me with current issues when I run train.py 

On EC2:
File ""train.py"", line 47, in <module>
    import tensorflow as tf
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/google/protobuf/descriptor.py"", line 46, in <module>
    from google.protobuf.pyext import _message
ImportError: /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/../lib/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/google/protobuf/pyext/_message.cpython-36m-x86_64-linux-gnu.so)

-----------------------------------------------------------------------------------
On Local Machine:
File ""train.py"", line 49, in <module>
    from object_detection.builders import dataset_builder
  File ""/anaconda3/lib/python3.6/site-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py"", line 27, in <module>
    from object_detection.data_decoders import tf_example_decoder
  File ""/anaconda3/lib/python3.6/site-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py"", line 27, in <module>
    from object_detection.protos import input_reader_pb2
  File ""/anaconda3/lib/python3.6/site-packages/object_detection-0.1-py3.6.egg/object_detection/protos/input_reader_pb2.py"", line 23, in <module>
    serialized_pb=_b('\n*object_detection/protos/input_reader.proto\x12\x17object_detection.protos\""\xd0\x06\n\x0bInputReader\x12\x18\n\x0elabel_map_path\x18\x01 \x01(\t:\x00\x12\x15\n\x07shuffle\x18\x02 \x01(\x08:\x04true\x12!\n\x13shuffle_buffer_size\x18\x0b \x01(\r:\x04\x32\x30\x34\x38\x12*\n\x1d\x66ilenames_shuffle_buffer_size\x18\x0c \x01(\r:\x03\x31\x30\x30\x12\x15\n\nnum_epochs\x18\x05 \x01(\r:\x01\x30\x12\x17\n\x0bnum_readers\x18\x06 \x01(\r:\x02\x36\x34\x12\x1f\n\x14num_parallel_batches\x18\x13 \x01(\r:\x01\x38\x12\x1f\n\x14num_prefetch_batches\x18\x14 \x01(\x05:\x01\x32\x12 \n\x0equeue_capacity\x18\x03 \x01(\r:\x04\x32\x30\x30\x30\x42\x02\x18\x01\x12#\n\x11min_after_dequeue\x18\x04 \x01(\r:\x04\x31\x30\x30\x30\x42\x02\x18\x01\x12\x1d\n\x11read_block_length\x18\x0f \x01(\r:\x02\x33\x32\x12\x1e\n\rprefetch_size\x18\r \x01(\r:\x03\x35\x31\x32\x42\x02\x18\x01\x12&\n\x16num_parallel_map_calls\x18\x0e \x01(\r:\x02\x36\x34\x42\x02\x18\x01\x12\""\n\x17num_additional_channels\x18\x12 \x01(\x05:\x01\x30\x12\x18\n\rnum_keypoints\x18\x10 \x01(\r:\x01\x30\x12 \n\x13max_number_of_boxes\x18\x15 \x01(\x05:\x03\x31\x30\x30\x12\""\n\x13load_instance_masks\x18\x07 \x01(\x08:\x05\x66\x61lse\x12M\n\tmask_type\x18\n \x01(\x0e\x32).object_detection.protos.InstanceMaskType:\x0fNUMERICAL_MASKS\x12\x1f\n\x10use_display_name\x18\x11 \x01(\x08:\x05\x66\x61lse\x12N\n\x16tf_record_input_reader\x18\x08 \x01(\x0b\x32,.object_detection.protos.TFRecordInputReaderH\x00\x12M\n\x15\x65xternal_input_reader\x18\t \x01(\x0b\x32,.object_detection.protos.ExternalInputReaderH\x00\x42\x0e\n\x0cinput_reader\"")\n\x13TFRecordInputReader\x12\x12\n\ninput_path\x18\x01 \x03(\t\""\x1c\n\x13\x45xternalInputReader*\x05\x08\x01\x10\xe8\x07*C\n\x10InstanceMaskType\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\x13\n\x0fNUMERICAL_MASKS\x10\x01\x12\r\n\tPNG_MASKS\x10\x02')
TypeError: __new__() got an unexpected keyword argument 'serialized_options'",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-08 21:23:20,open,,,['stat:awaiting response'],2018-08-09 07:37:30
720,tensorflow/models,models,5033,ericlevine,Syntax error: import matplotlibnmatplotlib.use('Agg')nimport matplotlib.pyplot as plt,"### System information
- **What is the top-level directory of the model you are using**: models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem

On Mac OSX, create_pycocotools_package.sh that's part of the object detection [running on cloud](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_cloud.md) instructions uses `sed` to change the import statements in `coco.py`. It inserts newlines, but that functionality doesn't work for OSX because of [quirks in the way sed works on OSX](https://stackoverflow.com/questions/24275070/sed-not-giving-me-correct-substitute-operation-for-newline-with-mac-difference). I've addressed the problem locally by changing the replacement from newlines to semicolons.

The error that I get when I try to run the job looks like this:

`Syntax error: import matplotlibnmatplotlib.use('Agg')nimport matplotlib.pyplot as plt`

### Source code / logs

Here's the specific line that I changed locally to make it work:

    -sed ""s/import matplotlib\.pyplot as plt/import matplotlib\nmatplotlib\.use\(\'Agg\'\)\nimport matplotlib\.pyplot as plt/g"" pycocotools/coco.py > coco.py.updated
    +sed ""s/import matplotlib\.pyplot as plt/import matplotlib; matplotlib\.use\(\'Agg\'\); import matplotlib\.pyplot as plt/g"" pycocotools/coco.py > coco.py.updated
",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-08-08 18:46:02,open,,,[],2018-08-17 00:58:54
721,tensorflow/models,models,5031,jsjs2009,AttributeError: 'NoneType' object has no attribute 'items'," stat:awaiting response  

https://github.com/tensorflow/models/blob/59f7e80ac8ad54913663a4b63ddf5a3db3689648/research/object_detection/model_lib.py#L130
WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f2bb627b500>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 10 to match input file shards.
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 107, in <module>
    tf.app.run()
  File ""/home/lc/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""object_detection/model_main.py"", line 103, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/lc/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 430, in train_and_evaluate
    executor.run_local()
  File ""/home/lc/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 609, in run_local
    hooks=train_hooks)
  File ""/home/lc/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/lc/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/lc/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/lc/anaconda3/envs/tensorflow/models/research/object_detection/model_lib.py"", line 213, in model_fn
    unpad_groundtruth_tensors=train_config.unpad_groundtruth_tensors)
  File ""/home/lc/anaconda3/envs/tensorflow/models/research/object_detection/model_lib.py"", line 131, in unstack_batch
    for key, tensor in tensor_dict.items()}
AttributeError: 'NoneType' object has no attribute 'items'

 ~/anaconda3/envs/tensorflow/models/research/
Ubuntu 16 
Anaconda3
python2.7
tensorflow1.4   from Tsinghua University mirror file 
cuda8.0

PIPELINE_CONFIG_PATH=""/home/lc/anaconda3/envs/tensorflow/models/research/models/model/faster_rcnn_inception_v2_pets.config""
MODEL_DIR=""/home/lc/anaconda3/envs/tensorflow/models/research/models/train/""
NUM_TRAIN_STEPS=50000
NUM_EVAL_STEPS=2000
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --num_eval_steps=${NUM_EVAL_STEPS} \
    --alsologtostderr

",4,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-08-08 13:50:20,open,,,[],2018-11-23 03:29:53
722,tensorflow/models,models,5030,lxyyang,"TFL Detect append flaskback, model is  ssd_mobilenet_v1_fpn_coco","i want to Transplant ssd_mobilenet_v1_fpn to the mobile phone, but i get a The problem of flashback。
can someone help me?
I follow ""Running on mobile with TensorFlow Lite"",
do ""object_detection/export_tflite_ssd_graph.py""、
“bazel run --config=opt tensorflow/contrib/lite/toco:toco”、
all ok！then ， i “cp /tmp/tflite/detect.tflite 
//tensorflow/contrib/lite/examples/android/app/src/main/assets”，but i get the problem of flashback.

my iphone  is nubia z11。

 i change the ""TF_OD_API_IS_QUANTIZED"" of DetectorActivity.java   to false, the problem of flashback is ok, but i can not get any boxes.",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-08 08:26:14,open,,,['stat:awaiting response'],2018-09-27 06:06:06
723,tensorflow/models,models,5028,highfly22,Object Detection:  cannot  finetune 10 classes or less by using the model ssd_resnet_50_fpn_coco ,"### System information
-  Linux Ubuntu 16.04
- TensorFlow installed from anaconda
- TensorFlow 1.7
- CUDA 9.0/cuDNN 7
- P40
- command

```
    python object_detection/model_main.py\
    --logtostderr \
    --pipeline_config_path=${DIR}/logo-detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640.config \
    --model_dir=${OUTPUT_DIR}
```

```
model {
  ssd {
    num_classes: 10
    image_resizer {
      fixed_shape_resizer {
        height: 640
        width: 640
      }
    }
    feature_extractor {
      type: ""ssd_resnet50_v1_fpn""
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 0.000399999989895
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.0299999993294
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.996999979019
          scale: true
          epsilon: 0.0010000000475
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      weight_shared_convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 0.000399999989895
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0
              stddev: 0.00999999977648
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.996999979019
            scale: true
            epsilon: 0.0010000000475
          }
        }
        depth: 256
        num_layers_before_predictor: 4
        kernel_size: 3
        class_prediction_bias_init: -4.59999990463
      }
    }
    anchor_generator {
      multiscale_anchor_generator {
        min_level: 3
        max_level: 7
        anchor_scale: 4.0
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        scales_per_octave: 2
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 0.300000011921
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 2.0
          alpha: 0.25
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    encode_background_as_zeros: true
    normalize_loc_loss_by_codesize: true
    inplace_batchnorm_update: true
    freeze_batchnorm: false
  }
}
train_config {
  batch_size: 1
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    random_crop_image {
      min_object_covered: 0.0
      min_aspect_ratio: 0.75
      max_aspect_ratio: 3.0
      min_area: 0.75
      max_area: 1.0
      overlap_thresh: 0.0
    }
  }
  sync_replicas: true
  optimizer {
    momentum_optimizer {
      learning_rate {
        cosine_decay_learning_rate {
          learning_rate_base: 0.0399999991059
          total_steps: 50000
          warmup_learning_rate: 0.0133330002427
          warmup_steps: 2000
        }
      }
      momentum_optimizer_value: 0.899999976158
    }
    use_moving_average: false
  }
  fine_tune_checkpoint: ""/opt/ml/data/logo-detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/model.ckpt""
  from_detection_checkpoint: true
  load_all_detection_checkpoint_vars: true
  fine_tune_checkpoint_type:  ""detection""
  # num_steps: 25000
  startup_delay_steps: 0.0
  replicas_to_aggregate: 8
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}
train_input_reader {
  label_map_path: ""/opt/ml/data/logo-detection/logo-label-map.pbtxt""
  tf_record_input_reader {
    input_path: ""/opt/ml/data/logo-detection/dataset-train.tfrecord""
  }
}
eval_config {
  num_examples: 8000
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""/opt/ml/data/logo-detection/logo-label-map.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""/opt/ml/data/logo-detection/dataset-val.tfrecord""
  }
}
```

### Describe the problem

With commit 02a9969e94feb51966f9bacddc1836d811f8ce69 , I try to finetune ssd_resnet_50_fpn_coco  for 10 classes object detection.

### Source code / logs

```
2018-08-08 03:26:17.852738: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at iterator_ops.cc:891 : Invalid argument: indices[2] = 2 is not in [0, 2)
	 [[Node: Gather_4 = Gather[Tindices=DT_INT64, Tparams=DT_INT64, validate_indices=true](cond/Merge, Reshape_8)]]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1420, in _call_tf_sessionrun
    status, run_metadata)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[2] = 2 is not in [0, 2)
	 [[Node: Gather_4 = Gather[Tindices=DT_INT64, Tparams=DT_INT64, validate_indices=true](cond/Merge, Reshape_8)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[1], [1,640,640,3], [1,3], [1,100], [1,100,4], [1,100,10], [1,100], [1,100], [1,100], [1]], output_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_BOOL, DT_FLOAT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
	 [[Node: IteratorGetNext/_3859 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_669_IteratorGetNext"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
```",8,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-08-08 04:09:51,open,,,[],2018-12-24 13:03:05
724,tensorflow/models,models,5027,Distance789,"object detection API，test the train result,report google.protobuf.message.DecodeError: Truncated message.","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:E:\
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0
- **GPU model and memory**:use CPU
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

test the train result,report google.protobuf.message.DecodeError: Truncated message.
**error detainls are as follow:**
E:\Python36\python.exe ""C:\Program Files\JetBrains\PyCharm 2018.2\helpers\pydev\pydevconsole.py"" 52401 52402
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['E:\\raccoon', 'E:/raccoon'])
Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 6.4.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 6.4.0
Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32
runfile('E:/raccoon/test.py', wdir='E:/raccoon')
Traceback (most recent call last):
  File ""E:\Python36\lib\site-packages\IPython\core\interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-587153575fe3>"", line 1, in <module>
    runfile('E:/raccoon/test.py', wdir='E:/raccoon')
  File ""C:\Program Files\JetBrains\PyCharm 2018.2\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 194, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""C:\Program Files\JetBrains\PyCharm 2018.2\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""E:/raccoon/test.py"", line 75, in <module>
    detecotr = TOD()
  File ""E:/raccoon/test.py"", line 23, in __init__
    self.detection_graph = self._load_model()
  File ""E:/raccoon/test.py"", line 32, in _load_model
    od_graph_def.ParseFromString(serialized_graph)
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\message.py"", line 185, in ParseFromString
    self.MergeFromString(serialized)
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\python_message.py"", line 1069, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\python_message.py"", line 1105, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\decoder.py"", line 633, in DecodeField
    if value._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\python_message.py"", line 1105, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\decoder.py"", line 612, in DecodeRepeatedField
    if value.add()._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\python_message.py"", line 1105, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\decoder.py"", line 743, in DecodeMap
    if submsg._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\python_message.py"", line 1095, in InternalParse
    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\decoder.py"", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\decoder.py"", line 799, in _SkipGroup
    new_pos = SkipField(buffer, pos, end, tag_bytes)
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\decoder.py"", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File ""E:\Python36\lib\site-packages\protobuf-3.4.0-py3.6.egg\google\protobuf\internal\decoder.py"", line 814, in _SkipFixed32
    raise _DecodeError('Truncated message.')
google.protobuf.message.DecodeError: Truncated message.

**there are the detail of related file:**
decoder.py:
# Protocol Buffers - Google's data interchange format
# Copyright 2008 Google Inc.  All rights reserved.
# https://developers.google.com/protocol-buffers/
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
#
#     * Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above
# copyright notice, this list of conditions and the following disclaimer
# in the documentation and/or other materials provided with the
# distribution.
#     * Neither the name of Google Inc. nor the names of its
# contributors may be used to endorse or promote products derived from
# this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

""""""Code for decoding protocol buffer primitives.

This code is very similar to encoder.py -- read the docs for that module first.

A ""decoder"" is a function with the signature:
  Decode(buffer, pos, end, message, field_dict)
The arguments are:
  buffer:     The string containing the encoded message.
  pos:        The current position in the string.
  end:        The position in the string where the current message ends.  May be
              less than len(buffer) if we're reading a sub-message.
  message:    The message object into which we're parsing.
  field_dict: message._fields (avoids a hashtable lookup).
The decoder reads the field and stores it into field_dict, returning the new
buffer position.  A decoder for a repeated field may proactively decode all of
the elements of that field, if they appear consecutively.

Note that decoders may throw any of the following:
  IndexError:  Indicates a truncated message.
  struct.error:  Unpacking of a fixed-width field failed.
  message.DecodeError:  Other errors.

Decoders are expected to raise an exception if they are called with pos > end.
This allows callers to be lax about bounds checking:  it's fineto read past
""end"" as long as you are sure that someone else will notice and throw an
exception later on.

Something up the call stack is expected to catch IndexError and struct.error
and convert them to message.DecodeError.

Decoders are constructed using decoder constructors with the signature:
  MakeDecoder(field_number, is_repeated, is_packed, key, new_default)
The arguments are:
  field_number:  The field number of the field we want to decode.
  is_repeated:   Is the field a repeated field? (bool)
  is_packed:     Is the field a packed field? (bool)
  key:           The key to use when looking up the field within field_dict.
                 (This is actually the FieldDescriptor but nothing in this
                 file should depend on that.)
  new_default:   A function which takes a message object as a parameter and
                 returns a new instance of the default value for this field.
                 (This is called for repeated fields and sub-messages, when an
                 instance does not already exist.)

As with encoders, we define a decoder constructor for every type of field.
Then, for every field of every message class we construct an actual decoder.
That decoder goes into a dict indexed by tag, so when we decode a message
we repeatedly read a tag, look up the corresponding decoder, and invoke it.
""""""

__author__ = 'kenton@google.com (Kenton Varda)'

import struct

import six

if six.PY3:
  long = int

from google.protobuf.internal import encoder
from google.protobuf.internal import wire_format
from google.protobuf import message


# This will overflow and thus become IEEE-754 ""infinity"".  We would use
# ""float('inf')"" but it doesn't work on Windows pre-Python-2.6.
_POS_INF = 1e10000
_NEG_INF = -_POS_INF
_NAN = _POS_INF * 0


# This is not for optimization, but rather to avoid conflicts with local
# variables named ""message"".
_DecodeError = message.DecodeError


def _VarintDecoder(mask, result_type):
  """"""Return an encoder for a basic varint value (does not include tag).

  Decoded values will be bitwise-anded with the given mask before being
  returned, e.g. to limit them to 32 bits.  The returned decoder does not
  take the usual ""end"" parameter -- the caller is expected to do bounds checking
  after the fact (often the caller can defer such checking until later).  The
  decoder returns a (value, new_pos) pair.
  """"""

  def DecodeVarint(buffer, pos):
    result = 0
    shift = 0
    while 1:
      b = six.indexbytes(buffer, pos)
      result |= ((b & 0x7f) << shift)
      pos += 1
      if not (b & 0x80):
        result &= mask
        result = result_type(result)
        return (result, pos)
      shift += 7
      if shift >= 64:
        raise _DecodeError('Too many bytes when decoding varint.')
  return DecodeVarint


def _SignedVarintDecoder(bits, result_type):
  """"""Like _VarintDecoder() but decodes signed values.""""""

  signbit = 1 << (bits - 1)
  mask = (1 << bits) - 1

  def DecodeVarint(buffer, pos):
    result = 0
    shift = 0
    while 1:
      b = six.indexbytes(buffer, pos)
      result |= ((b & 0x7f) << shift)
      pos += 1
      if not (b & 0x80):
        result &= mask
        result = (result ^ signbit) - signbit
        result = result_type(result)
        return (result, pos)
      shift += 7
      if shift >= 64:
        raise _DecodeError('Too many bytes when decoding varint.')
  return DecodeVarint

# We force 32-bit values to int and 64-bit values to long to make
# alternate implementations where the distinction is more significant
# (e.g. the C++ implementation) simpler.

_DecodeVarint = _VarintDecoder((1 << 64) - 1, int)
_DecodeSignedVarint = _SignedVarintDecoder(64, int)

# Use these versions for values which must be limited to 32 bits.
_DecodeVarint32 = _VarintDecoder((1 << 32) - 1, int)
_DecodeSignedVarint32 = _SignedVarintDecoder(32, int)


def ReadTag(buffer, pos):
  """"""Read a tag from the buffer, and return a (tag_bytes, new_pos) tuple.

  We return the raw bytes of the tag rather than decoding them.  The raw
  bytes can then be used to look up the proper decoder.  This effectively allows
  us to trade some work that would be done in pure-python (decoding a varint)
  for work that is done in C (searching for a byte string in a hash table).
  In a low-level language it would be much cheaper to decode the varint and
  use that, but not in Python.
  """"""

  start = pos
  while six.indexbytes(buffer, pos) & 0x80:
    pos += 1
  pos += 1
  return (buffer[start:pos], pos)


# --------------------------------------------------------------------


def _SimpleDecoder(wire_type, decode_value):
  """"""Return a constructor for a decoder for fields of a particular type.

  Args:
      wire_type:  The field's wire type.
      decode_value:  A function which decodes an individual value, e.g.
        _DecodeVarint()
  """"""

  def SpecificDecoder(field_number, is_repeated, is_packed, key, new_default):
    if is_packed:
      local_DecodeVarint = _DecodeVarint
      def DecodePackedField(buffer, pos, end, message, field_dict):
        value = field_dict.get(key)
        if value is None:
          value = field_dict.setdefault(key, new_default(message))
        (endpoint, pos) = local_DecodeVarint(buffer, pos)
        endpoint += pos
        if endpoint > end:
          raise _DecodeError('Truncated message.')
        while pos < endpoint:
          (element, pos) = decode_value(buffer, pos)
          value.append(element)
        if pos > endpoint:
          del value[-1]   # Discard corrupt value.
          raise _DecodeError('Packed element was truncated.')
        return pos
      return DecodePackedField
    elif is_repeated:
      tag_bytes = encoder.TagBytes(field_number, wire_type)
      tag_len = len(tag_bytes)
      def DecodeRepeatedField(buffer, pos, end, message, field_dict):
        value = field_dict.get(key)
        if value is None:
          value = field_dict.setdefault(key, new_default(message))
        while 1:
          (element, new_pos) = decode_value(buffer, pos)
          value.append(element)
          # Predict that the next tag is another copy of the same repeated
          # field.
          pos = new_pos + tag_len
          if buffer[new_pos:pos] != tag_bytes or new_pos >= end:
            # Prediction failed.  Return.
            if new_pos > end:
              raise _DecodeError('Truncated message.')
            return new_pos
      return DecodeRepeatedField
    else:
      def DecodeField(buffer, pos, end, message, field_dict):
        (field_dict[key], pos) = decode_value(buffer, pos)
        if pos > end:
          del field_dict[key]  # Discard corrupt value.
          raise _DecodeError('Truncated message.')
        return pos
      return DecodeField

  return SpecificDecoder


def _ModifiedDecoder(wire_type, decode_value, modify_value):
  """"""Like SimpleDecoder but additionally invokes modify_value on every value
  before storing it.  Usually modify_value is ZigZagDecode.
  """"""

  # Reusing _SimpleDecoder is slightly slower than copying a bunch of code, but
  # not enough to make a significant difference.

  def InnerDecode(buffer, pos):
    (result, new_pos) = decode_value(buffer, pos)
    return (modify_value(result), new_pos)
  return _SimpleDecoder(wire_type, InnerDecode)


def _StructPackDecoder(wire_type, format):
  """"""Return a constructor for a decoder for a fixed-width field.

  Args:
      wire_type:  The field's wire type.
      format:  The format string to pass to struct.unpack().
  """"""

  value_size = struct.calcsize(format)
  local_unpack = struct.unpack

  # Reusing _SimpleDecoder is slightly slower than copying a bunch of code, but
  # not enough to make a significant difference.

  # Note that we expect someone up-stack to catch struct.error and convert
  # it to _DecodeError -- this way we don't have to set up exception-
  # handling blocks every time we parse one value.

  def InnerDecode(buffer, pos):
    new_pos = pos + value_size
    result = local_unpack(format, buffer[pos:new_pos])[0]
    return (result, new_pos)
  return _SimpleDecoder(wire_type, InnerDecode)


def _FloatDecoder():
  """"""Returns a decoder for a float field.

  This code works around a bug in struct.unpack for non-finite 32-bit
  floating-point values.
  """"""

  local_unpack = struct.unpack

  def InnerDecode(buffer, pos):
    # We expect a 32-bit value in little-endian byte order.  Bit 1 is the sign
    # bit, bits 2-9 represent the exponent, and bits 10-32 are the significand.
    new_pos = pos + 4
    float_bytes = buffer[pos:new_pos]

    # If this value has all its exponent bits set, then it's non-finite.
    # In Python 2.4, struct.unpack will convert it to a finite 64-bit value.
    # To avoid that, we parse it specially.
    if (float_bytes[3:4] in b'\x7F\xFF' and float_bytes[2:3] >= b'\x80'):
      # If at least one significand bit is set...
      if float_bytes[0:3] != b'\x00\x00\x80':
        return (_NAN, new_pos)
      # If sign bit is set...
      if float_bytes[3:4] == b'\xFF':
        return (_NEG_INF, new_pos)
      return (_POS_INF, new_pos)

    # Note that we expect someone up-stack to catch struct.error and convert
    # it to _DecodeError -- this way we don't have to set up exception-
    # handling blocks every time we parse one value.
    result = local_unpack('<f', float_bytes)[0]
    return (result, new_pos)
  return _SimpleDecoder(wire_format.WIRETYPE_FIXED32, InnerDecode)


def _DoubleDecoder():
  """"""Returns a decoder for a double field.

  This code works around a bug in struct.unpack for not-a-number.
  """"""

  local_unpack = struct.unpack

  def InnerDecode(buffer, pos):
    # We expect a 64-bit value in little-endian byte order.  Bit 1 is the sign
    # bit, bits 2-12 represent the exponent, and bits 13-64 are the significand.
    new_pos = pos + 8
    double_bytes = buffer[pos:new_pos]

    # If this value has all its exponent bits set and at least one significand
    # bit set, it's not a number.  In Python 2.4, struct.unpack will treat it
    # as inf or -inf.  To avoid that, we treat it specially.
    if ((double_bytes[7:8] in b'\x7F\xFF')
        and (double_bytes[6:7] >= b'\xF0')
        and (double_bytes[0:7] != b'\x00\x00\x00\x00\x00\x00\xF0')):
      return (_NAN, new_pos)

    # Note that we expect someone up-stack to catch struct.error and convert
    # it to _DecodeError -- this way we don't have to set up exception-
    # handling blocks every time we parse one value.
    result = local_unpack('<d', double_bytes)[0]
    return (result, new_pos)
  return _SimpleDecoder(wire_format.WIRETYPE_FIXED64, InnerDecode)


def EnumDecoder(field_number, is_repeated, is_packed, key, new_default):
  enum_type = key.enum_type
  if is_packed:
    local_DecodeVarint = _DecodeVarint
    def DecodePackedField(buffer, pos, end, message, field_dict):
      value = field_dict.get(key)
      if value is None:
        value = field_dict.setdefault(key, new_default(message))
      (endpoint, pos) = local_DecodeVarint(buffer, pos)
      endpoint += pos
      if endpoint > end:
        raise _DecodeError('Truncated message.')
      while pos < endpoint:
        value_start_pos = pos
        (element, pos) = _DecodeSignedVarint32(buffer, pos)
        if element in enum_type.values_by_number:
          value.append(element)
        else:
          if not message._unknown_fields:
            message._unknown_fields = []
          tag_bytes = encoder.TagBytes(field_number,
                                       wire_format.WIRETYPE_VARINT)
          message._unknown_fields.append(
              (tag_bytes, buffer[value_start_pos:pos]))
      if pos > endpoint:
        if element in enum_type.values_by_number:
          del value[-1]   # Discard corrupt value.
        else:
          del message._unknown_fields[-1]
        raise _DecodeError('Packed element was truncated.')
      return pos
    return DecodePackedField
  elif is_repeated:
    tag_bytes = encoder.TagBytes(field_number, wire_format.WIRETYPE_VARINT)
    tag_len = len(tag_bytes)
    def DecodeRepeatedField(buffer, pos, end, message, field_dict):
      value = field_dict.get(key)
      if value is None:
        value = field_dict.setdefault(key, new_default(message))
      while 1:
        (element, new_pos) = _DecodeSignedVarint32(buffer, pos)
        if element in enum_type.values_by_number:
          value.append(element)
        else:
          if not message._unknown_fields:
            message._unknown_fields = []
          message._unknown_fields.append(
              (tag_bytes, buffer[pos:new_pos]))
        # Predict that the next tag is another copy of the same repeated
        # field.
        pos = new_pos + tag_len
        if buffer[new_pos:pos] != tag_bytes or new_pos >= end:
          # Prediction failed.  Return.
          if new_pos > end:
            raise _DecodeError('Truncated message.')
          return new_pos
    return DecodeRepeatedField
  else:
    def DecodeField(buffer, pos, end, message, field_dict):
      value_start_pos = pos
      (enum_value, pos) = _DecodeSignedVarint32(buffer, pos)
      if pos > end:
        raise _DecodeError('Truncated message.')
      if enum_value in enum_type.values_by_number:
        field_dict[key] = enum_value
      else:
        if not message._unknown_fields:
          message._unknown_fields = []
        tag_bytes = encoder.TagBytes(field_number,
                                     wire_format.WIRETYPE_VARINT)
        message._unknown_fields.append(
          (tag_bytes, buffer[value_start_pos:pos]))
      return pos
    return DecodeField


# --------------------------------------------------------------------


Int32Decoder = _SimpleDecoder(
    wire_format.WIRETYPE_VARINT, _DecodeSignedVarint32)

Int64Decoder = _SimpleDecoder(
    wire_format.WIRETYPE_VARINT, _DecodeSignedVarint)

UInt32Decoder = _SimpleDecoder(wire_format.WIRETYPE_VARINT, _DecodeVarint32)
UInt64Decoder = _SimpleDecoder(wire_format.WIRETYPE_VARINT, _DecodeVarint)

SInt32Decoder = _ModifiedDecoder(
    wire_format.WIRETYPE_VARINT, _DecodeVarint32, wire_format.ZigZagDecode)
SInt64Decoder = _ModifiedDecoder(
    wire_format.WIRETYPE_VARINT, _DecodeVarint, wire_format.ZigZagDecode)

# Note that Python conveniently guarantees that when using the '<' prefix on
# formats, they will also have the same size across all platforms (as opposed
# to without the prefix, where their sizes depend on the C compiler's basic
# type sizes).
Fixed32Decoder  = _StructPackDecoder(wire_format.WIRETYPE_FIXED32, '<I')
Fixed64Decoder  = _StructPackDecoder(wire_format.WIRETYPE_FIXED64, '<Q')
SFixed32Decoder = _StructPackDecoder(wire_format.WIRETYPE_FIXED32, '<i')
SFixed64Decoder = _StructPackDecoder(wire_format.WIRETYPE_FIXED64, '<q')
FloatDecoder = _FloatDecoder()
DoubleDecoder = _DoubleDecoder()

BoolDecoder = _ModifiedDecoder(
    wire_format.WIRETYPE_VARINT, _DecodeVarint, bool)


def StringDecoder(field_number, is_repeated, is_packed, key, new_default):
  """"""Returns a decoder for a string field.""""""

  local_DecodeVarint = _DecodeVarint
  local_unicode = six.text_type

  def _ConvertToUnicode(byte_str):
    try:
      return local_unicode(byte_str, 'utf-8')
    except UnicodeDecodeError as e:
      # add more information to the error message and re-raise it.
      e.reason = '%s in field: %s' % (e, key.full_name)
      raise

  assert not is_packed
  if is_repeated:
    tag_bytes = encoder.TagBytes(field_number,
                                 wire_format.WIRETYPE_LENGTH_DELIMITED)
    tag_len = len(tag_bytes)
    def DecodeRepeatedField(buffer, pos, end, message, field_dict):
      value = field_dict.get(key)
      if value is None:
        value = field_dict.setdefault(key, new_default(message))
      while 1:
        (size, pos) = local_DecodeVarint(buffer, pos)
        new_pos = pos + size
        if new_pos > end:
          raise _DecodeError('Truncated string.')
        value.append(_ConvertToUnicode(buffer[pos:new_pos]))
        # Predict that the next tag is another copy of the same repeated field.
        pos = new_pos + tag_len
        if buffer[new_pos:pos] != tag_bytes or new_pos == end:
          # Prediction failed.  Return.
          return new_pos
    return DecodeRepeatedField
  else:
    def DecodeField(buffer, pos, end, message, field_dict):
      (size, pos) = local_DecodeVarint(buffer, pos)
      new_pos = pos + size
      if new_pos > end:
        raise _DecodeError('Truncated string.')
      field_dict[key] = _ConvertToUnicode(buffer[pos:new_pos])
      return new_pos
    return DecodeField


def BytesDecoder(field_number, is_repeated, is_packed, key, new_default):
  """"""Returns a decoder for a bytes field.""""""

  local_DecodeVarint = _DecodeVarint

  assert not is_packed
  if is_repeated:
    tag_bytes = encoder.TagBytes(field_number,
                                 wire_format.WIRETYPE_LENGTH_DELIMITED)
    tag_len = len(tag_bytes)
    def DecodeRepeatedField(buffer, pos, end, message, field_dict):
      value = field_dict.get(key)
      if value is None:
        value = field_dict.setdefault(key, new_default(message))
      while 1:
        (size, pos) = local_DecodeVarint(buffer, pos)
        new_pos = pos + size
        if new_pos > end:
          raise _DecodeError('Truncated string.')
        value.append(buffer[pos:new_pos])
        # Predict that the next tag is another copy of the same repeated field.
        pos = new_pos + tag_len
        if buffer[new_pos:pos] != tag_bytes or new_pos == end:
          # Prediction failed.  Return.
          return new_pos
    return DecodeRepeatedField
  else:
    def DecodeField(buffer, pos, end, message, field_dict):
      (size, pos) = local_DecodeVarint(buffer, pos)
      new_pos = pos + size
      if new_pos > end:
        raise _DecodeError('Truncated string.')
      field_dict[key] = buffer[pos:new_pos]
      return new_pos
    return DecodeField


def GroupDecoder(field_number, is_repeated, is_packed, key, new_default):
  """"""Returns a decoder for a group field.""""""

  end_tag_bytes = encoder.TagBytes(field_number,
                                   wire_format.WIRETYPE_END_GROUP)
  end_tag_len = len(end_tag_bytes)

  assert not is_packed
  if is_repeated:
    tag_bytes = encoder.TagBytes(field_number,
                                 wire_format.WIRETYPE_START_GROUP)
    tag_len = len(tag_bytes)
    def DecodeRepeatedField(buffer, pos, end, message, field_dict):
      value = field_dict.get(key)
      if value is None:
        value = field_dict.setdefault(key, new_default(message))
      while 1:
        value = field_dict.get(key)
        if value is None:
          value = field_dict.setdefault(key, new_default(message))
        # Read sub-message.
        pos = value.add()._InternalParse(buffer, pos, end)
        # Read end tag.
        new_pos = pos+end_tag_len
        if buffer[pos:new_pos] != end_tag_bytes or new_pos > end:
          raise _DecodeError('Missing group end tag.')
        # Predict that the next tag is another copy of the same repeated field.
        pos = new_pos + tag_len
        if buffer[new_pos:pos] != tag_bytes or new_pos == end:
          # Prediction failed.  Return.
          return new_pos
    return DecodeRepeatedField
  else:
    def DecodeField(buffer, pos, end, message, field_dict):
      value = field_dict.get(key)
      if value is None:
        value = field_dict.setdefault(key, new_default(message))
      # Read sub-message.
      pos = value._InternalParse(buffer, pos, end)
      # Read end tag.
      new_pos = pos+end_tag_len
      if buffer[pos:new_pos] != end_tag_bytes or new_pos > end:
        raise _DecodeError('Missing group end tag.')
      return new_pos
    return DecodeField


def MessageDecoder(field_number, is_repeated, is_packed, key, new_default):
  """"""Returns a decoder for a message field.""""""

  local_DecodeVarint = _DecodeVarint

  assert not is_packed
  if is_repeated:
    tag_bytes = encoder.TagBytes(field_number,
                                 wire_format.WIRETYPE_LENGTH_DELIMITED)
    tag_len = len(tag_bytes)
    def DecodeRepeatedField(buffer, pos, end, message, field_dict):
      value = field_dict.get(key)
      if value is None:
        value = field_dict.setdefault(key, new_default(message))
      while 1:
        # Read length.
        (size, pos) = local_DecodeVarint(buffer, pos)
        new_pos = pos + size
        if new_pos > end:
          raise _DecodeError('Truncated message.')
        # Read sub-message.
        if value.add()._InternalParse(buffer, pos, new_pos) != new_pos:
          # The only reason _InternalParse would return early is if it
          # encountered an end-group tag.
          raise _DecodeError('Unexpected end-group tag.')
        # Predict that the next tag is another copy of the same repeated field.
        pos = new_pos + tag_len
        if buffer[new_pos:pos] != tag_bytes or new_pos == end:
          # Prediction failed.  Return.
          return new_pos
    return DecodeRepeatedField
  else:
    def DecodeField(buffer, pos, end, message, field_dict):
      value = field_dict.get(key)
      if value is None:
        value = field_dict.setdefault(key, new_default(message))
      # Read length.
      (size, pos) = local_DecodeVarint(buffer, pos)
      new_pos = pos + size
      if new_pos > end:
        raise _DecodeError('Truncated message.')
      # Read sub-message.
      if value._InternalParse(buffer, pos, new_pos) != new_pos:
        # The only reason _InternalParse would return early is if it encountered
        # an end-group tag.
        raise _DecodeError('Unexpected end-group tag.')
      return new_pos
    return DecodeField


# --------------------------------------------------------------------

MESSAGE_SET_ITEM_TAG = encoder.TagBytes(1, wire_format.WIRETYPE_START_GROUP)

def MessageSetItemDecoder(descriptor):
  """"""Returns a decoder for a MessageSet item.

  The parameter is the message Descriptor.

  The message set message looks like this:
    message MessageSet {
      repeated group Item = 1 {
        required int32 type_id = 2;
        required string message = 3;
      }
    }
  """"""

  type_id_tag_bytes = encoder.TagBytes(2, wire_format.WIRETYPE_VARINT)
  message_tag_bytes = encoder.TagBytes(3, wire_format.WIRETYPE_LENGTH_DELIMITED)
  item_end_tag_bytes = encoder.TagBytes(1, wire_format.WIRETYPE_END_GROUP)

  local_ReadTag = ReadTag
  local_DecodeVarint = _DecodeVarint
  local_SkipField = SkipField

  def DecodeItem(buffer, pos, end, message, field_dict):
    message_set_item_start = pos
    type_id = -1
    message_start = -1
    message_end = -1

    # Technically, type_id and message can appear in any order, so we need
    # a little loop here.
    while 1:
      (tag_bytes, pos) = local_ReadTag(buffer, pos)
      if tag_bytes == type_id_tag_bytes:
        (type_id, pos) = local_DecodeVarint(buffer, pos)
      elif tag_bytes == message_tag_bytes:
        (size, message_start) = local_DecodeVarint(buffer, pos)
        pos = message_end = message_start + size
      elif tag_bytes == item_end_tag_bytes:
        break
      else:
        pos = SkipField(buffer, pos, end, tag_bytes)
        if pos == -1:
          raise _DecodeError('Missing group end tag.')

    if pos > end:
      raise _DecodeError('Truncated message.')

    if type_id == -1:
      raise _DecodeError('MessageSet item missing type_id.')
    if message_start == -1:
      raise _DecodeError('MessageSet item missing message.')

    extension = message.Extensions._FindExtensionByNumber(type_id)
    if extension is not None:
      value = field_dict.get(extension)
      if value is None:
        value = field_dict.setdefault(
            extension, extension.message_type._concrete_class())
      if value._InternalParse(buffer, message_start,message_end) != message_end:
        # The only reason _InternalParse would return early is if it encountered
        # an end-group tag.
        raise _DecodeError('Unexpected end-group tag.')
    else:
      if not message._unknown_fields:
        message._unknown_fields = []
      message._unknown_fields.append((MESSAGE_SET_ITEM_TAG,
                                      buffer[message_set_item_start:pos]))

    return pos

  return DecodeItem

# --------------------------------------------------------------------

def MapDecoder(field_descriptor, new_default, is_message_map):
  """"""Returns a decoder for a map field.""""""

  key = field_descriptor
  tag_bytes = encoder.TagBytes(field_descriptor.number,
                               wire_format.WIRETYPE_LENGTH_DELIMITED)
  tag_len = len(tag_bytes)
  local_DecodeVarint = _DecodeVarint
  # Can't read _concrete_class yet; might not be initialized.
  message_type = field_descriptor.message_type

  def DecodeMap(buffer, pos, end, message, field_dict):
    submsg = message_type._concrete_class()
    value = field_dict.get(key)
    if value is None:
      value = field_dict.setdefault(key, new_default(message))
    while 1:
      # Read length.
      (size, pos) = local_DecodeVarint(buffer, pos)
      new_pos = pos + size
      if new_pos > end:
        raise _DecodeError('Truncated message.')
      # Read sub-message.
      submsg.Clear()
      if submsg._InternalParse(buffer, pos, new_pos) != new_pos:
        # The only reason _InternalParse would return early is if it
        # encountered an end-group tag.
        raise _DecodeError('Unexpected end-group tag.')

      if is_message_map:
        value[submsg.key].MergeFrom(submsg.value)
      else:
        value[submsg.key] = submsg.value

      # Predict that the next tag is another copy of the same repeated field.
      pos = new_pos + tag_len
      if buffer[new_pos:pos] != tag_bytes or new_pos == end:
        # Prediction failed.  Return.
        return new_pos

  return DecodeMap

# --------------------------------------------------------------------
# Optimization is not as heavy here because calls to SkipField() are rare,
# except for handling end-group tags.

def _SkipVarint(buffer, pos, end):
  """"""Skip a varint value.  Returns the new position.""""""
  # Previously ord(buffer[pos]) raised IndexError when pos is out of range.
  # With this code, ord(b'') raises TypeError.  Both are handled in
  # python_message.py to generate a 'Truncated message' error.
  while ord(buffer[pos:pos+1]) & 0x80:
    pos += 1
  pos += 1
  if pos > end:
    raise _DecodeError('Truncated message.')
  return pos

def _SkipFixed64(buffer, pos, end):
  """"""Skip a fixed64 value.  Returns the new position.""""""

  pos += 8
  if pos > end:
    raise _DecodeError('Truncated message.')
  return pos

def _SkipLengthDelimited(buffer, pos, end):
  """"""Skip a length-delimited value.  Returns the new position.""""""

  (size, pos) = _DecodeVarint(buffer, pos)
  pos += size
  if pos > end:
    raise _DecodeError('Truncated message.')
  return pos

def _SkipGroup(buffer, pos, end):
  """"""Skip sub-group.  Returns the new position.""""""

  while 1:
    (tag_bytes, pos) = ReadTag(buffer, pos)
    new_pos = SkipField(buffer, pos, end, tag_bytes)
    if new_pos == -1:
      return pos
    pos = new_pos

def _EndGroup(buffer, pos, end):
  """"""Skipping an END_GROUP tag returns -1 to tell the parent loop to break.""""""

  return -1

def _SkipFixed32(buffer, pos, end):
  """"""Skip a fixed32 value.  Returns the new position.""""""

  pos += 4
  if pos > end:
    raise _DecodeError('Truncated message.')
  return pos

def _RaiseInvalidWireType(buffer, pos, end):
  """"""Skip function for unknown wire types.  Raises an exception.""""""

  raise _DecodeError('Tag had invalid wire type.')

def _FieldSkipper():
  """"""Constructs the SkipField function.""""""

  WIRETYPE_TO_SKIPPER = [
      _SkipVarint,
      _SkipFixed64,
      _SkipLengthDelimited,
      _SkipGroup,
      _EndGroup,
      _SkipFixed32,
      _RaiseInvalidWireType,
      _RaiseInvalidWireType,
      ]

  wiretype_mask = wire_format.TAG_TYPE_MASK

  def SkipField(buffer, pos, end, tag_bytes):
    """"""Skips a field with the specified tag.

    |pos| should point to the byte immediately after the tag.

    Returns:
        The new position (after the tag value), or -1 if the tag is an end-group
        tag (in which case the calling loop should break).
    """"""

    # The wire type is always in the first byte since varints are little-endian.
    wire_type = ord(tag_bytes[0:1]) & wiretype_mask
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)

  return SkipField

SkipField = _FieldSkipper()






",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-08 03:43:46,open,,,[],2018-08-08 13:07:50
725,tensorflow/models,models,5022,moeedkundi,Deeplab readme,"Deeplab readme doesn't include any tutorial of how to bring in your own dataset, it is given in object detection and that is super helpful, so it should be added in deeplab too.",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-08-07 11:36:56,open,,,['stat:awaiting response'],2018-08-07 19:22:04
726,tensorflow/models,models,5020,aifollower,TFLite have not support SquaredDifference/TFLite_Detection_PostProcess?,"
### Describe the problem
2018-08-07 13:21:07.345473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: SquaredDifference
2018-08-07 13:21:07.361241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: TFLite_Detection_PostProcess
",40,"NamedUser(login=""jdduke"")","[NamedUser(login=""jdduke""), NamedUser(login=""achowdhery""), NamedUser(login=""robieta"")]",2018-08-07 08:26:03,open,,,['stat:awaiting response'],2019-04-05 15:30:34
727,tensorflow/models,models,5019,uelordi01,Issues exporting ssdlite_mobilenet_v2 to tensorflow-lite,"### System information
- **What is the top-level directory of the model you are using**:
  - ssdlite_mobilenet_v2_coco_2018_05_09 pretrained model
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux ubuntu 16.04
- **TensorFlow installed from binary (CPU)**:

- **TensorFlow version (use command below)**:
  -  1.5

### Describe the problem
Hi:

I am trying to export ssdlite_Mobilenet_v2 model to tf-lite:
I downloaded from [this path](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz)
I tried with these **two ways**: 
#### 1.- following [this tutorial](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) with *export_tflite_ssd_graph.py*, using the checkpoint

  - I use export_tflite_ssd_graph.py like this: 
``` javascript python export CONFIG_FILE=/home/VICOMTECH/uelordi/projects/tflite_models/ssdlite_mobilenet_v2_coco_2018_05_09/pipeline.config

  export CHECKPOINT_PATH=/home/VICOMTECH/uelordi/projects/tflite_models/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt

  export OUTPUT_DIR=/home/VICOMTECH/uelordi/projects/tflite_models/ssdlite_mobilenet_v2_coco_2018_05_09/tflite

    python /home/VICOMTECH/uelordi/SDK/tensorflow1/models/models/research/object_detection  /export_tflite_ssd_graph.py \
  --pipeline_config_path=$CONFIG_FILE \
  --trained_checkpoint_prefix=$CHECKPOINT_PATH \
  --output_directory=$OUTPUT_DIR \
  --add_postprocessing_op=true
  ```
 - then I created toco script: 
``` javascript
toco \
--input_file=tflite_graph.pb \
--output_file=latest_ssdlite_mobilenetv2.tflite \
  --input_format=TENSORFLOW_GRAPHDEF \
--input_shapes=1,300,300,3 \
  --output_format=TFLITE \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \
--inference_type=FLOAT \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
```
and I got this errors: 

> Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: DIV, Squeeze, TFLite_Detection_PostProcess. 

So  I added allow-custom-ops, and when I use the tf-lite interperter in android: 
``` javascript
d.tfLite = new Interpreter(loadModelFile(assetManager, modelFilename));
```
And as I supposed I have custom operation error.
> Internal error: Cannot create interpreter: Didn't find custom op for name 'DIV' with version 1
>    Didn't find custom op for name 'Squeeze' with version 1
>    Registration failed.


####  2.- using toco with the pretrained model. 
my toco script is: 
``` javascript
toco \
--input_file=frozen_inference_graph.pb \
--output_file=ssd_lite_v2.tflite \
  --input_format=TENSORFLOW_GRAPHDEF \
--input_shapes=1,300,300,3 \
  --output_format=TFLITE \
--input_arrays=normalized_input_image_tensor \
--output_arrays='detection_boxes,detection_scores,detection_classes,num_detections' \
--inference_type=FLOAT \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
```

As I have some errors with the operators I added allow-custom-ops, and when I use the tf-lite interperter in android: 
``` javascript
d.tfLite = new Interpreter(loadModelFile(assetManager, modelFilename));
```
But as I supposed I have custom operation error.
> Internal error: Cannot create interpreter: Didn't find custom op for name 'DIV' with version 1
>    Didn't find custom op for name 'Squeeze' with version 1
>    Registration failed.

*So my questions are:*
- What makes different export_tflite_ssd_graph.py (with the checkpoint) from a common toco script with pretrained model? It creates custom operations? complementary postprocessing operations?
*In case of toco pretrained problems: *
- This means that I have to create my custom operation of DIV and Squeeze operations?
",7,"NamedUser(login=""achowdhery"")","[NamedUser(login=""achowdhery""), NamedUser(login=""pkulzc"")]",2018-08-07 08:02:24,open,,,[],2019-03-29 21:48:49
728,tensorflow/models,models,5017,pusankim,Output is written twice to the console during training,"OS Platform and Distribution 
WINDOWS 10

TensorFlow installed from
anaconda

TensorFlow version
tensorflow-gpu      1.9.0

Bazel version


CUDA/cuDNN version
9.0/6.0

GPU model and memory
GeForce GTX 1050 Ti, 4GB

Exact command to reproduce


INFO:tensorflow:global step 199972: loss = 0.0452 (0.290 sec/step)
INFO:tensorflow:global step 199972: loss = 0.0452 (0.290 sec/step)
INFO:tensorflow:global step 199973: loss = 4351578088673814511616.0000 (0.276 sec/step)
INFO:tensorflow:global step 199973: loss = 4351578088673814511616.0000 (0.276 sec/step)
INFO:tensorflow:global step 199974: loss = 4637446326820970430464.0000 (0.283 sec/step)
INFO:tensorflow:global step 199974: loss = 4637446326820970430464.0000 (0.283 sec/step)
INFO:tensorflow:global step 199975: loss = 0.0367 (0.284 sec/step)
INFO:tensorflow:global step 199975: loss = 0.0367 (0.284 sec/step)
INFO:tensorflow:global step 199976: loss = 0.0421 (0.286 sec/step)
INFO:tensorflow:global step 199976: loss = 0.0421 (0.286 sec/step)
INFO:tensorflow:global step 199977: loss = 0.0378 (0.285 sec/step)
INFO:tensorflow:global step 199977: loss = 0.0378 (0.285 sec/step)
INFO:tensorflow:global step 199978: loss = 3157714078379565645824.0000 (0.285 sec/step)
INFO:tensorflow:global step 199978: loss = 3157714078379565645824.0000 (0.285 sec/step)
INFO:tensorflow:global step 199979: loss = 0.0269 (0.272 sec/step)
INFO:tensorflow:global step 199979: loss = 0.0269 (0.272 sec/step)
INFO:tensorflow:global step 199980: loss = 1563645005548432130048.0000 (0.287 sec/step)
INFO:tensorflow:global step 199980: loss = 1563645005548432130048.0000 (0.287 sec/step)
INFO:tensorflow:global step 199981: loss = 0.0456 (0.300 sec/step)
INFO:tensorflow:global step 199981: loss = 0.0456 (0.300 sec/step)
INFO:tensorflow:global step 199982: loss = 0.0381 (0.283 sec/step)
INFO:tensorflow:global step 199982: loss = 0.0381 (0.283 sec/step)
INFO:tensorflow:global step 199983: loss = 0.0855 (0.281 sec/step)
INFO:tensorflow:global step 199983: loss = 0.0855 (0.281 sec/step)
INFO:tensorflow:global step 199984: loss = 0.0408 (0.280 sec/step)
INFO:tensorflow:global step 199984: loss = 0.0408 (0.280 sec/step)
INFO:tensorflow:global step 199985: loss = 1553882608931176448000.0000 (0.288 sec/step)
INFO:tensorflow:global step 199985: loss = 1553882608931176448000.0000 (0.288 sec/step)
INFO:tensorflow:global step 199986: loss = 1558102903994487668736.0000 (0.293 sec/step)
INFO:tensorflow:global step 199986: loss = 1558102903994487668736.0000 (0.293 sec/step)
INFO:tensorflow:global step 199987: loss = 0.0442 (0.280 sec/step)
INFO:tensorflow:global step 199987: loss = 0.0442 (0.280 sec/step)
INFO:tensorflow:global step 199988: loss = 0.0528 (0.276 sec/step)
INFO:tensorflow:global step 199988: loss = 0.0528 (0.276 sec/step)
INFO:tensorflow:global step 199989: loss = 3111071422938796261376.0000 (0.279 sec/step)
INFO:tensorflow:global step 199989: loss = 3111071422938796261376.0000 (0.279 sec/step)
INFO:tensorflow:global step 199990: loss = 0.0866 (0.276 sec/step)
INFO:tensorflow:global step 199990: loss = 0.0866 (0.276 sec/step)
INFO:tensorflow:global step 199991: loss = 0.0375 (0.274 sec/step)
INFO:tensorflow:global step 199991: loss = 0.0375 (0.274 sec/step)
INFO:tensorflow:global step 199992: loss = 3141269465815198990336.0000 (0.274 sec/step)
INFO:tensorflow:global step 199992: loss = 3141269465815198990336.0000 (0.274 sec/step)
INFO:tensorflow:global step 199993: loss = 0.0908 (0.272 sec/step)
INFO:tensorflow:global step 199993: loss = 0.0908 (0.272 sec/step)
INFO:tensorflow:global step 199994: loss = 0.0389 (0.274 sec/step)
INFO:tensorflow:global step 199994: loss = 0.0389 (0.274 sec/step)
INFO:tensorflow:global step 199995: loss = 0.0317 (0.282 sec/step)
INFO:tensorflow:global step 199995: loss = 0.0317 (0.282 sec/step)
INFO:tensorflow:global step 199996: loss = 0.0332 (0.283 sec/step)
INFO:tensorflow:global step 199996: loss = 0.0332 (0.283 sec/step)
INFO:tensorflow:global step 199997: loss = 1525433229347588669440.0000 (0.274 sec/step)
INFO:tensorflow:global step 199997: loss = 1525433229347588669440.0000 (0.274 sec/step)
INFO:tensorflow:global step 199998: loss = 4603309604595455492096.0000 (0.278 sec/step)
INFO:tensorflow:global step 199998: loss = 4603309604595455492096.0000 (0.278 sec/step)
INFO:tensorflow:global step 199999: loss = 0.0320 (0.281 sec/step)
INFO:tensorflow:global step 199999: loss = 0.0320 (0.281 sec/step)
INFO:tensorflow:global step 200000: loss = 3127472125406796054528.0000 (0.277 sec/step)
INFO:tensorflow:global step 200000: loss = 3127472125406796054528.0000 (0.277 sec/step)
INFO:tensorflow:Stopping Training.
INFO:tensorflow:Stopping Training.
INFO:tensorflow:Finished training! Saving model to disk.
INFO:tensorflow:Finished training! Saving model to disk.

Does the same notification pop up twice, which means that it has been trained twice?


From the beginning to the end of the training, INFO came up twice.


I would be grateful if someone could tell me how to fix it.",5,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-07 07:29:51,open,,,[],2018-09-24 18:21:57
729,tensorflow/models,models,5016,Aksei,Create TFRecord files from the MNIST-M data set,"Hi, 

i am trying to create the TFRecord files from the MNIST-M dataset. I did download and extract the dataset, however when I run the command : 

python download_and_convert_mnist_m.py -- --dataset_dir $DSN_DATA_DIR

I always get the following error: 


Traceback (most recent call last):
  File ""download_and_convert_mnist_m.py"", line 240, in <module>
    tf.app.run()
  File ""/Users/myfolder/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""download_and_convert_mnist_m.py"", line 234, in main
    assert FLAGS.dataset_dir
AssertionError


I have been working at it for quite I while, but I can't fix it. 

If I use bazel (bazel run domain_adaptation/datasets:download_and_convert_mnist_m -- --dataset_dir $DSN_DATA_DIR), I run into the same problem. 

Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_myname/e9545550cccabf838338cbdd5e4c6269/execroot/__main__/bazel-out/darwin-fastbuild/bin/research/domain_adaptation/datasets/download_and_convert_mnist_m.runfiles/__main__/research/domain_adaptation/datasets/download_and_convert_mnist_m.py"", line 240, in <module>
    tf.app.run()
  File ""/Users/myFolder/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/private/var/tmp/_bazel_myname/e9545550cccabf838338cbdd5e4c6269/execroot/__main__/bazel-out/darwin-fastbuild/bin/research/domain_adaptation/datasets/download_and_convert_mnist_m.runfiles/__main__/research/domain_adaptation/datasets/download_and_convert_mnist_m.py"", line 235, in main
    run(FLAGS.dataset_dir)
  File ""/private/var/tmp/_bazel_myname/e9545550cccabf838338cbdd5e4c6269/execroot/__main__/bazel-out/darwin-fastbuild/bin/research/domain_adaptation/datasets/download_and_convert_mnist_m.runfiles/__main__/research/domain_adaptation/datasets/download_and_convert_mnist_m.py"", line 219, in run
    train_validation_filenames_to_class_ids, dataset_dir)
  File ""/private/var/tmp/_bazel_myname/e9545550cccabf838338cbdd5e4c6269/execroot/__main__/bazel-out/darwin-fastbuild/bin/research/domain_adaptation/datasets/download_and_convert_mnist_m.runfiles/__main__/research/domain_adaptation/datasets/download_and_convert_mnist_m.py"", line 123, in _convert_dataset
    os.path.join(png_directory, filename), 'r').read()
  File ""/Users/myFolder/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 127, in read
    pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))
  File ""/Users/myFolder/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 95, in _prepare_value
    return compat.as_str_any(val)
  File ""/Users/myFolder/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 113, in as_str_any
    return as_str(value)
  File ""/Users/myFolder/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 86, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte

Does anyone know how to fix it?

Thank you ",6,"NamedUser(login=""bousmalis"")","[NamedUser(login=""bousmalis"")]",2018-08-07 07:24:32,open,,,['stat:contributions welcome'],2018-08-22 22:05:12
730,tensorflow/models,models,5014,lucheng07082221,ssd_inceptionv2_coco error,"
I used ssd_inceptionv2_coco model , then got this error:
NotFoundError (see above for traceback): Key FeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta not found in checkpoint
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]


can you help me?
",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-07 02:24:27,open,,,['stat:awaiting response'],2018-08-07 13:07:59
731,tensorflow/models,models,5010,bsaendig,Evaluation does not support per category metrics,"### System information
- **What is the top-level directory of the model you are using**: research/object_detection
- **Have I written custom code**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0 (tensorflow-gpu 1.3.0) 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/7.1
- **GPU model and memory**:[GeForce GT 740]
- **Exact command to reproduce**: using the object_detection/model_main.py with include_metrics_per_category set true in the config file

Hi,
Although the parameter 'include_metrics_per_category' for the training pipeline config file theoretically exists, it does not actually work. Setting it to true leads to an error message ('ValueError: Category stats do not exist'). 
object_detection/metrics/coco_tools.py throws this error. It uses pycocotool's 'COCOeval' to calculate evaluation metrics and in case the include_metrics_per_category parameter is true it looks for a dictionary called 'category_stats' but this actually never gets calculated in pycocotools cocoeval.py.

This feature would be very useful and i would appreciate if it was added or alternatively any guidance in how i could implement it myself.

Thank you in advance.
",3,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-06 12:35:30,open,,,[],2018-08-31 08:57:58
732,tensorflow/models,models,5009,chowkamlee81,Adding decoder module for NASNet for DeeplabV3+,"I need to use NASNet-A_Large_331 as backend architecture rather than xception65/xception71 to do semantic segmentation. Kindly suggest DECODER_END_POINTS  feature_extractor.py for NASNet-A_Large_331 model.

Kindly help

",3,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-08-06 10:41:44,open,,,['stat:awaiting tensorflower'],2018-08-18 00:28:57
733,tensorflow/models,models,5008,yswang0522,how to pretrain model on MS-COCO (Deeplabv3+) ,"I'm training deeplabv3+ with my own backbone network and I want to pretrain my models on MS-COCO dataset.  the deeplab paper shows that pre-train models on coco dataset can much improve the results, but the coco dataset just give the instance level labels, how to transfer data to satisfy the category level requirement, or anyone can give me some examples on how to pre-train models on coco datasets? and Do you have any idea how to handle overlapping segments in the MS-COCO ground truth while training the network?

Thank you for your quick reponse. The followings are details of my version.
What is the top-level directory of the model you are using
/home/semantic_segmentation/models/research/deeplab/
Have I written custom code
No
OS Platform and Distribution
Ubuntu 16.04.3 LTS ( GNU/Linux 4.4.0-98-generic x86_64)
TensorFlow installed from
pip command
TensorFlow version
1.6.0
Bazel version
None
CUDA/cuDNN version
None
GPU model and memory
TITAN Xp 12GB * 8
Exact command to reproduce
None.",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-08-06 08:49:29,open,,,[],2018-09-21 10:08:25
734,tensorflow/models,models,5005,sxsxsx,Is there any accuracy  loss when transform frozen_eval_graph.pb to TFLITE by TensorFlow Lite Optimizing Converter (TOCO) ,Is there any accuracy  loss when transform frozen_eval_graph.pb to TFLITE by TensorFlow Lite Optimizing Converter (TOCO) ,2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-08-05 12:19:21,open,,,['stat:awaiting response'],2018-08-07 07:24:17
735,tensorflow/models,models,5004,netanel-s,Is it possible to save checkpoints based on validation performance?,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code **: stock
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**: training and evaluating ssdlite_mobilenet_v2_coco_2018_05_09 (on COCO dataset) from checkpoint of mobilenet_v2_1.0_224 using object_detection/model_main.py.

### Describe the problem
Is it possible to save checkpoints based on validation performance?
I want to train my model, and decide which checkpoints to save based on the performance of the model on the validation set. I want to do so in order to save certain checkpoints (not only the latest ones) and maybe stop the training in some case when I cannot monitor the training.
I know that ValidationMonitor had a similar functionality, but it is deprecated, so I wonder if this functionality exists somewhere else which I couldn't find.
Thanks in advance.
",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-08-05 10:05:03,open,,,[],2019-01-15 12:46:36
736,tensorflow/models,models,5003,apriandito,Key Conv/biases not found in checkpoint in Resnet 101,"I found this error when running this command 

`python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet101_coco.config`

and the error comes up :   
```
`C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
WARNING:tensorflow:From C:\Tensorflow\models\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py:2008: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/shortcut/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/shortcut/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/shortcut/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/shortcut/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:tensorflow:From C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py:737: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
WARNING:tensorflow:From C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py:737: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-08-05 12:37:49.766998: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-08-05 12:37:49.984755: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1392] Found device 0 with properties:
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 4.97GiB
2018-08-05 12:37:49.992318: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1471] Adding visible gpu devices: 0
2018-08-05 12:37:50.648988: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-05 12:37:50.652889: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:958]      0
2018-08-05 12:37:50.655167: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   N
2018-08-05 12:37:50.657520: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4734 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from training/model.ckpt-5297
INFO:tensorflow:Restoring parameters from training/model.ckpt-5297
2018-08-05 12:37:51.787240: W T:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Conv/biases not found in checkpoint
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Key Conv/biases not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Tensorflow\models\research\object_detection\trainer.py"", line 332, in train
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1284, in __init__
    self.build()
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1296, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1333, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 781, in _build_internal
    restore_sequentially, reshape)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 400, in _AddRestoreOps
    restore_sequentially)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 832, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1546, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key Conv/biases not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Key Conv/biases not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Tensorflow\models\research\object_detection\trainer.py"", line 332, in train
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1284, in __init__
    self.build()
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1296, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1333, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 781, in _build_internal
    restore_sequentially, reshape)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 400, in _AddRestoreOps
    restore_sequentially)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 832, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1546, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key Conv/biases not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Traceback (most recent call last):
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Key Conv/biases not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Tensorflow\models\research\object_detection\trainer.py"", line 370, in train
    saver=saver)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py"", line 748, in train
    master, start_standard_services=False, config=session_config) as sess:
  File ""C:\Users\Dhito\Anaconda3\lib\contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 1005, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 833, in stop
    ignore_live_threads=ignore_live_threads)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 994, in managed_session
    start_standard_services=start_standard_services)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 731, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\session_manager.py"", line 281, in prepare_session
    config=config)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\session_manager.py"", line 211, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1768, in restore
    six.reraise(exception_type, exception_value, exception_traceback)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1752, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key Conv/biases not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Tensorflow\models\research\object_detection\trainer.py"", line 332, in train
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1284, in __init__
    self.build()
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1296, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1333, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 781, in _build_internal
    restore_sequentially, reshape)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 400, in _AddRestoreOps
    restore_sequentially)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 832, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1546, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\Users\Dhito\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key Conv/biases not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]`
```

I use the config file from the sample/config folder  and download the checkpoint from [Here](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz)

I use tensorflow 1.9 GPU with GTX1060, Windows 10, before this error i got this error message.
 I got this error [issues#3705](https://github.com/tensorflow/models/issues/3705#issuecomment-375563179) , when i solved that, this error comes up. I already make sure there's no miss/false directory. Please Help :(",25,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-05 05:49:20,open,,,[],2019-04-07 05:49:28
737,tensorflow/models,models,5002,jbasquiat,[Object detection] OutOfRangeError with Oxford-IIIT Pets Dataset,"### System information
- **What is the top-level directory of the model you are using**: models-master/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0/7.0.5
- **GPU model and memory**: gtx 950M 4Gb
- **Exact command to reproduce**:
```
python object_detection/model_main.py \
    --pipeline_config_path=pets/models/faster_rcnn_resnet101_pets.config \
    --model_dir=pets/models/ \
    --num_train_steps=50000 \
    --num_eval_steps=2000 \
    --alsologtostderr

```
Hi, I'm triying to run Oxford-IIIT Pets Dataset example locally. I ran _object_detection/dataset_tools/create_pet_tf_record.py_ to get tfrecord file. And I download _faster_rcnn_resnet101_pets.config_ and I changed path in the cofig file. I have this directory structure:
```
+data
    -label_map file
    -train TFRecord file
    -eval TFRecord file
+models
    -pipeline config file
    -model.ckpt, .pb and .pbtxt

```

But when I run the python file to train with this:

```
python object_detection/model_main.py \
    --pipeline_config_path=pets/models/faster_rcnn_resnet101_pets.config \
    --model_dir=pets/models/ \
    --num_train_steps=50000 \
    --num_eval_steps=2000 \
    --alsologtostderr

```

I get this error:

```

WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f5f6e9a90c8>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:From /home/juan/TFG/models-master/research/object_detection/core/box_predictor.py:403: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/juan/TFG/models-master/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2008: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
WARNING:tensorflow:From /home/juan/TFG/models-master/research/object_detection/core/losses.py:317: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
2018-08-04 16:49:15.436297: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-08-04 16:49:15.508705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-04 16:49:15.509470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 2.97GiB
2018-08-04 16:49:15.509486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-08-04 16:49:16.708317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-04 16:49:16.708344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-08-04 16:49:16.708351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-08-04 16:49:16.708519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2688 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2018-08-04 16:49:52.851978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-08-04 16:49:52.852048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-04 16:49:52.852055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-08-04 16:49:52.852061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-08-04 16:49:52.852274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2688 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0, compute capability: 5.0)
creating index...
index created!
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.00s).
Accumulating evaluation results...
Please run evaluate() first
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 85, in <module>
    tf.app.run()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 81, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 439, in train_and_evaluate
    executor.run()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 518, in run
    self.run_local()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 657, in run_local
    eval_result = evaluator.evaluate_and_export()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 847, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 425, in evaluate
    name=name)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1117, in _evaluate_model
    config=self._session_config)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/training/evaluation.py"", line 212, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 679, in __exit__
    self._close_internal(exception_type)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 711, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 811, in end
    feed_dict=self._final_ops_feed_dict)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[1], [1,?], [1,?,4], [1,?,37], [1,?], [1,?], [1,?], [1,?], [1,?,?,3], [1], [1], [1,?,?,3], [1], [1,3]], output_types=[DT_STRING, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_INT64, DT_BOOL, DT_FLOAT, DT_FLOAT, DT_STRING, DT_INT32, DT_UINT8, DT_STRING, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/non_max_suppression_8/NonMaxSuppressionV2/_4669 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_5335_...pressionV2"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op u'IteratorGetNext', defined at:
  File ""object_detection/model_main.py"", line 85, in <module>
    tf.app.run()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 81, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 439, in train_and_evaluate
    executor.run()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 518, in run
    self.run_local()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 657, in run_local
    eval_result = evaluator.evaluate_and_export()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 847, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 425, in evaluate
    name=name)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1085, in _evaluate_model
    input_fn, model_fn_lib.ModeKeys.EVAL))
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 691, in _get_features_and_labels_from_input_fn
    result = self._call_input_fn(input_fn, mode)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 798, in _call_input_fn
    return input_fn(**kwargs)
  File ""/home/juan/TFG/models-master/research/object_detection/inputs.py"", line 379, in _eval_input_fn
    input_dict = dataset_util.make_initializable_iterator(dataset).get_next()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 370, in get_next
    name=name)), self._output_types,
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1466, in iterator_get_next
    output_shapes=output_shapes, name=name)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[1], [1,?], [1,?,4], [1,?,37], [1,?], [1,?], [1,?], [1,?], [1,?,?,3], [1], [1], [1,?,?,3], [1], [1,3]], output_types=[DT_STRING, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_INT64, DT_BOOL, DT_FLOAT, DT_FLOAT, DT_STRING, DT_INT32, DT_UINT8, DT_STRING, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/non_max_suppression_8/NonMaxSuppressionV2/_4669 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_5335_...pressionV2"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]


```",3,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-04 15:12:41,open,,,[],2019-01-02 15:56:07
738,tensorflow/models,models,5001,rohitsaluja22,attention_ocr: using multiple gpu's on same machine for training,"What is the top-level directory of the model you are using: attention_ocr/python
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): pip install --upgrade tensorflow-gpu
TensorFlow version (use command below): 1.4.1.
Bazel version: N/A
CUDA/cuDNN version: cuda/8.0 cudnn/7.1.2
GPU model and memory: 8 x Tesla K80
Exact command to reproduce: CUDA_VISIBLE_DEVICES=6,7 python train.py --batch_size=4

Hi, I am trying to train attention_ocr on a new dataset. Images are of size 600X600.
When I use following command, I am only able to use one GPU and batch size of 2:-
CUDA_VISIBLE_DEVICES=7 python train.py --batch_size=2

I try to increase GPU's externally with following command:-
CUDA_VISIBLE_DEVICES=6,7 python train.py --batch_size=4

But I get allocation error with batch size of 4, with batch size of 2 only one GPU is in use(checked through nvidia-smi).
Can you help me how to use attention_ocr with multiple GPU's on same machine (to increase batch size as well preferably)?",7,"NamedUser(login=""alexgorban"")","[NamedUser(login=""alexgorban"")]",2018-08-04 13:22:56,open,,,['stat:awaiting tensorflower'],2018-10-02 01:54:36
739,tensorflow/models,models,5000,1213999170,Found an extremely huge element in [Embedding PCA parameters] of VGGish model,"I have run the [VGGish](https://github.com/tensorflow/models/tree/master/research/audioset) model to extracting features from .wav files. But the 128-embedding features seem quite different to the published features of [audioset ](https://research.google.com/audioset/download.html).

Finally I found there is an extremely huge element in [Embedding PCA parameters](https://storage.googleapis.com/audioset/vggish_pca_params.npz) as following: 

![image](https://user-images.githubusercontent.com/3352303/43671410-2320005a-97cc-11e8-9722-c240d414f068.png)

All the pca_eigen_vectors' elements' absolute values are less than 5, except for pca_eigen_vector[127][65]  which is 44.999. Meanwhile, the pca_means[65] is -0.251, and 44.999 * 0.251 = 11.295. It's quite beyond the QUANTIZE_MAX_VAL = +2.0. 

So all the last elements of the 128-embedding features extracted by VGGish model are 255. There must be something wrong with the PCA parameters.",4,"NamedUser(login=""plakal"")","[NamedUser(login=""plakal"")]",2018-08-04 01:55:09,open,,,[],2018-08-09 01:34:06
740,tensorflow/models,models,4995,zhengw060024,there is some error in model tutorials\image\mnist,"
In the  convolutional.py 
line 201 
![image](https://user-images.githubusercontent.com/7269014/43637157-cb98682a-9746-11e8-9cc8-be935a1f3418.png)
the max_pooling padding is ""same""
but line173
![image](https://user-images.githubusercontent.com/7269014/43637274-1eb05716-9747-11e8-8aba-97e0b22908de.png)
the size is [IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64
if the image_size is 33（a odd num）after two ""same"" padding max_pooling ((33+1) /2+1)/2) ,the size is 9,not 8,error may be happened",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-03 10:04:03,open,,,['stat:awaiting response'],2018-08-03 19:22:48
741,tensorflow/models,models,4994,twangnh,Memory Leak: Dst tensor is not  initialized pops out even I have enough GPU memory,"I'm using tf object detection api
```
INFO:tensorflow:global step 11202: loss = 3.6547 (0.945 sec/step)
INFO:tensorflow:global step 11203: loss = 4.0109 (0.905 sec/step)
INFO:tensorflow:global step 11204: loss = 3.9925 (0.968 sec/step)
INFO:tensorflow:global step 11205: loss = 4.3680 (0.955 sec/step)
INFO:tensorflow:global step 11206: loss = 4.3539 (0.919 sec/step)
INFO:tensorflow:global step 11207: loss = 4.3368 (0.921 sec/step)
INFO:tensorflow:global step 11208: loss = 3.7202 (0.907 sec/step)
INFO:tensorflow:global step 11209: loss = 3.2813 (0.928 sec/step)
INFO:tensorflow:global step 11210: loss = 3.5669 (0.939 sec/step)
INFO:tensorflow:global step 11211: loss = 3.4885 (0.985 sec/step)
INFO:tensorflow:global step 11212: loss = 3.1678 (0.924 sec/step)
INFO:tensorflow:global step 11213: loss = 3.2577 (0.924 sec/step)
INFO:tensorflow:global step 11214: loss = 4.0300 (1.147 sec/step)
INFO:tensorflow:Recording summary at step 11214.
INFO:tensorflow:global step 11215: loss = 3.7617 (0.998 sec/step)
INFO:tensorflow:global step 11216: loss = 3.3286 (0.941 sec/step)
INFO:tensorflow:global step 11217: loss = 3.2815 (0.932 sec/step)
INFO:tensorflow:global step 11218: loss = 4.1242 (0.966 sec/step)
INFO:tensorflow:global step 11219: loss = 4.5689 (0.966 sec/step)
INFO:tensorflow:global step 11220: loss = 3.6092 (1.072 sec/step)
INFO:tensorflow:global step 11221: loss = 4.2938 (1.001 sec/step)
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.
framework.errors_impl.InternalError'>, Dst tensor is not initialized.
         [[Node: clone_3/prefetch_queue_Dequeue/_8301 = _Recv[client_term
inated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:3"",
 send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_
incarnation=1, tensor_name=""edge_1660_clone_3/prefetch_queue_Dequeue"", te
nsor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:3
""]()]]
Traceback (most recent call last):                              [65/1841]
  File ""train.py"", line 216, in <module>
    tf.app.run()
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 212, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/home/user/prj/tf-object-api-orig/object_detection/trainer.py""
, line 435, in train
    saver=saver)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/contrib/slim/python/slim/learning.py"", line 769, in train
    sess, train_op, global_step, train_step_kwargs)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_
step
    run_metadata=run_metadata)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not 
initialized.
         [[Node: clone_3/prefetch_queue_Dequeue/_8301 = _Recv[client_term
inated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:3"",
 send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", sen[69/1841]
incarnation=1, tensor_name=""edge_1660_clone_3/prefetch_queue_Dequeue"", te
nsor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:3
""]()]]
Traceback (most recent call last):
  File ""train.py"", line 216, in <module>
    tf.app.run()
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 212, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/home/user/prj/tf-object-api-orig/object_detection/trainer.py""
, line 435, in train
    saver=saver)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/contrib/slim/python/slim/learning.py"", line 769, in train
    sess, train_op, global_step, train_step_kwargs)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_
step
    run_metadata=run_metadata)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/user/anaconda2/envs/tensorflow_/lib/python2.7/site-packa
ges/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not 
initialized.
         [[Node: clone_3/prefetch_queue_Dequeue/_8301 = _Recv[client_term
inated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:3"",
 send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_
incarnation=1, tensor_name=""edge_1660_clone_3/prefetch_queue_Dequeue"", te
nsor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:3
""]()]]
```
I got this error even if I have checked that I have enough GPU memory, I'm train on 4 GPU, the memory consumption holds at about 8G/12G for several thousands step, then the error pops out, I suspect there is memory leak, can someone help?",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-08-03 09:39:49,open,,,['stat:awaiting response'],2018-08-04 04:40:28
742,tensorflow/models,models,4993,jixiaonanzhuaizhuai,Object detect : Batchsize is 2 and cannot be increased more when use model fasterRcnn_inception_resnet_v2 ?,"
When I use model fasterRcnn_inception_resnet_v2 with my own data for training, I set batchsize=2 to use my 1 GPU. 

I found that my training loss curve has not been convergent, and the shock is quite severe. Is there any way to help me increase my batchsize",6,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-08-03 07:13:45,open,,,[],2018-10-11 16:53:48
743,tensorflow/models,models,4992,chowkamlee81,OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key xception_65/entry_flow/block1/unit_1/xception_module/separable_conv1_depthwise/BatchNorm/beta not found in checkpoint,"I was training deeplabv3 on cityscape dataset using command below.

python train.py --logtostderr --training_number_of_steps=90000 --train_split=""train"" --model_variant=""xception_65"" --atrous_rates=6 --atrous_rates=12 --atrous_rates=18 --output_stride=16 --decoder_output_stride=4 --train_crop_size=769 --train_crop_size=769 --train_batch_size=1 --dataset=""cityscapes"" --tf_initial_checkpoint=""/home/administrator/Documents.orig/FileSystem/TensorFlow/models-master/research/deeplab/pretrained/xception_65/model.ckpt"" --train_logdir=""/home/administrator/Documents.orig/SEMANTIC_SEGMENTATION_DATASET/CITYSCAPES_RAWDATASET/exp/train_on_train_set/train"" --dataset_dir=""/home/administrator/Documents.orig/SEMANTIC_SEGMENTATION_DATASET/CITYSCAPES_RAWDATASET/tfrecord""


Iam using 2 nividia geforce 1080 Ti each of 11GB.
 **Please help on this.**
But iam getting error below :-1: 
pciBusID: 0000:17:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2018-08-03 01:04:35.431712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-08-03 01:04:35.691451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-03 01:04:35.691486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-08-03 01:04:35.691508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-08-03 01:04:35.691786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10405 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from /home/administrator/Documents.orig/SEMANTIC_SEGMENTATION_DATASET/CITYSCAPES_RAWDATASET/exp/train_on_train_set/train/model.ckpt-0
2018-08-03 01:04:36.551027: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key xception_65/entry_flow/block1/unit_1/xception_module/separable_conv1_depthwise/BatchNorm/beta not found in checkpoint
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Key xception_65/entry_flow/block1/unit_1/xception_module/separable_conv1_depthwise/BatchNorm/beta not found in checkpoint
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op u'save/RestoreV2', defined at:
  File ""train.py"", line 396, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 389, in main
    save_interval_secs=FLAGS.save_interval_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 658, in train
    saver = saver or tf_saver.Saver()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1338, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 835, in _build_internal
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 472, in _AddRestoreOps
    restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 886, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key xception_65/entry_flow/block1/unit_1/xception_module/separable_conv1_depthwise/BatchNorm/beta not found in checkpoint
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Traceback (most recent call last):
  File ""train.py"", line 396, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 389, in main
    save_interval_secs=FLAGS.save_interval_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 747, in train
    master, start_standard_services=False, config=session_config) as sess:
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 828, in stop
    ignore_live_threads=ignore_live_threads)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 989, in managed_session
    start_standard_services=start_standard_services)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 726, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 279, in prepare_session
    config=config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 207, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1802, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key xception_65/entry_flow/block1/unit_1/xception_module/separable_conv1_depthwise/BatchNorm/beta not found in checkpoint
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op u'save/RestoreV2', defined at:
  File ""train.py"", line 396, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 389, in main
    save_interval_secs=FLAGS.save_interval_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 658, in train
    saver = saver or tf_saver.Saver()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1338, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 835, in _build_internal
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 472, in _AddRestoreOps
    restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 886, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key xception_65/entry_flow/block1/unit_1/xception_module/separable_conv1_depthwise/BatchNorm/beta not found in checkpoint
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",5,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-08-03 05:08:21,open,,,"['models: research', 'stat:awaiting response']",2019-02-01 22:33:11
744,tensorflow/models,models,4988,b0noI,Broken link on TensroRT page,"page: https://github.com/tensorflow/models/tree/master/research/tensorrt
link that is broken: http://download.tensorflow.org/models/official/resnetv2_imagenet_savedmodel.tar.gz (SavedModel)

attempt to open shows:
```
<Error>
<Code>AccessDenied</Code>
<Message>Access denied.</Message>
<Details>
Anonymous caller does not have storage.objects.get access to download.tensorflow.org/models/official/resnetv2_imagenet_savedmodel.tar.gz.
</Details>
</Error>
```",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-08-02 18:09:17,open,,,['stat:awaiting response'],2018-08-03 01:40:33
745,tensorflow/models,models,4986,WZMIAOMIAO,Executor failed to create kernel,"### System information
- **What is the top-level directory of the model you are using**: /home/wz/models-master-20180312
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip3 install
- **TensorFlow version (use command below)**:1.6 GPU
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:  CUDA7.0 / cuDNN 9.0
- **GPU model and memory**: GTX 1060 6G
- **Python version**: 3.5
- **Exact command to reproduce**:  #!/usr/bin/env bash
export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim

PIPELINE_CONFIG_PATH='/home/wz/Boat_test/rfcn_resnet101_coco.config'
MODEL_DIR='/home/wz/Boat_test/train_20180802_rfcn'
NUM_TRAIN_STEPS=30000
NUM_EVAL_STEPS=1
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --num_eval_steps=${NUM_EVAL_STEPS} \
    --alsologtostderr

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I copy configure from object_detection/sample file, modify some path information. 
I run model_main.py flowing runing locally file. Then get this problem.

### Source code / logs
/home/wz/models-master-20180802/research/object_detection/utils/visualization_utils.py:25: UserWarning: 
This call to matplotlib.use() has no effect because the backend has already
been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

The backend was *originally* set to 'TkAgg' by the following code:
  File ""object_detection/model_main.py"", line 26, in <module>
    from object_detection import model_lib
  File ""/home/wz/models-master-20180802/research/object_detection/model_lib.py"", line 26, in <module>
    from object_detection import eval_util
  File ""/home/wz/models-master-20180802/research/object_detection/eval_util.py"", line 28, in <module>
    from object_detection.metrics import coco_evaluation
  File ""/home/wz/models-master-20180802/research/object_detection/metrics/coco_evaluation.py"", line 20, in <module>
    from object_detection.metrics import coco_tools
  File ""/home/wz/models-master-20180802/research/object_detection/metrics/coco_tools.py"", line 47, in <module>
    from pycocotools import coco
  File ""/home/wz/models-master-20180802/research/pycocotools/coco.py"", line 49, in <module>
    import matplotlib.pyplot as plt
  File ""/home/wz/.local/lib/python3.5/site-packages/matplotlib/pyplot.py"", line 71, in <module>
    from matplotlib.backends import pylab_setup
  File ""/home/wz/.local/lib/python3.5/site-packages/matplotlib/backends/__init__.py"", line 16, in <module>
    line for line in traceback.format_stack()


  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f476e65dea0>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /home/wz/models-master-20180802/research/object_detection/utils/ops.py:713: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/wz/models-master-20180802/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2070: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
WARNING:tensorflow:From /home/wz/models-master-20180802/research/object_detection/core/losses.py:317: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See tf.nn.softmax_cross_entropy_with_logits_v2.

/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
2018-08-02 20:18:28.257853: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-08-02 20:18:28.350196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-02 20:18:28.350449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.8475
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.51GiB
2018-08-02 20:18:28.350464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-08-02 20:18:28.501682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5292 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-08-02 20:18:41.291284: E tensorflow/core/common_runtime/executor.cc:645] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
2018-08-02 20:18:41.307261: E tensorflow/core/common_runtime/executor.cc:645] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
2018-08-02 20:18:41.323339: E tensorflow/core/common_runtime/executor.cc:645] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
2018-08-02 20:18:41.338391: E tensorflow/core/common_runtime/executor.cc:645] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
2018-08-02 20:18:41.353837: E tensorflow/core/common_runtime/executor.cc:645] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
2018-08-02 20:18:41.369156: E tensorflow/core/common_runtime/executor.cc:645] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
2018-08-02 20:18:41.384667: E tensorflow/core/common_runtime/executor.cc:645] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
2018-08-02 20:18:41.400290: E tensorflow/core/common_runtime/executor.cc:645] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
2018-08-02 20:18:41.400669: W tensorflow/core/framework/op_kernel.cc:1202] OP_REQUIRES failed at iterator_ops.cc:870 : Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
2018-08-02 20:18:41.416507: E tensorflow/core/common_runtime/executor.cc:645] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
Traceback (most recent call last):
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1361, in _do_call
    return fn(*args)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1340, in _run_fn
    target_list, status, run_metadata)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[1], [1,?,?,3], [1,3], [1,100], [1,100,4], [1,100,1], [1,100], [1,100], [1,100], [1]], output_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_BOOL, DT_FLOAT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
	 [[Node: map/TensorArrayUnstack_1/Shape/_4913 = _HostRecv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1238_map/TensorArrayUnstack_1/Shape"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 421, in train_and_evaluate
    executor.run()
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 494, in run
    self.run_local()
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 626, in run_local
    hooks=train_hooks)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 352, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 891, in _train_model
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 546, in run
    run_metadata=run_metadata)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1022, in run
    run_metadata=run_metadata)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1113, in run
    raise six.reraise(*original_exc_info)
  File ""/home/wz/.local/lib/python3.5/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1098, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1170, in run
    run_metadata=run_metadata)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 950, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/home/wz/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[1], [1,?,?,3], [1,3], [1,100], [1,100,4], [1,100,1], [1,100], [1,100], [1,100], [1]], output_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_BOOL, DT_FLOAT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
	 [[Node: map/TensorArrayUnstack_1/Shape/_4913 = _HostRecv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1238_map/TensorArrayUnstack_1/Shape"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-08-02 20:18:41.433357: E tensorflow/core/common_runtime/executor.cc:645] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]

	 [[Node: Pad_9 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Slice_9, stack_9)]]

",9,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-08-02 12:36:24,open,,,['type:bug/performance'],2018-08-27 22:55:14
746,tensorflow/models,models,4985,irmowan,[Feature Request] Show training image before and after augmentation,"When I want to see what these augmentation operations really do, it is very adhoc for me to summary some images to tensorboard, and have no way to log them to files.

Want to show sample training images before and after augmentation will help me debug my model.",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-08-02 11:55:37,open,,,['stat:awaiting response'],2018-08-29 16:23:57
747,tensorflow/models,models,4977,spicyramen,Update directory path and update bashrc file,"- Update directory path with Tensorflow directoy folder: tensorflow/models/research/
- Add option to refresh source ~/.bashrc",0,,[],2018-08-02 06:30:28,open,,,['cla: yes'],2018-11-05 07:32:09
748,tensorflow/models,models,4974,theJiangYu,Change vocab_size from 33708 to 33945 in model_params.py,"Change vocab_size from 33708 to 33945 in models/official/transformer/model/model_params.py
The original vocab_size causes InvalidArgumentError when training Transformer using CPU.",2,,[],2018-08-01 19:05:50,open,,,['cla: yes'],2018-08-01 20:00:16
749,tensorflow/models,models,4973,AbhirupKamath,updated the path for fsns dataset in the code  fsns.py,"Fixed the path for fsns dataset (DEFAULT_DATASET_DIR) on windows . 

The previous code would create an error : 

""C:\Users\DELL\Anaconda3\envs\python35DL\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open .. The system cannot find the path specified .

Above changes have fixed this error. 

",0,,[],2018-08-01 17:06:23,open,,,['cla: yes'],2018-08-02 16:12:44
750,tensorflow/models,models,4970,javadhelali,Added missing parsey_universal directory to syntaxnet,"Fixed #3001
the broken sample code [here](https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/universal.md) is now valid and runnable.",5,,[],2018-08-01 12:18:27,open,,,['cla: yes'],2018-08-01 12:26:42
751,tensorflow/models,models,4968,tdf1995,Cannot assign a device for operation,"when i tried to train the resnet_v2 in the flowers datasets using train_image_classifier.py，i got this error:

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'gradients/softmax_cross_entropy_loss/xentropy_grad/LogSoftmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.

NVIDIA GeForce GTX 1060 5GB
CUDA9.0
CUDNN v7
I found some related solutions: set the clone_on_CPU to True,but it's too slow,and I need to use GPU to train,how can i solve this problem",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-08-01 07:31:53,open,,,['stat:awaiting response'],2018-08-01 19:15:36
752,tensorflow/models,models,4954,alexgorban,Fix FSNS dataset set path construction on Windows.,,1,,[],2018-07-31 19:23:09,open,,,['cla: yes'],2018-09-21 19:06:21
753,tensorflow/models,models,4952,sxsxsx,TypeError: create_training_graph() got an unexpected keyword argument 'quant_delay',"ubantu16.04 64bit
pip install tensorflow-gpu==1.6
python27
cuda 9.1 cudnn7.0

pwd
/home/icare/yqli/models-master/research/slim

 python nets/mobilenet_v1_train.py --dataset_dir ""/tmp/data/flowers"" --fine_tune_checkpoint  ""/tmp/checkpoints/mobilenet_v1_quant/mobilenet_v1_1.0_224_quant.ckpt"" --quantize=True --checkpoint_dir ""/tmp/mobilenet/quantize_int8_train""

WARNING:tensorflow:From /home/icare/.local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:731: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See tf.nn.softmax_cross_entropy_with_logits_v2.

Traceback (most recent call last):
  File ""nets/mobilenet_v1_train.py"", line 213, in <module>
    tf.app.run(main)
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""nets/mobilenet_v1_train.py"", line 209, in main
    train_model()
  File ""nets/mobilenet_v1_train.py"", line 192, in train_model
    g, train_tensor = build_model()
  File ""nets/mobilenet_v1_train.py"", line 140, in build_model
    tf.contrib.quantize.create_training_graph(quant_delay=get_quant_delay())
TypeError: create_training_graph() got an unexpected keyword argument 'quant_delay'
",4,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-07-31 12:33:19,open,,,[],2018-08-03 19:22:03
754,tensorflow/models,models,4951,wgshun,"After the last update, the training problem.","After the update of July 13, 2018, when I was training, i encountered a problem.
```
2018-07-31 18:30:45.568587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-31 18:30:45.569107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:04:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-07-31 18:30:45.570029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1
2018-07-31 18:30:45.962995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-31 18:30:45.963022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 
2018-07-31 18:30:45.963027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y 
2018-07-31 18:30:45.963046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N 
2018-07-31 18:30:45.963359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10143 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
2018-07-31 18:30:46.055702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10407 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path /home/vcaadmin/models/research/object_detection/model/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:Saving checkpoint to path /home/vcaadmin/models/research/object_detection/model/model.ckpt
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
```
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.67                 Driver Version: 390.67                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:03:00.0  On |                  N/A |
|  1%   50C    P8    17W / 250W |  10676MiB / 11177MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |
|  1%   50C    P8    17W / 250W |  10631MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1074      G   /usr/lib/xorg/Xorg                           124MiB |
|    0      1619      G   compiz                                       138MiB |
|    0     24459      C   python                                     10387MiB |
|    0     29566      G   /opt/teamviewer/tv_bin/TeamViewer             21MiB |
|    1     24459      C   python                                     10619MiB |
+-----------------------------------------------------------------------------+
```
Why does it take up memory, but not use it?
Can someone know why?
3Q",5,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-31 10:48:39,open,,,['stat:awaiting tensorflower'],2018-08-11 13:33:59
755,tensorflow/models,models,4941,mbenami,Save Best Model option for eval.py,"- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/a
- **TensorFlow installed from (source or binary)**:source 
- **TensorFlow version (use command below)**:N/a
- **Bazel version (if compiling from source)**:N/a
- **CUDA/cuDNN version**:N/a
- **GPU model and memory**:N/a
- **Exact command to reproduce**:N/a

New feature, 
Hi I know that in keras, when you train there is an option `save_best` for saving the best model evaluation depend on the matric you want (`loss`, `acc`, or custom )

is there an option to add this when using eval.py for evaluating the train model? 
the matric can be one of the `metrics_set` in the eval_config 
that can save space when training for long time and you don't want to save all the train checkpoint 

Thanks.

 

Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-30 15:00:29,open,,,[],2018-08-02 16:26:19
756,tensorflow/models,models,4939,AakashKumarNain,Changes in the API not documented ,"### System information
- **What is the top-level directory of the model you are using**: Object Detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: Cuda 9.0
- **GPU model and memory**: K80
- **Exact command to reproduce**: N/A


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Since the day OpenImageData challenge has started, the API has changed a lot. I have two repos cloned in my computer. The older one contains `train.py`, `eval.py`, etc and everything worked as it was documented in the source code. Now, this whole API has changed and the worst thing is nothing in the documentation has been updated how to run the models or evaluate them. Also, it feels like everything is in favour of TPUs, which of course most people don't use. Also, earlier API was compatible with both Python2 and Python3. Now the `coco_evaluation` py file used is incompatible with Python3.5. 

Can you please clear what's going on here?
",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-07-30 09:26:42,open,,,[],2018-07-31 17:01:05
757,tensorflow/models,models,4924,vsuthichai,transformer model evaluation fails translating test data,"### System information
- **What is the top-level directory of the model you are using**:
transformer
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.4 LTS
Release:	16.04
Codename:	xenial
- **TensorFlow installed from (source or binary)**:
AWS DLAMI 12.0
- **TensorFlow version (use command below)**:
v1.9.0-0-g25c197e023 1.9.0
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA 9.0 and Intel MKL-DNN
- **GPU model and memory**:
Tesla V100-SXM2-16GB 
x8 gpus
- **Exact command to reproduce**:
```
#!/bin/bash
WORKING_DIR=/home/ubuntu/benchmarks/scripts/models/official/transformer
cd $WORKING_DIR
export PYTHONPATH=/home/ubuntu/benchmarks/scripts/models
export PARAM_SET=tiny
export DATA_DIR=$WORKING_DIR/data
export MODEL_DIR=$WORKING_DIR/model_$PARAM_SET
export VOCAB_FILE=$DATA_DIR/vocab.ende.32768

rm -rf model_$PARAM_SET

python transformer_main.py --data_dir=$DATA_DIR --model_dir=$MODEL_DIR \
    --vocab_file=$VOCAB_FILE --param_set=$PARAM_SET \
    --train_epochs=2 \
    --bleu_source=test_data/newstest2014.en --bleu_ref=test_data/newstest2014.de
```
You can collect some of this information using our environment capture script:
[tf_env.txt](https://github.com/tensorflow/models/files/2238130/tf_env.txt)
```
     active environment : tensorflow_p36
    active env location : /home/ubuntu/anaconda3/envs/tensorflow_p36
            shell level : 1
       user config file : /home/ubuntu/.condarc
 populated config files :
          conda version : 4.4.10
    conda-build version : 3.4.1
         python version : 3.6.4.final.0
       base environment : /home/ubuntu/anaconda3  (writable)
           channel URLs : https://repo.continuum.io/pkgs/main/linux-64
                          https://repo.continuum.io/pkgs/main/noarch
                          https://repo.continuum.io/pkgs/free/linux-64
                          https://repo.continuum.io/pkgs/free/noarch
                          https://repo.continuum.io/pkgs/r/linux-64
                          https://repo.continuum.io/pkgs/r/noarch
                          https://repo.continuum.io/pkgs/pro/linux-64
                          https://repo.continuum.io/pkgs/pro/noarch
          package cache : /home/ubuntu/anaconda3/pkgs
                          /home/ubuntu/.conda/pkgs
       envs directories : /home/ubuntu/anaconda3/envs
                          /home/ubuntu/.conda/envs
               platform : linux-64
             user-agent : conda/4.4.10 requests/2.18.4 CPython/3.6.4 Linux/4.4.0-1062-aws ubuntu/16.04 glibc/2.23
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False
```

### Describe the problem
BLEU score evaluation for the transformer model happens after every iteration (train iteration/eval).  The evaluation process translates the BLEU test data source file and writes this translation to a temp file.  This translated file is then compared with the BLEU test data reference file to compute the model's BLEU score.  Occasionally some translations will be an empty line, ie. there appears to be no translation.  This isn't necessarily a problem I think but it becomes a problem when the very first translation, ie. the first line in the translated tmp file, is just a newline.  This newline gets stripped away before split lines is called, so the length check fails.

### Source code / logs

```
I0728 01:38:33.188780 140138513848064 tf_logging.py:115] Writing to file /tmp/tmpnuei9hi1
Traceback (most recent call last):
  File ""transformer_main.py"", line 635, in <module>
    absl_app.run(main)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/absl/app.py"", line 274, in run
    _run_main(main, args)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/absl/app.py"", line 238, in _run_main
    sys.exit(main(argv))
  File ""transformer_main.py"", line 629, in main
    run_transformer(flags.FLAGS)
  File ""transformer_main.py"", line 611, in run_transformer
    vocab_file=flags_obj.vocab_file)
  File ""transformer_main.py"", line 350, in run_loop
    estimator, bleu_source, bleu_ref, vocab_file)
  File ""transformer_main.py"", line 233, in evaluate_and_log_bleu
    estimator, subtokenizer, bleu_source, bleu_ref)
  File ""transformer_main.py"", line 217, in translate_and_compute_bleu
    uncased_score = compute_bleu.bleu_wrapper(bleu_ref, tmp_filename, False)
  File ""/home/ubuntu/benchmarks/scripts/models/official/transformer/compute_bleu.py"", line 93, in bleu_wrapper
    raise ValueError(""Reference and translation files have different number of ""
ValueError: Reference and translation files have different number of lines.
```

Within `bleu_wrapper` function of `compute_bleu.py`, line 89:
```python
  ref_lines = tf.gfile.Open(ref_filename).read().strip().splitlines()
  hyp_lines = tf.gfile.Open(hyp_filename).read().strip().splitlines()

  if len(ref_lines) != len(hyp_lines):
    raise ValueError(""Reference and translation files have different number of ""
                     ""lines."")
```
Temp file generated during BLEU score evaluation
[tmpnuei9hi1.txt](https://github.com/tensorflow/models/files/2237697/tmpnuei9hi1.txt)",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-28 05:08:25,open,,,['stat:awaiting response'],2018-09-17 22:08:50
758,tensorflow/models,models,4923,fdkssdks,Train.py gives AttributeError: 'InputReader' object has no attribute 'mask_type',"When running  
python legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet50_coco.config

```PYTHON
WARNING:tensorflow:From /home/ubuntu/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/platform/app.py:124: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From /home/ubuntu/models/research/object_detection/legacy/trainer.py:262: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
Traceback (most recent call last):
  File ""legacy/train.py"", line 184, in <module>
    tf.app.run()
  File ""/home/ubuntu/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""/home/ubuntu/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 136, in new_func
    return func(*args, **kwargs)
  File ""legacy/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/home/ubuntu/models/research/object_detection/legacy/trainer.py"", line 276, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""/home/ubuntu/models/research/object_detection/legacy/trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""legacy/train.py"", line 121, in get_next
    dataset_builder.build(config)).get_next()
  File ""/home/ubuntu/models/research/object_detection/builders/dataset_builder.py"", line 120, in build
    instance_mask_type=input_reader_config.mask_type,
AttributeError: 'InputReader' object has no attribute 'mask_type'
```
How can I solve this issue?
------------------------

**System information**
**OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Amazon AWS p2.xlarge Ubuntu 16.04 LTS 
**TensorFlow installed from (source or binary):** binary
**TensorFlow version (use command below):** 1.5.0
Bazel version (if compiling from source):
**CUDA/cuDNN version:** 9.0 / 7.0
**GPU model and memory:** Amazon AWS k series 100GB
**Exact command to reproduce:** python legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet50_coco.config",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-07-27 19:42:29,open,,,['stat:awaiting response'],2018-07-28 06:50:51
759,tensorflow/models,models,4922,tingxingdong,mobileNet_v1 GEMM percentage not consistent with the paper on my profiling ,"### System information
- **What is the top-level directory of the model you are using**: mobileNet v1
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **TensorFlow installed from (source or binary)**:
binary 
- **TensorFlow version (use command below)**:
1.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 7.5
- **GPU model and memory**: P5000
- **Exact command to reproduce**:
nvprof python -m scripts.label_image \
    --graph=tf_files/retrained_graph.pb  \
    --image=tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg

I am testing float32 precision. 
According to the mobileNet v1 paper: page 4. MobileNet v1 spend 95% time on the 1x1 convolution which is implemented with GEMM ( guess should be SGEMM).  There is no a graph related to the statement in the paper. 

I used nvprof to profile mobielNet V1 with inference to verify the paper's statement on my P5000.  However, nvprof shows FFT takes 37.5% of the entire GPU time. The CGEMM  (I guess should be used with FFT in FFT based conv2d) takes 50%+ of the GPU time.    While the denoted SGEMM routine only takes less than 3% of the GPU time.  The FFT is not mentioned in the paper at all.  So I want to know where the 95% GEMM come from? 

part of the profiling results 
==28906== Profiling application: python -m scripts.label_image --graph=tf_files/retrained_graph.pb --image=tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg
==28906== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   47.29%  11.971ms         9  1.3301ms  346.56us  4.9989ms  maxwell_cgemm_32x64_tn
                   18.79%  4.7556ms        10  475.56us  16.352us  2.4202ms  void fft2d_r2c_16x16<float>(float2*, float const *, int, int, int, int, int, int, int, int)
                    5.64%  1.4280ms         9  158.66us  4.3200us  703.65us  void flip_filter<float, float>(float*, float const *, int, int, int, int)
                    4.73%  1.1978ms         9  133.09us  7.1370us  558.37us  void transpose_readWrite_alignment_kernel<float2, float2, int=1, bool=0, int=6, int=4, int=4>(cublasTransposeParams<float2>, float2 const *, float2*, float2 const *)
                    4.52%  1.1444ms         1  1.1444ms  1.1444ms  1.1444ms  maxwell_cgemm_64x64_tn
                    2.26%  571.81us        92  6.2150us     608ns  167.52us  [CUDA memcpy HtoD]
                    2.20%  557.41us        17  32.788us  18.592us  60.897us  maxwell_scudnn_128x128_relu_interior_nn
                    1.93%  487.88us         2  243.94us  168.93us  318.95us  void fft2d_r2c_32x32<float, unsigned int=1, bool=0>(float2*, float const *, int, int, int, int, int, int, int, int, int, cudnn::reduced_divisor, bool)
                    1.92%  486.18us         4  121.55us  18.144us  288.96us  void fft2d_r2c_64x64<float>(float2*, float const *, int, int, int, int, int, int, int, int)
                    1.81%  458.27us        11  41.661us  11.552us  74.689us  void im2col4d_kernel<float, int>(im2col4d_params, cudnnConvolutionStruct, cudnnTensor4dStruct, float const *, float*, int)
                    1.71%  434.05us        11  39.459us  21.472us  69.857us  void cudnn::detail::implicit_convolve_sgemm<float, float, int=128, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::implicit_convolve_sgemm<float, float, int=128, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>*, kernel_conv_params, int, float, float, int, float, float, int, int)
                    1.17%  297.15us         2  148.58us  20.576us  276.58us  void DSE::vector_fft<int=0, int=1, int=128, int=8, int=8, int=1, float, float, float2>(float2*, float2, int, int3, float2*)
                    0.89%  225.54us         2  112.77us  34.112us  191.43us  void DSE::regular_fft_pad<int=0, int=1, int=128, int=16, int=32, int=1, float, float, float2>(float2*, float*, int, int3, float*, int, flo

....


mobileNet v1 paper: https://arxiv.org/abs/1704.04861
The profile case: https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#5
",1,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-07-27 19:36:59,open,,,['stat:awaiting owner'],2018-08-01 18:42:25
760,tensorflow/models,models,4921,KevinLucidyne,Rewrite models/tutorials/image/cifar10_estimator to remove deprecated methods,"What is the top-level directory of the model you are using: models/tutorials/image/cifar10_estimator
Have I written custom code; No.
OS Platform and Distribution: Ubuntu 17.10
TensorFlow installed from: N/A
TensorFlow version: 1.9 (master branch)
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: See https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator

The master branch version of models/tutorials/image/cifar10_estimator/cifar10_main.py is using deprecated functions.  Please rewrite because this is the most advanced example posted for using multiple GPUs.

When you run this program you get the warnings

```
WARNING:tensorflow:From pslinknet.py:397: Experiment.__init__ (from tensorflow.contrib.learn.python.learn.experiment) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.estimator.train_and_evaluate. You will also have to convert to a tf.estimator.Estimator.
WARNING:tensorflow:From /home/gradescan/miniconda3/envs/untitled/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:279: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.
Instructions for updating:
Monitors are deprecated. Please use tf.train.SessionRunHook.

```",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-27 17:48:15,open,,,[],2019-03-26 14:58:46
761,tensorflow/models,models,4918,dheera,Support batch inference for exported frozen models,"This PR allows exported models to be fed with batches of images for inference, e.g.
`    feed_dict = { ""ImageTensor:0"": [ image0, image1, image2, ... ] }`
Inference times for batches of size 1 are unaffected.

Sorry for the mess of commits -- I had to sync this to upstream/master since I had accidentally used dheera/master for another pull request in-progress and created another branch for this particular PR which is separate. This pull request contains only the code necessary for the batch inference. The only file changed should be export_model.py (see the Files changed section)",2,,[],2018-07-27 06:04:38,open,,,['cla: yes'],2018-08-01 16:48:52
762,tensorflow/models,models,4911,AakashKumarNain,SSD-MobileNetv1-focal loss config missing,"### System information
- **What is the top-level directory of the model you are using**: Object Detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: K80
- **Exact command to reproduce**: N/A

I cloned this repository a few weeks back. At that time, in the sample configs, `ssd_mobilentv1_focal_loss_coco.config` was present but now in the updated repository, the config file is nowhere to be found. Is there any particular reason why it has been removed?",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-07-26 17:37:05,open,,,[],2018-07-27 19:07:10
763,tensorflow/models,models,4905,shauviks,"google.protobuf.text_format.ParseError: 135:19 : Message type ""object_detection.protos.TFRecordInputReader"" has no field named ""D""","Hi When i am trying to run train.py, I am getting the above error . Can someone please help me . The complete error is below.
`(tfobj) D:\tensorflow\models\research\object_detection>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config
C:\Users\kkran\Anaconda3\envs\tfobj\lib\importlib\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
C:\Users\kkran\Anaconda3\envs\tfobj\lib\importlib\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
WARNING:tensorflow:From C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\tensorflow\python\platform\app.py:124: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\tensorflow\python\platform\app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\tensorflow\python\util\deprecation.py"", line 136, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 93, in main
    FLAGS.pipeline_config_path)
  File ""D:\tensorflow\models\research\object_detection\utils\config_util.py"", line 94, in get_configs_from_pipeline_file
    text_format.Merge(proto_str, pipeline_config)
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\google\protobuf\text_format.py"", line 533, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\google\protobuf\text_format.py"", line 587, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\google\protobuf\text_format.py"", line 620, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\google\protobuf\text_format.py"", line 635, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\google\protobuf\text_format.py"", line 735, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\google\protobuf\text_format.py"", line 823, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\google\protobuf\text_format.py"", line 735, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\google\protobuf\text_format.py"", line 823, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""C:\Users\kkran\Anaconda3\envs\tfobj\lib\site-packages\google\protobuf\text_format.py"", line 703, in _MergeField
    (message_descriptor.full_name, name))
google.protobuf.text_format.ParseError: 135:19 : Message type ""object_detection.protos.TFRecordInputReader"" has no field named ""D"".`",5,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-26 05:43:37,open,,,[],2018-11-24 06:31:50
764,tensorflow/models,models,4901,kant,Typo on string #237,Plus minor formatting proposals,0,,[],2018-07-26 02:40:57,open,,,['cla: yes'],2018-07-26 02:41:00
765,tensorflow/models,models,4900,theJiangYu,[Transformer] InvalidArgumentError at the beginning of training a transformer model on CPU ,"It throws the following InvalidArgumentError at the beginning of training a Transformer model (models/official/transformer/transformer_main.py) using CPUs. 
The training steps is on: 
https://github.com/tensorflow/models/tree/master/official/transformer

Full Error Trace is as follows: 
2018-07-25 16:16:27.734555: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
I0725 16:16:28.569362 139892177725248 tf_logging.py:115] Benchmark run: {'model_name': 'transformer', 'dataset': {'name': 'wmt_translate_ende'}, 'machine_config': {'cpu_info': {'num_cores': 112, 'cpu_info': 'Intel(R) Xeon(R) Platinum 8180 CPU @ 2.50GHz', 'mhz_per_cpu': 2500.0}, 'gpu_info': {'count': 0}, 'memory_total': 201245483008, 'memory_available': 179603804160}, 'test_id': None, 'run_date': '2018-07-25T23:16:27.746402Z', 'tensorflow_version': {'version': '1.9.0', 'git_hash': 'v1.9.0-0-g25c197e023'}, 'tensorflow_environment_variables': [], 'run_parameters': [{'name': 'allow_ffn_pad', 'bool_value': 'True'}, {'name': 'alpha', 'float_value': 0.6}, {'name': 'attention_dropout', 'float_value': 0.1}, {'name': 'batch_size', 'long_value': 32768}, {'name': 'beam_size', 'long_value': 4}, {'name': 'data_dir', 'string_value': '$HOME/transformer/data'}, {'name': 'default_batch_size', 'long_value': 2048}, {'name': 'default_batch_size_tpu', 'long_value': 32768}, {'name': 'extra_decode_length', 'long_value': 50}, {'name': 'filter_size', 'long_value': 2048}, {'name': 'hidden_size', 'long_value': 512}, {'name': 'initializer_gain', 'float_value': 1.0}, {'name': 'label_smoothing', 'float_value': 0.1}, {'name': 'layer_postprocess_dropout', 'float_value': 0.1}, {'name': 'learning_rate', 'float_value': 2.0}, {'name': 'learning_rate_decay_rate', 'float_value': 1.0}, {'name': 'learning_rate_warmup_steps', 'long_value': 16000}, {'name': 'max_length', 'long_value': 256}, {'name': 'model_dir', 'string_value': '$HOME/logs/transformer/model_base'}, {'name': 'num_heads', 'long_value': 8}, {'name': 'num_hidden_layers', 'long_value': 6}, {'name': 'num_parallel_calls', 'long_value': 112}, {'name': 'optimizer_adam_beta1', 'float_value': 0.9}, {'name': 'optimizer_adam_beta2', 'float_value': 0.997}, {'name': 'optimizer_adam_epsilon', 'float_value': 1e-09}, {'name': 'relu_dropout', 'float_value': 0.1}, {'name': 'repeat_dataset', 'long_value': 1}, {'name': 'static_batch', 'bool_value': 'False'}, {'name': 'tpu', 'string_value': 'None'}, {'name': 'use_synthetic_data', 'bool_value': 'False'}, {'name': 'use_tpu', 'bool_value': 'False'}, {'name': 'vocab_size', 'long_value': 33708}]}
I0725 16:16:32.669298 139892177725248 tf_logging.py:115] Using config: {'_model_dir': '$HOME/logs/transformer/model_base', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.one_device_strategy.OneDeviceStrategy object at 0x7f397c1b69e8>, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f397c1b6a58>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0725 16:16:32.672403 139892177725248 tf_logging.py:115] Training schedule:
I0725 16:16:32.672613 139892177725248 tf_logging.py:115] 	1. Train for 1 epochs.
I0725 16:16:32.672733 139892177725248 tf_logging.py:115] 	2. Evaluate model.
I0725 16:16:32.672834 139892177725248 tf_logging.py:115] 	3. Compute BLEU score.
I0725 16:16:32.672941 139892177725248 tf_logging.py:115] Repeat above steps until the BLEU score reaches 25.000000
I0725 16:16:32.675400 139892177725248 tf_logging.py:115] Starting iteration 1
I0725 16:16:32.763708 139892177725248 tf_logging.py:115] Calling model_fn.
$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
I0725 16:16:45.845091 139892177725248 tf_logging.py:115] Done calling model_fn.
I0725 16:16:46.302198 139892177725248 tf_logging.py:115] Create CheckpointSaverHook.
I0725 16:16:49.258668 139892177725248 tf_logging.py:115] Graph was finalized.
I0725 16:16:49.283038 139892177725248 tf_logging.py:115] Restoring parameters from $HOME/logs/transformer/model_base/model.ckpt-0
I0725 16:16:53.075307 139892177725248 tf_logging.py:115] Running local_init_op.
I0725 16:16:53.208708 139892177725248 tf_logging.py:115] Done running local_init_op.
I0725 16:17:01.174737 139892177725248 tf_logging.py:115] Saving checkpoints for 0 into $HOME/logs/transformer/model_base/model.ckpt.
Traceback (most recent call last):
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[168,32] = 33748 is not in [0, 33708)
	 [[Node: model/Transformer/encode/embedding_shared_weights/embedding/Gather = ResourceGather[Tindices=DT_INT64, _class=[""loc:@model...ad/Reshape""], dtype=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](model/Transformer/embedding_shared_weights/embedding_and_softmax/weights, FunctionBufferingResourceGetNext)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""transformer_main.py"", line 632, in <module>
    absl_app.run(main)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/absl/app.py"", line 274, in run
    _run_main(main, args)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/absl/app.py"", line 238, in _run_main
    sys.exit(main(argv))
  File ""transformer_main.py"", line 626, in main
    run_transformer(flags.FLAGS)
  File ""transformer_main.py"", line 608, in run_transformer
    vocab_file=flags_obj.vocab_file)
  File ""transformer_main.py"", line 332, in run_loop
    hooks=train_hooks)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1117, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1253, in _train_model_distributed
    saving_listeners)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1336, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 577, in run
    run_metadata=run_metadata)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1053, in run
    run_metadata=run_metadata)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1144, in run
    raise six.reraise(*original_exc_info)
  File ""$HOME/.local/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1129, in run
    return self._sess.run(*args, **kwargs)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1201, in run
    run_metadata=run_metadata)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 981, in run
    return self._sess.run(*args, **kwargs)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[168,32] = 33748 is not in [0, 33708)
	 [[Node: model/Transformer/encode/embedding_shared_weights/embedding/Gather = ResourceGather[Tindices=DT_INT64, _class=[""loc:@model...ad/Reshape""], dtype=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](model/Transformer/embedding_shared_weights/embedding_and_softmax/weights, FunctionBufferingResourceGetNext)]]

Caused by op 'model/Transformer/encode/embedding_shared_weights/embedding/Gather', defined at:
  File ""transformer_main.py"", line 632, in <module>
    absl_app.run(main)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/absl/app.py"", line 274, in run
    _run_main(main, args)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/absl/app.py"", line 238, in _run_main
    sys.exit(main(argv))
  File ""transformer_main.py"", line 626, in main
    run_transformer(flags.FLAGS)
  File ""transformer_main.py"", line 608, in run_transformer
    vocab_file=flags_obj.vocab_file)
  File ""transformer_main.py"", line 332, in run_loop
    hooks=train_hooks)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1117, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1160, in _train_model_distributed
    self.config)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 794, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/one_device_strategy.py"", line 77, in _call_for_each_tower
    return fn(*args, **kwargs)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1107, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""transformer_main.py"", line 78, in model_fn
    logits = model(inputs, targets)
  File ""$HOME/models/official/transformer/model/transformer.py"", line 91, in __call__
    encoder_outputs = self.encode(inputs, attention_bias)
  File ""$HOME/models/official/transformer/model/transformer.py"", line 114, in encode
    embedded_inputs = self.embedding_softmax_layer(inputs)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 329, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 703, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""$HOME/models/official/transformer/model/embedding_layer.py"", line 76, in call
    embeddings = tf.gather(self.shared_weights, x)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 2664, in gather
    return params.sparse_read(indices, name=name)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 767, in sparse_read
    self._handle, indices, dtype=self._dtype, name=name)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 586, in resource_gather
    validate_indices=validate_indices, name=name)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""$HOME/miniconda3/envs/models-cpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): indices[168,32] = 33748 is not in [0, 33708)
	 [[Node: model/Transformer/encode/embedding_shared_weights/embedding/Gather = ResourceGather[Tindices=DT_INT64, _class=[""loc:@model...ad/Reshape""], dtype=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](model/Transformer/embedding_shared_weights/embedding_and_softmax/weights, FunctionBufferingResourceGetNext)]]

",5,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta""), NamedUser(login=""k-w-w"")]",2018-07-25 23:45:07,open,,,[],2019-03-29 12:26:52
766,tensorflow/models,models,4899,raymond-yuan,Serving blogpost,,2,,[],2018-07-25 18:51:04,open,,,['cla: yes'],2018-08-13 20:52:50
767,tensorflow/models,models,4898,fdkssdks,ImportError: cannot import name 'graph_rewriter_builder' when running train.py,"I followed the guide to generate and place all the files required for training in the proper models/research/object_detection directory structure.

I found out the train.py file is in legacy folder so I run:
```
python legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config
```
I get this error:
```
Traceback (most recent call last):
  File ""legacy/train.py"", line 50, in <module>
    from object_detection.builders import graph_rewriter_builder
ImportError: cannot import name 'graph_rewriter_builder'
```
I checked the file is present in the directory.

I copied train.py out of legacy directory ran it and same error again.

I also passed the path `export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim` and no success.

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.1
- **GPU model and memory**: GTX960M 4GB
- **Exact command to reproduce**: python legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config

[Link to Tensorflow tutorial](https://pythonprogramming.net/creating-tfrecord-files-tensorflow-object-detection-api-tutorial/)
",3,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-25 18:15:33,open,,,[],2018-11-11 16:51:11
768,tensorflow/models,models,4892,superbug7,Training Faster-RCNN for different size images ,"I am in process of training Faster RCNN model for custom size images and I only wish to train first stage ( proposal stage ). 

For example, I am trying to train first stage for lower resolution. My first question is that whether this is a correct approach to train for lower resolution resnet proposal extractor. I see very bad accuracy and not sure what all parameters I should look for in order to train for custom size image rather than using 
min_dim of 600 and max_dim of 1024. For example, below is the lower resolution config I tried but results/proposals are not good. 

Here is how my config looks like:
model {
  faster_rcnn {
    num_classes: 90
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 300
        max_dimension: 512
      }
    }
    number_of_stages: 1
    feature_extractor {
      type: 'faster_rcnn_resnet101'
      first_stage_features_stride: 8
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 8
        width_stride: 8
      }
    }

Details on Setup:
What is the top-level directory of the model you are using: samples/configs/faster_rcnn_resnet101_coco.config
Have I written custom code: No
OS Platform and Distribution: Ubuntu 16.04 x86_64 
TensorFlow installed from: pip install tensorflow-gpu
TensorFlow version: 1.7.0
Bazel version: N/A
CUDA/cuDNN version: NA
GPU model and memory: 1080Ti
Exact command to reproduce: NA
",2,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-07-25 06:33:57,open,,,[],2018-07-26 07:01:37
769,tensorflow/models,models,4891,sudheer-thanna-SS,RuntimeError: There was no new checkpoint after the training. Eval status: no new checkpoint,"Hi,

I have completed the tensorflow-gpu setup on EC2 P2xLarge instance on ubuntu. Working on a custom object detect solution. 

When i have kick started the training, the program is getting struck at the following line. I cant see any additional details around why the training is not proceeding furthr.

Can someone provide me the insights around what i need to do to make the training work on a GPU. please note that the computing capacity of GPU is 3.7?

Please note that i have set the custom folders for the images, intergraph, training etc.


Command that I ran to kick start training:

python model_main.py --pipeline_config_path=/home/ubuntu/image-recognition/training/faster_rcnn_inception_v2_pets.config --model_dir=/home/ubuntu/image-recognition/training/ --num_train_steps=50000 --num_eval_steps=1000 --alsologtostderr

Logs:
(tensorflow_p36) ubuntu@ip-:~$ cd tensorflow/models/research/object_detection/
(tensorflow_p36) ubuntu@ip-:~/tensorflow/models/research/object_detection$ python model_main.py --pipeline_config_path=/home/ubuntu/image-recognition/training/faster_rcnn_inception_v2_pets.config --model_dir=/home/ubuntu/image-recognition/training/ --num_train_steps=50000 --num_eval_steps=1000 --alsologtostderr
/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model_main.py"", line 62, in main
    eval_steps=FLAGS.num_eval_steps)
  File ""/home/ubuntu/tensorflow/models/research/object_detection/model_lib.py"", line 489, in create_estimator_and_inputs
    configs = get_configs_from_pipeline_file(pipeline_config_path)
  File ""/home/ubuntu/tensorflow/models/research/object_detection/utils/config_util.py"", line 93, in get_configs_from_pipeline_file
    proto_str = f.read()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 125, in read
    self._preread_check()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 85, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: /home/ubuntu/image-recognition/training/faster_rcnn_inception_v2_pets.config; No such file or directory
(tensorflow_p36) ubuntu@ip-:~/tensorflow/models/research/object_detection$ cd
(tensorflow_p36) ubuntu@ip-:~$ s3fs -o allow_other,uid=1000,gid=1000,umask=7777,use_cache=/tmp ss-artificial-intelligence:/image-recognition image-recognition
(tensorflow_p36) ubuntu@ip-:~$ cd tensorflow/models/research/object_detection/                                           (tensorflow_p36) ubuntu@ip-:~/tensorflow/models/research/object_detection$ python model_main.py --pipeline_config_path=/home/ubuntu/image-recognition/training/faster_rcnn_inception_v2_pets.config --model_dir=/home/ubuntu/image-recognition/training/ --num_train_steps=50000 --num_eval_steps=1000 --alsologtostderr
/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f14ef4bef28>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /home/ubuntu/tensorflow/models/research/object_detection/core/box_predictor.py:407: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/ubuntu/tensorflow/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2037: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
WARNING:root:Variable [SecondStageBoxPredictor/BoxEncodingPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [SecondStageBoxPredictor/BoxEncodingPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [SecondStageBoxPredictor/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [SecondStageBoxPredictor/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [global_step] is not available in checkpoint
WARNING:tensorflow:From /home/ubuntu/tensorflow/models/research/object_detection/core/losses.py:317: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

WARNING:tensorflow:From /home/ubuntu/tensorflow/models/research/object_detection/core/losses.py:317: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  **""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""** -- I cant see any logs after this and also in the training folder the model file is not getting created.


Any help to resolve this is highly appreciated

",7,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-25 06:20:39,open,,,[],2018-10-18 03:29:21
770,tensorflow/models,models,4890,SubWayss,The problem of  protos/ssd.proto,"when I  protoc the files in protos,I found a question:
`object_detection/protos/ssd.proto:87:3: Expected ""required"", ""optional"", or ""repeated"".`
line 87 in ssd.proto, you should  comment out the line 
`reserved 6;`",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-25 04:49:40,open,,,[],2018-08-07 22:48:00
771,tensorflow/models,models,4888,mahmoud-abuzaina,Adding support for inter-op and intra-op thread settings,Setting inter-op and intra-op thread settings will help improving the performance of CPU if Tensorflow is built with MKL support.,0,,[],2018-07-24 23:41:10,open,,,['cla: yes'],2018-07-24 23:41:13
772,tensorflow/models,models,4887,hadim,Make the object_detection folder pip installable,"Please see https://github.com/tensorflow/models/issues/917#issuecomment-407552473 for history.

In short, that would allow to really use `object_detection` as a lib and an API without having to clone the whole repository.

It would make it more easy to use and everyone's code using this API more reproducible.

Now it's up to you whether you want to keep it in this repo or create a separate repo from `models`. The advantage of the later is that you can easily install whatever git commit you to want with something like that: `pip install https://github.com/tensorflow/models/archive/6c21084503b27a9ab118e1db25f79957d5ef540b.zip`.

That being said I am totally fine with regularly upload wheels to PyPi from only the `research/object_detection` folder.

The last thing for a pip installable package, you would have to compile PB files in the `setup.py` file, but that should not be too hard. I can submit a PR if you want/need.

Hope this makes sense to you.",7,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-07-24 21:35:23,open,,,"['models: research', 'type:feature']",2019-03-02 06:44:54
773,tensorflow/models,models,4881,Luca3424,"Training custom model crashes with ""ERROR:tensorflow:Model diverged with loss = NaN.""","### System information
- **What is the top-level directory of the model you are using**: Object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.0.5
- **GPU model and memory**: Nvidia GeForce GTX 1050 Ti
- **Exact command to reproduce**: 
`python model_main.py --model_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco.config --logtostderr`

### Describe the problem
The model_main.py script crashes before even one training step. Normally I'd say it's because of my GPU, but with the now deprecated train.py script it worked well. I'm training a custom model with the ssd_inception_v2_coco config file and the model as finetune checkpoint.

### Source code / logs
`(tensorflow2) c:\tensorflow2\models\research\object_detection>python model_main.py --model_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco.config --eval_training_data --alsologtostderr
C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\importlib\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\importlib\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
C:\tensorflow2\models\research\object_detection\utils\visualization_utils.py:25: UserWarning:
This call to matplotlib.use() has no effect because the backend has already
been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

The backend was *originally* set to 'TkAgg' by the following code:
  File ""model_main.py"", line 26, in <module>
    from object_detection import model_lib
  File ""C:\tensorflow2\models\research\object_detection\model_lib.py"", line 26, in <module>
    from object_detection import eval_util
  File ""C:\tensorflow2\models\research\object_detection\eval_util.py"", line 28, in <module>
    from object_detection.metrics import coco_evaluation
  File ""C:\tensorflow2\models\research\object_detection\metrics\coco_evaluation.py"", line 20, in <module>
    from object_detection.metrics import coco_tools
  File ""C:\tensorflow2\models\research\object_detection\metrics\coco_tools.py"", line 47, in <module>
    from pycocotools import coco
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\pycocotools\coco.py"", line 49, in <module>
    import matplotlib.pyplot as plt
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\matplotlib\pyplot.py"", line 71, in <module>
    from matplotlib.backends import pylab_setup
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\matplotlib\backends\__init__.py"", line 16, in <module>
    line for line in traceback.format_stack()


  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x0000013642613C80>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
2018-07-24 16:28:36.695781: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-07-24 16:28:37.044293: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1392] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.4175
pciBusID: 0000:26:00.0
totalMemory: 4.00GiB freeMemory: 3.30GiB
2018-07-24 16:28:37.053468: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1471] Adding visible gpu devices: 0
2018-07-24 16:28:37.688722: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-24 16:28:37.692253: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:958]      0
2018-07-24 16:28:37.694498: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   N
2018-07-24 16:28:37.696812: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3025 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)
ERROR:tensorflow:Model diverged with loss = NaN.
Traceback (most recent call last):
  File ""model_main.py"", line 101, in <module>
    tf.app.run()
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\estimator\training.py"", line 447, in train_and_evaluate
    return executor.run()
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\estimator\training.py"", line 531, in run
    return self.run_local()
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\estimator\training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1119, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1135, in _train_model_default
    saving_listeners)
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1336, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 577, in run
    run_metadata=run_metadata)
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1053, in run
    run_metadata=run_metadata)
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1144, in run
    raise six.reraise(*original_exc_info)
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1129, in run
    return self._sess.run(*args, **kwargs)
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1209, in run
    run_metadata=run_metadata))
  File ""C:\Users\Luca\Anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py"", line 635, in after_run
    raise NanLossDuringTrainingError
tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.`
",40,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-24 14:38:40,open,,,[],2019-02-27 17:17:43
774,tensorflow/models,models,4878,gsaibro,[deeplab] fine_tune_batch_norm and backbone network,"Hi everyone, 

I'm training deeplab on my own dataset (medical images) and I have some questions about the network.

**First Question:**
I didn't understand what ""fine_tune_batch_norm=false or true"" is doing, what is the different between them and why ""fine_tune_batch_norm=true"" needs larger batch size?

**Second Question:**
Is it sensible to initialize the backbone Xception (or other one) with a pre-trained imageNet checkpoint even if my images are really different from the ones on imageNet?

Thanks for your help.


_What is the top-level directory of the model you are using_
N/A
_Have I written custom code_
**Yes**
_OS Platform and Distribution_
N/A
_TensorFlow installed from_
N/A
_TensorFlow version_
N/A
_Bazel version_
N/A
_CUDA/cuDNN version_
N/A
_GPU model and memory_
**2 GTX 1080-Ti 11GB**
_Exact command to reproduce_
N/A",4,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-07-24 10:01:10,open,,,['stat:awaiting response'],2018-10-30 12:32:57
775,tensorflow/models,models,4876,SolsticeDante,Learn Capsule Net,-,1,,[],2018-07-24 06:20:42,open,,,['cla: no'],2018-07-24 06:20:46
776,tensorflow/models,models,4875,irmowan,Slow when hard example mining iou threshold = 1.0,"### System information
- **What is the top-level directory of the model you are using**:  Skip
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0 gpu
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: CUDA 9 cuDNN 7
- **GPU model and memory**: 1080Ti, 12GB
- **Exact command to reproduce**: No

### Describe the problem
When I config hard example mining iou_threshold = 1.0 other than 0.99 or 0.7 (ssd paper use 1.0, no OHEM), the model run very slow when training. I digged into the code and found it will do a full-boxes NMS, which is a waste of CPU resources.

As iou_threshold = 1.0, it is no need to do NMS, just return all boxes is enough. Here is a pull request to fix this problem: https://github.com/tensorflow/models/pull/4874.

Test on a customized model I use:
Before fix: 4.5 secs/step
After fix: 0.4 secs/step",0,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-07-24 03:04:22,open,,,[],2018-07-24 08:07:05
777,tensorflow/models,models,4873,Jilliansea,Error in Freezing the exported Graph,"After i have trained my classifier based on slim, i want to Freezing the exported Graph as introduced on the tutorial, but i got the error like that:
ERROR: Skipping '/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools:freeze_graph': not a valid absolute pattern (absolute target patterns must start with exactly two slashes): '/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools:freeze_graph'
WARNING: Target pattern parsing failed.
ERROR: not a valid absolute pattern (absolute target patterns must start with exactly two slashes): '/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools:freeze_graph'
INFO: Elapsed time: 0.035s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
export_inference_graph.sh: 3: export_inference_graph.sh: bazel-bin/tensorflow/python/tools/freeze_graph: not found


and my shell file write like this:
bazel build /usr/local/lib/python2.7/dist-packages/tensorflow/python/tools:freeze_graph

bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=/nfs/project/wangzhihui_i/tf_api/experiment_cascade/graph/vgg16_inf_graph.pb \
  --input_checkpoint=/nfs/project/wangzhihui_i/tf_api/experiment_cascade/train_logs/all/model.ckpt-50000 \
  --input_binary=true --output_graph=/tmp/VGG16_50000.pb \
  --output_node_names=/nfs/project/wangzhihui_i/tf_api/experiment_cascade/graph/VGG16/Predictions/Reshape_1 \
  --output_file=/nfs/project/wangzhihui_i/tf_api/experiment_cascade/graph/test


have anyone could give me some advice?


",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-07-24 02:23:06,open,,,['stat:awaiting response'],2018-08-22 08:54:01
778,tensorflow/models,models,4863,manipopopo,[object_detection] image summary 'Detections_Left_Groundtruth_Right' in eval_metric_ops caused OutOfRangeError,"### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary 
- **TensorFlow version (use command below)**: 1.10.0-dev20180721
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: Tesla K80
- **Exact command to reproduce**: 

Set `eval_input_reader.num_epochs` of `${PIPELINE_CONFIG}` to `1`, and set `eval_config.num_examples` of `${PIPELINE_CONFIG}` to some value greater than the number of examples in `eval_input_reader`. 

Run
`python3 object_detection/model_main.py --pipeline_config_path=${PIPELINE_CONFIG} --checkpoint_dir=${MODEL_DIR}`

---

After [evaluating all the examples](https://github.com/tensorflow/tensorflow/blob/fa9d8aab41249cfc901338dfcb38cedb7ed1e603/tensorflow/python/training/evaluation.py#L211) in the input source, `Estimator` will [run](https://github.com/tensorflow/tensorflow/blob/fa9d8aab41249cfc901338dfcb38cedb7ed1e603/tensorflow/python/training/evaluation.py#L211) `value_op`s of `eval_metric_ops` to get the `eval_results`.

The `value_op` of `eval_metric_ops['Detections_Left_Groundtruth_Right']` is [`img_summary`](https://github.com/tensorflow/models/blob/85dd5fa4ba406b953550f20269cadddac5303241/research/object_detection/model_lib.py#L390), which is a tensor depending on the input source. Running `img_summary` will cause `OutOfRangeError` if the input source has been exhausted.

[Removing `Detections_Left_Groundtruth_Right` from `eval_metric_ops`](https://github.com/tensorflow/models/blob/85dd5fa4ba406b953550f20269cadddac5303241/research/object_detection/model_lib.py#L388-L390) or save the summary to a `Variable` may be temporary workarounds.",0,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-07-22 06:21:01,open,,,"['stat:awaiting owner', 'type:bug/performance']",2018-08-01 18:45:10
779,tensorflow/models,models,4862,navinkeshava,WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta/Momentum] is not available in checkpoint,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform WINDOWS 10
- **TensorFlow 
- **TensorFlow version 1.5
- **CUDA 9.0 cuDNN 7.0 version

python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config

when i am training the model it is showing these types of warning what is the cause of this what is to do correct it 


WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma/Momentum] is not available in checkpoint
",11,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-07-22 03:22:37,open,,,[],2018-12-24 08:36:30
780,tensorflow/models,models,4861,sculd,adding the sentimen analysis model to research folder's README.md file,,2,,[],2018-07-21 22:26:40,open,,,['cla: yes'],2018-07-23 21:58:20
781,tensorflow/models,models,4859,YJHMITWEB,Is there any resnet model which could be used for FPN?,"I notice that in keras, there are resnet models that could output features and FPN could use, for example, if the input size is 1024x1024, output nodes are [512x512,256x256,128x128,64x64]. But in tensorflow, no matter resnet_v2 or resnet_v1, output_nodes are always [128x128, 64x64, 64x64, 64x64] which could not be used by FPN.
I am wonder is there any available resnet options in pure tensorflow that could be used by FPN(Feature Pyramid Network)?",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-21 13:11:17,open,,,[],2018-08-08 02:07:08
782,tensorflow/models,models,4850,MjdMahasneh,how can i train faster R-CNN using a new customised network model? ,"What is the top-level directory of the model you are using: research/object_detection
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution: Windows 10
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.9.0
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: release 9.0, V9.0.176
GPU model and memory: GeForce GTX 1080 Ti 11GB
Exact command to reproduce: N/A


I would like to know, If it is possible to train faster R-CNN using a new customised network model? 

for example: i want to create a model that takes multiple inputs (2 images per class object), can i adapt this model to faster R-CNN? if so, what do i need to modify in the implementation to achieve it?",2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-20 13:36:57,open,,,[],2018-07-27 06:07:28
783,tensorflow/models,models,4849,Rayndell,Loss diverges to very high values in Object Detection API,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Bazel version (if compiling from source)**: no
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: NVIDIA GeForce GTX 1080 Ti
- **Exact command to reproduce**: python train.py --logtostderr --pipeline_config_path=rfcn_resnet101_custom.config --train_dir=custom_train_dir

### Describe the problem
While training a detection model with the object detection API (RFCN with Resnet v1 101 feature extractor) I encountered what looks like a bug to me (maybe an overflow). After learning for a little while without problems (with a custom dataset) the loss begins to display very large values that grow very quickly. At first I thought there was an overflow on the loss value but it seems more progressive:
INFO:tensorflow:global step 69: loss = 0.5207 (0.422 sec/step)
INFO:tensorflow:global step 70: loss = 0.2422 (0.406 sec/step)
INFO:tensorflow:global step 71: loss = 0.5621 (0.391 sec/step)
INFO:tensorflow:global step 72: loss = 1.5380 (0.422 sec/step)
INFO:tensorflow:global step 73: loss = 0.3814 (0.422 sec/step)
INFO:tensorflow:global step 74: loss = 0.6671 (0.406 sec/step)
INFO:tensorflow:global step 75: loss = 1.8687 (0.422 sec/step)
INFO:tensorflow:global step 76: loss = 1.4388 (0.438 sec/step)
INFO:tensorflow:global step 77: loss = 5.7769 (0.391 sec/step)
INFO:tensorflow:global step 78: loss = 15.7012 (0.391 sec/step)
INFO:tensorflow:global step 79: loss = 18.4020 (0.391 sec/step)
INFO:tensorflow:global step 80: loss = 0.1409 (0.422 sec/step)
INFO:tensorflow:global step 81: loss = 41.4890 (0.391 sec/step)
INFO:tensorflow:global step 82: loss = 391.0323 (0.391 sec/step)
INFO:tensorflow:global step 83: loss = 1184.6780 (0.391 sec/step)
INFO:tensorflow:global step 84: loss = 113833.9297 (0.406 sec/step)
INFO:tensorflow:global step 85: loss = 1028554.8125 (0.422 sec/step)
INFO:tensorflow:global step 86: loss = 2339703.2500 (0.406 sec/step)
INFO:tensorflow:global step 87: loss = 4837331.0000 (0.406 sec/step)
INFO:tensorflow:global step 88: loss = 120685024.0000 (0.406 sec/step)
INFO:tensorflow:global step 89: loss = 3053880832.0000 (0.375 sec/step)
INFO:tensorflow:global step 90: loss = 74913587200.0000 (0.406 sec/step)
INFO:tensorflow:global step 91: loss = 299197333504.0000 (0.406 sec/step)
INFO:tensorflow:global step 92: loss = 22885305417728.0000 (0.406 sec/step)
INFO:tensorflow:global step 93: loss = 710914261123072.0000 (0.406 sec/step)

See the attached log file for more details. I was not able to locate any memory allocation problems during the execution. I also included the custom code I used to transform my data into a tfrecord file (taken from the create_pascal_tf_record.py script), the label map and config I used. The strange thing is that when I do exactly the same thing with a label map with only 1 label, the training works fine and converges. I know there are some mistakes for some labels associated to the bounding boxes, but they are not that frequent, and if it was a problem caused for example by too few data for some classes, isn't the training supposed only not to converge instead of displaying very high values? Do you have any idea of what can cause this behavior? Thanks a lot for your help.


### Source code / logs
[object_detection.zip](https://github.com/tensorflow/models/files/2213972/object_detection.zip)

",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-20 12:58:18,open,,,[],2018-07-27 15:16:18
784,tensorflow/models,models,4848,AliceDinh,Tensorflow Object Detection API using Faster RCNN on Android devices. ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Window 10
- **TensorFlow installed from (source or binary)**: binary, installed from pip: pip install tensorflow
- **TensorFlow version (use command below)**: 1.8
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I used **TensorFlow Android Camera Demo**, which is **TF Detect**, at this link:
(https://github.com/tensorflow/tensorflow/tree/r1.9/tensorflow/examples/android)
I trained my own model using **SSD w/Mobilenet detector** using my own data, then I imported the frozen model (about **20MB**) to Android project and it was working well on my phone: **Samsung S7Edge, 4GB Ram**.
Now, I changed from SSD w/Mobilenet detector to **Faster RCNN** because Faster RCNN gives higher accuracy as many people stated. The frozen model is checked on Jupyter Notebook and the result is good. The frozen model is about **110MB** and I imported to Android project. With 4GB Ram of Samsung S7Edge I believe it will be okies but there is **no output result and the app crashed**. So, my questions are:

**Is Faster RCNN model not suitable for mobile? what should I do to make it work on Mobile?**

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",38,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-07-20 10:40:42,open,,,['stat:awaiting owner'],2019-02-05 12:05:07
785,tensorflow/models,models,4846,antoajayraj,Training Nasnet_large results in Out of memory,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
models/research/slim/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Redhat 7.5
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.8
- **Bazel version (if compiling from source)**:0.10.0
- **CUDA/cuDNN version**:9.2/7.1.2
- **GPU model and memory**: V100/16GB
- **Exact command to reproduce**:
python train_image_classifier.py --dataset_dir=/data/TF_records/ --dataset_name=imagenet --dataset_split_name=train --model_name=nasnet_large --num_clones=4

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

== cat /etc/issue ===============================================
Linux paiws7 4.14.0-49.el7a.ppc64le #1 SMP Wed Mar 14 13:58:40 UTC 2018 ppc64le ppc64le ppc64le GNU/Linux
VERSION=""7.5 (Maipo)""
VERSION_ID=""7.5""
REDHAT_BUGZILLA_PRODUCT_VERSION=7.5
REDHAT_SUPPORT_PRODUCT_VERSION=""7.5""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux paiws7 4.14.0-49.el7a.ppc64le #1 SMP Wed Mar 14 13:58:40 UTC 2018 ppc64le ppc64le ppc64le GNU/Linux

== check pips ===================================================
numpy                              1.14.4
numpydoc                           0.7.0
protobuf                           3.5.0
tensorflow                         1.8.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)
/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/lib64:/usr/lib:/usr/local/cuda-9.1/lib:/usr/local/cuda/nvvm/lib64:/usr/local/cuda-9.2/lib64:/usr/local/cuda-9.2/extras/CUPTI/lib64:/opt/DL/tensorflow/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Jul 20 00:56:20 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.15                 Driver Version: 396.15                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |
| N/A   38C    P0    39W / 300W |      0MiB / 15360MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |
| N/A   42C    P0    40W / 300W |      0MiB / 15360MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |
| N/A   39C    P0    42W / 300W |      0MiB / 15360MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |
| N/A   42C    P0    39W / 300W |      0MiB / 15360MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.2/targets/ppc64le-linux/lib/libcudart.so.9.2.64
/usr/local/cuda-9.2/targets/ppc64le-linux/lib/libcudart_static.a

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Was trying to train nasnet_large using models/research/slim using imagenet dataset, and I encounter Out of memory.

python train_image_classifier.py --dataset_dir=/data/TF_records/ --dataset_name=imagenet --dataset_split_name=train --model_name=nasnet_large --num_clones=4


2018-07-18 23:04:09.439260: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                 14813580493
InUse:                 14813580288
MaxInUse:              14813580288
NumAllocs:                    9334
MaxAllocSize:            338608128

2018-07-18 23:04:09.439375: W tensorflow/core/common_runtime/bfc_allocator.cc:279] ****************************************************************************************************
2018-07-18 23:04:09.439462: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at fused_batch_norm_op.cc:274 : Resource exhausted: OOM when allocating tensor with shape[32,336,21,21] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
INFO:tensorflow:Recording summary at step 0.
Traceback (most recent call last):
  File ""train_image_classifier.py"", line 581, in <module>
    tf.app.run()
  File ""/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train_image_classifier.py"", line 577, in main
    sync_optimizer=optimizer if FLAGS.sync_replicas else None)
  File ""/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 769, in train
    sess, train_op, global_step, train_step_kwargs)
  File ""/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_step
    run_metadata=run_metadata)
  File ""/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 332, in __init__
    def __init__(self, node_def, op, message):
KeyboardInterrupt


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-20 06:00:56,open,,,[],2018-07-20 13:02:39
786,tensorflow/models,models,4844,F-Chan,Unable to convert Python 2 code to Python 3 code,"### System information
- **What is the top-level directory of the model you are using**: models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**:tensorflow ubuntu (venv, CPU support only)
- **TensorFlow version (use command below)**:v1.9
- **Bazel version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:Intel(R) Core(TM) i5-7500 CPU @ 3.40GHz , 8GB RAM
- **Exact command to reproduce**:
2to3 --output-dir=""C:\models-master_new"" -W -n ""C:\models-master""


### Describe the problem
I tried to use 2to3 to convert python2 code to python3 code but there were 17 errors.

### Source code / logs
RefactoringTool: Warnings/messages while refactoring:
RefactoringTool: ### In file C:\models-master\research\efficient-hrl\environments\maze_env.py ###
RefactoringTool: Line 48: could not convert: raise ""MODEL_CLASS unspecified!""
RefactoringTool: Python 3 does not support string exceptions
RefactoringTool: There were 17 errors:
RefactoringTool: Can't parse C:\models-master\official\transformer\data_download.py: ParseError: bad input: type=22, value='=', context=('', (141, 46))
RefactoringTool: Can't parse C:\models-master\research\inception\inception\data\process_bounding_boxes.py: ParseError: bad input: type=22, value='=', context=('', (125, 46))
RefactoringTool: Can't parse C:\models-master\research\lexnet_nc\path_model.py: ParseError: bad input: type=22, value='=', context=('', (547, 65))
RefactoringTool: Can't parse C:\models-master\research\minigo\gtp_extensions.py: ParseError: bad input: type=22, value='=', context=('', (90, 38))
RefactoringTool: Can't parse C:\models-master\research\minigo\gtp_wrapper.py: ParseError: bad input: type=22, value='=', context=('', (68, 38))
RefactoringTool: Can't parse C:\models-master\research\minigo\selfplay_mcts.py: ParseError: bad input: type=22, value='=', context=('', (92, 66))
RefactoringTool: Can't parse C:\models-master\research\minigo\strategies.py: ParseError: bad input: type=22, value='=', context=('', (124, 18))
RefactoringTool: Can't parse C:\models-master\research\real_nvp\real_nvp_multiscale_dataset.py: ParseError: bad input: type=22, value='=', context=('', (1441, 35))
RefactoringTool: Can't parse C:\models-master\research\slim\datasets\process_bounding_boxes.py: ParseError: bad input: type=22, value='=', context=('', (124, 46))
RefactoringTool: Can't parse C:\models-master\research\swivel\glove_to_shards.py: ParseError: bad input: type=22, value='=', context=('', (131, 14))
RefactoringTool: Can't parse C:\models-master\research\swivel\nearest.py: ParseError: bad input: type=22, value='=', context=('', (29, 15))
RefactoringTool: Can't parse C:\models-master\research\swivel\prep.py: ParseError: bad input: type=22, value='=', context=('', (133, 23))
RefactoringTool: Can't parse C:\models-master\research\swivel\swivel.py: ParseError: bad input: type=22, value='=', context=('', (371, 20))
RefactoringTool: Can't parse C:\models-master\research\swivel\text2bin.py: ParseError: bad input: type=22, value='=', context=('', (53, 15))
RefactoringTool: Can't parse C:\models-master\research\swivel\wordsim.py: ParseError: bad input: type=22, value='=', context=('', (47, 15))
RefactoringTool: Can't parse C:\models-master\tutorials\embedding\word2vec.py: ParseError: bad input: type=22, value='=', context=('', (417, 46))
RefactoringTool: Can't parse C:\models-master\tutorials\embedding\word2vec_optimized.py: ParseError: bad input: type=22, value='=', context=('', (334, 15))
",0,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-07-20 05:16:16,open,,,[],2018-07-20 19:19:47
787,tensorflow/models,models,4843,mefathy,models/research/slim/nets/inception_v2.py: Missing batch normalizations after conv1,"Attempts to load the provided model checkpoint for inception_v2 fails complaining that the checkpoint does not contain values for the biases of the various convolutions defined in the network. Inspecting the checkpoint file and the graph generated by inception_v2.py reveals that the checkpoint file contains batch-normalization for convolutions starting from conv2 and no bias except at the conv1 layer (which does not have a following batch normalization). The graph generated by inception_v2.py does not show any batch normalization layers.

Since the inception_v2 is supposed to have batch normalization, I think the issue here is that inception_v2.py should provide batch normalization parameters to the slim.conv2d invocations after conv1 so that slim adds batch normalization and avoids defining biases.",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-20 02:40:23,open,,,['stat:awaiting response'],2018-07-20 13:02:49
788,tensorflow/models,models,4842,lixinso,fix print in object_detection_evaluation.py to make it work with python3,,5,,[],2018-07-20 00:54:55,open,,,['cla: yes'],2018-07-20 01:05:50
789,tensorflow/models,models,4835,technosite20,"I got Error while using protoc command, can anyone solve it?","p(tensorflow) C:\Anaconda3\envs\tensorflow\models\research\object_detection\protos>protoc -I=./ --python_out=./ .\anchor_generator.proto
object_detection/protos/grid_anchor_generator.proto: File not found.
object_detection/protos/ssd_anchor_generator.proto: File not found.
object_detection/protos/multiscale_anchor_generator.proto: File not found.
anchor_generator.proto: Import ""object_detection/protos/grid_anchor_generator.proto"" was not found or had errors.
anchor_generator.proto: Import ""object_detection/protos/ssd_anchor_generator.proto"" was not found or had errors.
anchor_generator.proto: Import ""object_detection/protos/multiscale_anchor_generator.proto"" was not found or had errors.
anchor_generator.proto:13:5: ""GridAnchorGenerator"" is not defined.
anchor_generator.proto:14:5: ""SsdAnchorGenerator"" is not defined.
anchor_generator.proto:15:5: ""MultiscaleAnchorGenerator"" is not defined.",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-07-19 09:22:15,open,,,[],2018-07-20 01:35:25
790,tensorflow/models,models,4833,feifaxiaoming,two computer distributed Train model,"there i have two machine for train model 
one is ubuntu  one is win10
i set as below:
 cluster = tf.train.ClusterSpec({
        ""worker"": [
            ""xxxxx:2222"",
            ""xxxxxxx:3333"",
        ],
    })
    job_name = 'worker'
    server = tf.train.Server(cluster, job_name=job_name, task_index=0)

i don't set ps job,
when i running my worker ,i found is slow step then on computer. 
my model is inception-v4.
",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-19 08:57:46,open,,,[],2018-07-20 19:19:43
791,tensorflow/models,models,4830,irmowan,Does object detection API has a plan to migrate to python3,"I have to use python3 on object detection API for project. But there is many conflicts when I use python3. Does API has a plan to migrate to python3, as python2.7 will not be supported soon?",8,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-19 06:41:45,open,,,[],2019-02-22 23:00:39
792,tensorflow/models,models,4824,ravikantgupta9,Feature request: Support for Eager Execution during debugging,Currently the eager execution does not for tensorflow 1.7. Even when you upgrade to 1.8 there are issues with the queue for the eager execution. Having the capability of eager execution would be great for debugging.,1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-18 20:44:41,open,,,['stat:awaiting response'],2018-07-19 20:01:26
793,tensorflow/models,models,4821,mtli,[DeepLab v3+] Hardware configuration and hyper-parameters to reproduce results on Cityscapes,"Hi, I am trying to reproduce the 78.79% of the released xception_cityscapes_trainfine and it seems that the hyper-paramters listed in https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/cityscapes.md is for demonstrative purpose.

What are the hardware configuration (e.g. how many GPUs and vRAM per GPU) and the hyper-parameters (e.g. batch size, number of steps, learning rate) to reproduce such result on Cityscapes?

Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: N/A
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A",4,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-07-18 18:03:35,open,,,[],2018-09-24 09:10:05
794,tensorflow/models,models,4819,austinmw,New Object Detection Estimator-based doesn't have GPU options?,"
### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: 4 GTX 1080 TI 
- **Exact command to reproduce**: `python model_main.py ...`

### Describe the problem

The `train.py` script now in `/legacy` had simple options to set number of GPUs to use. I don't see any corresponding options in the new `model_main.py`. Is this a bug? There must still be functionality for selecting GPUs without rewriting the model code, right? Or else this seems like a step backwards?


",1,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort""), NamedUser(login=""pkulzc"")]",2018-07-18 15:22:24,open,,,[],2018-07-19 21:28:06
795,tensorflow/models,models,4817,pereirfe,Fix undefined variable error,"The usage of the variable 'base_name' raise a NameError since it was never defined.
The value which 'base_name' is expected to have is contained in 'checkpoint_name' 
The solution was to change the variable name 'base_name' to 'checkpoint_name'",3,,[],2018-07-18 13:45:16,open,,,['cla: yes'],2018-07-18 13:47:22
796,tensorflow/models,models,4814,Changkou,How to train a pretrained quantlize model by myself,"I trained a mobile_v1 quantlize model with my own datasets, and the accuracy decayed for almost 10% in compare with the float model, all the other parameters are the same, so i want to know how to train a quantlize pretrained  model.
I know it was transfered from a float model, but i want to know the detail information like how many steps used to train a float model before tranfer to a quantlize model.
Thanks, and sorry for my poor english.
",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-07-18 11:21:58,open,,,[],2018-07-20 17:03:08
797,tensorflow/models,models,4813,varun19299,[object-detection] Paddings must be non-negative: 0 -16,"## Bug

### System information
- **What is the top-level directory of the model you are using**: models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, but not relevant to bug.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: 0.14.0
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: 1080 Ti, 12 GB
- **Exact command to reproduce**:

```
python object_detection/model_main.py \
--logtostderr \
--model_dir /media/ssd1/sat_data_models/frcnn/train \
--pipeline_config_path {path_to_pipeline/pipeline.config} \
--num_train_steps 800000 \
--num_train_steps 20000
```

_Note: abstracted pipeline_config_path and model_dir, but these were set correctly._

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Seems like a padding issue with an op using tf.pad.

Unable to trace stack due to `Estimator's` abstraction.

Trailing error lines:

```
session.run(eval_ops, feed_dict)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 567, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1043, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1134, in run
    raise six.reraise(*original_exc_info)
  File ""/home/user/.local/lib/python3.5/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1119, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1191, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 971, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Paddings must be non-negative: 0 -16
	 [[Node: Pad_11 = Pad[T=DT_FLOAT, Tpaddings=DT_INT32](cond_2/Merge, stack_11)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[1], [1,300,300,3], [1,?,?,3], [1,3], [1,100], [1,100,4], [1,100,60], [1,100], [1,100], [1,100], [1]], output_types=[DT_INT32, DT_FLOAT, DT_UINT8, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_BOOL, DT_FLOAT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
```

The legacy train script was however doing okay.

Also seems like the Estimator built `model_main` doesn't output logs to std (of loss, global step). Only warnings and info are updated.

###  Config File (train config only uploaded)
 
```
train_config: {
  batch_size: 4
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.000003
          schedule {
            step: 900000
            learning_rate: .00000003
          }
          schedule {
            step: 1200000
            learning_rate: .000000003
          }
        }

  
  data_augmentation_options {
    random_horizontal_flip {
    }
    random_adjust_brightness{
    }
    random_adjust_contrast{
    }
    random_adjust_hue{
    }
    random_adjust_saturation{
    }
    random_distort_color{
    }
    random_crop_pad_image{
    }
    random_vertical_flip{
    }
    random_rotation90{
    }
  }
}


```",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-18 11:15:10,open,,,[],2018-07-19 16:45:25
798,tensorflow/models,models,4811,marcbelmont,Update scope name to match name from trained model,Use same scope name as in https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md trained model weight file.,0,,[],2018-07-18 09:58:43,open,,,['cla: yes'],2018-07-18 09:58:45
799,tensorflow/models,models,4809,Sangsooko,Accuracy of MobileNet_v2 evaluation,"I evaluated Slim models in my GPU server with pre-trained checkpoint in TF slim models.
But, Top-1 Accuracy of MobileNet_v2_1.0_224 is too lower than that in README.
The gap is 1.774% ( Top-1 Accuracy in README = 71.9, The result in my GPU server = 70.13 )

Please let me know what I should do to get same top-1 accuracy of MobileNet_v2.

Please check the following table to check my results.
-------------------------+----------+-----------+---------
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,|_Top-1 Accuracy(%)__| 
-------------------------+----------+-----------+---------
model,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,|_readme_ |_by myself_|_difference 
-------------------------+----------+-----------+---------
Inception_V3,,,,,,,,,,,,,,,,,,,,,|____78.00 |______77.98 |____0.022
VGG16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,|____71.50 |______70.80 |____0.700
VGG19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,|____71.10 |______71.00 |____0.100
NASNet-A_Mobile_224,|____74.00 |______73.97 |____0.030
MobileNet_v1_1.0_224,,|____70.90 |______70.75 |____0.152
MobileNet_v2_1.4_224,,|____74.90 |______74.11 |____0.790
MobileNet_v2_1.0_224,|____71.90 |______70.13 |____1.774
-------------------------+----------+-----------+---------

readme data of Top-1 Accuracy is from the following link.
https://github.com/tensorflow/models/tree/master/research/slim#Pretrained

other settings are following.
TF version 1.8, ILSVRC-2012, latest tensorflow/models codes and check point.

The following is setting for eval_image_classifier.py
// ==================================
python eval_image_classifier.py \
--checkpoint_path=${CHECKPOINT_DIR} \
--eval_dir=${EVAL_DIR} \
--dataset_dir=${DATASET_DIR} \
--dataset_name=imagenet \
--dataset_split_name=validation \
--preprocessing_name=mobilenet_v1 \
--model_name=mobilenet_v2 \
--eval_image_size=224",6,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-07-18 08:57:39,open,,,[],2019-01-23 01:51:48
800,tensorflow/models,models,4808,mhusseinsh,Another Dataset,"Hello,
I want to ask, if there is a possibility to use your deep lab implementation on my own dataset for semantic segmentation task.
I have a dataset with semantic segmentation labels of 12 classes, is it possible to use your model ?






Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-07-18 08:12:13,open,,,['stat:awaiting tensorflower'],2018-07-20 04:51:50
801,tensorflow/models,models,4807,shihehe73,Multiple Parameter Servers for cifar10_estimator,"### System information
- **What is the top-level directory of the model you are using**: [cifar10_estimator](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator)
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04 on Google Cloud Platform
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.0
- **GPU model and memory**: Tesla K80
- **Exact command to reproduce**: as below

### Describe the problem
I'm recently working on distributed TensorFlow training using cifar10_estimator in the tutorial folder.

It's totally fine when I running the code with 1 PS(parameter server), 1 Master, and multiple workers. But when it comes to multiple PS, the master crashed while raising the error:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: /job:ps/replica:0/task:1/device:CPU:0 unknown device.
```
I'm using TF_CONFIG as follow **(2 PS, 1 master, 3 workers)**:
```
export TF_CONFIG='{""task"": {""task"": 0, ""type"": ""ps""}, ""environment"": ""cloud"", 
 ""cluster"": {""worker"": [""10.140.0.3:8000"", ""10.140.0.5:8000"", ""10.140.0.6:8000""], 
 ""master"": [""10.140.0.2:8000""], ""ps"": [""10.140.0.4:8000"", ""10.140.0.19:8000""]}, 
 ""model_dir"": ""gs://[bucket]/cifar10_model/""}'
```
and run the code as follow.
```
Master and Workers:
    python cifar10_main.py --data-dir=gs://[bucket]/cifar-10-data --job-dir=gs://[bucket]/cifar10_model/ --num-gpus=1 --train-steps=1000 --sync

PS:
    python cifar10_main.py --data-dir=gs://[bucket]/cifar-10-data --job-dir=gs://[bucket]/cifar10_model/ --num-gpus=0
```

All of the instances are on Google Cloud Platform.

Am I doing wrong in my TF_CONFIG or what, or I simply cannot run multiple PS using the cifar10_estimator code provided?

If so, what can I do to test out multiple Parameter Server on ResNet model?
",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-18 06:43:44,open,,,[],2018-07-18 13:07:53
802,tensorflow/models,models,4805,jinfagang,object_detection should update to python3,"It's 8021 year now, why still using python2? the object_detection many python files still has python2 gramar... ",5,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-07-18 03:33:44,open,,,"['stat:awaiting owner', 'type:feature']",2018-11-16 15:59:56
803,tensorflow/models,models,4804,boris0430,how to export pb files for resnet model?,"In models version 1.4.0, this is an exporting function to save pb files of mnist model, just as following code: 

`  # Export the model
  if FLAGS.export_dir is not None:
    image = tf.placeholder(tf.float32, [None, 28, 28])
    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({
        'image': image,
    })
    mnist_classifier.export_savedmodel(FLAGS.export_dir, input_fn) `

However, i don`t find any exporting functions for resnet and wide_deep models, and when i use function in mnist to export pb file to resnet, there is an error that '**_Failed to convert object of type<class 'dict'> to Tensor. Contents: {'feature': <tf.Tensor 'input_tensor: 0' shape=(20, 32, 32, 3) dtype=float32>}. Consider casting elements to as supported type_**'.

does anyone find a method to export pb files to these models(resnet、wide_deep)? 

Thanks a lot!",5,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-07-18 01:45:31,open,,,"['models: official', 'stat:awaiting response', 'stat:awaiting tensorflower']",2019-02-01 22:36:21
804,tensorflow/models,models,4801,jj19940807,"[object_detection] ""RuntimeError: main thread is not in main loop""and""tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocat ing tensor with shape[64,6426,48] and type float on /job:localhost/replica:0/tas k:0/device:GPU:0 by allocator GPU_0_bfc"" ","the config file I used is ""ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config"",
""batch_size"" default value is 64.  an error occurred at run time
-------------------------------------------------------------------------------------------------------------------
2018-07-17 23:38:53.847260: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\bfc_allocator.cc:680] Stats:
Limit:                  3532349440
InUse:                  3437157888
MaxInUse:               3531101696
NumAllocs:                    6995
MaxAllocSize:           1090912256

2018-07-17 23:38:53.848260: W T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\bfc_allocator.cc:279] ****************************************************
***********************************************x
Traceback (most recent call last):
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_session
run
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocat
ing tensor with shape[64,6426,48] and type float on /job:localhost/replica:0/tas
k:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: swap_in_Loss/Loss_1/sub_1_1 = _CopyFromHostToGpu[T=DT_FLOAT, _c
lass=[""loc@Loss/Loss_1/sub_1_1""], _device=""/job:localhost/replica:0/task:0/devic
e:GPU:0""](swap_out_Loss/Loss_1/sub_1_1, ^WeightSharedConvolutionalBoxPredictor/C
lassPredictor/BiasAdd)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add repor
t_tensor_allocations_upon_oom to RunOptions for current allocation info.
-------------------------------------------------------------------------------------------------------------------

I don't know why. I felt like there might be not enough memory.
I tried to change the value of ""batch_size"" to a smaller value,but a new error has emerged .
------------------------------------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""object_detection\model_main.py"", line 101, in <module>
    tf.app.run()
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection\model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\training.py"", line 447, in train_and_eva
luate
    return executor.run()
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\training.py"", line 531, in run
    return self.run_local()
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\estimator.py"", line 1119, in _train_mode
l
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\estimator.py"", line 1135, in _train_mode
l_default
    saving_listeners)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\estimator.py"", line 1336, in _train_with
_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\training\monitored_session.py"", line 577, in run
    run_metadata=run_metadata)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\training\monitored_session.py"", line 1053, in run
    run_metadata=run_metadata)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\training\monitored_session.py"", line 1144, in run
    raise six.reraise(*original_exc_info)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\six.py"", line 693, in reraise
    raise value
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\training\monitored_session.py"", line 1129, in run
    return self._sess.run(*args, **kwargs)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\training\monitored_session.py"", line 1201, in run
    run_metadata=run_metadata)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\training\monitored_session.py"", line 981, in run
    return self._sess.run(*args, **kwargs)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: RuntimeError: main thread
is not in main loop
Traceback (most recent call last):

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\ops\script_ops.py"", line 158, in __call__
    ret = func(*args)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\si
te-packages\tensorflow\models-master\research\object_detection\utils\visualizati
on_utils.py"", line 693, in cdf_plot
    fig = plt.figure(frameon=False)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\matplotlib\pyplot.py"", line 548, in figure
    **kwargs)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\matplotlib\backend_bases.py"", line 161, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\matplotlib\backends\_backend_tk.py"", line 1053, in new_figure_manage
r_given_figure
    icon_img = Tk.PhotoImage(file=icon_fname)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\tk
inter\__init__.py"", line 3542, in __init__
    Image.__init__(self, 'photo', name, cnf, master, **kw)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\tk
inter\__init__.py"", line 3498, in __init__
    self.tk.call(('image', 'create', imgtype, name,) + options)

RuntimeError: main thread is not in main loop


         [[Node: Loss/PyFunc_1 = PyFunc[Tin=[DT_FLOAT], Tout=[DT_UINT8], token=""
pyfunc_1"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/Squeeze_
1/_5297)]]
         [[Node: train/update_FeatureExtractor/resnet_v1_50/block4/unit_1/bottle
neck_v1/conv1/weights/ApplyMomentum/_8102 = _Recv[client_terminated=false, recv_
device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localho
st/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_
10150...lyMomentum"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/tas
k:0/device:GPU:0""]()]]

Caused by op 'Loss/PyFunc_1', defined at:
  File ""object_detection\model_main.py"", line 101, in <module>
    tf.app.run()
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection\model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\training.py"", line 447, in train_and_eva
luate
    return executor.run()
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\training.py"", line 531, in run
    return self.run_local()
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\estimator.py"", line 1119, in _train_mode
l
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\estimator.py"", line 1132, in _train_mode
l_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\estimator\estimator.py"", line 1107, in _call_model
_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\si
te-packages\tensorflow\models-master\research\object_detection\model_lib.py"", li
ne 282, in model_fn
    prediction_dict, features[fields.InputDataFields.true_image_shape])
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\si
te-packages\tensorflow\models-master\research\object_detection\meta_architecture
s\ssd_meta_arch.py"", line 597, in loss
    flattened_class_ids, flattened_classification_losses)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\si
te-packages\tensorflow\models-master\research\object_detection\meta_architecture
s\ssd_meta_arch.py"", line 660, in _summarize_anchor_classification_loss
    'NegativeAnchorLossCDF')
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\si
te-packages\tensorflow\models-master\research\object_detection\utils\visualizati
on_utils.py"", line 703, in add_cdf_image_summary
    cdf_plot = tf.py_func(cdf_plot, [values], tf.uint8)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\ops\script_ops.py"", line 384, in py_func
    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\ops\script_ops.py"", line 227, in _internal_py_func

    input=inp, token=token, Tout=Tout, name=name)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\ops\gen_script_ops.py"", line 130, in py_func
    ""PyFunc"", input=input, token=token, Tout=Tout, name=name)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_
op_helper
    op_def=op_def)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

UnknownError (see above for traceback): RuntimeError: main thread is not in main
 loop
Traceback (most recent call last):

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\tensorflow\python\ops\script_ops.py"", line 158, in __call__
    ret = func(*args)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\Lib\si
te-packages\tensorflow\models-master\research\object_detection\utils\visualizati
on_utils.py"", line 693, in cdf_plot
    fig = plt.figure(frameon=False)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\matplotlib\pyplot.py"", line 548, in figure
    **kwargs)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\matplotlib\backend_bases.py"", line 161, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\matplotlib\backends\_backend_tk.py"", line 1053, in new_figure_manage
r_given_figure
    icon_img = Tk.PhotoImage(file=icon_fname)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\tk
inter\__init__.py"", line 3542, in __init__
    Image.__init__(self, 'photo', name, cnf, master, **kw)

  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\tk
inter\__init__.py"", line 3498, in __init__
    self.tk.call(('image', 'create', imgtype, name,) + options)

RuntimeError: main thread is not in main loop


         [[Node: Loss/PyFunc_1 = PyFunc[Tin=[DT_FLOAT], Tout=[DT_UINT8], token=""
pyfunc_1"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/Squeeze_
1/_5297)]]
         [[Node: train/update_FeatureExtractor/resnet_v1_50/block4/unit_1/bottle
neck_v1/conv1/weights/ApplyMomentum/_8102 = _Recv[client_terminated=false, recv_
device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localho
st/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_
10150...lyMomentum"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/tas
k:0/device:GPU:0""]()]]

Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\matplotlib\_pylab_helpers.py"", line 78, in destroy_all
    manager.destroy()
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\si
te-packages\matplotlib\backends\_backend_tk.py"", line 558, in destroy
    self.window.destroy()
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\tk
inter\__init__.py"", line 2058, in destroy
    for c in list(self.children.values()): c.destroy()
  File ""D:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\tk
inter\__init__.py"", line 2302, in destroy
    self.tk.call('destroy', self._w)
RuntimeError: main thread is not in main loop",4,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-17 15:56:25,open,,,[],2018-07-23 16:55:01
805,tensorflow/models,models,4800,austinmw,[Feature request] Documentation for Object Detection API GPU parameters,"### System information
- **What is the top-level directory of the model you are using**: `/tensorflow/models/research/object_detection/`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: conda tensorflow-gpu
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: not compiled from source
- **CUDA/cuDNN version**: 9.0/7?
- **GPU model and memory**: (4) GTX 1080 TI
- **Exact command to reproduce**: Feature request

### Describe the problem
Normally to train on all 4 gpus in my single server with object detection models, I use the parameters `--num_clones=4 --ps_tasks=1` when calling `train.py`. However with a few models (ex. NasNet, RetinaNet), increasing `num_clones`>1 errors out (`ValueError: ('In Synchronous SGD mode num_clones must ', 'be 1. Found num_clones: 4')`). I tried every combination I could think of including the other parameter `worker_replicas` and  modifying `batch_size`, but nothing I do will utilize more than the first GPU. 

I can't find any documentation for how to set this up, so this would be really helpful to include for beginners.

**EDIT**: just noticed that `train.py` is now in `legacy/` so I can no longer use multi GPU for _any_ model, not just the ones listed. I created a new issue for this.
",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta""), NamedUser(login=""pkulzc"")]",2018-07-17 14:42:45,open,,,[],2018-08-20 13:47:15
806,tensorflow/models,models,4798,dnuffer,[object_detection] model_main.py failure: tensorflow.python.framework.errors_impl.InvalidArgumentError: Paddings must be non-negative,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.9.0-0-g25c197e023', '1.9.0')
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7.1
- **GPU model and memory**: 1080/8GB
- **Exact command to reproduce**: python /models/research/object_detection/model_main.py --pipeline_config_path=faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid_2018_01_28.config --model_dir=. --num_train_steps=1000 --num_eval_steps=1000 --alsologtostderr 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I am trying to use transfer learning to train a model for the open images challenge. I prepared the data as tfrecord files. I downloaded faster_rcnn_inception_resnet_v2_atrous_oid from the model zoo. I created a config by modifying the number of classes and paths. When I run model_main.py to start training, it fails with the following exception:

```
Traceback (most recent call last):                                                                                                                                                                                                                            
  File ""/models/research/object_detection/model_main.py"", line 101, in <module>                                                                                                                                                                               
    tf.app.run()                                                                                                                                                                                                                                              
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run                                                                                                                                                           
    _sys.exit(main(argv))                                                                                                                                                                                                                                     
  File ""/models/research/object_detection/model_main.py"", line 97, in main                                                                                                                                                                                    
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])                                                                                                                                                                                     
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 447, in train_and_evaluate                                                                                                                                      
    return executor.run()                                                                                                                                                                                                                                     
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 531, in run                                                                                                                                                     
    return self.run_local()                                                                                                                                                                                                                                   
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 669, in run_local                                                                                                                                               
    hooks=train_hooks)                                                                                                                                                                                                                                        
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 366, in train                                                                                                                                                  
    loss = self._train_model(input_fn, hooks, saving_listeners)                                                                                                                                                                                               
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1119, in _train_model                                                                                                                                          
    return self._train_model_default(input_fn, hooks, saving_listeners)                                                                                                                                                                                       
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1135, in _train_model_default                                                                                                                                  
    saving_listeners)                                                                                                                                                                                                                                         
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1336, in _train_with_estimator_spec                                                                                                                            
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])                                                                                                                                                                                    
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 577, in run                                                                                                                                             
    run_metadata=run_metadata)                                                                                                                                                                                                                                
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1053, in run                                                                                                                                            
    run_metadata=run_metadata)                                                                                                                                                                                                                                
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1144, in run                                                                                                                                            
    raise six.reraise(*original_exc_info)                                                                                                                                                                                                                     
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1129, in run                                                                                                                                            
    return self._sess.run(*args, **kwargs)                                                                                                                                                                                                                    
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1201, in run                                                                                                                                            
    run_metadata=run_metadata)                                                                                                                                                                                                                                
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 981, in run                                                                                                                                             
    return self._sess.run(*args, **kwargs)                                                                                                                                                                                                                    
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run                                                                                                                                                         
    run_metadata_ptr)                                                                                                                                                                                                                                         
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run                                                                                                                                                       
    feed_dict_tensor, options, run_metadata)                                                                                                                                                                                                                  
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run                                                                                                                                                    
    run_metadata)                                                                                                                                                                                                                                             
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call                                                                                                                                                   
    raise type(e)(node_def, op, message)                                                                                                                                                                                                                      
tensorflow.python.framework.errors_impl.InvalidArgumentError: Paddings must be non-negative: 0 -54                                                                                                                                                            
         [[Node: Pad_9 = Pad[T=DT_FLOAT, Tpaddings=DT_INT32, _device=""/device:CPU:0""](cond_2/Merge, stack_9)]]                                                                                                                                                
         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[1], [1,?,?,3], [1,3], [1,100], [1,100,4], [1,100,500], [1,100], [1,100], [1,100], [1]], output_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_BOOL, DT_FLO
AT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]                                                                                                                                                                            
```


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

This is the config I used:
```
model {
  faster_rcnn {
    num_classes: 500
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
      }
    }
    feature_extractor {
      type: ""faster_rcnn_inception_resnet_v2""
      first_stage_features_stride: 8
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        height_stride: 8
        width_stride: 8
        scales: 0.25
        scales: 0.5
        scales: 1.0
        scales: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 1.0
        aspect_ratios: 2.0
      }
    }
    first_stage_atrous_rate: 2
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.00999999977648
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.699999988079
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 17
    maxpool_kernel_size: 1
    maxpool_stride: 1
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
        use_dropout: false
        dropout_keep_probability: 1.0
      }
    }
    second_stage_batch_size: 20
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.300000011921
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}
train_config {
  batch_size: 1
  optimizer {
    momentum_optimizer {
      learning_rate {
        manual_step_learning_rate {
          initial_learning_rate: 5.99999964379e-07
          schedule {                                                                                                      
            step: 1000                                                                                                    
            learning_rate: 5.99999984843e-05                                                                              
          }                                                                                                               
          schedule {                                                                                                      
            step: 60000                                                                                                   
            learning_rate: 6.00000021223e-06                                                                              
          }                                                                                                               
          schedule {                                                                                                      
            step: 70000                                                                                                   
            learning_rate: 6.00000021223e-07                                                                              
          }                                                                                                               
        }                                                                                                                 
      }                                                                                                                   
      momentum_optimizer_value: 0.899999976158                                                                            
    }                                                                                                                     
    use_moving_average: false                                                                                             
  }                                                                                                                       
  gradient_clipping_by_norm: 10.0                                                                                         
  fine_tune_checkpoint: ""/data/object_detection_models/faster_rcnn_inception_resnet_v2_atrous_oid_2018_01_28/model.ckpt""  
  num_steps: 1000                                                                                                         
  load_all_detection_checkpoint_vars: true                                                                                
  fine_tune_checkpoint_type: ""detection""                                                                                  
}                                                                                                                         
train_input_reader {                                                                                                      
  label_map_path: ""/models/research/object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt""             
  num_readers: 1                                                                                                          
  tf_record_input_reader {                                                                                                
    input_path: ""/data/images/train_tfrecords/tfrecord-00000-of-00001""                                                    
  }                                                                                                                       
}                                                                                                                         
eval_config {                                                                                                             
  num_examples: 1000                                                                                                      
  max_evals: 10                                                                                                           
  metrics_set: ""open_images_metrics""                                                                                      
  use_moving_averages: false                                                                                              
  retain_original_images: true                                                                                            
}                                                                                                                         
eval_input_reader {                                                                                                       
  label_map_path: ""/models/research/object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt""             
  shuffle: false                                                                                                          
  num_readers: 1                                                                                                          
  tf_record_input_reader {                                                                                                
    input_path: ""/data/images/validation_tfrecords/tfrecord-00000-of-00001""                                               
  }                                                                                                                       
}                                                                                                                         
```

This is the complete output from model_main.py:

```
/usr/local/lib/python2.7/dist-packages/object_detection/utils/visualization_utils.py:25: UserWarning:
This call to matplotlib.use() has no effect because the backend has already
been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

The backend was *originally* set to 'TkAgg' by the following code:
  File ""/models/research/object_detection/model_main.py"", line 26, in <module>
    from object_detection import model_lib
  File ""/usr/local/lib/python2.7/dist-packages/object_detection/model_lib.py"", line 26, in <module>
    from object_detection import eval_util
  File ""/usr/local/lib/python2.7/dist-packages/object_detection/eval_util.py"", line 28, in <module>
    from object_detection.metrics import coco_evaluation
  File ""/usr/local/lib/python2.7/dist-packages/object_detection/metrics/coco_evaluation.py"", line 20, in <module>
    from object_detection.metrics import coco_tools
  File ""/usr/local/lib/python2.7/dist-packages/object_detection/metrics/coco_tools.py"", line 47, in <module>
    from pycocotools import coco
  File ""build/bdist.linux-x86_64/egg/pycocotools/coco.py"", line 49, in <module>
    import matplotlib.pyplot as plt
  File ""/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py"", line 71, in <module>
    from matplotlib.backends import pylab_setup
  File ""/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py"", line 16, in <module>
    line for line in traceback.format_stack()


  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f09e1527488>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/object_detection/core/box_predictor.py:407: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2037: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
WARNING:root:Variable [SecondStageBoxPredictor/BoxEncodingPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [SecondStageBoxPredictor/BoxEncodingPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [SecondStageBoxPredictor/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [SecondStageBoxPredictor/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable.
WARNING:root:Variable [global_step] is not available in checkpoint
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/object_detection/core/losses.py:317: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/object_detection/core/losses.py:317: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
2018-07-17 12:48:43.762318: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-07-17 12:48:44.053889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:05:00.0
totalMemory: 7.93GiB freeMemory: 7.81GiB
2018-07-17 12:48:44.053946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-07-17 12:48:44.250098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-17 12:48:44.250181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0
2018-07-17 12:48:44.250200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N
2018-07-17 12:48:44.250436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7541 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:05:00.0
, compute capability: 6.1)
2018-07-17 12:49:28.560860: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.53GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-17 12:49:28.562139: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.56GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-17 12:49:38.242122: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.11GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-17 12:49:38.267979: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-17 12:49:38.300060: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-17 12:49:38.651405: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.62GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-17 12:49:42.910921: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.11GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-17 12:49:42.935985: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-17 12:49:42.967801: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-17 12:49:45.355076: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
Traceback (most recent call last):
  File ""/models/research/object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/models/research/object_detection/model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 447, in train_and_evaluate
    return executor.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 531, in run
    return self.run_local()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1119, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1135, in _train_model_default
    saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1336, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 577, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1053, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1144, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1129, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1201, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 981, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Paddings must be non-negative: 0 -54
         [[Node: Pad_9 = Pad[T=DT_FLOAT, Tpaddings=DT_INT32, _device=""/device:CPU:0""](cond_2/Merge, stack_9)]]
         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[1], [1,?,?,3], [1,3], [1,100], [1,100,4], [1,100,500], [1,100], [1,100], [1,100], [1]], output_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_BOOL, DT_FLOAT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
```",24,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort""), NamedUser(login=""pkulzc"")]",2018-07-17 13:09:03,open,,,[],2018-08-18 10:05:21
807,tensorflow/models,models,4797,isaac-lapworth,tensorflow-posenet installation fails,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: __This issue relates to the tfjs-models repository.__ You cannot submit issues there so I'm posting this here - please let me know where best to post this.
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Enterprise v1803
- **TensorFlow installed from (source or binary)**: Installed via npm
- **TensorFlow version (use command below)**: @tensorflow/tfjs v0.12.0
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: npm install @tensorflow-models/posenet

### Describe the problem

Tried installing posenet via npm for use in my project. When installing the latest version (0.2.0) using `npm install @tensorflow-models/posenet` the postinstall script fails on running the command `yarn update --pattern @tensorflow`. This gives the error: 
```
error No lockfile in this directory. Run `yarn install` to generate one.
```

After running yarn install the same error occurs when a lockfile is present. 

Attempted deleting and reinstalling node_modules and installing posenet in a new clean project, both with the lockfile present. These both gave the same error.

__The install worked fine when specifically installing posenet version 0.1.3__

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Here is the output with a yarn lockfile present.
```
> @tensorflow-models/posenet@0.2.0 postinstall C:\dev\project\node_modules\@tensorflow-models\posenet
> yarn upgrade --pattern @tensorflow

yarn upgrade v1.7.0
error No lockfile in this directory. Run `yarn install` to generate one.
info Visit https://yarnpkg.com/en/docs/cli/upgrade for documentation about this command.
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {""os"":""darwin"",""arch"":""any
""} (current: {""os"":""win32"",""arch"":""x64""})

npm ERR! code ELIFECYCLE
npm ERR! errno 1
npm ERR! @tensorflow-models/posenet@0.2.0 postinstall: `yarn upgrade --pattern @tensorflow`
npm ERR! Exit status 1
npm ERR!
npm ERR! Failed at the @tensorflow-models/posenet@0.2.0 postinstall script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.

npm ERR! A complete log of this run can be found in:
npm ERR!     C:\Users\username\AppData\Roaming\npm-cache\_logs\2018-07-17T11_05_31_879Z-debug.log
```
",3,"NamedUser(login=""nsthorat"")","[NamedUser(login=""nsthorat"")]",2018-07-17 11:38:46,open,,,[],2018-08-07 07:01:37
808,tensorflow/models,models,4796,ZhangXinNan,add slim/scripts/finetune_inception_v4_on_flowers.sh,add slim/scripts/finetune_inception_v4_on_flowers.sh,3,,[],2018-07-17 11:04:01,open,,,['cla: yes'],2018-07-17 11:21:23
809,tensorflow/models,models,4795,feixuedudiao,How Can I finetune the deeplabv3+ on ms coco dataset,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

### **System information**
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tf1.8
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:cuda9.0/cuDNN7.15
- **GPU model and memory**: titan xp * 4 12G * 4 
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am finetuning the deeplabv3+ using the mobilenetv2 model with pretrain  imagenets, on the ms coco dataset. I change some code in the scripts.
segmentation_dataset.py:
`_MS_COCO_SEG_INFORMATION = DatasetDescriptor(
    splits_to_sizes={
        'train': 118286,
        'val': 5000,
        #'trainval': 118286,
    },
    num_classes=81,
    ignore_label=255,
)`

_DATASETS_INFORMATION = {
    'cityscapes': _CITYSCAPES_INFORMATION,
    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
    'ms_coco': _MS_COCO_SEG_INFORMATION,
    'ade20k': _ADE20K_INFORMATION,
}

train_utils.py:
change   the code
`exclude_list = ['global_step']` 
with
`  exclude_list = ['global_step','logits']'

and set the variables 
flags.DEFINE_boolean('initialize_last_layer', False,
                     'Initialize the last layer.')

flags.DEFINE_boolean('last_layers_contain_logits_only', True,
                     'Only consider logits as last layers or not.')

but when I run the script trian.py ,it has no problem; when I run the script eval.py, it report :
assertion failed: [`predictions` out of bound] [Condition x < y did not hold element-wise:] [x (mean_iou/confusion_matrix/control_dependency_1:0) = ] [0 0 0...] [y (mean_iou/ToInt64_2:0) = ] [81]

what's wrong? who can tell me? thinks.


",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-17 10:15:03,open,,,[],2018-07-27 08:42:46
810,tensorflow/models,models,4794,237014845,absl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --model_dir before flags were parsed.,"
when I run python research/object_detection/model_main.py --pipeline_config_path='./research/object_detection/trainmodels/ssd_mobilenet_v1_pets.config' --model_dir='./research/object_detection/trained2012/'

Traceback (most recent call last):
  File ""research/object_detection/model_main.py"", line 103, in <module>
    tf.app.run()
  File ""/home/hd/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""research/object_detection/model_main.py"", line 57, in main
    config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir)
  File ""/home/hd/.local/lib/python3.5/site-packages/absl/flags/_flagvalues.py"", line 488, in __getattr__
    raise _exceptions.UnparsedFlagAccessError(error_message)
absl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --model_dir before flags were parsed.

",9,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-17 07:34:10,open,,,['stat:awaiting response'],2018-09-04 14:07:39
811,tensorflow/models,models,4793,zhangax1,WARNING:root:image 0 does not have groundtruth difficult flag specified,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-07-17 05:59:38,open,,,['stat:awaiting response'],2018-07-23 20:16:57
812,tensorflow/models,models,4792,SimeonZhang,Fix small mistakes in docstring.,correct the description of padding or clipping tensor method.,3,,[],2018-07-17 03:29:42,open,,,['cla: yes'],2018-07-17 03:35:41
813,tensorflow/models,models,4781,ycui123,Fater RCNN VGG19 Config file for object detection,Has this been implemented ? Thank you,5,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-16 14:51:45,open,,,['stat:contributions welcome'],2018-07-17 13:06:51
814,tensorflow/models,models,4779,PhoneixZhou,update to newest version,"update to newest version ,especially for object detection",1,,[],2018-07-16 14:33:41,open,,,['cla: no'],2018-07-16 14:50:51
815,tensorflow/models,models,4778,netanel-s,TF's COCO evaluation wrapper doesn't actually support include_metrics_per_category and all_metrics_per_category,"### System information
- **What is the top-level directory of the model you are using**: models/research/object_detection/
- **Have I written custom code**: stock script of COCO's detection evaluation
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 9
- **GPU model and memory**: Titan XP, 12GB
- **Exact command to reproduce**:  object_detection/eval.py with checkpoint and config file of ssdlite_mobilenet_v2_coco_2018_05_09 with added metrics_set: ""coco_detection_metrics"" and include_metrics_per_category: true in eval_config.

### Describe the problem
Flags include_metrics_per_category and all_metrics_per_category in eval_config of the config file of the model are not used, and therefore are always False.
Hence, the output metrics are always the usual 12 metrics of COCO's AP and AR for all categories together.

### Source code / logs
In get_evaluators() of object_detection/evaluator.py:
`EVAL_METRICS_CLASS_DICT[eval_metric_fn_key](categories=categories)`
should also be using include_metrics_per_category and all_metrics_per_category in case they're existing attributes of eval_config.
But then I get the 'Category stats do not exist' in ComputeMetrics() of object_detection/metrics/coco_tools.py since COCOEvalWrapper instance doesn't have category_stats attribute.
I tried figuring out how these are supposed to be calculated and fed, but I'm not sure the code supports per category stats (while ComputeMetrics() suggests that it is).

A fix would be highly appreciated.
Thank you very much in advance.",14,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-07-16 13:16:19,open,,,"['stat:awaiting owner', 'type:bug/performance']",2019-04-07 18:42:05
816,tensorflow/models,models,4777,szm2015,Getting RuntimeError: main thread is not in main loop when running model_main.py,"
### System information
- **What is the top-level directory of the model you are using**:
Well, I'm trying to fine-tune ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync model on my own custom dataset.
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **TensorFlow installed from (source or binary)**:
From source
- **TensorFlow version (use command below)**:
1.9.0
- **Bazel version (if compiling from source)**:
0.15.0
- **CUDA/cuDNN version**:
9.2/7.1
- **GPU model and memory**:
Geforce GTX 850M 4GB
- **Exact command to reproduce**:
python3 object_detection/model_main.py --pipeline_config_path=/home/szm/Work/TensorFlow/Models/ObjectDetection/ssd_mobilenet_v1_fpn_DETRAC_516x292_4C_RA1/ssd_mobilenet_v1_fpn.config --model_dir=/home/szm/Work/TensorFlow/Models/ObjectDetection/ssd_mobilenet_v1_fpn_DETRAC_516x292_4C_RA1 --alsologtostderr

### Describe the problem
OK! I've been using an older release of Object Detection API (from the README it seems to be February 9, 2018 release) and have trained multiple models with it. Recently, I decided to use the July 13, 2018 release so as to be able to train newly added models like ssd_mobilenet_v1_fpn. 

First, I finished the steps mentioned in the installation with model_builder_test.py resulting in:
>Ran 18 tests in 0.095s
>OK

Then I tried to train my model using the command I mentioned above, which is what I found in [Running locally](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md). I'm using the provided config file ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config with minor changes in the following parts:
>num_classes (changed number)
>fixed_shape_resizer (changed number)
>matched_threshold (changed number)
>unmatched_threshold (changed number)
>iou_threshold (changed number)
>batch_size (changed number)
>fine_tune_checkpoint_type: 'detection' (added)
>num_steps (commented)
>learning_rate_base (changed number)
>warmup_learning_rate (changed number)

The first problem is that the codes (or at least some of them) seem to have problems running with python3 (which is the python I'm using, It's actually version 3.6.5). Here are the errors I got from running model_main.py and their solutions:

```
File ""/.../models-master/research/object_detection/utils/object_detection_evaluation.py"", line 842
    print 'Scores and tpfp per class label: {}'.format(class_index)
    solution: print() instead of print 
```
```
File ""/.../models-master/research/object_detection/models/feature_map_generators.py"", line 225, in fpn_top_down_feature_maps
    reversed(zip(output_feature_map_keys, output_feature_maps_list)))
TypeError: 'zip' object is not reversible
	solution: reversed(list(zip(output_feature_map_keys, output_feature_maps_list))))
``` 
```
File ""/.../models-master/research/object_detection/model_lib.py"", line 282, in model_fn
    losses = [loss_tensor for loss_tensor in losses_dict.itervalues()]
AttributeError: 'dict' object has no attribute 'itervalues'

	solution: using values() instead of itervalues()
```

I really don't understand why should such errors exist? Is the API meant to be used with python 2 or it's that I'm missing something?!

Anyway, the actual problem is the one mentioned in the title. That's the error I get after fixing the above problems (I'm not exactly sure about the fixes though!) and I don't know how to deal with it. It's worth to note that I also repeated the above steps on an Ubuntu 16.04 with CUDA 9.0,tensorflow 1.7.0 and Geforce GTX 1070 8GB and got the same errors.

### Source code / logs
Here's the full Traceback:

```
2018-07-16 12:37:39.541264: W tensorflow/core/framework/op_kernel.cc:1263] Unknown: RuntimeError: main thread is not in main loop
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 206, in __call__
    ret = func(*args)

  File ""/home/szm/Work/TensorFlow/Codes/models-master/research/object_detection/utils/visualization_utils.py"", line 693, in cdf_plot
    fig = plt.figure(frameon=False)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 548, in figure
    **kwargs)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/backend_bases.py"", line 161, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_tk.py"", line 1053, in new_figure_manager_given_figure
    icon_img = Tk.PhotoImage(file=icon_fname)

  File ""/usr/lib/python3.6/tkinter/__init__.py"", line 3542, in __init__
    Image.__init__(self, 'photo', name, cnf, master, **kw)

  File ""/usr/lib/python3.6/tkinter/__init__.py"", line 3498, in __init__
    self.tk.call(('image', 'create', imgtype, name,) + options)

RuntimeError: main thread is not in main loop


Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: RuntimeError: main thread is not in main loop
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 206, in __call__
    ret = func(*args)

  File ""/home/szm/Work/TensorFlow/Codes/models-master/research/object_detection/utils/visualization_utils.py"", line 693, in cdf_plot
    fig = plt.figure(frameon=False)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 548, in figure
    **kwargs)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/backend_bases.py"", line 161, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_tk.py"", line 1053, in new_figure_manager_given_figure
    icon_img = Tk.PhotoImage(file=icon_fname)

  File ""/usr/lib/python3.6/tkinter/__init__.py"", line 3542, in __init__
    Image.__init__(self, 'photo', name, cnf, master, **kw)

  File ""/usr/lib/python3.6/tkinter/__init__.py"", line 3498, in __init__
    self.tk.call(('image', 'create', imgtype, name,) + options)

RuntimeError: main thread is not in main loop


	 [[Node: Loss/PyFunc_1 = PyFunc[Tin=[DT_FLOAT], Tout=[DT_UINT8], token=""pyfunc_1"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/Squeeze_1/_3561)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 451, in train_and_evaluate
    return executor.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 590, in run
    return self.run_local()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 691, in run_local
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1143, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1171, in _train_model_default
    saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1449, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 583, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1059, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1150, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/lib/python3/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1135, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1207, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 987, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: RuntimeError: main thread is not in main loop
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 206, in __call__
    ret = func(*args)

  File ""/home/szm/Work/TensorFlow/Codes/models-master/research/object_detection/utils/visualization_utils.py"", line 693, in cdf_plot
    fig = plt.figure(frameon=False)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 548, in figure
    **kwargs)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/backend_bases.py"", line 161, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_tk.py"", line 1053, in new_figure_manager_given_figure
    icon_img = Tk.PhotoImage(file=icon_fname)

  File ""/usr/lib/python3.6/tkinter/__init__.py"", line 3542, in __init__
    Image.__init__(self, 'photo', name, cnf, master, **kw)

  File ""/usr/lib/python3.6/tkinter/__init__.py"", line 3498, in __init__
    self.tk.call(('image', 'create', imgtype, name,) + options)

RuntimeError: main thread is not in main loop


	 [[Node: Loss/PyFunc_1 = PyFunc[Tin=[DT_FLOAT], Tout=[DT_UINT8], token=""pyfunc_1"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/Squeeze_1/_3561)]]

Caused by op 'Loss/PyFunc_1', defined at:
  File ""object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 451, in train_and_evaluate
    return executor.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 590, in run
    return self.run_local()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 691, in run_local
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1143, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1168, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1131, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/szm/Work/TensorFlow/Codes/models-master/research/object_detection/model_lib.py"", line 281, in model_fn
    prediction_dict, features[fields.InputDataFields.true_image_shape])
  File ""/home/szm/Work/TensorFlow/Codes/models-master/research/object_detection/meta_architectures/ssd_meta_arch.py"", line 597, in loss
    flattened_class_ids, flattened_classification_losses)
  File ""/home/szm/Work/TensorFlow/Codes/models-master/research/object_detection/meta_architectures/ssd_meta_arch.py"", line 660, in _summarize_anchor_classification_loss
    'NegativeAnchorLossCDF')
  File ""/home/szm/Work/TensorFlow/Codes/models-master/research/object_detection/utils/visualization_utils.py"", line 703, in add_cdf_image_summary
    cdf_plot = tf.py_func(cdf_plot, [values], tf.uint8)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 456, in py_func
    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 281, in _internal_py_func
    input=inp, token=token, Tout=Tout, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_script_ops.py"", line 128, in py_func
    ""PyFunc"", input=input, token=token, Tout=Tout, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3212, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1702, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

UnknownError (see above for traceback): RuntimeError: main thread is not in main loop
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 206, in __call__
    ret = func(*args)

  File ""/home/szm/Work/TensorFlow/Codes/models-master/research/object_detection/utils/visualization_utils.py"", line 693, in cdf_plot
    fig = plt.figure(frameon=False)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 548, in figure
    **kwargs)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/backend_bases.py"", line 161, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)

  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_tk.py"", line 1053, in new_figure_manager_given_figure
    icon_img = Tk.PhotoImage(file=icon_fname)

  File ""/usr/lib/python3.6/tkinter/__init__.py"", line 3542, in __init__
    Image.__init__(self, 'photo', name, cnf, master, **kw)

  File ""/usr/lib/python3.6/tkinter/__init__.py"", line 3498, in __init__
    self.tk.call(('image', 'create', imgtype, name,) + options)

RuntimeError: main thread is not in main loop


	 [[Node: Loss/PyFunc_1 = PyFunc[Tin=[DT_FLOAT], Tout=[DT_UINT8], token=""pyfunc_1"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/Squeeze_1/_3561)]]

Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/_pylab_helpers.py"", line 78, in destroy_all
    manager.destroy()
  File ""/home/szm/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_tk.py"", line 558, in destroy
    self.window.destroy()
  File ""/usr/lib/python3.6/tkinter/__init__.py"", line 2058, in destroy
    for c in list(self.children.values()): c.destroy()
  File ""/usr/lib/python3.6/tkinter/__init__.py"", line 2302, in destroy
    self.tk.call('destroy', self._w)
RuntimeError: main thread is not in main loop
```
",19,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-07-16 11:24:19,open,,,[],2018-08-13 18:19:17
817,tensorflow/models,models,4774,varun19299,Estimators for training: Multi GPU Support seems missing,"## Feature Request:

Estimators seem to make data parallelism much easier with the replicate_model_fn and TowerOptimizer decorators. This doesn't seem to be included in the Estimator definitions at `model_lib.py`.

Could Multi GPU use be clarified (if already present)?

For my present use case, I happen to be modifying the `model_lib.py` defention with the decorators to accommodate tower cloning. 

------------------------

_System Information doesn't seem relevant to this, but included nevertheless_

### System information
- **What is the top-level directory of the model you are using**: model/research 
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, but not with any dependencies with the Estimator defenitions.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, 16.04 (ubuntu)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: 0.14.0
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: 1080 Ti, 12 GB
- **Exact command to reproduce**: `python object_detection/model_main.py`

",14,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-15 13:20:29,open,,,['stat:awaiting owner'],2018-08-06 16:47:12
818,tensorflow/models,models,4773,twangnh,AVG pool cause OOM,"Hi, I'm using tf object detection api, I modified my code for the roialign, my code runs smoothly with many steps, but after some step, like 3000, suddenly it throws:
```
2018-07-15 18:52:23.371349: W tensorflow/core/common_runtime/bfc_allocato
r.cc:279] ***************************************************************
*************************************
2018-07-15 18:52:23.371397: W tensorflow/core/framework/op_kernel.cc:1318
] OP_REQUIRES failed at transpose_op.cc:199 : Resource exhausted: OOM whe
n allocating tensor with shape[64,4032,34,34] and type float on /job:mast
er/replica:0/task:0/device:GPU:7 by allocator GPU_7_bfc
INFO:tensorflow:Error reported to Coordinator: OOM when allocating tensor
 with shape[64,4032,34,34] and type float on /job:master/replica:0/task:0
/device:GPU:1 by allocator GPU_1_bfc
         [[Node: clone_1/AvgPool-0-TransposeNHWCToNCHW-LayoutOptimizer = 
Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=""/job:master/replica:0/task
:0/device:GPU:1""](clone_1/CropAndResize, PermConstNHWCToNCHW-LayoutOptimi
zer_G59595)]]
Hint: If you want to see a list of allocated tensors when OOM happens, ad
d report_tensor_allocations_upon_oom to RunOptions for current allocation
 info.

         [[Node: clone_0/Loss/BoxClassifierLoss/localization_loss_G76959 
= _Recv[client_terminated=false, recv_device=""/job:master/replica:0/task:
0/device:CPU:0"", send_device=""/job:master/replica:0/task:0/device:GPU:0"",
 send_device_incarnation=7212908817941306581, tensor_name=""edge_62089_clo
ne_0/Loss/BoxClassifierLoss/localization_loss"", tensor_type=DT_FLOAT, _de
vice=""/job:master/replica:0/task:0/device:CPU:0""]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, ad
d report_tensor_allocations_upon_oom to RunOptions for current allocation
 info.
Traceback (most recent call last):
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exc
eption
    yield
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/training/coordinator.py"", line 495, in run
    self.run_loop()
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/training/supervisor.py"", line 1030, in run_loop
    self._sv.global_step])
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
ResourceExhaustedError: OOM when allocating tensor with shape[64,4032,34,
34] and type float on /job:master/replica:0/task:0/device:GPU:1 by alloca
tor GPU_1_bfc
         [[Node: clone_1/AvgPool-0-TransposeNHWCToNCHW-LayoutOptimizer = 
Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=""/job:master/replica:0/task
:0/device:GPU:1""](clone_1/CropAndResize, PermConstNHWCToNCHW-LayoutOptimi
zer_G59595)]]
Hint: If you want to see a list of allocated tensors when OOM happens, ad
d report_tensor_allocations_upon_oom to RunOptions for current allocation
 info.

         [[Node: clone_0/Loss/BoxClassifierLoss/localization_loss_G76959 
= _Recv[client_terminated=false, recv_device=""/job:master/replica:0/task:
0/device:CPU:0"", send_device=""/job:master/replica:0/task:0/device:GPU:0"",
 send_device_incarnation=7212908817941306581, tensor_name=""edge_62089_clo
ne_0/Loss/BoxClassifierLoss/localization_loss"", tensor_type=DT_FLOAT, _de
vice=""/job:master/replica:0/task:0/device:CPU:0""]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, ad
d report_tensor_allocations_upon_oom to RunOptions for current allocation
 info.

INFO:tensorflow:Error reported to Coordinator: OOM when allocating tensor
 with shape[64,4032,34,34] and type float on /job:master/replica:0/task:0
/device:GPU:1 by allocator GPU_1_bfc
         [[Node: clone_1/AvgPool-0-TransposeNHWCToNCHW-LayoutOptimizer = 
Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=""/job:master/replica:0/task
:0/device:GPU:1""](clone_1/CropAndResize, PermConstNHWCToNCHW-LayoutOptimi
zer_G59595)]]
Hint: If you want to see a list of allocated tensors when OOM happens, ad
d report_tensor_allocations_upon_oom to RunOptions for current allocation
 info.

         [[Node: clone_0/Loss/BoxClassifierLoss/localization_loss_G76959 
= _Recv[client_terminated=false, recv_device=""/job:master/replica:0/task:
0/device:CPU:0"", send_device=""/job:master/replica:0/task:0/device:GPU:0"",
 send_device_incarnation=7212908817941306581, tensor_name=""edge_62089_clo
ne_0/Loss/BoxClassifierLoss/localization_loss"", tensor_type=DT_FLOAT, _de
vice=""/job:master/replica:0/task:0/device:CPU:0""]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, ad
d report_tensor_allocations_upon_oom to RunOptions for current allocation
 info.
Traceback (most recent call last):
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exc
eption
    yield
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/training/coordinator.py"", line 495, in run
    self.run_loop()
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/training/supervisor.py"", line 1030, in run_loop
    self._sv.global_step])
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/user/anaconda2/lib/python2.7/site-pack
ages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
ResourceExhaustedError: OOM when allocating tensor with shape[64,4032,34,
34] and type float on /job:master/replica:0/task:0/device:GPU:1 by alloca
tor GPU_1_bfc
         [[Node: clone_1/AvgPool-0-TransposeNHWCToNCHW-LayoutOptimizer = 
Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=""/job:master/replica:0/task
:0/device:GPU:1""](clone_1/CropAndResize, PermConstNHWCToNCHW-LayoutOptimi
zer_G59595)]]
Hint: If you want to see a list of allocated tensors when OOM happens, ad
d report_tensor_allocations_upon_oom to RunOptions for current allocation
 info.
         [[Node: clone_0/Loss/BoxClassifierLoss/localization_loss_G76959 
= _Recv[client_terminated=false, recv_device=""/job:master/replica:0/task:
0/device:CPU:0"", send_device=""/job:master/replica:0/task:0/device:GPU:0"",
 send_device_incarnation=7212908817941306581, tensor_name=""edge_62089_clo
ne_0/Loss/BoxClassifierLoss/localization_loss"", tensor_type=DT_FLOAT, _de
vice=""/job:master/replica:0/task:0/device:CPU:0""]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, ad
d report_tensor_allocations_upon_oom to RunOptions for current allocation
 info.

```
seems like avg pool was optimized to use NCHW format, so it first transpose to NCHW then transpose back. the part of code is 
```
ret = tf.nn.avg_pool(ret, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME', data_format='NHWC')
```
when explicitly write it as 
```
ret = tf.transpose(ret, [0,3,1,2])
ret = tf.nn.avg_pool(ret, [1, 1, 2, 2], [1, 1, 2, 2], padding='SAME', data_format='NCHW')
ret = tf.transpose(ret, [0, 2,3, 1])
```
for several thousands of steps there is no OOM, but later OOM again happened, the problem is strange since I have fixed input size, so memory consumption should be fixed, dont know why it OOM later",2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-15 11:40:51,open,,,[],2018-07-17 19:40:37
819,tensorflow/models,models,4772,jbasquiat,Object detection error: No such file or directory,"Hi,

I'm trying to run locally a training with object detection API  on ubuntu. I've followed instruction [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md
).  I have my pipeline.config, my tfrecord files and model.ckpt from model zoo in a recommended directory structure.

I try to run this script:
```
python object_detection/model_main.py \
    --pipeline_config_path= ""/home/juan/TFG/prueba1/models/model/pipeline.config"" \
    --model_dir=""/home/juan/TFG/prueba1/models/model"" \
    --checkpoint_dir= ""/home/juan/TFG/prueba1/faster_rcnn_resnet101_kitti/model.ckpt.data-00000-of-00001"" \
    --num_train_steps=2000 \
    --num_eval_steps=200 \
    --alsologtostderr
```

But I get this error:
```
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 86, in <module>
    tf.app.run()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 57, in main
    eval_steps=FLAGS.num_eval_steps)
  File ""/home/juan/TFG/models-master/research/object_detection/model_lib.py"", line 469, in create_estimator_and_inputs
    configs = get_configs_from_pipeline_file(pipeline_config_path)
  File ""/home/juan/TFG/models-master/research/object_detection/utils/config_util.py"", line 93, in get_configs_from_pipeline_file
    proto_str = f.read()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 120, in read
    self._preread_check()
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 80, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""/home/juan/.local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory

```

If I understand correctly, the script can't find the pipeline.config, but I've checked that _/home/juan/TFG/prueba1/models/model/pipeline.config_ exist

I have no idea what I'm doing wrong

Thanks!",4,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-15 08:42:39,open,,,[],2018-07-17 01:17:44
820,tensorflow/models,models,4771,phmagic,Fixed Open Images TF Record creation for Python 3,,3,,[],2018-07-15 07:31:06,open,,,['cla: no'],2018-07-15 07:34:12
821,tensorflow/models,models,4770,josealb,Feature request: SSD Mobilenet needs instance keys for ML Engine,"### System information
- **What is the top-level directory of the model you are using**: Object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Stock scripts only
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ML Engine
- **TensorFlow installed from (source or binary)**: ML Engine
- **TensorFlow version (use command below)**: 1.5
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: ML Engine
- **Exact command to reproduce**: Submit batch prediction

### Describe the problem
I am attempting to use an SSD mobilenet object detection model in ML Engine. I trained the model and the detections are correct. However, I just learned that batch predictions from ML Engine can return scrambled and instance keys must be used to assign each prediction to its input.
If I try doing this with the current models I get errors.

### Source code / logs
This is how I attempt to create a TF_Record with keys:
```
with tf.python_io.TFRecordWriter(predict_instance_tfr) as tfr_writer:
        with open(predict_instance_json, ""w"") as fp:
            blobs = bucket.list_blobs(prefix=folder_name)
            for blob in blobs:
                image_name=blob.name
                image_name = (image_name.split('/'))[1]
                if (image_name.split('.')[-1:] == ['jpg'] or image_name.split('.')[-1:] == ['jpeg']):
                    blob.download_to_filename(image_name)
                    img = Image.open(image_name)
                    img = img.resize((width, height), Image.ANTIALIAS)
                    output_str = io.BytesIO()
                    img.save(output_str, ""JPEG"")
                                        example = tf.train.Example(
                    features=tf.train.Features(
                        feature={
                            #'key': _int64_feature(instance_key),
                            'image': _bytes_feature(output_str.getvalue())
                        }))
                    tfr_writer.write(example.SerializeToString())
```
And here is the error I get:

> ('Exception during running the graph: assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]\n\t [[Node: map/while/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert = Assert[T=[DT_STRING], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/while/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp, map/while/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert/data_0)]]', 1)

Thank you",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel""), NamedUser(login=""pkulzc"")]",2018-07-15 07:07:01,open,,,['stat:awaiting response'],2018-07-19 21:29:17
822,tensorflow/models,models,4768,lamberta,Import keras-estimator notebook from kashif/tf-keras-tutorial,"Bringing in keras-estimator notebook from kashif/tf-keras-tutorial (see: https://github.com/kashif/tf-keras-tutorial/issues/1) ---Thanks, Dr. Kashif Rasul!

Needs a technical and editorial review before adding to tensorflow.org

Stage: https://colab.research.google.com/github/lamberta/models/blob/keras-estimator-tutorial/samples/core/tutorials/estimators/keras_estimator.ipynb
",0,,[],2018-07-14 21:58:41,open,,,['cla: yes'],2018-07-14 22:04:05
823,tensorflow/models,models,4760,smakaviani,which section of code made it necessary to have Bazel installed ,"Hi Thanks for implementation of this paper, I can not figure out why author used bazel , has the code sth borrowed from the c functions or because of the tensorflow version used here? 

What is the top-level directory of the model you are using  N/A
Have I written custom code N/A
OS Platform and Distribution N/A
TensorFlow installed from   N/A
TensorFlow version 1.4
Bazel version  N/A
CUDA/cuDNN version  N/A
GPU model and memory  N/A
Exact command to reproduce  N/A",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-13 07:15:22,open,,,['stat:awaiting response'],2018-07-15 04:29:16
824,tensorflow/models,models,4755,mbenami,Add image name to tensorboard ,"[object detection]
if it possible to add the name of the image when display on tensorboard 
it can help for debugging when the dataset is changing all the time 
one simple way to do that is reusing the keep_image_id_for_visualization_export flag and 
adding in eval_utils.py: 

```
  if export_dir:
    if keep_image_id_for_visualization_export and result_dict[fields.
                                                              InputDataFields()
                                                              .key]:
      export_path = os.path.join(export_dir, 'export-{}-{}.png'.format(
          tag, result_dict[fields.InputDataFields().key]))
    else:
      export_path = os.path.join(export_dir, 'export-{}.png'.format(tag))
    vis_utils.save_image_array_as_png(image, export_path)
  elif keep_image_id_for_visualization_export:
    tag += ' - ' + result_dict[fields.InputDataFields().key]
```
thanks ",2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-12 20:28:36,open,,,['stat:awaiting tensorflower'],2018-07-16 20:41:51
825,tensorflow/models,models,4754,Strange369,AssertionError: Model diverged with loss = NaN,"When I am trying the tutorial to apply the multiple GPUs, there is an error:
python3 cifar10_multi_gpu_train.py --num_gpus=2
Traceback (most recent call last):
  File ""cifar10_multi_gpu_train.py"", line 277, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""cifar10_multi_gpu_train.py"", line 273, in main
    train()
  File ""cifar10_multi_gpu_train.py"", line 246, in train
    assert not np.isnan(loss_value), 'Model diverged with loss = NaN'
AssertionError: Model diverged with loss = NaN
I have modified the LEARNING_RATE from 0.1 to 0.01, however it doesn't help.
Any suggestions?

Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-12 15:27:08,open,,,[],2018-07-13 01:43:36
826,tensorflow/models,models,4753,ruimarquesXesol,[deeplab] add inference_batch_size option to export_model.py,"### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.8.0-3413-g8e86dcd', '1.9.0-rc0')
- **Bazel version (if compiling from source)**: 0.13
- **CUDA/cuDNN version**: 9.1/7.0
- **GPU model and memory**: 2*(Nvidia Titan X (Pascal) with 12194MiB memory)
- **Exact command to reproduce**:
```python
python export_model.py \
  --logtostderr \
  --checkpoint_path=""${CKPT_PATH}"" \
  --export_path=""${EXPORT_PATH}"" \
  --num_classes=11 \
  --crop_size=320 \
  --crop_size=320 \
  --output_stride=16 \
```

### Describe the problem
It would be nice to be able to export a frozen graph which accepted a batch of images instead of a single image as input. Similar to what [vis.py#L47](https://github.com/tensorflow/models/blob/ee6fdda13b2cb79d96303a8ef06ad50dee325611/research/deeplab/vis.py#L47) already does.

My initial idea was to simply change [export_model.py#L77](https://github.com/tensorflow/models/blob/ee6fdda13b2cb79d96303a8ef06ad50dee325611/research/deeplab/export_model.py#L77)
```python
 # input_preprocess takes 4-D image tensor as input.
input_image = tf.placeholder(tf.uint8, [1, None, None, 3], name=_INPUT_NAME)
```
to
```python
 # input_preprocess takes 4-D image tensor as input.
input_image = tf.placeholder(tf.uint8, [FLAGS.inference_batch_size, None, None, 3], name=_INPUT_NAME)
```
but obviously the whole input generator pipeline is more complex than that and I also cannot use the code from vis.py because it uses [deeplab.utils.input_generator.get()](https://github.com/tensorflow/models/blob/ee6fdda13b2cb79d96303a8ef06ad50dee325611/research/deeplab/vis.py#L209) whereas export_model.py uses [deeplab.input_preprocess.preprocess_image_and_label()](https://github.com/tensorflow/models/blob/ee6fdda13b2cb79d96303a8ef06ad50dee325611/research/deeplab/export_model.py#L83) which are completely different functions with different purposes.

It would be nice if someone knows a simple way of modifying the code to make this happen. I believe this is a necessary feature because in many real world application there is a huge time advantage in doing inference in batches.

For example, one could divide a high resolution image into small sections, each with the correct input size of the first layer, put them in a 4D input tensor batch and infer all of them at once. In principle, this would be faster than inferring with the original size.

### Source code / logs
NA
",4,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-07-12 14:41:06,open,,,"['stat:contributions welcome', 'type:feature']",2019-01-24 12:39:05
827,tensorflow/models,models,4745,digitalized-hfut,how to fine tune another layers？,"Type the code like this  ""python train_image_classifier.py"", but i don't understand how to fine tune another layer? Not the last fully connected layer",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-11 15:00:00,open,,"NamedUser(login=""digitalized-hfut"")",['stat:awaiting response'],2018-07-12 03:14:33
828,tensorflow/models,models,4742,SriramganeshMarimuthu,How can I retrain the object detection tensorflow model? or How can I retrain the existing ssd_mobilenet_v1_coco model with one addition class.?,"I would like to know, If it is possible can we retrain the object detection model?

Say Example: I have trained my own model to identify red and green light. Now I need to update/retrain my model to identify the yellow color light.

When we use TF- Image Classifier, we can add new image sets to test and train directory and we can execute retrain.py to update the .pb file to get new results.

Could you please guide me, how to implement retrain mechanism for object detection model with TF?",4,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel""), NamedUser(login=""pkulzc"")]",2018-07-11 08:47:54,open,,,[],2018-07-19 21:29:45
829,tensorflow/models,models,4741,David-Lee-1990,How can i get the probability of the predicted bounding box?,"I train tensorflow's object detection api on my own data with ssd_mobilenet_v1 on Windows 10. 
I have GPU NVIDIA GeForce GTX 1070, 8G. 
CUDA version: 8.0
Tensorflow version: 1.8.0

I want to get the probabilities of predicted bounding boxes of my trained  model. 
`   (boxes, scores, classes, num) = sess.run(
    [detection_boxes, detection_scores, detection_classes, num_detections],
    feed_dict={image_tensor: image_np_expanded})`

I print the scores out, but i find those scores are so small while actually they should be near 0.9.
`5.2126852e-05
9.6937576e-05
7.9559584e-05
6.452701e-05
7.0217706e-05
0.00013549675
0.00012261284
0.00017910718
0.00035809493
0.00014258255
0.00013030451
0.00015346507
0.00034742578
0.00070519955
0.0013604913
0.0013002576
0.0008962038
0.001834011
0.00083002774
0.0013717125
0.0011037914
0.0029161684
0.003989096
0.0040172455
0.0067180903
0.013149556
0.030326849
0.0118160825
0.01653535`

so what does the score really mean?",2,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-07-11 07:42:33,open,,,[],2018-07-15 15:30:04
830,tensorflow/models,models,4739,kanul,[tflite][quantization][deeplabv3] Constant array MobilenetV2/expanded_conv_7/depthwise/depthwise_weights lacks MinMax information,"**System information**
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:Source
- **TensorFlow version (use command below)**:1.9.0
- **Python version**:2.7.12
- **Bazel version (if compiling from source)**:0.12.0
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:cuda-9.0/7.0
- **GPU model and memory**:GeForce GTX 1080/8105MiB
- **Phone**:xiaomi5 (Snapdragon 820)
- **Exact command to reproduce**:
bazel run --config=opt //tensorflow/contrib/lite/toco:toco --
--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb
--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite
--inference_type=QUANTIZED_UINT8
--input_shape=1,513,513,3
--input_array=sub_7
--output_array=ResizeBilinear_3

**Describe the problem**
I have tried to quantize MobileNetV2 for deeplabV3+ with TFlite. But I fail to convert the model.
From the following issue, I saw that the operations were not supported for the option of quantization.

https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md
Checkpoint name: mobilenetv2_coco_voc_trainaug

As we can see graphs from the tensorboard, there is one big problem.

In ""import/MobilenetV2/expanded_conv_7/depthwise/depthwise"",

the operation of depthwise consists of the subgraph with 3 nodes: (depthwise) and BatchToSpaceND, SpaceToBatchND.

But, in ""import/MobilenetV2/expanded_conv_6/depthwise/depthwise"",

the operation of depthwise is DepthwiseConv2dNative itself.

From the difference, we can not quantize deeplabv3 based on mobilenetv2.

The one thing is that MobilenetV2/expanded_conv_7~16 does not have min/max value to be needed for quantization with tflite.

Although I implement the needed min/max value in hardcode_min_max.cc,

This model does not run well in mobile environments.

The ultimate problem is caused by the fact that depthwise_conv_7~16 consist of 3 nodes including BatchToSpaceND and SpaceToBatchND.

I request you to notify the method to resolve above issues.

**Source code / logs**

bazel run --config=opt //tensorflow/contrib/lite/toco:toco --
--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb
--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite
--inference_type=QUANTIZED_UINT8
--input_shape=1,513,513,3
--input_array=sub_7
--output_array=ResizeBilinear_3

2018-07-11 04:40:01.330069: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 166 operators, 340 arrays (1 quantized)
2018-07-11 04:40:01.330711: W tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:339] Tweaking the MinMax of array ResizeBilinear_1, which is an input to {Concatenation operator with output concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.
2018-07-11 04:40:01.332983: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 111 operators, 285 arrays (1 quantized)
2018-07-11 04:40:01.335731: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 111 operators, 285 arrays (1 quantized)
2018-07-11 04:40:01.337575: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_7/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.337670: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_7/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.337695: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_7/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.338553: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_8/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.338711: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_8/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.338786: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_8/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.339777: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_9/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.339918: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_9/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.339985: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_9/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.340933: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_10/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.341034: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_10/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.341059: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_10/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.342497: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_11/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.342593: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_11/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.342620: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_11/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.344311: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_12/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.344422: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_12/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.344452: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_12/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.345978: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_13/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.346094: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_13/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.346122: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_13/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.349163: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_14/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.349318: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_14/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.349351: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_14/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.353356: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_15/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.353511: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_15/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.353545: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_15/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.357264: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_16/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.357400: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_16/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.357432: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_16/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
",2,"NamedUser(login=""YknZhu"")","[NamedUser(login=""YknZhu"")]",2018-07-11 05:19:43,open,,,['type:feature'],2018-07-16 21:37:44
831,tensorflow/models,models,4738,cjr0106,No module named official,"
When i run the code""cifia_test.py"",it got wrong in line 25:

`from official.resnet import cifar10_main
from official.utils.testing import integration`

No module named official. 
i add another code  import official ,it got the same wrong,could you give me a hand?
",4,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-07-11 04:29:38,open,,,[],2018-07-12 06:56:02
832,tensorflow/models,models,4735,leccyril,blank in filename or label mask_rcnn_inception_v2,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

System information
What is the top-level directory of the model you are using:
/models/object_detection

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
use data_tools/create_pet_tf_record.py with faces_only False (i want mask)

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Debian 9

TensorFlow installed from (source or binary):
install from pip3

TensorFlow version (use command below):
1.8.0

Bazel version (if compiling from source):

CUDA/cuDNN version:

GPU model and memory:

Exact command to reproduce:


put blank in png and xml and jpg files and in label
launch create_pet_record_coco_tf.py

WARNING:root:Could not find /home/tensorflow/object_detection/images/mask/annotations/xmls/.xml, ignoring example.


https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

the problem is i have blank name like ""Laptop Lenovo Yoga.xml/jpg/png"" and same lable in label xml, so i cut after the first blank


Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.",4,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-07-10 18:44:54,open,,,[],2018-07-14 06:51:46
833,tensorflow/models,models,4732,RoytenBerge,"Uneven class distribution, object detection","What is the top-level directory of the model you are using
tensorflow/models/research
Have I written custom code
No
OS Platform and Distribution
Windows 10
TensorFlow installed from
Pip install
TensorFlow version
up-to-date 
Bazel version
CUDA/cuDNN version
not in use
GPU model and memory
not used
Exact command to reproduce
python train.py --logtostderr --train_dir=trainingBigSet/ --pipeline_config_path=object_detection/training/pipeline.config

------------------------------------------------------------------------------------------------------------

I am running the faster_rcnn_inception_v2_coco_2018_01_28 neural network on my home made dataset. It is not possible (due to my lack of data) to make an evenly divided dataset. Therefor I was wondering if there is a way of adding more weight to the classes that occur less in my dataset? 

I was thinking about multiplying the loss per class with a different weight, but i don't exactly know how to implement this. 

Your help is very much appreciated!

Kind regards,

Roy

--update
After further research I assume that I should edit some value in the faster_rcnn_meta_arch.py script:
```
def loss(self, prediction_dict, true_image_shapes, scope=None):
    """"""Compute scalar loss tensors given prediction tensors.

    If number_of_stages=1, only RPN related losses are computed (i.e.,
    `rpn_localization_loss` and `rpn_objectness_loss`).  Otherwise all
    losses are computed.

    Args:
      prediction_dict: a dictionary holding prediction tensors (see the
        documentation for the predict method.  If number_of_stages=1, we
        expect prediction_dict to contain `rpn_box_encodings`,
        `rpn_objectness_predictions_with_background`, `rpn_features_to_crop`,
        `image_shape`, and `anchors` fields.  Otherwise we expect
        prediction_dict to additionally contain `refined_box_encodings`,
        `class_predictions_with_background`, `num_proposals`, and
        `proposal_boxes` fields.
      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
        of the form [height, width, channels] indicating the shapes
        of true images in the resized images, as resized images can be padded
        with zeros.
      scope: Optional scope name.

    Returns:
      a dictionary mapping loss keys (`first_stage_localization_loss`,
        `first_stage_objectness_loss`, 'second_stage_localization_loss',
        'second_stage_classification_loss') to scalar tensors representing
        corresponding loss values.
    """"""
    with tf.name_scope(scope, 'Loss', prediction_dict.values()):
      (groundtruth_boxlists, groundtruth_classes_with_background_list,
       groundtruth_masks_list) = self._format_groundtruth_data(
           true_image_shapes)
      loss_dict = self._loss_rpn(
          prediction_dict['rpn_box_encodings'],
          prediction_dict['rpn_objectness_predictions_with_background'],
          prediction_dict['anchors'],
          groundtruth_boxlists,
          groundtruth_classes_with_background_list)
      if self._number_of_stages > 1:
        loss_dict.update(
            self._loss_box_classifier(
                prediction_dict['refined_box_encodings'],
                prediction_dict['class_predictions_with_background'],
                prediction_dict['proposal_boxes'],
                prediction_dict['num_proposals'],
                groundtruth_boxlists,
                groundtruth_classes_with_background_list,
                prediction_dict['image_shape'],
                prediction_dict.get('mask_predictions'),
                groundtruth_masks_list,
            ))
    return loss_dict

```
There is a function def _loss_box_classifier that holds the classification loss calculation:
`      classification_loss = tf.multiply(self._second_stage_cls_loss_weight,
                                        second_stage_cls_loss,
                                        name='classification_loss')`
On line 1828. This might be the line that needs to be multiplied with the class multiplier?",1,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-10 14:05:36,open,,,"['models: research', 'stat:awaiting response']",2019-02-06 21:55:04
834,tensorflow/models,models,4731,leccyril,mask is not added for mask_rcnn_inception_v2,"
### System information
- **What is the top-level directory of the model you are using**:

_/models/object_detection_

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

NO only launch `data_tools/create_pet_tf_record.py` with parameter _faces_only False (i want mask)_

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

_Debian 9_

- **TensorFlow installed from (source or binary)**:

installation from pip3, python 3.6

- **TensorFlow version (use command below)**:

_1.8.0 cpu installation_

protobuf 3.6


### Describe the problem

_create files as gear_1.jpg /2,3,4....
create files as gear_1.xml
create mask file as gear_1.png

path /images/mask/images
images/mask/test_images
images/mask/annotations/xmls
images/mask/annotations/trimaps (mask png into)

put all files created into above directory

config file is configured to take mask and in PNG format

`data_tools/create_pet_tf_record.py`

i verified there is image/object/mask in record files

train-0000-of-00010.record (several files like this - 10)_

then i launch train.py /eval.py but mask is not displayed (my mask is just line who delimiter object not complete object ,juste delimitation). 

How i can add the mask or see it in eval ? or detection  ?

### Source code / logs

no error log juste bounding box displayed, not mask.... i become crazy!

please no one could make it work with precise command to execute ?  it is not clear and i made exaclty as all tutorial but with pet dataset (i have not json ...)
",14,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-10 08:35:38,open,,,[],2019-02-02 16:19:24
835,tensorflow/models,models,4723,mktozk,Fix MobileNet example notebook,"According to [official source code](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet), inception's preprocessing is used during training time. So same method should be used for evaluation.

Note: this modification will decrease prediction accuracy, but increase fairness.",0,,[],2018-07-09 12:35:04,open,,,['cla: yes'],2018-07-10 02:13:02
836,tensorflow/models,models,4722,Iorest,A problem happened when I run models/research/slim/eval_image_classifier.py,"I am running this program in tensorflow 1.8. Then the power supply to the GPU is stopped, which causes the GPU to not work，unless restart the computer.
However,this problem does not appear when running other programs.

## System information
GPU model and memory:Titan X, 12G(ubuntu18.04); 
Tensorflow version: 1.8.0(GPU)(install from anaconda)
",2,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-07-09 09:39:07,open,,,[],2018-07-12 18:34:03
837,tensorflow/models,models,4719,Jilliansea,"loss decrease to 0 after 200 steps, which isn't nomal","Q：when I train faster_rcnn_resnet101_coco_2018_01_28 on my own dataset(2 classes), my loss decrease to 0 too fast, but when i run eval.py, the precision on tensorboard is always 0.

so i add tf.logging.info(result[""groundtruth""]) in eval_util.py, and say the groundtruth is empty [],
i think the train process is also wrong, but i have no idea where to change, who can help me ,thx a lot`




WARNING:tensorflow:From /data/models/research/object_detection/trainer.py:263: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
INFO:tensorflow:(<tf.Tensor 'Slice_6:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'Slice_18:0' shape=(?, 4) dtype=float32>)
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From /data/models/research/object_detection/core/box_predictor.py:407: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From /data/models/research/object_detection/core/losses.py:317: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See tf.nn.softmax_cross_entropy_with_logits_v2.

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /data/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2017: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_1/bottleneck_v1/shortcut/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_2/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block1/unit_3/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_1/bottleneck_v1/shortcut/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_2/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_3/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block2/unit_4/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_1/bottleneck_v1/shortcut/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_10/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_11/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_12/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_13/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_14/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_15/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_17/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_18/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_19/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_2/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_20/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_21/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_22/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_23/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_3/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_4/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_5/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_6/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_7/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_8/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/block3/unit_9/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_101/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_1/bottleneck_v1/shortcut/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_2/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv3/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv3/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [SecondStageFeatureExtractor/resnet_v1_101/block4/unit_3/bottleneck_v1/conv3/weights/Momentum] is not available in checkpoint
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:736: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:736: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-07-09 15:54:32.237949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:83:00.0
totalMemory: 22.38GiB freeMemory: 22.21GiB
2018-07-09 15:54:32.238053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:83:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from train_logs/model.ckpt-0
INFO:tensorflow:Restoring parameters from train_logs/model.ckpt-0
INFO:tensorflow:Starting Session.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path train_logs/model.ckpt
INFO:tensorflow:Saving checkpoint to path train_logs/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:Starting Queues.
INFO:tensorflow:Starting Queues.
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:global step 1: loss = 1.9438 (12.236 sec/step)
INFO:tensorflow:global step 1: loss = 1.9438 (12.236 sec/step)
INFO:tensorflow:global step 2: loss = 1.8968 (0.769 sec/step)
INFO:tensorflow:global step 2: loss = 1.8968 (0.769 sec/step)
INFO:tensorflow:global step 3: loss = 1.6575 (0.859 sec/step)
INFO:tensorflow:global step 3: loss = 1.6575 (0.859 sec/step)
INFO:tensorflow:global step 4: loss = 1.5547 (0.906 sec/step)
INFO:tensorflow:global step 4: loss = 1.5547 (0.906 sec/step)
INFO:tensorflow:global step 5: loss = 1.3830 (0.882 sec/step)
INFO:tensorflow:global step 5: loss = 1.3830 (0.882 sec/step)
INFO:tensorflow:global step 6: loss = 1.1714 (0.799 sec/step)
INFO:tensorflow:global step 6: loss = 1.1714 (0.799 sec/step)
INFO:tensorflow:global step 7: loss = 0.9430 (0.708 sec/step)
INFO:tensorflow:global step 7: loss = 0.9430 (0.708 sec/step)
INFO:tensorflow:global step 8: loss = 0.9529 (0.637 sec/step)
INFO:tensorflow:global step 8: loss = 0.9529 (0.637 sec/step)
INFO:tensorflow:global step 9: loss = 0.8300 (0.651 sec/step)
INFO:tensorflow:global step 9: loss = 0.8300 (0.651 sec/step)
INFO:tensorflow:global step 10: loss = 0.7526 (0.667 sec/step)
INFO:tensorflow:global step 10: loss = 0.7526 (0.667 sec/step)
INFO:tensorflow:global step 11: loss = 0.6422 (0.643 sec/step)
INFO:tensorflow:global step 11: loss = 0.6422 (0.643 sec/step)
INFO:tensorflow:global step 12: loss = 0.6573 (0.637 sec/step)
INFO:tensorflow:global step 12: loss = 0.6573 (0.637 sec/step)
INFO:tensorflow:global step 13: loss = 0.5969 (0.661 sec/step)
INFO:tensorflow:global step 13: loss = 0.5969 (0.661 sec/step)
INFO:tensorflow:global step 14: loss = 0.5862 (0.662 sec/step)
INFO:tensorflow:global step 14: loss = 0.5862 (0.662 sec/step)
INFO:tensorflow:global step 15: loss = 0.5760 (0.635 sec/step)
INFO:tensorflow:global step 15: loss = 0.5760 (0.635 sec/step)
INFO:tensorflow:global step 16: loss = 0.5324 (0.640 sec/step)
INFO:tensorflow:global step 16: loss = 0.5324 (0.640 sec/step)
INFO:tensorflow:global step 17: loss = 0.5235 (0.636 sec/step)
INFO:tensorflow:global step 17: loss = 0.5235 (0.636 sec/step)
INFO:tensorflow:global step 18: loss = 0.4793 (0.624 sec/step)
INFO:tensorflow:global step 18: loss = 0.4793 (0.624 sec/step)
INFO:tensorflow:global step 19: loss = 0.4736 (0.669 sec/step)
INFO:tensorflow:global step 19: loss = 0.4736 (0.669 sec/step)
INFO:tensorflow:global step 20: loss = 0.4509 (0.635 sec/step)
INFO:tensorflow:global step 20: loss = 0.4509 (0.635 sec/step)
INFO:tensorflow:global step 21: loss = 0.4199 (0.634 sec/step)
INFO:tensorflow:global step 21: loss = 0.4199 (0.634 sec/step)
INFO:tensorflow:global step 22: loss = 0.4185 (0.634 sec/step)
INFO:tensorflow:global step 22: loss = 0.4185 (0.634 sec/step)
INFO:tensorflow:global step 23: loss = 0.3794 (0.759 sec/step)
INFO:tensorflow:global step 23: loss = 0.3794 (0.759 sec/step)",2,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-07-09 08:01:17,open,,,['stat:awaiting response'],2018-07-13 09:41:06
838,tensorflow/models,models,4718,Drasidur,Change Dataset PASCAL VOC2012,"### System information
- **What is the top-level directory of the model you are using**:deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: /
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux - Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: 1.5
- **Bazel version (if compiling from source)**: / 
- **CUDA/cuDNN version**: CUDA 9 - cuDNN 7 
- **GPU model and memory**: Nvidia GTX 1080 TI 
- **Exact command to reproduce**: / 

### Describe the problem
Hello,
I am currently working with the deeplab module and i need to change the annotations (SegmentationClass folder). 
I tried to change the images in the folder and run the code but it didn't worked.
I also tried to generate the tfrecords and then run the code but it didn't worked.
Can please someone explain me how to do this ?

Thanks, Julien.",3,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-07-09 07:40:32,open,,,['stat:awaiting response'],2018-07-16 21:41:24
839,tensorflow/models,models,4716,JingLiJJ,UnrecognizedFlagError: Unknown command line flag 'p',"For Attention_ocr, while executing python -m unittest discover -p '*_test.py', I am getting an error unknown command line flag 'p'. I don't know how to solve it....


======================================================================
ERROR: test_correct_results_on_test_data (demo_inference_test.DemoInferenceTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jj301440/models/research/attention_ocr/python/demo_inference_test.py"", line 49, in test_correct_results_on_test_data
    image_path_pattern)
  File ""/home/jj301440/models/research/attention_ocr/python/demo_inference.py"", line 75, in run
    dataset_name)
  File ""/home/jj301440/models/research/attention_ocr/python/demo_inference.py"", line 59, in create_model
    dataset = common_flags.create_dataset(split_name=FLAGS.split_name)
  File ""/home/jj301440/.conda/envs/jj_27/lib/python2.7/site-packages/tensorflow/python/platform/flags.py"", line 84, in __getattr__
    wrapped(_sys.argv)
  File ""/home/jj301440/.conda/envs/jj_27/lib/python2.7/site-packages/absl/flags/_flagvalues.py"", line 630, in __call__
    name, value, suggestions=suggestions)
UnrecognizedFlagError: Unknown command line flag 'p'

======================================================================
ERROR: test_moving_variables_properly_loaded_from_a_checkpoint (demo_inference_test.DemoInferenceTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jj301440/models/research/attention_ocr/python/demo_inference_test.py"", line 26, in test_moving_variables_properly_loaded_from_a_checkpoint
    dataset_name)
  File ""/home/jj301440/models/research/attention_ocr/python/demo_inference.py"", line 59, in create_model
    dataset = common_flags.create_dataset(split_name=FLAGS.split_name)
  File ""/home/jj301440/.conda/envs/jj_27/lib/python2.7/site-packages/tensorflow/python/platform/flags.py"", line 84, in __getattr__
    wrapped(_sys.argv)
  File ""/home/jj301440/.conda/envs/jj_27/lib/python2.7/site-packages/absl/flags/_flagvalues.py"", line 630, in __call__
    name, value, suggestions=suggestions)
UnrecognizedFlagError: Unknown command line flag 'p'

",3,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-08 12:59:25,open,,,[],2018-09-05 07:12:36
840,tensorflow/models,models,4714,MasterofPLM,load checkpoint files ./ckpt/resnet_v2_50.ckpt,"Thanks for official response, I find a **get_vars_to_restore** function in the program and solve my problem! The reason is biases are not saved in .ckpt.
--------------------------------------------------------------------------------------
When I try to load model resnet_v2_50 like this:
saver.restore(sess, './ckpt/resnet_v2_50.ckpt')
I get the error: ""resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/biases"" not found in checkpoint files ./ckpt/resnet_v2_50.ckpt
I use the res_net given in the program so I don't know why",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-07-08 12:21:39,open,,,['stat:awaiting response'],2018-07-13 14:26:21
841,tensorflow/models,models,4712,philomathic-sponge,How to do multi-scale testing and horizontal flipping during testing?,Could you please give some ideas on how to perform horizontal flipping during test-time and also perform multi-scale testing using this repository for object detection?,2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-07-07 10:30:26,open,,,[],2018-07-08 06:39:30
842,tensorflow/models,models,4711,Ty-Won,Small typo correction,"Changed line 74 from ""There many Tensorflow APIs""->""There are many Tensorflow APIs""",3,,[],2018-07-07 00:58:16,open,,,['cla: yes'],2018-07-09 15:30:31
843,tensorflow/models,models,4709,feixuedudiao,"how can solve the error ""errors_impl.InvalidArgumentError: assertion failed: [`predictions` out of bound] [Condition x < y did not hold element-wise:] [x (mean_iou/confusion_matrix/control_dependency_1:0) = ] [0 0 0...]"", when running deeplabv3+ eval.py at training on ms coco2014 dataset with the mobilenetv2 model ","when I training on ms coco 2014 datasets with the mobilenet pretrained mode, I have the error reported ""errors_impl.InvalidArgumentError: assertion failed: [`predictions` out of bound] [Condition x < y did not hold element-wise:] [x (mean_iou/confusion_matrix/control_dependency_1:0) = ] [0 0 0...]"". I set the cropsize[641, 641] at running eval.py, how can solve this problem ,thanks.

",7,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-07-06 09:29:48,open,,,['stat:awaiting tensorflower'],2019-03-19 06:30:28
844,tensorflow/models,models,4708,zhengtianyu1996,[Deeplab] eval.py does not print the miou value on windows,"### System information
- **What is the top-level directory of the model you are using**:deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10; Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary; binary
- **TensorFlow version (use command below)**:1.7.0(windows); 1.8.0(ubuntu)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:V9.0(windows); V9.0(ubuntu)
- **GPU model and memory**:Titan XP, 16G(windows); GT 710, 2G(ubuntu)
- **Exact command to reproduce**:


### Describe the problem
I prefer to use DeeplabV3+ on windows. DeeplabV3+ may be based on python2, but tensorflow on windows only supports python3. So there is some code I need to rewrite. When I'm running eval.py on ubuntu, terminal can finally print a miou value after Evaluation. But I can't get the value in Spyder on windows although the code is nearly the same.


### Source code / logs

**run eval.py in terminal on Ubuntu:**

root@zhengtianyu-DellPC:/home/zhengtianyu/models-master/research# bash deeplab/local_test.sh
INFO:tensorflow:Evaluating on val set
INFO:tensorflow:Performing single-scale test.
INFO:tensorflow:Eval num images 64
INFO:tensorflow:Eval batch size 4 and num batch 16
INFO:tensorflow:Waiting for new checkpoint at /home/zhengtianyu/models-master/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train
INFO:tensorflow:Found new checkpoint at /home/zhengtianyu/models-master/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train/model.ckpt-30000
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/training/python/training/evaluation.py:301: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
INFO:tensorflow:Graph was finalized.
2018-07-06 15:46:59.329226: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-07-06 15:46:59.395507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-06 15:46:59.395910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GT 710 major: 3 minor: 5 memoryClockRate(GHz): 0.954
pciBusID: 0000:01:00.0
totalMemory: 1.95GiB freeMemory: 1.53GiB
2018-07-06 15:46:59.395925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-07-06 15:46:59.623073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-06 15:46:59.623107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-07-06 15:46:59.623113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-07-06 15:46:59.623318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1307 MB memory) -> physical GPU (device: 0, name: GeForce GT 710, pci bus id: 0000:01:00.0, compute capability: 3.5)
INFO:tensorflow:Restoring parameters from /home/zhengtianyu/models-master/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train/model.ckpt-30000
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting evaluation at 2018-07-06-07:47:02
2018-07-06 15:47:04.837138: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 15:47:14.477138: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 15:47:16.152807: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
INFO:tensorflow:Evaluation [1/16]
INFO:tensorflow:Evaluation [2/16]
INFO:tensorflow:Evaluation [3/16]
INFO:tensorflow:Evaluation [4/16]
INFO:tensorflow:Evaluation [5/16]
INFO:tensorflow:Evaluation [6/16]
INFO:tensorflow:Evaluation [7/16]
INFO:tensorflow:Evaluation [8/16]
INFO:tensorflow:Evaluation [9/16]
INFO:tensorflow:Evaluation [10/16]
INFO:tensorflow:Evaluation [11/16]
INFO:tensorflow:Evaluation [12/16]
INFO:tensorflow:Evaluation [13/16]
INFO:tensorflow:Evaluation [14/16]
INFO:tensorflow:Evaluation [15/16]
INFO:tensorflow:Evaluation [16/16]
INFO:tensorflow:Finished evaluation at 2018-07-06-07:49:01
miou_1.0[0.873398125]


**run eval.py in Spyder on Windows:**

runfile('D:/Postgraduate/models-master/research/deeplab/eval_windows.py')
WARNING:tensorflow:From C:\Users\zty\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
INFO:tensorflow:Evaluating on val set
C:\Users\zty\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\absl\flags\_validators.py:359: UserWarning: Flag --checkpoint_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\zty\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\absl\flags\_validators.py:359: UserWarning: Flag --eval_logdir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\zty\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\absl\flags\_validators.py:359: UserWarning: Flag --dataset_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
INFO:tensorflow:Performing single-scale test.
INFO:tensorflow:Eval num images 320
INFO:tensorflow:Eval batch size 4 and num batch 80
INFO:tensorflow:Waiting for new checkpoint at ./deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train
INFO:tensorflow:Found new checkpoint at ./deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train\model.ckpt-20
WARNING:tensorflow:From C:\Users\zty\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\contrib\training\python\training\evaluation.py:303: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from ./deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train\model.ckpt-20
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting evaluation at 2018-07-06-03:26:12
INFO:tensorflow:Evaluation [8/80]
INFO:tensorflow:Evaluation [16/80]
INFO:tensorflow:Evaluation [24/80]
INFO:tensorflow:Evaluation [32/80]
INFO:tensorflow:Evaluation [40/80]
INFO:tensorflow:Evaluation [48/80]
INFO:tensorflow:Evaluation [56/80]
INFO:tensorflow:Evaluation [64/80]
INFO:tensorflow:Evaluation [72/80]
INFO:tensorflow:Evaluation [80/80]
INFO:tensorflow:Finished evaluation at 2018-07-06-04:24:56
An exception has occurred, use %tb to see the full traceback.

SystemExit

C:\Users\zty\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn(""To exit: use 'exit', 'quit', or Ctrl-D."", stacklevel=1)
",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-07-06 09:16:35,open,,,[],2018-07-06 19:00:55
845,tensorflow/models,models,4703,jjahanip,Extract deep features for each bounding box in object detection module in inception_resnet_v2,"I am interested to have the deep features of each bounding box that is passed to the classifier to be classified. I found [this gist](https://gist.github.com/markdtw/02ece6b90e75832bd44787c03a664e8d) that explained I can get the features using the following tensor:
```python
feat_avg = graph.get_tensor_by_name('SecondStageBoxPredictor/AvgPool:0')
```

But the issue that I have is that in `pipeline.config` there are two stages for bounding box proposals:

1. `first_stage_max_proposals: 300`
2. ```
    second_stage_post_processing {
    ...
    max_detections_per_class: 100
    max_total_detections: 100
    }
    ```

The output_dict boxes, classes and scores contain information of 100 boxes (second_stage_post_processing); but `feat_avg` tensor returns features for 300 bounding boxes (first_stage_max_proposals).

Does anybody know which tensor returns the deep features of detected bounding boxes? Or, is there any helper function I can use to get the features?

I think that it would be nice to add the array of deep features of detected bounding boxes to the `output_dict` in `object_detection_tutorial.ipynb` too.

 
### System information
- **What is the top-level directory of the model you are using**: object_detectoin
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows
- **TensorFlow installed from (source or binary)**: Anaconda
- **TensorFlow version (use command below)**: 1.8
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0 / 7.0.5
- **GPU model and memory**: Geforce GTX 1080 TI
- **Exact command to reproduce**: Could be found [here](https://gist.github.com/markdtw/02ece6b90e75832bd44787c03a664e8d)",3,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-07-05 23:38:11,open,,,[],2019-02-28 12:49:35
846,tensorflow/models,models,4700,rjkrnkmr,Datasets/Imagenet.py_Unable to retrieve the label to names,"Hi, @marksandler2 @nealwu 

I am trying to use the **Imagenet.py** in the datasets and it seems that the below link is not working and i am getting any values for **labels_to_names**.

I have this error as **IOError: [Errno socket error] [Errno 111] Connection refused**

base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/inception/inception/data/'



Could you please help me to fix this??

![image](https://user-images.githubusercontent.com/33498870/42314974-8e94a14e-8046-11e8-8005-3021b79ae4c5.png)


",3,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-07-05 09:18:12,open,,,['stat:awaiting response'],2018-07-09 19:53:12
847,tensorflow/models,models,4695,dketterer,Update eval_util.py: force metrics to file,"This fixes a problem when running evaluation with run_once. Examples get evaluated and logging shows the result, but the summary_writer doesn't write to disk.
Force the summary_writer to write the changes to file.",3,,[],2018-07-04 13:45:40,open,,,['cla: yes'],2018-07-04 13:49:56
848,tensorflow/models,models,4694,xiadeye,No variables to save Error when set the mobilenet-ssd depth_multiplier value<1,"I used the ssd_mobilenet_v1_pets.config and ssd_mobilenet_v1_coco_2017_11_17 pretrained model.I want to adjust the depth_multiplier value in config file,so I set it to 0.5(default is 1),but it raised a error:

ValueError: No variables to save

btw,depth_multiplier=1.0 is normal.

I also asked in stackoverflow:https://stackoverflow.com/questions/51173225/no-variables-to-save-error-when-set-the-mobilenet-ssd-depth-multiplier-value1
this is my config file:

model {
  ssd {
    num_classes: 5
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 540
        width: 360
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.03
              mean: 0.0
            }
          }
          batch_norm {
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v1'
      min_depth: 16
      depth_multiplier: 0.5
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.9997,
          epsilon: 0.001,
        }
      }
    }
    loss {
      classification_loss {
        weighted_sigmoid {
        }
      }
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 0
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 5
        max_total_detections: 20
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  batch_size: 5
  optimizer {
    rms_prop_optimizer: {
      learning_rate: {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.004
          decay_steps: 800720
          decay_factor: 0.95
        }
      }
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""/media/xxlyu/Newsmy1/temp/ssd_mobilenet_v1_coco_2017_11_17/model.ckpt""
  from_detection_checkpoint: true
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  # num_steps: 200000
  num_steps: 2000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
  batch_queue_capacity: 150
  num_batch_queue_threads: 8
  prefetch_queue_capacity: 10
}
",2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-04 12:32:22,open,,,"['stat:awaiting response', 'stat:awaiting tensorflower']",2018-08-18 00:29:33
849,tensorflow/models,models,4691,aliakbarhamzeh1378, Make sure the Op and Kernel are registered in the binary running in this process.,"hello.
sorry for taking your time.
I have a problem
when I trained my custom dataset by any of pre-trained model like [this](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz) 
that give an error to me. the error is :
`python Object_detection_webcam.py `

`Traceback (most recent call last):
  File ""Object_detection_webcam.py"", line 67, in <module>
    tf.import_graph_def(od_graph_def, name='')
  File ""/home/aliakbar/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/home/aliakbar/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 489, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'NonMaxSuppressionV3' in binary running on aliakbar-X556UR. Make sure the Op and Kernel are registered in the binary running in this process.
`

but when I use the original pre-trained model (not my custom model) that good work.

and my system is :
asus-k556
2g graphic nvidia

sorry if my English is bad",6,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-07-03 22:22:12,open,,,[],2019-01-15 03:35:29
850,tensorflow/models,models,4681,live-lively,How to build icp op in vid2depth?,How to build icp op in vid2depth?,4,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-07-03 06:04:43,open,,,[],2018-08-23 21:29:24
851,tensorflow/models,models,4676,erfan-matroid,Training Keypoints Detection Model using Object Detection API,"### System information
- **What is the top-level directory of the model you are using**: `tensorflow/models/research/object_detection/samples/configs/mask_rcnn_*`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not yet
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: `v1.8.0-0-g93bc2e2072 1.8.0`
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: `V9.0.176` / `V7.0.5`
- **GPU model and memory**: Tesla K80, 12GB
- **Exact command to reproduce**: 
```
# From the tensorflow/models/research/ directory
python object_detection/train.py \
    --logtostderr \
    --pipeline_config_path=${PATH_TO_YOUR_PIPELINE_CONFIG} \
    --train_dir=${PATH_TO_TRAIN_DIR}
```
[source](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md#running-the-training-job)

### Describe the problem
Mask-RCNN model can also be trained for keypoints detection and there is a [`keypoint_box_coder`](https://github.com/tensorflow/models/blob/master/research/object_detection/box_coders/keypoint_box_coder.py) for the Mask RCNN model. However there is no documentation about preparing data and training a model for keypoints detection and judging from [this test case](https://github.com/tensorflow/models/blob/master/research/object_detection/core/box_predictor_test.py#L150-L159) it appears that it's not supported yet. Is there a plan to add keypoints detection to this API or are there any pointers to help add the implementation?
",2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-07-02 22:07:41,open,,,['type:feature'],2019-03-20 08:40:58
852,tensorflow/models,models,4672,gdelab,Adding RefineDet to Tensorflow object detection module,"- **What is the top-level directory of the model you are using**:  https://github.com/tensorflow/models/tree/master/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A



### Feature request
An object detection model called [RefineDet ](https://arxiv.org/pdf/1711.06897.pdf) was proposed during CVPR 2018, that seems to achieve state-of-the-art performance at high speed.
 Is it planned to add it to the TF object detection models ?  


",5,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-07-02 14:13:07,open,,,['stat:awaiting owner'],2019-03-13 06:51:33
853,tensorflow/models,models,4662,kalanityL,[vid2depth] tiny readme typos,"Hi Reza and thank you very much for the code. 
2 typos you might want to update in the readme:
- for running the inference, parameter is written ""--video"" instead of ""--kitti_video
- for the wget of the kitti_archives_to_download.txt, link is to the git page. The raw file address instead is 
https://raw.githubusercontent.com/mrharicot/monodepth/master/utils/kitti_archives_to_download.txt

Thank you again.
K. 
",2,"NamedUser(login=""rezama"")","[NamedUser(login=""rezama"")]",2018-07-01 09:36:14,open,,,"['type:bug/performance', 'type:docs']",2018-09-26 17:48:39
854,tensorflow/models,models,4658,live-lively,"How to build icp op in vid2depth? I failed to do it many times, please help me!",,0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-30 06:34:11,open,,"NamedUser(login=""live-lively"")",[],2018-07-01 00:29:18
855,tensorflow/models,models,4655,harshilpatel312,ValueError: Tried to convert 't' to a tensor and failed.,"### System information
- **What is the top-level directory of the model you are using**: models/research/object-detection
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.8.0    
- **GPU model and memory**: GeForce GTX 980 Ti
- **CUDA/cuDNN version**: N/A
- **Bazel version**: N/A
- **Exact command to reproduce**: python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet101_coco.config --num_clones=2 --ps_tasks=1

### Describe the problem
Not able to train the model. It gives a 'ValueError' as shown below. 

Things I tried:
1. I found out a solution from [#3705](https://github.com/tensorflow/models/issues/3705#issuecomment-375563179) which doesn't work for me. Ran `python setup.py build` and `python setup.py install` from the 'models/research/' directory too. Still doesn't work.

2. I commented out Line 169 (+added random print statements), built and installed setup.py, and ran the 'train.py' command again, and it still shows the traceback as show below (notice that it still calls Line 169). I think the problem is it doesn't apply the changes I make to the 'utils/learning_schedules.py' file. Can someone tell me how to apply these changes or if possible, the solution to this entire problem?  

### Traceback
```
Traceback (most recent call last):
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1104, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 235, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 214, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 441, in make_tensor_proto
    _GetDenseDimensions(values)))
ValueError: Argument must be a dense tensor: range(0, 3) - got shape [3], but wanted [].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 524, in _apply_op_helper
    values, as_ref=input_arg.is_ref).dtype.name
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1104, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 235, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 214, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 441, in make_tensor_proto
    _GetDenseDimensions(values)))
ValueError: Argument must be a dense tensor: range(0, 3) - got shape [3], but wanted [].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/home/smr/tensorflow/models/research/object_detection/trainer.py"", line 284, in train
    train_config.optimizer)
  File ""/home/smr/tensorflow/models/research/object_detection/builders/optimizer_builder.py"", line 50, in build
    learning_rate = _create_learning_rate(config.learning_rate)
  File ""/home/smr/tensorflow/models/research/object_detection/builders/optimizer_builder.py"", line 109, in _create_learning_rate
    learning_rate_sequence, config.warmup)
  File ""/home/smr/tensorflow/models/research/object_detection/utils/learning_schedules.py"", line 169, in manual_stepping
    [0] * num_boundaries))
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 2681, in where
    return gen_math_ops.select(condition=condition, x=x, y=y, name=name)
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 6699, in select
    ""Select"", condition=condition, t=x, e=y, name=name)
  File ""/home/smr/tf/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 528, in _apply_op_helper
    (input_name, err))
ValueError: Tried to convert 't' to a tensor and failed. Error: Argument must be a dense tensor: range(0, 3) - got shape [3], but wanted [].
```",7,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-06-29 20:18:06,open,,,[],2018-07-20 18:31:32
856,tensorflow/models,models,4653,twangnh,Object detection API num_clones and must be 1 when using synchronous training,"In the recent update of object detection API, there is modified code lines:
https://github.com/tensorflow/models/blob/2dc6b914b5ce8b98451b48c291ad26c8be3afdc4/research/object_detection/trainer.py#L262-L264

How ever I cant figure out the meaning of this modification, naturally we have multiple workers to sync, and there is multiple clones on each worker.",2,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-06-29 13:05:14,open,,,"['type:docs', 'type:feature']",2018-07-03 22:39:09
857,tensorflow/models,models,4650,manilasmoker,ssd_mobilenet_v1_coco_2017_11_17 training issue,"###  **System information**
- **OS Platform and Distribution:** Ubuntu 16.04.4 LTS
- **TensorFlow installed from**: `pip install tensorflow `
- **TensorFlow version:** 1.8.0
- **Bazel version:** 0.15.0
-  **CUDA/cuDNN version:** I use Tensorflow CPU 
-  **GPU model and memory:** Nvidia Quadro P2000


- **Exact commands to reproduce:** `python train.py --logtosderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config`

` python eval.py         --logtostderr         --checkpoint_dir=training         --eval_dir=eval         --pipeline_config_path=training/ssd_mobilenet_v1_pets.config`

### Describe the problem
I am attempting to train the pretrained ssd_mobilenet_v1_coco_11_06_2017 to detect custom object (following this tutorial https://pythonprogramming.net/training-custom-objects-tensorflow-object-detection-api-tutorial/). Every training (with every dataset of images) I have the following issues:
- even if the total loss converges to nearly 1, the mAP remains stable at 0.0 
- in _Tensorboard images_ all the bounding boxes disappear (see attachments) after the first checkpoint and, when there is a detection, they are so small I can not even see them.

( the attachment images have a very low number of  steps but I uploaded them to show you the common trend; if I let the training reach 30-40k I have the same issues).

Thank you for your help!

### Source code / logs

When I run the **training**

```
giggio@giggio-ws:~/models/research/object_detection$ python train.py --logtosderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config
WARNING:tensorflow:From /home/giggio/models/research/object_detection/trainer.py:260: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
```

When I run the **evaluation**
```
giggio@giggio-ws:~/models/research/object_detection$ python eval.py         --logtostderr         --checkpoint_dir=training         --eval_dir=eval         --pipeline_config_path=training/ssd_mobilenet_v1_pets.config
/home/giggio/models/research/object_detection/utils/visualization_utils.py:25: UserWarning: 
This call to matplotlib.use() has no effect because the backend has already
been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

The backend was *originally* set to 'TkAgg' by the following code:
  File ""eval.py"", line 50, in <module>
    from object_detection import evaluator
  File ""/home/giggio/models/research/object_detection/evaluator.py"", line 24, in <module>
    from object_detection import eval_util
  File ""/home/giggio/models/research/object_detection/eval_util.py"", line 28, in <module>
    from object_detection.metrics import coco_evaluation
  File ""/home/giggio/models/research/object_detection/metrics/coco_evaluation.py"", line 20, in <module>
    from object_detection.metrics import coco_tools
  File ""/home/giggio/models/research/object_detection/metrics/coco_tools.py"", line 47, in <module>
    from pycocotools import coco
  File ""/home/giggio/models/research/pycocotools/coco.py"", line 49, in <module>
    import matplotlib.pyplot as plt
  File ""/home/giggio/.local/lib/python2.7/site-packages/matplotlib/pyplot.py"", line 71, in <module>
    from matplotlib.backends import pylab_setup
  File ""/home/giggio/.local/lib/python2.7/site-packages/matplotlib/backends/__init__.py"", line 16, in <module>
    line for line in traceback.format_stack()


  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
2018-06-29 00:41:11.575407: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:tensorflow:Restoring parameters from training/model.ckpt-120
INFO:tensorflow:Restoring parameters from training/model.ckpt-120
WARNING:root:image 0 does not have groundtruth difficult flag specified
```
**ssd_mobilenet_v1_pets.config**
```
# SSD with Mobilenet v1, configured for Oxford-IIIT Pets Dataset.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for ""PATH_TO_BE_CONFIGURED"" to find the fields that
# should be configured.

model {
  ssd {
    num_classes: 1
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.03
              mean: 0.0
            }
          }
          batch_norm {
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v1'
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.9997,
          epsilon: 0.001,
        }
      }
    }
    loss {
      classification_loss {
        weighted_sigmoid {
          anchorwise_output: true
        }
      }
      localization_loss {
        weighted_smooth_l1 {
          anchorwise_output: true
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 0
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  batch_size: 15
  optimizer {
    rms_prop_optimizer: {
      learning_rate: {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.001
          decay_steps: 800720
          decay_factor: 0.95
        }
      }
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""ssd_mobilenet_v1_coco_2017_11_17/model.ckpt""
  from_detection_checkpoint: true
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""data/train.record""
  }
  label_map_path: ""training/object_detection.pbtxt""
}

eval_config: {
  num_examples: 2000
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""data/test.record""
  }
  label_map_path: ""training/object_detection.pbtxt""
  shuffle: false
  num_readers: 1
}
```
script Python to convert a csv file to a **record** one


```
#!/usr/bin/env python


import os
import io
import pandas as pd
import tensorflow as tf

from PIL import Image
from object_detection.utils import dataset_util
from collections import namedtuple, OrderedDict

flags = tf.app.flags
flags.DEFINE_string('csv_input', '', 'Path to the CSV input')
flags.DEFINE_string('output_path', '', 'Path to output TFRecord')
FLAGS = flags.FLAGS


def class_text_to_int(row_label):
    # if you working with your own classes chenge the label
    if row_label == 'macncheese':
        return 1
    #elif row_label == 'elon musk':
    #    return 2
    else:
        return None


def split(df, group):
    data = namedtuple('data', ['filename', 'object'])
    gb = df.groupby(group)
    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]


def create_tf_example(group, path):
    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:
        encoded_jpg = fid.read()
    encoded_jpg_io = io.BytesIO(encoded_jpg)
    image = Image.open(encoded_jpg_io)
    width, height = image.size


    filename = group.filename.encode('utf8')
    image_format = b'jpg'
    xmins = []
    xmaxs = []
    ymins = []
    ymaxs = []
    classes_text = []
    classes = []

    for index, row in group.object.iterrows():
        xmins.append(row['xmin'] / width)
        xmaxs.append(row['xmax'] / width)
        ymins.append(row['ymin'] / height)
        ymaxs.append(row['ymax'] / height)
        classes_text.append(row['class'].encode('utf8'))
        classes.append(class_text_to_int(row['class']))

    tf_example = tf.train.Example(features=tf.train.Features(feature={
        'image/height': dataset_util.int64_feature(height),
        'image/width': dataset_util.int64_feature(width),
        'image/filename': dataset_util.bytes_feature(filename),
        'image/source_id': dataset_util.bytes_feature(filename),
        'image/encoded': dataset_util.bytes_feature(encoded_jpg),
        'image/format': dataset_util.bytes_feature(image_format),
        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
        'image/object/class/label': dataset_util.int64_list_feature(classes),
    }))
    return tf_example


def main(_):
    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
    path = os.path.join(os.getcwd(), 'images')
    examples = pd.read_csv(FLAGS.csv_input)
    grouped = split(examples, 'filename')
    for group in grouped:
        tf_example = create_tf_example(group, path)
        writer.write(tf_example.SerializeToString())

    writer.close()
    output_path = os.path.join(os.getcwd(), FLAGS.output_path)
    print('Successfully created the TFRecords: {}'.format(output_path))


if __name__ == '__main__':
    tf.app.run()

```


### **Issue Images`**

![screenshot-2018-6-29 tensorboard](https://user-images.githubusercontent.com/20823471/42063984-a7f67d54-7b34-11e8-91fa-f68e1692e36e.png)
![screenshot-2018-6-29 tensorboard 1](https://user-images.githubusercontent.com/20823471/42064001-b9b13eda-7b34-11e8-954a-646e88d7918b.png)
",6,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-06-28 22:59:58,open,,,[],2018-07-21 22:34:32
858,tensorflow/models,models,4649,kmeng01,"Adversarial_text with Bidirectional LSTM on IMDB: Matmul dimensions must be equal, but are 512 and 1024","### System information
- **OS**: Ubuntu 16.04.4 LTS (GNU/Linux 4.4.0-104-generic x86_64)
- **TensorFlow installed from**: `pip install --upgrade tensorflow-gpu`
- **TensorFlow version**: 1.8.0
- **CUDA/cuDNN version**: CUDA Version 9.0.176
- **GPU model and memory**: GTX 1080Ti 11GB GDDR5X
- **Exact command to reproduce**: 
```bash
$ python pretrain.py \
--train_dir=$PTDIR \
--data_dir=$IMDBDIR \
--vocab_size=87007 \
--embedding_dims=256 \
--rnn_cell_size=512 \
--num_candidate_samples=1024 \
--batch_size=64 \
--learning_rate=0.001 \
--learning_rate_decay_factor=0.9999 \
--max_steps=100000 \
--max_grad_norm=1.0 \
--num_timesteps=400 \
--keep_prob_emb=0.5 \
--normalize_embeddings \
--bidir_lstm=True

$ python train_classifier.py \
--train_dir=$TDIR \
--pretrained_model_dir=$PTDIR \
--data_dir=$IMDBDIR \
--vocab_size=87007 \
--embedding_dims=256 \
--rnn_cell_size=512 \
--cl_num_layers=1 \
--cl_hidden_size=30 \
--batch_size=64 \
--learning_rate=0.0005 \
--learning_rate_decay_factor=0.9998 \
--max_steps=15000 \
--max_grad_norm=1.0 \
--num_timesteps=400 \
--keep_prob_emb=0.5 \
--normalize_embeddings \
--adv_training_method=vat \
--perturb_norm_length=5.0 \
--bidir_lstm=True
```

### Describe the problem
I am attempting to train the adversarial_text model with a bidirectional lstm on the example `IMDB` dataset. The uni-lstm has already worked correctly. To accomplish this, I have:
1. copied over the `pretrain.py` and `train_classifier.py` commands
2. added the `bidir_lstm=True` flag
3. changed the `rnn_cell_size` to `512` and `embedding_dims` to `256`, which is what the [paper](https://arxiv.org/pdf/1605.07725.pdf) for this code states is the difference in hyperparameters between training the uni-lstm and bidir-lstm.

When I run `pretrain.py`, the code works perfectly fine. However, afterwards, when I run `train_classifier.py`, the code errors with the stack trace shown below. 
I have noticed that if I run `pretrain.py` with the `bidir_lstm=True` flag and `train_classifier.py` without, it works fine. But is that the intended behavior? Thank you for the assistance.

### Stack track log
```bash
Traceback (most recent call last):
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1567, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 512 and 1024 for 'cl_logits/dense_2/Tensordot/MatMul' (op: 'MatMul') with input shapes: [51200,512], [1024,30].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_classifier.py"", line 68, in <module>
    tf.app.run()
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train_classifier.py"", line 58, in main
    train_op, loss, global_step = model.classifier_training()
  File ""/home/exx/vat-repro/gan-text/models/research/adversarial_text/graphs.py"", line 157, in classifier_training
    loss = self.classifier_graph()
  File ""/home/exx/vat-repro/gan-text/models/research/adversarial_text/graphs.py"", line 432, in classifier_graph
    embedded, return_intermediates=True)
  File ""/home/exx/vat-repro/gan-text/models/research/adversarial_text/graphs.py"", line 545, in cl_loss_from_embedding
    logits = self.layers['cl_logits'](lstm_out)
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py"", line 314, in __call__
    output = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 717, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py"", line 634, in call
    mask=masks)
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py"", line 805, in _run_internal_graph
    layer.call(computed_tensor, **kwargs))
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/layers/core.py"", line 157, in call
    [0]])
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 3004, in tensordot
    ab_matmul = matmul(a_reshape, b_reshape)
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 2122, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4279, in mat_mul
    name=name)
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1734, in __init__
    control_input_ops)
  File ""/home/exx/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1570, in _create_c_op
    raise ValueError(str(e))
ValueError: Dimensions must be equal, but are 512 and 1024 for 'cl_logits/dense_2/Tensordot/MatMul' (op: 'MatMul') with input shapes: [51200,512], [1024,30].
````
",2,"NamedUser(login=""a-dai"")","[NamedUser(login=""a-dai"")]",2018-06-28 19:57:36,open,,,"['models: research', 'stat:awaiting response']",2019-02-06 21:54:00
859,tensorflow/models,models,4648,MilesZhao,adversarial text code does output right thing,"### System information
run a toy example in my mac
 

### Describe the problem
My test set has 101 samples (12 classes) at my first cross validation fold. I use 10-cross validation. So I need to collect the predications and corresponding real labels for calculating combined F1-score. 

At the source code of adversarial text, I adjusted two functions as following:
` 
 def eval_graph(self, dataset='test'):

    inputs = _inputs(dataset, pretrain=False)
    
    embedded = self.layers['embedding'](inputs.tokens)
    _, next_state, logits, _ = self.cl_loss_from_embedding(
        embedded, inputs=inputs, return_intermediates=True)

    if FLAGS.single_label:
      indices = tf.stack([tf.range(FLAGS.batch_size), inputs.length - 1], 1)
      labels = tf.expand_dims(tf.gather_nd(inputs.labels, indices), 1)
      weights = tf.expand_dims(tf.gather_nd(inputs.weights, indices), 1)
    else:
      labels = inputs.labels
      weights = inputs.weights
    eval_ops = {
        'accuracy':
            tf.contrib.metrics.streaming_accuracy(
                layers_lib.predictions(logits), labels, weights)
    }

    with tf.control_dependencies([inputs.save_state(next_state)]):
      acc, acc_update = eval_ops['accuracy']
      acc_update = tf.identity(acc_update)
      eval_ops['accuracy'] = (acc, acc_update)

    var_restore_dict = make_restore_average_vars_dict()
    preds_op = layers_lib.predictions(logits)
    actual_op = tf.identity(labels)
    return eval_ops, var_restore_dict, preds_op, actual_op
`

and

`
def run_eval(eval_ops,preds_op,actuals_op, summary_writer, saver):
 
  sv = tf.train.Supervisor(
      logdir=FLAGS.eval_dir, saver=None, summary_op=None, summary_writer=None)
  with sv.managed_session(
      master=FLAGS.master, start_standard_services=False) as sess:
    if not restore_from_checkpoint(sess, saver):
      return
    sv.start_queue_runners(sess)

    metric_names, ops = zip(*eval_ops.items())
    value_ops, update_ops = zip(*ops)

    value_ops_dict = dict(zip(metric_names, value_ops))
    cnt = 0
    # Run update ops
    p = []
    flatten = lambda l: [item for sublist in l for item in sublist]
    num_batches = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))
    tf.logging.info('Running %d batches for evaluation.', num_batches)
    for i in range(num_batches):
      if (i + 1) % 10 == 0:
        tf.logging.info('Running batch %d/%d...', i + 1, num_batches)
      if (i + 1) % 50 == 0:
        _log_values(sess, value_ops_dict)
      _, preds,actuals =sess.run([update_ops,preds_op,actuals_op])
      cnt = cnt + len(preds)
      
      if i!=26:
        p = p + flatten(actuals)
      else:
        p = p + flatten(actuals[:5])
    print('total number is :', cnt,len(p))
    counter=collections.Counter(p)
    print(counter)
    _log_values(sess, value_ops_dict, summary_writer=summary_writer)
`
It turns out variable 

`actuals` 

does not output the right labels and its numbers like the input.

Input: Counter({5: 30, 1: 20, 11: 14, 3: 8, 6: 7, 2: 7, 10: 4, 8: 3, 4: 3, 0: 2, 9: 2, 7: 1}) 5:30 means class of '5' has 30 samples in test set.
`actual`: Counter({0: 41, 1: 20, 11: 14, 3: 8, 6: 7, 10: 4, 8: 3, 9: 2, 7: 1, 4: 1}). Miss two classes and the numbers also partially incorrect. 

I suspect the function of `tf.contrib.training.batch_sequences_with_states` messes around my code. But I do not know how to fix it.

One more thing is my batch size is 32 and there are 101 examples. SO the inputs will be exhausted at last batch  (only 5 sample left). I found that I still can get 32 samples at final batch. When I count it.

Thanks.


code link https://github.com/tensorflow/models/tree/master/research/adversarial_text",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-06-28 19:01:04,open,,,['stat:awaiting response'],2018-06-29 01:05:54
860,tensorflow/models,models,4645,saurabhhjjain,train data format for lexnet_nc,I am trying to train lexnet_nc model. Upto now I figured out that relation feature should be named as `rel_id` with int64 datatype and words feature should be named as `lemmas` with data type int64. But in input we have two words W1 and W2. So how to store these two words in training data to lookup embeddings file ?,1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-06-28 09:57:31,open,,,['stat:awaiting response'],2018-06-28 19:15:00
861,tensorflow/models,models,4638,anuragsodhi,Issue in /research/global_objectives code,"The loss_layers_example.py returns the following error:

""TypeError: Fetch argument dict_values([<tf.Variable 'precision_at_recall/lambdas:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'precision_at_recall/div:0' shape=(1,) dtype=float32>, <tf.Tensor 'precision_at_recall/Sum_2:0' shape=(1,) dtype=float32>, <tf.Tensor 'precision_at_recall/Sum_3:0' shape=(1,) dtype=float32>]) has invalid type <class 'dict_values'>, must be a string or Tensor. (Can not convert a dict_values into a Tensor or Operation.""

Also, the loss layers code is not functioning as it should. I tried P@R and R@P but it does not train the model such that target value of precision or recall is kept as the specified level.

What is the top-level directory of the model you are using: /tensorflow/models/tree/master/research/global_objectives
Have I written custom code: No
OS Platform and Distribution: Windows 10, Python 3.6.5
TensorFlow installed from: pip installed official tensorflow 1.8 
TensorFlow version: v1.8
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A - Intel Onboard graphics 
Exact command to reproduce: ran loss_layers_example.py",4,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-06-26 19:52:55,open,,,[],2019-03-01 01:34:07
862,tensorflow/models,models,4635,snkreddy,https://github.com/tensorflow/models/tree/master/official/wide_deep/wide_deep.py is not present as it is mentioned in documentation,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-26 16:02:36,open,,,[],2018-06-27 01:03:52
863,tensorflow/models,models,4633,SinDongHwan,tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4 
- **CUDA/cuDNN version**:  CUDA 8.0 & cuDNN 6
- **GPU model and memory** : Geforce GTX 1080ti & 11GB

### Describe the problem
I faced error if i tests model( [""faster_rcnn_resnet101_coco""](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)).
On the past, when i had tested that model, not error. and when i tested my model, not error.
i worked same source code. i just modify model_ckpt & label path.

### Source code / logs
`2018-06-26 13:21:31.368972: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: NodeDef mentions attr 'identical_element_shapes' not in Op<name=TensorArrayV3; signature=size:int32 -> handle:resource, flow:float; attr=dtype:type; attr=element_shape:shape,default=<unknown>; attr=dynamic_size:bool,default=false; attr=clear_after_read:bool,default=true; attr=tensor_array_name:string,default=""""; is_stateful=true>; NodeDef: Preprocessor/map/TensorArray = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Preprocessor/map/TensorArrayUnstack/strided_slice). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
2018-06-26 13:21:31.369024: E tensorflow/core/common_runtime/executor.cc:643] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'identical_element_shapes' not in Op<name=TensorArrayV3; signature=size:int32 -> handle:resource, flow:float; attr=dtype:type; attr=element_shape:shape,default=<unknown>; attr=dynamic_size:bool,default=false; attr=clear_after_read:bool,default=true; attr=tensor_array_name:string,default=""""; is_stateful=true>; NodeDef: Preprocessor/map/TensorArray = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Preprocessor/map/TensorArrayUnstack/strided_slice). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
         [[Node: Preprocessor/map/TensorArray = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Preprocessor/map/TensorArrayUnstack/strided_slice)]]
Traceback (most recent call last):
  File ""/home/actionrecog/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
    return fn(*args)
  File ""/home/actionrecog/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""/home/actionrecog/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'identical_element_shapes' not in Op<name=TensorArrayV3; signature=size:int32 -> handle:resource, flow:float; attr=dtype:type; attr=element_shape:shape,default=<unknown>; attr=dynamic_size:bool,default=false; attr=clear_after_read:bool,default=true; attr=tensor_array_name:string,default=""""; is_stateful=true>; NodeDef: Preprocessor/map/TensorArray = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Preprocessor/map/TensorArrayUnstack/strided_slice). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
         [[Node: Preprocessor/map/TensorArray = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Preprocessor/map/TensorArrayUnstack/strided_slice)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""server.py"", line 121, in <module>
    main()
  File ""server.py"", line 100, in main
    (boxes, scores, classes, num) = sess.run([detection_boxes, detection_scores, detection_classes, num_detections],feed_dict={image_tensor: image_np_expanded}
  File ""/home/actionrecog/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/actionrecog/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/actionrecog/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/actionrecog/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'identical_element_shapes' not in Op<name=TensorArrayV3; signature=size:int32 -> handle:resource, flow:float; attr=dtype:type; attr=element_shape:shape,default=<unknown>; attr=dynamic_size:bool,default=false; attr=clear_after_read:bool,default=true; attr=tensor_array_name:string,default=""""; is_stateful=true>; NodeDef: Preprocessor/map/TensorArray = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Preprocessor/map/TensorArrayUnstack/strided_slice). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
         [[Node: Preprocessor/map/TensorArray = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Preprocessor/map/TensorArrayUnstack/strided_slice)]]

Caused by op 'Preprocessor/map/TensorArray', defined at:
  File ""server.py"", line 121, in <module>
    main()
  File ""server.py"", line 49, in main
    tf.import_graph_def(od_grapth_def, name="""")
  File ""/home/actionrecog/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""/home/actionrecog/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/actionrecog/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): NodeDef mentions attr 'identical_element_shapes' not in Op<name=TensorArrayV3; signature=size:int32 -> handle:resource, flow:float; attr=dtype:type; attr=element_shape:shape,default=<unknown>; attr=dynamic_size:bool,default=false; attr=clear_after_read:bool,default=true; attr=tensor_array_name:string,default=""""; is_stateful=true>; NodeDef: Preprocessor/map/TensorArray = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Preprocessor/map/TensorArrayUnstack/strided_slice). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
         [[Node: Preprocessor/map/TensorArray = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Preprocessor/map/TensorArrayUnstack/strided_slice)]]
`
",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-06-26 04:33:26,open,,,['stat:awaiting response'],2018-06-26 12:56:48
864,tensorflow/models,models,4627,austinmw,[Feature Request] half_precision=true in object detection config files,I don't see any simple way to switch back and forth between 32 and 16 bit while running models from the object detection model zoo. I think adding the ability to set this in the model config file and then propagate it throughout would be really helpful.,2,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-06-25 19:19:06,open,,,['stat:awaiting owner'],2018-07-11 16:16:04
865,tensorflow/models,models,4626,azemZejnil,Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel''is:issue is:open ,"So, I downloaded whole repository and I tried to run the 'android' application:
tensorflow-master/tensorflow/examples/android
but I get the runtime error from title.

there is this line of code in build.gradle:
def nativeBuild='bazel';
and when I change it, for example, to 'none', I get problem with camera.
However, I would like to solve this bazel buildSystem because it seems that is default.

Thanks in advance",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-06-25 18:37:33,open,,,['stat:awaiting response'],2018-06-26 07:23:58
866,tensorflow/models,models,4620,aeloyq,A Few Works Of Quantization on object_detection,"I added a few new features for supporting Quantization on object_detection and also fixed some small bugs in this project.
1. add .gitignore for ignoring proto generated files
2. suppress double logging info
3. use growth gpu memory allocating strategy when evaluating and training on one gpu.
4. solved several py2/py3 compatibility problems by six moudule
5. fix the bug that learning_rate may sometimes not be added to summary and tensorboard
6. add new options when restore variables from a checkpoint
(Which may important for quantize. tf.contrib.quantize.create_training_graph slows down the speed of training. So you may train the model without graph rewriter. After the first-time's convergence of model, the rewriter will be activated and then keep on training to refine the accuracy. However, the variable restoration from previous checkpoint will restore the variable generated by optimizer which slows down the second-time's convergence because of low learning rate.)
7. support for exporting tflite compatible subgraph
8. add a few documents and config samples for quantize

Tests passed:
faster_rcnn_meta_arch_test.py
rfcn_meta_arch_test.py
ssd_meta_arch_test.py
model_lib_test.py
trainer_test.py
eval_util_test
exporter_test.py",0,,[],2018-06-25 11:47:09,open,,,['cla: yes'],2018-07-12 09:51:05
867,tensorflow/models,models,4611,rohitsaluja22,Regarding optimizer and parameters for learning in attention_ocr,"Hey @alexgorban  and @bitfort  and others, 

This is regarding optimizer and parameters for learning in attention_ocr.
In the paper, you mention stochastic gradient descent optimizer to work and following parameters:-

--label_smoothing=0.9 --momentum=0.75 -- weight  decay=0.00004 --lstm_state_clip_value=10 --learning_rate=0.002 --learning_rate_decay=0.1 --decay_after=1,200,000 


Whereas in implementation(in train.py) you use tf.train.MomentumOptimizer and following parameters:-

--label_smoothing=0.1 --momentum=0.9 -- weight  decay=0.00004 --lstm_state_clip_value=10  --learning_rate=0.004  


Is the later better than former(which I think so)? Or shall I change to former to replicate results or try it on my data?


Yours
Rohit",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-23 01:15:06,open,,,[],2018-06-24 06:40:56
868,tensorflow/models,models,4609,NHDLarry,Remove outdated tf.contrib.data and replace with appropriate tf.data …,Remove outdated tf.contrib.data references and replace with appropriate tf.data API's so tutorial will run in current versions of TF.  ,0,,[],2018-06-22 22:24:07,open,,,['cla: yes'],2018-06-23 23:24:44
869,tensorflow/models,models,4605,R-Miner,Can we save this optimized tenosrrt model for inerence purposes instead of running it again and again?,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-06-22 19:19:32,open,,,['stat:awaiting response'],2018-06-26 02:01:29
870,tensorflow/models,models,4600,HalfLemon01,"Why? when i train faster-rcnn , it's global step view twice.","###Situation
when i try object-detection  model
### Describe the problem
before today, the model and project is normal. however  my server exchange place. appear the problem.


### Source code / logs


INFO:tensorflow:Restoring parameters from /usr/local/tensorflow_models/models-master/research/object_detection/MODEL_FASTER/model.ckpt
INFO:tensorflow:Restoring parameters from /usr/local/tensorflow_models/models-master/research/object_detection/MODEL_FASTER/model.ckpt
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path ./research/object_detection/TRAIN_EVAL/TRAIN_RESULT/model.ckpt
INFO:tensorflow:Saving checkpoint to path ./research/object_detection/TRAIN_EVAL/TRAIN_RESULT/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:global step 1: loss = 2.1770 (10.038 sec/step)
INFO:tensorflow:global step 1: loss = 2.1770 (10.038 sec/step)
INFO:tensorflow:global step 2: loss = 1.7647 (0.673 sec/step)
INFO:tensorflow:global step 2: loss = 1.7647 (0.673 sec/step)
INFO:tensorflow:global step 3: loss = 1.8594 (1.599 sec/step)
INFO:tensorflow:global step 3: loss = 1.8594 (1.599 sec/step)
INFO:tensorflow:global step 4: loss = 1.7591 (0.168 sec/step)
INFO:tensorflow:global step 4: loss = 1.7591 (0.168 sec/step)
INFO:tensorflow:global step 5: loss = 1.4718 (0.150 sec/step)
INFO:tensorflow:global step 5: loss = 1.4718 (0.150 sec/step)
INFO:tensorflow:global step 6: loss = 1.4042 (0.123 sec/step)
INFO:tensorflow:global step 6: loss = 1.4042 (0.123 sec/step)
INFO:tensorflow:global step 7: loss = 1.4349 (0.145 sec/step)
INFO:tensorflow:global step 7: loss = 1.4349 (0.145 sec/step)
INFO:tensorflow:global step 8: loss = 1.1939 (0.120 sec/step)
INFO:tensorflow:global step 8: loss = 1.1939 (0.120 sec/step)
INFO:tensorflow:global step 9: loss = 1.2642 (0.127 sec/step)
INFO:tensorflow:global step 9: loss = 1.2642 (0.127 sec/step)

every step  run  twice.",2,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-06-22 04:20:52,open,,,"['models: research', 'stat:awaiting response']",2019-02-06 21:53:01
871,tensorflow/models,models,4599,89douner,How to check generalization (validation loss) ?,"### System information
-What is the top-level directory of the model you are using:  /models/research
-Have I written custom code: N/A
-OS Platform and Distribution: Linux Ubuntu 16.04
-TensorFlow installed from: pip installation
-TensorFlow version: Tensorflow: 1.5.0
-Bazel version: N/A
-CUDA/cuDNN version: CUDA:9.0/ cuDNN:7.1
- **GPU model and memory**: GeForce GTX 1070 Ti
- **Exact command to reproduce**:python3 train.py --logtostderr --train_dir=training/ --
### Describe the problem
When I train ssd_inception_v2 model, as below process, that message ""global step~ loss ~"" is showed on command screen. is the loss validation loss?
In addition, like below second image that shows a various type of loss, I couldn't know whether the loss is training loss or validation loss.
How could I check the validation loss of my training model??

### Source code / logs
![2018-06-22 12-26-51](https://user-images.githubusercontent.com/31752297/41756632-7a96bbf6-7618-11e8-9192-305377341bcf.png)
![2018-06-22 12-27-30](https://user-images.githubusercontent.com/31752297/41756638-7fd2a74c-7618-11e8-98fd-5f8d90410796.png)
",3,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-06-22 03:37:58,open,,,[],2018-09-13 06:08:12
872,tensorflow/models,models,4598,velociraptor111,NEW FEATURE: Added a flag option for users to choose of how much GPU memory training will occupy,"Having this feature will allow users to run both training and testing on a single GPU.
Users will have the ability to partition a fraction of their GPU for training and also run evaluation loop at the same time in order to monitor the accuracy/recall.",2,,[],2018-06-21 22:01:00,open,,,['cla: yes'],2018-06-28 14:37:21
873,tensorflow/models,models,4597,KSLHacks,Changing file doc for sample consistency,"Changing `.pbtxt` to `.config` to align with samples provided in `research\object_detection\samples\configs\`

Consistency will help avoid confusion when supplying pipeline file to train and eval scripts.

Files:
`research/object_detection/eval.py`
`research/object_detection/train.py`

___
Note:
`export_inference_graph.py` uses the consistent extensions proposed.
```
Example Usage:
--------------
python export_inference_graph \
    --input_type image_tensor \
    --pipeline_config_path path/to/ssd_inception_v2.config \
    --trained_checkpoint_prefix path/to/model.ckpt \
    --output_directory path/to/exported_model_directory
```

",0,,[],2018-06-21 20:06:52,open,,,['cla: yes'],2018-06-21 20:57:35
874,tensorflow/models,models,4593,santanu-2018,UnicodeDecodeError,"### System Information

-  **What is the top-level directory of the model I am using**: /models/research/differential_privacy/multiple_teachers 

- **Have I written custom code**: No

- **OS Platform and Distribution**:Linux Ubuntu 18.04

- **TensorFlow installed from**: conda

- **TensorFlow version**:1.8.0

- **Bazel version**: N/A

- **CUDA/cuDNN version**:N/A (I dont have GPU)

- **GPU model and memory**: N/A

- **Exact command to reproduce**:Train two teachers,
python train_teachers.py --nb_teachers=250 --teacher_id=0 --dataset=mnist --train_dir=train_dir --data_dir=data_dir
python train_teachers.py --nb_teachers=250 --teacher_id=1 --dataset=mnist --train_dir=train_dir --data_dir=data_dir

### Describe the Problem
Hi, I am getting an error when I try to run the code of differential_privacy using multiple teachers. When I run the command 
""python train_teachers.py --nb_teachers=250 --teacher_id=0 --dataset=mnist --train_dir=train_dir --data_dir=data_dir"", 
I get the following error:

### Source Code/Logs

Traceback (most recent call last):
  File ""train_teachers.py"", line 104, in <module>
    tf.app.run()
  File ""/home/santanu/miniconda/envs/fedlearn/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train_teachers.py"", line 101, in main
    assert train_teacher(FLAGS.dataset, FLAGS.nb_teachers, FLAGS.teacher_id)
  File ""train_teachers.py"", line 63, in train_teacher
    train_data, train_labels, test_data, test_labels = input.ld_mnist()
  File ""/home/santanu/Downloads/models-master/research/differential_privacy/multiple_teachers/input.py"", line 386, in ld_mnist
    train_data = extract_mnist_data(local_urls[0], 60000, 28, 1)
  File ""/home/santanu/Downloads/models-master/research/differential_privacy/multiple_teachers/input.py"", line 274, in extract_mnist_data
    return np.load(file_obj)
  File ""/home/santanu/.local/lib/python3.6/site-packages/numpy/lib/npyio.py"", line 404, in load
    magic = fid.read(N)
  File ""/home/santanu/miniconda/envs/fedlearn/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 127, in read
    pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))
  File ""/home/santanu/miniconda/envs/fedlearn/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 95, in _prepare_value
    return compat.as_str_any(val)
  File ""/home/santanu/miniconda/envs/fedlearn/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 113, in as_str_any
    return as_str(value)
  File ""/home/santanu/miniconda/envs/fedlearn/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 86, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte

Any suggestion how to overcome it?
",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-06-21 11:33:45,open,,,['stat:awaiting response'],2018-06-22 04:22:56
875,tensorflow/models,models,4592,victorherbemontagne,"Error on object segmentation script to generate TFrecords file, create_pet_tf_record.py or in the doc","### System information
- **What is the top-level directory of the model you are using**: model/research/object_detection
- **Have I written custom code **: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: No GPU
- **Exact command to reproduce**:


### Describe the problem

In the tutorial for training a new model on object segmentation,  you should provide masks of course and from the doc:

> The spatial dimensions of each mask must agree with the image. Each mask has only a single channel, 
> and the pixel values are either 0 (background) or 1 (object mask).

But in the files provided to produce TFRecords for the pet records dataset `create_pet_tf_record.py` (in `dataset_util/`), in the function `dict_to_tf_example()`, when the mask is opened, `nonbackground_indices_x `and
`nonbackground_indices_y` are computed as:
```
nonbackground_indices_x = np.any(mask_np != 2, axis=0)
nonbackground_indices_y = np.any(mask_np != 2, axis=1)

```
Shouldn't be `mask_np == 1` ? As we are checking which pixel of the mask is not nul to compute:

```
nonzero_x_indices = np.where(nonbackground_indices_x)
nonzero_y_indices = np.where(nonbackground_indices_y)

```

Actually I was modifying this script to adapt it to my dataset (LFW part label) and I use the `face_only= False` to compute the bounding box around the face in each image.  My  bounding box where always all nul. I think that, as this script as a default of `face_only= True` and when this option is used the script doesn't use nonzero_x_indices  and nonzero_y_indices,  it was painless.

I perhaps miss something as I really don't understand how this error could have happened, waiting for your appraisal to eventually prepare a PR.

Cheers !

Victor",12,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-06-21 09:00:22,open,,,[],2018-09-14 13:25:02
876,tensorflow/models,models,4591,shmilyhh,Skip_thoughts word expansion,"### System information
- **What is the top-level directory of the model you are using**: N/A
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
```# Path to checkpoint file or a directory containing checkpoint files (the script
# will select the most recent).
CHECKPOINT_PATH=""${HOME}/skip_thoughts/model/train""

# Vocabulary file generated by the preprocessing script.
SKIP_THOUGHTS_VOCAB=""${HOME}/skip_thoughts/data/vocab.txt""

# Path to downloaded word2vec model.
WORD2VEC_MODEL=""${HOME}/skip_thoughts/googlenews/GoogleNews-vectors-negative300.bin""

# Output directory.
EXP_VOCAB_DIR=""${HOME}/skip_thoughts/exp_vocab""

# Build the vocabulary expansion script.
cd tensorflow-models/skip_thoughts
bazel build -c opt //skip_thoughts:vocabulary_expansion

# Run the vocabulary expansion script.
bazel-bin/skip_thoughts/vocabulary_expansion \
  --skip_thoughts_model=${CHECKPOINT_PATH} \
  --skip_thoughts_vocab=${SKIP_THOUGHTS_VOCAB} \
  --word2vec_model=${WORD2VEC_MODEL} \
  --output_dir=${EXP_VOCAB_DIR}
```

### Describe the problem
When I tried to expand the vocabulary, the model loaded based on function _load_skip_thoughts_embeddings() in vocabulary_expansion.py file is an old one. Its shape is (20000, 620). Since in #1268 @cshallue said, 

> Vocabulary expansion has already been done on the pre-trained models

However, the loaded model is one without vocabulary expansion. I think the reason is that the checkpoint file in [pretrained model folder](http://download.tensorflow.org/models/skip_thoughts_uni_2017_02_02.tar.gz) is not correct so that the following code did not load correct word_embedding.
```
reader = tf.train.NewCheckpointReader(checkpoint_file) 
word_embedding = reader.get_tensor(""word_embedding"")
```

### Source code / logs
```
INFO:tensorflow:Loading skip-thoughts embedding matrix from /home/hao/Workplace/HaoXu/Data/skip_thoughts/pretrained/skip_thoughts_uni_2017_02_02/model.ckpt-501424
INFO:tensorflow:Loaded skip-thoughts embedding matrix of shape (20000, 620)
INFO:tensorflow:Reading vocabulary from /home/hao/Workplace/HaoXu/Data/skip_thoughts/pretrained/skip_thoughts_uni_2017_02_02/vocab.txt
index is 677444
origin index is 393736
index is 918116
origin index is 393736
INFO:tensorflow:Read vocabulary of size 930912
Traceback (most recent call last):
  File ""/home/hao/Workplace/HaoXu/Library/models/research/skip_thoughts/bazel-bin/skip_thoughts/vocabulary_expansion.runfiles/__main__/skip_thoughts/vocabulary_expansion.py"", line 218, in <module>
    tf.app.run()
  File ""/home/hao/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/hao/Workplace/HaoXu/Library/models/research/skip_thoughts/bazel-bin/skip_thoughts/vocabulary_expansion.runfiles/__main__/skip_thoughts/vocabulary_expansion.py"", line 202, in main
    word2vec)
  File ""/home/hao/Workplace/HaoXu/Library/models/research/skip_thoughts/bazel-bin/skip_thoughts/vocabulary_expansion.runfiles/__main__/skip_thoughts/vocabulary_expansion.py"", line 151, in _expand_vocabulary
    skip_thoughts_vocab[w] for w in shared_words
IndexError: index 929024 is out of bounds for axis 0 with size 20000
```",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-20 19:00:37,open,,,[],2018-07-27 13:39:56
877,tensorflow/models,models,4590,kcsodetz,Add code blocks under the bazel portion of Manual Installation,"Under ""Manual Installation"" section, the code blocks for the bazel portion were missing ",3,,[],2018-06-20 15:59:56,open,,"NamedUser(login=""kcsodetz"")",['cla: yes'],2019-03-02 23:38:41
878,tensorflow/models,models,4586,jngiam,PiperOrigin-RevId: 201234832,,0,,[],2018-06-19 23:42:09,open,,,['cla: yes'],2018-06-19 23:42:11
879,tensorflow/models,models,4584,axelsly,Added caret to Inception-ResNet-v2 in README.md,"This symbol notes that post-processing input size to the model must be of size
299 per the linked arxiv paper.",0,,[],2018-06-19 21:13:51,open,,,['cla: yes'],2018-06-19 21:13:53
880,tensorflow/models,models,4581,bigsnarfdude,correct base_name to checkpoint_name,reference to base_name appears to be incorrect. changing base_name to checkpoint_name resolves unpacking error and broken notebook example,4,,[],2018-06-19 17:44:47,open,,,['cla: yes'],2018-06-19 17:54:20
881,tensorflow/models,models,4576,jitongjitong,Mobilenet SSD network structure write by tensorflow,"I know that there's a mobilenet SSD training program in the project, but the network structure  is read directly from the CKPT file.  I want to know if there is a  network structure of mobilenet SSD written directly by tensorflow
",3,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-19 07:18:11,open,,,[],2018-11-05 05:56:06
882,tensorflow/models,models,4575,HaiDang2001VN,Inference Problem of Vid2Depth? @rezema,"Hello @rezama 
I have cloned your Vid2Depth and modify the inference to make it able to read  my custom video (as RGB-frames).
I used the pretrained model (model-119496) to infere it and I got the results with 100 for all elements, I have check the io but it seems like ok (picture below are result with your depth output format)
Please tell me if you need more info to identify my problem, @rezama !
I'm using Tensorflow 1.8 with Python 3.5 (other necessary python packages are latest up-to-date on PyPI) and I ran on Windows 10 (use Git bash cmd).
![0000](https://user-images.githubusercontent.com/22542993/41581903-50ef0466-73ca-11e8-9473-6d0174cc9969.png)
Thanks in advance.",5,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-06-19 07:15:49,open,,,[],2019-01-19 02:28:39
883,tensorflow/models,models,4569,shmilyhh,Skip-Thoughts: decode error in encoder_manager.load_model,"### System information
- **What is the top-level directory of the model you are using**: N/A
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
- **Python version: Python 3.6.4 :: Anaconda custom (64-bit)
```
ipython  # Launch iPython.

In [0]:

# Imports.
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
import os.path
import scipy.spatial.distance as sd
from skip_thoughts import configuration
from skip_thoughts import encoder_manager

In [1]:
# Set paths to the model.
VOCAB_FILE = ""/path/to/vocab.txt""
EMBEDDING_MATRIX_FILE = ""/path/to/embeddings.npy""
CHECKPOINT_PATH = ""/path/to/model.ckpt-9999""
# The following directory should contain files rt-polarity.neg and
# rt-polarity.pos.
MR_DATA_DIR = ""/dir/containing/mr/data""

In [2]:
# Set up the encoder. Here we are using a single unidirectional model.
# To use a bidirectional model as well, call load_model() again with
# configuration.model_config(bidirectional_encoder=True) and paths to the
# bidirectional model's files. The encoder will use the concatenation of
# all loaded models.
encoder = encoder_manager.EncoderManager()
encoder.load_model(configuration.model_config(),
                   vocabulary_file=VOCAB_FILE,
                   embedding_matrix_file=EMBEDDING_MATRIX_FILE,
                   checkpoint_path=CHECKPOINT_PATH)
```

### Describe the problem
I am using python3.6 and trying to load vocabulary from vocab.txt file. But it occurs error. The error is in lineno 66. It seems like the type of line is str which does not have decode() function.


### Source code / logs
The output error:
```
AttributeError                            Traceback (most recent call last)
<ipython-input-11-d2fd1b801170> in <module>()
      3                    vocabulary_file=VOCAB_FILE,
      4                    embedding_matrix_file=EMBEDDING_MATRIX_FILE,
----> 5                    checkpoint_path=CHECKPOINT_PATH)
      6

~/Workplace/HaoXu/Library/models/research/skip_thoughts/skip_thoughts/encoder_manager.py in load_model(self, model_config, vocabulary_file, e
mbedding_matrix_file, checkpoint_path)
     64     with tf.gfile.GFile(vocabulary_file, mode=""r"") as f:
     65       lines = list(f.readlines())
---> 66     reverse_vocab = [line.decode(""utf-8"").strip() for line in lines]
     67     tf.logging.info(""Loaded vocabulary with %d words."", len(reverse_vocab))
     68

~/Workplace/HaoXu/Library/models/research/skip_thoughts/skip_thoughts/encoder_manager.py in <listcomp>(.0)
     64     with tf.gfile.GFile(vocabulary_file, mode=""r"") as f:
     65       lines = list(f.readlines())
---> 66     reverse_vocab = [line.decode(""utf-8"").strip() for line in lines]
     67     tf.logging.info(""Loaded vocabulary with %d words."", len(reverse_vocab))
     68

AttributeError: 'str' object has no attribute 'decode'
```",6,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-18 17:12:09,open,,,[],2018-07-04 14:45:13
884,tensorflow/models,models,4568,kurita236,"In slim/export_inference_graph.py, I can only 3 channels can be specified.","### Describe the problem

In slim/export_inference_graph.py, I can only 3 channels can be specified, because it is hard-coded.

### System information
- **What is the top-level directory of the model you are using**:https://github.com/tensorflow/models/blob/master/research/slim/export_inference_graph.py
- **Have I written custom code**:No
- **OS Platform and Distribution**: Windows 7 64 bit
- **TensorFlow installed from (source or binary)**: pip install --upgrade tensorflow==1.8.0
- **TensorFlow version (use command below)**: (""b'v1.8.0-0-g93bc2e2072'"", '1.8.0')
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**:None",6,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada""), NamedUser(login=""k-w-w"")]",2018-06-18 09:18:13,open,,,['stat:awaiting owner'],2018-10-23 20:05:47
885,tensorflow/models,models,4567,dreamgonfly,Update gen_vocab.py to be python3 compatible,"Dictionary's .iteritem() raises an error in Python3.
Replaced it with iteritem from six library.",3,,[],2018-06-18 08:57:22,open,,,['cla: yes'],2018-10-22 07:07:28
886,tensorflow/models,models,4562,natha1008,"Deeplab v3+ I WANT TO TRAIN MY OWN DATASET. TF_INITIALK_CHECKPOINT, INITIALIZE_LAST_LAYER OR EXCLUDE_LIST  ","System information
What is the top-level directory of the model you are using: deeplab
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.6.0
Bazel version (if compiling from source):
CUDA/cuDNN version: 9.0/7.0.4
GPU model and memory: 1080Ti * 2 , 10Gb * 2
Exact command to reproduce:

Describe the problem

This is a feature request. I am trying to train the Deeplab model with my own dataset. I already trained a small set using tf_initial_checkpoint=pascal_train_aug.tar.gz  and initialize_last_layer= False and i got it a MioU accuracy around 60%. However, I need to use my whole dataset that is almost 6K images, and i would like to initialize my new train with the weights from my last train (60%). But, I'm not sure which parameters i have to change.  If, should be like this:
tf_initial_checkpoint = last_checkpoint (60%)
Initialize_last_layer = true 
I also, read some issues with exclude_list={""global_step""}
I don't know if in my case i need to change for exclude_list={""global_step"",""logits""} 
I'm very confusing with this parameters. Help Pleaseee. 
any idea about my issue   
",0,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-06-18 02:39:58,open,,,[],2018-06-18 17:03:25
887,tensorflow/models,models,4557,Li-Yun,How do use the pre-trained resnet model to classify my own images,"I am trying to apply the pre-trained model on this website to do some experiments given my own testing images. However, I did not figure out which the script I should use? It seems that imagenet_main.py is to train the model. Does anyone have a similar experience?

Thanks,
Li-Yun",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-06-17 00:17:07,open,,,['stat:awaiting response'],2018-06-17 06:35:59
888,tensorflow/models,models,4553,shmilyhh,Skip-Thought Vectors: Alternative Data Source to expand vocabulary,"### System information
- **What is the top-level directory of the model you are using**: N/A
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**:  N/A
- **Bazel version (if compiling from source)**:  N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:N/A

### Describe the problem
For the vocabulary expansion in Skip-Thought Vectors, could I use the [GloVe word2vec for Twitter](https://nlp.stanford.edu/projects/glove/)? Since I want to build Skip-Thought Vectors for Tweets.
Thanks.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
N/A",1,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-06-15 18:54:03,open,,,['stat:awaiting response'],2018-06-18 01:54:30
889,tensorflow/models,models,4550,hellbago,Using a boolean placeholder for switching between train and validation (attention ocr model),"I want to evaluate the result of the training procedure with a validation set. For doing so i set a boolean placeholder that i fill with True for the training step and with False for the validation step. 
In the function get_input() in sequence_layer.py I've replaced the orginal code with this one, where self.is_training is a class variable inizialed with my boolean placeholder
  ```
def get_input(self, prev, i):
    """"""A wrapper for get_train_input and get_eval_input.

    Args:
      prev: output tensor from previous step of the RNN. A tensor with shape:
        [batch_size, num_char_classes].
      i: index of a character in the output sequence.

    Returns:
      A tensor with shape [batch_size, ?] - depth depends on implementation
      details.
    """"""
    # if self.is_training():
    #   return self.get_train_input(prev, i)
    # else:
    #   return self.get_eval_input(prev, i)

    return tf.cond(tf.equal(self._is_training,True),
                   lambda:self.get_train_input(prev, i),
                   lambda:self.get_eval_input(prev, i))
```
The problem is that when I feed the value True inside the placeholder I get this error: **tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value**

While using False the program run correctly. Any hint for solving this issue? @alexgorban ",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-06-15 11:18:16,open,,,['stat:awaiting response'],2018-06-15 18:53:26
890,tensorflow/models,models,4548,hyybuaa,[bug] Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available,"InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'InceptionV4/Logits/Predictions': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
         [[Node: InceptionV4/Logits/Predictions = Softmax[T=DT_FLOAT, _device=""/device:GPU:0""](InceptionV4/Logits/Logits/BiasAdd)]]",5,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-06-15 08:33:52,open,,,"['models: research', 'stat:awaiting response']",2019-02-01 22:37:08
891,tensorflow/models,models,4547,twangnh,How could resnet101 first conv take 360MB and first conv in first block take 2.13GB memory with 2x600x900x3 input,"Hi! I'm using tf object detection API, with res101 backbone, training on COCO, all are official setting, except I added run_metadata in summary, so I can view the memory consumption, with input of images of batch size 2, which is 2x600x900x3, I got very large memory consumption:
For the first 7x7 conv, it is 360MB:
![image](https://user-images.githubusercontent.com/18298163/41458316-d82eb40e-70b8-11e8-96b1-545d2a51cf8d.png)

For the first 1x1 conv in first block, it is 2.13GB:

![image](https://user-images.githubusercontent.com/18298163/41458417-1eafcc74-70b9-11e8-8d0c-df39c4517a3b.png)
I find some post says those measurement are only watermark of the memory usage, not counting deallocating, and also conv are implemented with im2col, so addition memory is needed for the operation, so what is meaning of this feature, how to use it for diagnose memory usage, is there any suggesting tool or way to effectively mapping actual GPU usage to model architecture? I have tried tfdbg, it can print tensor sizes, but seems it only count all named tensors, not useful for actual GPU memory usage.",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-15 08:33:37,open,,,['stat:awaiting response'],2018-06-15 18:53:13
892,tensorflow/models,models,4543,darthsuogles,Fix MobileNet example notebook issue,"Fix variable name issue in MobileNet example notebook.
Make sure all `base_name` variables are renamed to `checkpoint_name`.",4,,[],2018-06-15 02:08:16,open,,,['cla: yes'],2018-12-17 21:15:43
893,tensorflow/models,models,4539,dheera,add gamma and hue adjustments to image augmentation,"Allows input data to be augmented using power law (gain, gamma) and hue adjustments.
Flags added with defaults set to not performing any adjustments.",4,,[],2018-06-14 19:58:39,open,,,['cla: yes'],2018-07-27 05:48:07
894,tensorflow/models,models,4538,edisonchensy," Import ""object_detection/protos/ssd.proto"" was not found or had errors.","I got this problem when i try to make installation. please help me.


edison@amax-server:~/google_TF_object_detection/models/research$ protoc object_detection/protos/*.proto --python_out=.
object_detection/protos/ssd.proto:87:3: Expected ""required"", ""optional"", or ""repeated"".
object_detection/protos/ssd.proto:87:12: Expected field name.
object_detection/protos/model.proto: Import ""object_detection/protos/ssd.proto"" was not found or had errors.
object_detection/protos/model.proto:12:5: ""Ssd"" is not defined.





",4,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-06-14 14:58:03,open,,,"['models: research', 'stat:awaiting response', 'stat:awaiting tensorflower']",2019-02-01 22:32:35
895,tensorflow/models,models,4536,heidongxianhau,"when python train.py ,if occured this .","WARNING:tensorflow:From /home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/trainer.py:260: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 0 to match input file shards.
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/trainer.py"", line 274, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""train.py"", line 121, in get_next
    dataset_builder.build(config)).get_next()
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py"", line 186, in build
    process_fn, config.input_path[:], input_reader_config)
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/utils/dataset_util.py"", line 141, in read_dataset
    sloppy=config.shuffle))
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 901, in apply
    dataset = transformation_func(self)
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/interleave_ops.py"", line 167, in _apply_fn
    buffer_output_elements, prefetch_input_elements)
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/interleave_ops.py"", line 70, in __init__
    self._map_func.add_to_graph(ops.get_default_graph())
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/python/framework/function.py"", line 486, in add_to_graph
    self._create_definition_if_needed()
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/python/framework/function.py"", line 321, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/python/framework/function.py"", line 338, in _create_definition_if_needed_impl
    outputs = self._func(*inputs)
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/interleave_ops.py"", line 58, in tf_map_func
    dataset = map_func(nested_args)
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/python/data/ops/readers.py"", line 90, in __init__
    filenames, dtypes.string, name=""filenames"")
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 923, in convert_to_tensor
    as_ref=False)
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1013, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/deeplearning/anaconda2/envs/py27_tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 857, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(""arg0:0"", shape=(), dtype=float32)'",7,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-06-14 13:36:03,open,,,[],2019-03-02 09:08:04
896,tensorflow/models,models,4535,Shiro-LK,Tensorflow API multi channel,"### System information
- **What is the top-level directory of the model you are using**: ssd mobilenet v2
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: anaconda
- **TensorFlow version (use command below)**: 1.8
- **Bazel version (if compiling from source)**: N/AN
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: k80
- **Exact command to reproduce**:


### Describe the problem
I am trying to train the Tensorflow API on multiple channels (different from 3 channels). I followed this tutorial : https://github.com/minhnhat93/tf_object_detection_multi_channels but it seems there is a little problem. I tried to understand where the problem comes from and maybe it comes from  line 196 in data_decoder/tf_example_decoder.py : 

```
image = slim_example_decoder.ItemHandlerCallback(
          keys=['image/encoded', 'image/height', 'image/width', 'image/channels'],
          func=self._read_image
        )

  def _read_image(self, keys_to_tensors):
      image_encoded = keys_to_tensors['image/encoded']
      height = keys_to_tensors['image/height']
      width = keys_to_tensors['image/width']
      channels = keys_to_tensors['image/channels']
      to_shape = tf.cast(tf.stack([height, width, channels]), tf.int32)
      image = tf.reshape(tf.decode_raw(image_encoded, tf.uint8), to_shape)
      return image

```
Moreover, I think it could be a good feature to add to the original tensorflow repository. It can be usefull when we work on grayscale image(1 channel) or more channels (3<)
### Source code / logs
```
INFO:tensorflow:Error reported to Coordinator: Nan in summary histogram for: ModelVars/BoxPredictor_5/ClassPredictor_depthwise/BatchNorm/moving_variance
	 [[Node: ModelVars/BoxPredictor_5/ClassPredictor_depthwise/BatchNorm/moving_variance = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ModelVars/BoxPredictor_5/ClassPredictor_depthwise/BatchNorm/moving_variance/tag, BoxPredictor_5/ClassPredictor_depthwise/BatchNorm/moving_variance/read)]]
	 [[Node: FeatureExtractor/MobilenetV2/expanded_conv_5/expand/kernel/Regularizer/l2_regularizer/_551 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_3224_...egularizer"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op 'ModelVars/BoxPredictor_5/ClassPredictor_depthwise/BatchNorm/moving_variance', defined at:
  File ""models_multi/research/object_detection/train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""models_multi/research/object_detection/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/content/models_multi/research/object_detection/trainer.py"", line 338, in train
    model_var.op.name, model_var))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/summary.py"", line 203, in histogram
    tag=tag, values=values, name=scope)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_logging_ops.py"", line 283, in histogram_summary
    ""HistogramSummary"", tag=tag, values=values, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

```

EDIT: I also tried on image of 3 channels (instead of 4), it works for a while but I got after 400 steps the same error.
EDIT2 : Training on faster RCNN resnet50 (for 3 channels) seems to work better. But I cannot load the pre trained weight I get this error :  

`WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/weights/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/beta/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/gamma/Momentum] is not available in checkpoint
WARNING:root:Variable [FirstStageFeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/weights/Momentum] is not available in checkpoint`

EDIT 3: with Faster RCNN (4 channels), I got this error : 

```
Traceback (most recent call last):
  File ""models_multi/research/object_detection/train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""models_multi/research/object_detection/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/content/models_multi/research/object_detection/trainer.py"", line 401, in train
    saver=saver)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 747, in train
    master, start_standard_services=False, config=session_config) as sess:
  File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py"", line 828, in stop
    ignore_live_threads=ignore_live_threads)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py"", line 989, in managed_session
    start_standard_services=start_standard_services)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py"", line 726, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 279, in prepare_session
    config=config)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 207, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1802, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [7,7,4,64] rhs shape= [7,7,3,64]
	 [[Node: save/Assign_404 = Assign[T=DT_FLOAT, _class=[""loc:@FirstStageFeatureExtractor/resnet_v1_50/conv1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](FirstStageFeatureExtractor/resnet_v1_50/conv1/weights, save/RestoreV2:404)]]

Caused by op 'save/Assign_404', defined at:
  File ""models_multi/research/object_detection/train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""models_multi/research/object_detection/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/content/models_multi/research/object_detection/trainer.py"", line 361, in train
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1338, in __init__
    self.build()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 835, in _build_internal
    restore_sequentially, reshape)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 494, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 185, in restore
    self.op.get_shape().is_fully_defined())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py"", line 283, in assign
    validate_shape=validate_shape)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 60, in assign
    use_locking=use_locking, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access


```

I need probably to change some other parameters but I do not know where exactly.",4,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-06-14 12:08:08,open,,,[],2018-06-18 11:59:03
897,tensorflow/models,models,4533,liangxiao05,A display bug  when training ssd model with pretrained mobilenet ,"Please go to the end lines,the training informations are printed twice when training ssd model with pretrained mobilenet_v2_1.0_224 (downloaded at https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)

**concerned train_configs in pipeline.config:
  fine_tune_checkpoint: ""/XXXXXXXX/mobilenet_v2_0.75_224/mobilenet_v2_0.75_224.ckpt""
  fine_tune_checkpoint_type:  ""classification""
  num_steps: 200000**

INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/moving_mean] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/moving_variance] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:736: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:736: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-06-14 12:05:36.095070: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-06-14 12:05:36.578163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:05:00.0
totalMemory: 11.90GiB freeMemory: 11.37GiB
2018-06-14 12:05:36.822183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:09:00.0
totalMemory: 11.90GiB freeMemory: 11.74GiB
2018-06-14 12:05:36.823139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix
2018-06-14 12:05:36.826738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1 
2018-06-14 12:05:36.826748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y Y 
2018-06-14 12:05:36.826752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   Y Y 
2018-06-14 12:05:36.826760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN Xp, pci bus id: 0000:05:00.0, compute capability: 6.1)
2018-06-14 12:05:36.826766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from /home/eco/test_trans_learning3/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt
INFO:tensorflow:Restoring parameters from /home/eco/test_trans_learning3/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt
INFO:tensorflow:Starting Session.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path /home/eco/test_trans_learning3/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/eco/test_trans_learning3/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
**INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:global step 1: loss = 85.5841 (20.408 sec/step)
INFO:tensorflow:global step 1: loss = 85.5841 (20.408 sec/step)
INFO:tensorflow:global step 2: loss = 78.7843 (0.534 sec/step)
INFO:tensorflow:global step 2: loss = 78.7843 (0.534 sec/step)
INFO:tensorflow:global step 3: loss = 72.4312 (0.785 sec/step)
INFO:tensorflow:global step 3: loss = 72.4312 (0.785 sec/step)
INFO:tensorflow:global step 4: loss = 68.7148 (0.732 sec/step)
INFO:tensorflow:global step 4: loss = 68.7148 (0.732 sec/step)
INFO:tensorflow:global step 5: loss = 67.2820 (0.754 sec/step)
INFO:tensorflow:global step 5: loss = 67.2820 (0.754 sec/step)
INFO:tensorflow:global step 6: loss = 66.5206 (0.772 sec/step)
INFO:tensorflow:global step 6: loss = 66.5206 (0.772 sec/step)
INFO:tensorflow:global step 7: loss = 64.2771 (0.728 sec/step)
INFO:tensorflow:global step 7: loss = 64.2771 (0.728 sec/step)
INFO:tensorflow:global step 8: loss = 63.6546 (0.800 sec/step)
INFO:tensorflow:global step 8: loss = 63.6546 (0.800 sec/step)
INFO:tensorflow:global step 9: loss = 61.9096 (0.898 sec/step)
INFO:tensorflow:global step 9: loss = 61.9096 (0.898 sec/step)
INFO:tensorflow:global step 10: loss = 60.7216 (0.944 sec/step)
INFO:tensorflow:global step 10: loss = 60.7216 (0.944 sec/step)
INFO:tensorflow:global step 11: loss = 60.6402 (0.751 sec/step)
INFO:tensorflow:global step 11: loss = 60.6402 (0.751 sec/step)
INFO:tensorflow:global step 12: loss = 58.8133 (0.721 sec/step)
INFO:tensorflow:global step 12: loss = 58.8133 (0.721 sec/step)
INFO:tensorflow:global step 13: loss = 58.4023 (0.713 sec/step)
INFO:tensorflow:global step 13: loss = 58.4023 (0.713 sec/step)
INFO:tensorflow:global step 14: loss = 57.8213 (0.821 sec/step)
INFO:tensorflow:global step 14: loss = 57.8213 (0.821 sec/step)
INFO:tensorflow:global step 15: loss = 56.2347 (0.966 sec/step)
INFO:tensorflow:global step 15: loss = 56.2347 (0.966 sec/step)**
",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-14 06:46:18,open,,,[],2018-06-14 19:03:33
898,tensorflow/models,models,4531,kekeller,[deeplab] Key aspp1_depthwise/BatchNorm/beta not found in checkpoint,"I am getting an issue running eval on a custom dataset. When I actually load the checkpoints into the model, I get an error about missing parameters (Momentum, etc) in the checkpoint file. 

### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 3.10.0-693.11.6.el7.x86_64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.8.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: cuda/9.0.176   cudnn/7.0
- **GPU model and memory**: (not sure, running on university cluster)
- **Exact command to reproduce**:

> DATASET=""./datasets/tfrecord""
> 
> TRAIN_LOGDIR=""./datasets/exp/train""
>EVAL_LOGDIR=""./datasets/exp/val""
> 
> python ./eval.py \
>   --logtostderr \
>   --eval_split=""val"" \
>   --model_variant=""xception_65"" \
>   --atrous_rates=6 \
>   --atrous_rates=12 \
>   --atrous_rates=18 \
>   --output_stride=16 \
>   --decoder_output_stride=4 \
>   --eval_crop_size=256 \
>   --eval_crop_size=256 \
>   --checkpoint_dir=""${TRAIN_LOGDIR}"" \
>   --eval_logdir=""${EVAL_LOGDIR}"" \
>   --dataset_dir=""${DATASET}"" \
>   --max_number_of_evaluations=1


### Describe the problem
I am using a custom dataset from a project. We have 960 jpg images with corresponding png masks. We also have 180 validation images (mask and image combo). We have two classes and all the png masks are converted to binary label images (checked in matlab and all the masks are just 0 or 1). 

I exported these images as tfrecord using the scripts in the dataset folder. Though I had to hardcode this part to properly get the expected output:

>   FLAGS.image_format = ""jpg""
>   FLAGS.label_format = ""png""

I then trained the model using:

> 
> DATASET=""./datasets/tfrecord""
> 
> TRAIN_LOGDIR=""./datasets/exp/train""
> 
> CKPT=""./xception/model.ckpt""
> 
> NUM_ITERATIONS=20
> python ./train.py \
>   --logtostderr \
>   --train_split=""train"" \
>   --model_variant=""xception_65"" \
>   --output_stride=16 \
>   --train_crop_size=256 \
>   --train_crop_size=256 \
>   --train_batch_size=4 \
>   --training_number_of_steps=""${NUM_ITERATIONS}"" \
>   --tf_initial_checkpoint=""${CKPT}""  \
>   --fine_tune_batch_norm=true \
>   --train_logdir=""${TRAIN_LOGDIR}"" \
>   --dataset_dir=""${DATASET}""

The training output is what I expected : 

> 
> INFO:tensorflow:Training on train set
> WARNING:tensorflow:From /cluster/home/kellerke/.local/lib64/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:731: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> 
> Future major versions of TensorFlow will allow gradients to flow
> into the labels input on backprop by default.
> 
> See tf.nn.softmax_cross_entropy_with_logits_v2.
> 
> INFO:tensorflow:Initializing model from path: ./xception_65/model.ckpt
> WARNING:tensorflow:Variable image_pooling/weights missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable image_pooling/BatchNorm/gamma missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable image_pooling/BatchNorm/beta missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable image_pooling/BatchNorm/moving_mean missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable image_pooling/BatchNorm/moving_variance missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable aspp0/weights missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable aspp0/BatchNorm/gamma missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable aspp0/BatchNorm/beta missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable aspp0/BatchNorm/moving_mean missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable aspp0/BatchNorm/moving_variance missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable concat_projection/weights missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable concat_projection/BatchNorm/gamma missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable concat_projection/BatchNorm/beta missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable concat_projection/BatchNorm/moving_mean missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable concat_projection/BatchNorm/moving_variance missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable logits/semantic/weights missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable logits/semantic/biases missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable image_pooling/weights/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable image_pooling/BatchNorm/gamma/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable image_pooling/BatchNorm/beta/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable aspp0/weights/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable aspp0/BatchNorm/gamma/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable aspp0/BatchNorm/beta/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable concat_projection/weights/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable concat_projection/BatchNorm/gamma/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable concat_projection/BatchNorm/beta/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable logits/semantic/weights/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:Variable logits/semantic/biases/Momentum missing in checkpoint ./xception_65/model.ckpt
> WARNING:tensorflow:From /cluster/home/kellerke/.local/lib64/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
> Instructions for updating:
> Please switch to tf.train.MonitoredTrainingSession
> 2018-06-13 22:48:58.842532: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
> INFO:tensorflow:Restoring parameters from ./xception_65/model.ckpt
> INFO:tensorflow:Running local_init_op.
> INFO:tensorflow:Done running local_init_op.
> INFO:tensorflow:Starting Session.
> INFO:tensorflow:Saving checkpoint to path ./datasets/exp/train/model.ckpt
> INFO:tensorflow:Starting Queues.
> INFO:tensorflow:global_step/sec: 0
> INFO:tensorflow:Recording summary at step 0.
> INFO:tensorflow:global step 10: loss = 3.2193 (4.939 sec/step)
> INFO:tensorflow:global step 20: loss = 3.1794 (4.912 sec/step)
> INFO:tensorflow:Stopping Training.
> INFO:tensorflow:Finished training! Saving model to disk.

The issue arises when I try to evaluate the model. 

I added the following lines to the segmentation_dataset.py file:

> _SOYBEAN_INFORMATION = DatasetDescriptor(
>     splits_to_sizes={
>         'train': 960,
>         'val': 180,
>     },
>     num_classes=2,
>     ignore_label=255,
> )
> _DATASETS_INFORMATION = {
>     'cityscapes': _CITYSCAPES_INFORMATION,
>     'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
>     'ade20k': _ADE20K_INFORMATION,
>     'soybean': _SOYBEAN_INFORMATION
> }

And I changed the eval.py dataset setting here:

> flags.DEFINE_string('dataset', 'soybean',
>                     'Name of the segmentation dataset.')
> 

When I run the evaluation code the system just freezes waiting for a checkpoint:

> DATASET=""./datasets/tfrecord""
> 
> TRAIN_LOGDIR=""./datasets/exp/train""
>EVAL_LOGDIR=""./datasets/exp/val""
> 
> CKPT=""./xception/model.ckpt""
> 
> python ./eval.py \
>   --logtostderr \
>   --eval_split=""val"" \
>   --model_variant=""xception_65"" \
>   --atrous_rates=6 \
>   --atrous_rates=12 \
>   --atrous_rates=18 \
>   --output_stride=16 \
>   --decoder_output_stride=4 \
>   --eval_crop_size=256 \
>   --eval_crop_size=256 \
>   --checkpoint_dir=""${TRAIN_LOGDIR}"" \
>   --eval_logdir=""${EVAL_LOGDIR}"" \
>   --dataset_dir=""${DATASET}"" \
>   --max_number_of_evaluations=1

### Issue

> INFO:tensorflow:Evaluating on val set
> INFO:tensorflow:Performing single-scale test.
> INFO:tensorflow:Eval num images 180
> INFO:tensorflow:Eval batch size 1 and num batch 180
> INFO:tensorflow:Waiting for new checkpoint at ./datasets/exp/train
> INFO:tensorflow:Found new checkpoint at ./datasets/exp/train/model.ckpt-21
> WARNING:tensorflow:From /cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.py:301: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
> Instructions for updating:
> Please switch to tf.train.get_or_create_global_step
> INFO:tensorflow:Graph was finalized.
> 2018-06-13 21:47:51.795975: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
> INFO:tensorflow:Restoring parameters from ./datasets/exp/train/model.ckpt-21
> 2018-06-13 21:47:52.012550: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key aspp1_depthwise/BatchNorm/beta not found in checkpoint
> Traceback (most recent call last):
>   File ""./eval.py"", line 176, in <module>
>     tf.app.run()
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
>     _sys.exit(main(argv))
>   File ""./eval.py"", line 169, in main
>     eval_interval_secs=FLAGS.eval_interval_secs)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/evaluation.py"", line 301, in evaluation_loop
>     timeout=timeout)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.py"", line 445, in evaluate_repeatedly
>     session_creator=session_creator, hooks=hooks) as session:
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 816, in __init__
>     stop_grace_period_secs=stop_grace_period_secs)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 539, in __init__
>     self._sess = _RecoverableSession(self._coordinated_creator)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1002, in __init__
>     _WrappedSession.__init__(self, self._create_session())
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1007, in _create_session
>     return self._sess_creator.create_session()
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 696, in create_session
>     self.tf_sess = self._session_creator.create_session()
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 467, in create_session
>     init_fn=self._scaffold.init_fn)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 279, in prepare_session
>     config=config)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 191, in _restore_checkpoint
>     saver.restore(sess, checkpoint_filename_with_path)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1802, in restore
>     {self.saver_def.filename_tensor_name: save_path})
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 900, in run
>     run_metadata_ptr)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
>     feed_dict_tensor, options, run_metadata)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
>     run_metadata)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.NotFoundError: Key aspp1_depthwise/BatchNorm/beta not found in checkpoint
> 	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
> 
> Caused by op u'save/RestoreV2', defined at:
>   File ""./eval.py"", line 176, in <module>
>     tf.app.run()
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
>     _sys.exit(main(argv))
>   File ""./eval.py"", line 169, in main
>     eval_interval_secs=FLAGS.eval_interval_secs)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/evaluation.py"", line 301, in evaluation_loop
>     timeout=timeout)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.py"", line 445, in evaluate_repeatedly
>     session_creator=session_creator, hooks=hooks) as session:
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 816, in __init__
>     stop_grace_period_secs=stop_grace_period_secs)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 539, in __init__
>     self._sess = _RecoverableSession(self._coordinated_creator)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1002, in __init__
>     _WrappedSession.__init__(self, self._create_session())
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1007, in _create_session
>     return self._sess_creator.create_session()
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 696, in create_session
>     self.tf_sess = self._session_creator.create_session()
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 458, in create_session
>     self._scaffold.finalize()
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 212, in finalize
>     self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 910, in _get_saver_or_default
>     saver = Saver(sharded=True, allow_empty=True)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1338, in __init__
>     self.build()
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1347, in build
>     self._build(self._filename, build_save=True, build_restore=True)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1384, in _build
>     build_save=build_save, build_restore=build_restore)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 829, in _build_internal
>     restore_sequentially, reshape)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 525, in _AddShardedRestoreOps
>     name=""restore_shard""))
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 472, in _AddRestoreOps
>     restore_sequentially)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 886, in bulk_restore
>     return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
>     shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
>     op_def=op_def)
>   File ""/cluster/home/kellerke/models/research/deeplab/env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
>     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
> 
> NotFoundError (see above for traceback): Key aspp1_depthwise/BatchNorm/beta not found in checkpoint
> 	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]


",6,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-06-13 20:56:58,open,,,"['stat:awaiting owner', 'stat:awaiting response']",2019-01-22 19:11:28
899,tensorflow/models,models,4525,cheneeheng,[deeplab + cityscape] Frozen inference graph provided is slower than a self-exported graph.,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.7.0
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 5.1
- **GPU model and memory**: Quadro M4000
- **Exact command to reproduce**: NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
1. Ran inference on cityscapes image using the frozen inference graph provided and got a run time of ~3.7s
2. Ran inference on cityscapes image using the frozen inference graph exported using the checkpoint provided and got a run time of ~0.9s
(Although both are running less than the 5s runtime given in model_zoo.md)

I have used the official codes to do the export. Arguments passed to it is shown below :
--checkpoint_path=""/path/to/model.ckpt""
--export_path=""/path/to/frozen_inference_graph.pb""
--model_variant=""xception_65""
--atrous_rates=6
--atrous_rates=12
--atrous_rates=18
--output_stride=16
--decoder_output_stride=4
--num_classes=19
--crop_size=1025
--crop_size=2049
--inference_scales=1.0

The only change i made is pulling the inference code out of ipynb, added time.time() for timing, and added some util function to loop through the directory of images.

A quick check with tensorboard shows that my exported graph has just 1216 nodes compared to the 1311 nodes in the graph provided.

**Question**:
1. Is the difference in runtime due to my export argument being wrong?
2. Is the frozen graph created from another checkpoint leading to the difference in runtime?
3. Am i missing something here?

Thank you.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-06-13 18:08:41,open,,,[],2018-06-15 15:54:00
900,tensorflow/models,models,4524,bleedingfight,Error to train on my data ,"My environment;
cuda V9.0.176,cudnn 7.0,tensorflow-gpu 1.8,Titan X x8
My data:20000 num picture,2labels,object is lane(4 pixle width ),other is background.Every picture size is[512,512],segmentation annotation picture is grayscale picture,lane pix is 1,other's(background) is 0.
my alter include:

    splits_to_sizes={
        'train': 18000,
        'trainval': 20000,
        'val': 2000,
    },
    num_classes=2,
    ignore_label=255,
)```
and add:
```_DATASETS_INFORMATION = {
    'cityscapes': _CITYSCAPES_INFORMATION,
    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
    'lane_seg': _LANE_SEG_INFORMATION,
    'ade20k': _ADE20K_INFORMATION,
}

train:

NUM_ITERATIONS=10000
python ""${WORK_DIR}""/train.py \
  --logtostderr \
  --initialize_last_layer=False \
  --num_clones=1 \
  --last_layers_contain_logits_only=False \
  --dataset='lane_seg' \
  --train_split=""trainval"" \
  --model_variant=""xception_65"" \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=16 \
  --decoder_output_stride=4 \
  --train_crop_size=513 \
  --train_crop_size=513 \
  --train_batch_size=4 \
  --training_number_of_steps=""${NUM_ITERATIONS}"" \
  --fine_tune_batch_norm=true \
  --tf_initial_checkpoint=""${INIT_FOLDER}/deeplabv3_pascal_train_aug/model.ckpt"" \
  --train_logdir=""${TRAIN_LOGDIR}"" \
  --dataset_dir=""${LANE_DATASET}""

python ""${WORK_DIR}""/eval.py \
  --logtostderr \
  --eval_split=""val"" \
  --dataset=""lane_seg"" \
  --model_variant=""xception_65"" \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=16 \
  --decoder_output_stride=4 \
  --eval_crop_size=513 \
  --eval_crop_size=513 \
  --checkpoint_dir=""${TRAIN_LOGDIR}"" \
  --eval_logdir=""${EVAL_LOGDIR}"" \
  --dataset_dir=""${LANE_DATASET}"" \
  --max_number_of_evaluations=1

python ""${WORK_DIR}""/vis_lane.py \
  --logtostderr \
  --vis_split=""val"" \
  --dataset=""lane_seg"" \
  --model_variant=""xception_65"" \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=16 \
  --decoder_output_stride=4 \
  --vis_crop_size=513 \
  --vis_crop_size=513 \
  --checkpoint_dir=""${TRAIN_LOGDIR}"" \
  --vis_logdir=""${VIS_LOGDIR}"" \
  --dataset_dir=""${LANE_DATASET}"" \
  --max_number_of_iterations=1

python ""${WORK_DIR}""/export_model.py \
  --logtostderr \
  --checkpoint_path=""${CKPT_PATH}"" \
  --export_path=""${EXPORT_PATH}"" \
  --model_variant=""xception_65"" \
  --atrous_rates=6 \
  --atrous_rates=12 \
  --atrous_rates=18 \
  --output_stride=16 \
  --decoder_output_stride=4 \
  --num_classes=2 \
  --crop_size=513 \
  --crop_size=513 \
  --inference_scales=1.0
```
I alter the train_utils.py's exclude_list=['global_step',logits]
when i train the dataset,i error list:

Input checkpoint '/home/models/research/deeplab/datasets/lane_seg/exp/train_on_trainval_set/train/model.ckpt-10000' doesn't exist!

my questions is:

    when I vis the picture,The result is like:
    i think it only use pretrain of voc to inference,so the result is so bad,Is my opinion right?
![](http://img2.ph.126.net/5aK_IRuxMATmbE52dhRRkQ==/2990108677697661746.png)
![](http://img1.ph.126.net/uHDFeEL3U9BzbjvPfuqkmw==/6597626119542484963.png)
    my dataset mybe the data samples may be strongly biased to background,How can i finetune the weight of loss ?
    Your num_classes should be greater than the max pixel value in the images
    Someone said that should finetune loss's weight,what I shold do,scale label 1's weight to 10x,100x,or change to 0.1,0.2or change label 0's weight?what others i need to finetune?

Thanks for your help!",23,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-13 17:55:59,open,,,[],2019-02-06 13:36:12
901,tensorflow/models,models,4522,austinmw,[feature request] timing metrics in tensorboard,"
I don't see anything relating to inference speed in tensorboard. 

Also, how does the model zoo obtain inference speed metrics? I see that training iterations display a fractional seconds per step, but when separately running the `eval.py` I don't see any way to obtain ms inference speed metrics. Am I just missing this feature or does it not exist?


### System information
- **What is the top-level directory of the model you are using**:
N/A
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
N/A
- **TensorFlow installed from (source or binary)**:
N/A
- **TensorFlow version (use command below)**:
N/A
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A",5,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-06-13 14:25:43,open,,,"['stat:awaiting owner', 'type:feature']",2018-06-19 23:06:09
902,tensorflow/models,models,4519,baihualinxin,TFLite toco failed to conver quantized model ( mobilenet_v1_1.0_224 ) to tflite format ,"Download the network model

MobileNet_v1_1.0_224_quant

https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md
Turn tflite error

 bazel run --config=opt \
   tensorflow/contrib/lite/toco:toco -- \
   --input_file=/Users/dchealth/Desktop/mobilenet/quantized_graph.pb \
   --output_file=/Users/dchealth/Desktop/mobilenet/frozen_graphnew.tflite \
   --input_type=FLOAT \
   --input_shape=1,128,128,3 \
   --input_arrays=input \
   --output_arrays=MobilenetV1/Predictions/Reshape_1
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=102
INFO: Reading rc options for 'run' from /Users/dchealth/tensorflow/tools/bazel.rc:
  Inherited 'build' options: --distinct_host_configuration=false --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt
ERROR: Config value opt is not defined in any .rc file
DCHealthdeMac-mini:toco dchealth$   --inference_type=QUANTIZED_UINT8
-bash: --inference_type=QUANTIZED_UINT8: command not found
DCHealthdeMac-mini:toco dchealth$   --std_values=128
-bash: --std_values=128: command not found
DCHealthdeMac-mini:toco dchealth$   --mean_values=128

------------------------
System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):mac os 10.13.5

TensorFlow installed from (source or binary):pip

TensorFlow version (use command below):'1.8.0

Python version: 3.6.4

Bazel version (if compiling from source):
Build label: 0.14.0-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jun 1 14:26:58 2018 (1527863218)
Build timestamp: 1527863218
Build timestamp as int: 1527863218

GCC/Compiler version (if compiling from source):no

CUDA/cuDNN version:no

GPU model and memory:no

Exact command to reproduce:no",4,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-06-13 06:23:01,open,,,[],2018-06-14 15:59:24
903,tensorflow/models,models,4517,achraf-boussaada,Tensorflow logs everything twice while training,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS Sierra version 10.12.6
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.8.0
- **Python version**: 
3.6.5
- **Bazel version (if compiling from source)**:
0.13.0
- **GCC/Compiler version (if compiling from source)**:
GCC 4.2.1


### Describe the problem
I'm training an object detection model using the new `ssdlite_mobilenet_v2_coco_2018_05_09` and it's configuration file `ssdlite_mobilenet_v2_coco.config` and tensorflow installed from source. When I launch the training tensorflow starts printing the same info twice. 

This problem didn't happen while training the same network I'm trying to get, with a different model (checkpoint)  `ssd_mobilenet_v1_coco_2017_11_17` and the configuration file `ssd_mobilenet_v1_pets.config` and with tensorflow installed from pip (I tested with version 1.6.0 and 1.8.0) 

- NOTE : I didn't change the code in both cases and I wonder what's the cause of this.

- I'm using CPU only and the command to execute the training is (for both cases) : 

    python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/name_of_config_file.config

### Source code / logs

```
INFO:tensorflow:global step 3292: loss = 3.2832 (2.960 sec/step)
INFO:tensorflow:global step 3292: loss = 3.2832 (2.960 sec/step)
INFO:tensorflow:global step 3293: loss = 3.5285 (3.675 sec/step)
INFO:tensorflow:global step 3293: loss = 3.5285 (3.675 sec/step)
INFO:tensorflow:global step 3294: loss = 2.3972 (3.564 sec/step)
INFO:tensorflow:global step 3294: loss = 2.3972 (3.564 sec/step)
INFO:tensorflow:Recording summary at step 3294.
INFO:tensorflow:Recording summary at step 3294.
INFO:tensorflow:global_step/sec: 0.294019
INFO:tensorflow:global_step/sec: 0.294019
```
",7,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-06-13 05:53:18,open,,,['stat:awaiting response'],2018-09-04 03:38:48
904,tensorflow/models,models,4516,lixiaohui2020,The MACs of mobilenet_v2_1.0_224 is 314M not 300 M.,"Hi, 
**System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**
no
**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**
Linux Ubuntu 16.04
**TensorFlow installed from (source or binary):**
source
**TensorFlow version (use command below):**
1.8.0
**Python version:**
3.5
**Bazel version (if compiling from source):**
0.13.0
**GCC/Compiler version (if compiling from source):**
GCC 4.2.1

**Describe the problem**
    I  want to ask a question about the MACs of of mobilenet_v2_1.0_224. I get 3.47M parameters the same to you, but the MACs is 314M but not 300M.  I don't know why.
For example,
**conv3X3**  Filter Size/Pad,Stride 3 X 3 X 3 X 32 /1,2, Output Size 112 X 112 X 32, Mult_operations 10,838,016.
**bottleneck_1:**
**conv1X1**  Filter Size/Pad,Stride 1 X 1 X 32 X 32 /0, 1, Output Size 112 X 112 X 32 , Mult_operations 12,845,056.
**conv dw**  Filter Size/Pad,Stride 3 X 3 X 1 X 32 /1, 1 , Output Size 112 X 112 X 32, Mult_operations 3,612,672.
**conv1X1** Filter Size/Pad,Stride 1 X 1 X 32 X 16 /0, 1 , Output Size  112 X 112 X 16, Mult_operations  6,422,528.
I hope you help me see where the compution is wrong.
Thanks!









",2,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-06-13 02:52:41,open,,,[],2018-07-11 16:22:26
905,tensorflow/models,models,4514,Kurtoid,Update installation.md,added python setup step,1,,[],2018-06-12 23:22:17,open,,,['cla: no'],2018-06-12 23:22:21
906,tensorflow/models,models,4513,cklsoft,Distributed resnet look like run in serial,"### System information
- **What is the top-level directory of the model you are using**: official/resnet
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: 15471MiB, 2 GPUs
- **Exact command to reproduce**: set TF_CONFIG, and run resnet

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

I ran resnet in multi-gpus in one gpu and 15 gpus, respectively. The result show in worker log is : 

I0613 06:23:03.042901 140540576401216 tf_logging.py:116] loss = 1.5084058, step = 2114
INFO:tensorflow:loss = 1.7659625, step = 3616 (45.301 sec)
I0613 06:23:48.344144 140540576401216 tf_logging.py:116] loss = 1.7659625, step = 3616 (45.301 sec)
INFO:tensorflow:loss = 1.2500832, step = 5124 (45.533 sec)


It cost 45 secs  to run 1500 steps. While in the scenario only use one gpu, the result is:

INFO:tensorflow:loss = 1.4370964, step = 300 (3.163 sec)
I0613 06:35:03.420171 140657753802560 tf_logging.py:116] loss = 1.4370964, step = 300 (3.163 sec)
INFO:tensorflow:global_step/sec: 29.615
I0613 06:35:06.795655 140657753802560 tf_logging.py:116] global_step/sec: 29.615
INFO:tensorflow:loss = 1.3290867, step = 400 (3.377 sec)
I0613 06:35:06.796813 140657753802560 tf_logging.py:116] loss = 1.3290867, step = 400 (3.377 sec)

It seems that, one core (1 worker) cost 3s to run 100 steps, and 15 workers(each worker has one gpu) cost 45s to run 1500steps, 3*15=45! 

### Source code / logs

I modify `resnet_run_loop.resnet_main` to fix the distributed trainning, and add TF_CONFIG in each worker. Since it will hang in distributed mode when using estimator.train, I use estimator.train_and_evaluate. On the other hand, train_and_evaluate only support one iteration, and will cause 'address already in use' in the second iteration, so I use the estimator.train after the first iteration.

`def resnet_main(
    flags_obj, model_function, input_function, dataset_name, shape=None):

   .... # no change

  total_training_cycle = (flags_obj.train_epochs //
                          flags_obj.epochs_between_evals)
  train_spec=tf.estimator.TrainSpec(input_fn=input_fn_train,max_steps=1000)
  eval_spec=tf.estimator.EvalSpec(input_fn=input_fn_eval,steps=1,throttle_secs=1,start_delay_secs=1)
  max_steps=0
  for cycle_index in range(total_training_cycle):
    print(""cycle_index...."")
    tf.logging.info('Starting a training cycle: %d/%d',
                    cycle_index, total_training_cycle)

    tf.logging.info('Starting to evaluate.')

    max_steps+=train_spec.max_steps
    if cycle_index==0:
      tf.logging.info('Starting to evaluate==.')
      tf.estimator.train_and_evaluate(classifier,train_spec,eval_spec)
    else:
      tf.logging.info('Starting to evaluate+++.')
eval_spec=tf.estimator.EvalSpec(input_fn=input_fn_eval,steps=1,throttle_secs=1,start_delay_secs=1)
      classifier.train(input_fn=train_spec.input_fn,max_steps=max_steps,hooks=list(train_spec.hooks))
    #tf.estimator.train_and_evaluate(classifier,train_spec,eval_spec)

    # print('eval_results: '+str(eval_results))
    # benchmark_logger.log_evaluation_result(eval_results)

    # if model_helpers.past_stop_threshold(
    #     flags_obj.stop_threshold, eval_results['accuracy']):
    #   print('past_stop_threshold break')
    #   break

  if flags_obj.export_dir is not None:
    # Exports a saved model for the given classifier.
    input_receiver_fn = export.build_tensor_serving_input_receiver_fn(
        shape, batch_size=flags_obj.batch_size)
    classifier.export_savedmodel(flags_obj.export_dir, input_receiver_fn)

`",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-12 22:42:57,open,,,[],2018-06-16 00:17:55
907,tensorflow/models,models,4510,AndriiTsok,Renamed base_name variable to checkpoint_name,Fixed invalid variable name for checkpoint filename prefix,3,,[],2018-06-12 15:40:42,open,,,['cla: yes'],2018-06-12 15:42:16
908,tensorflow/models,models,4502,gustavz,[object_detection] Graph Transformations applied to frozen graphs in the Model Zoo,"### System information
- **What is the top-level directory of the model you are using**: object_detection and deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes and No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: Source 
- **TensorFlow version (use command below)**: 1.7
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: 9 / 7
- **GPU model and memory**: 4GB GTX 1050
- **Exact command to reproduce**:

**Question:** Which graph Transformations (or other additional operations) are applied to the models which are released in the Model Zoo?

**Background:** When re-training Mask R-CNN Models on the checkpoints provided in the Model Zoo for a few steps on COCO and then re-exporting it as a frozen_inference_graph.pb the Model is much slower than the one which is released.

**Guess:** There are Transforms of the `tensorflow/tools/graph_transforms/transform_graph` tool applied. Probably amongst others `strip_unused_nodes`, `remove_nodes`, `remove_attribute`.

It would be great to get information onthat. Thank you TF Team !

",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-11 07:35:26,open,,,[],2018-06-12 01:17:57
909,tensorflow/models,models,4501,sj6077,Skip thoughts decode error,"### System information
- **What is the top-level directory of the model you are using**:
skip_thoughts
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**
No:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.6.1
- **Bazel version (if compiling from source)**:
0.5.4
- **CUDA/cuDNN version**:
9/7
- **GPU model and memory**:
Titan XP, 12G
- **Exact command to reproduce**:
bazel-bin/skip_thoughts/vocabulary_expansion \
  --skip_thoughts_model=${CHECKPOINT_PATH} \
  --skip_thoughts_vocab=${SKIP_THOUGHTS_VOCAB} \
  --word2vec_model=${WORD2VEC_MODEL} \
  --output_dir=${EXP_VOCAB_DIR}

### Describe the problem
I got the below error message.
`UnicodeDecodeError: 'utf8' codec can't decode byte 0xa8 in position 0: invalid start byte`
The word in the vocab.txt is ¨C. I wonder how can I fix it. 
I cannot download BookCorpus now, so I can't regenerate the preprocessed data, too. Is there any idea to handle it?

### Source code / logs
`UnicodeDecodeError: 'utf8' codec can't decode byte 0xa8 in position 0: invalid start byte`

",0,"NamedUser(login=""cshallue"")","[NamedUser(login=""cshallue""), NamedUser(login=""yhliang2018"")]",2018-06-11 07:22:14,open,,,['stat:awaiting owner'],2018-06-14 04:47:45
910,tensorflow/models,models,4495,dennywangtenk,Request new documents on performance fine tuning,"**Description**,

 There are at least 20% questions and issues on github and SO directly related to out-of-memory issues, training performance issues, inference performance issues, etc. It could save lots of waiting time and hardware resources if the team can publish an official guideline for performance tuning and achieve performance close to numbers on [Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).

**New documents can include (not limit to) these topics**,

1. Monitor actual memory usage, and detect and prevent out-of-memory error.
2. FAQ for train/eval/inference on GPU, and tips on improving GPU utilization.
3. Tutorial for video inferencing(realtime).
4. Tips for inferencing on mobile devices (with very limited resources)


Overall my experience with TF object detection API is positive, fairly easy to use, pretty robust.  It could be much more popular if train/eval/inference performance is improved.


Thank you.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.7
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX1060, 6GB
- **Exact command to reproduce**:
",1,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-06-09 19:24:44,open,,,['stat:awaiting owner'],2018-06-11 16:36:56
911,tensorflow/models,models,4490,Abduoit,WARNING:root:Invalid example: with create_pet_tf_record.py ???,"**System information**

**What is the top-level directory of the model you are using: /home/abdu/py3-gpu/models/research/object_detection
Have I written custom code (NO):
OS Platform and Distribution (Linux Ubuntu 16.04):
TensorFlow installed from (binary[using pip3):
TensorFlow version (1.6.0):
Python version: (3.5.2)
Bazel version (N/A):
GCC/Compiler version (N/A):
CUDA/cuDNN version: CUDA 9.0/ CUDDN 7
GPU model and memory: Nvidia GTX 1080/ 8GB
Exact command to reproduce:**

`(py3-gpu) abdu@grasp-uoit:~/py3-gpu/models/research$ python object_detection/dataset_tools/create_pet_tf_record.py     --label_map_path=object_detection/data/pet_label_map.pbtxt     --data_dir=`pwd`     --output_dir=/home/abdu/py3-gpu/models/research/ttt/`


I am trying to generate TFRecords files using `create_pet_tf_record.py` based on this instructions [HERE](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/preparing_inputs.md#generating-the-oxford-iiit-pet-tfrecord-files) I also made `'faces_only', False,`

I  get 20 TFRecords files 10 for train and 10 for eval

```
pets_fullbody_with_masks_train.record-00000-of-00010
.
.
pets_fullbody_with_masks_train.record-00009-of-00010
```

and
```
pets_fullbody_with_masks_val.record-00000-of-00010
.
.
pets_fullbody_with_masks_val.record-00009-of-00010
```
How do I suppose to refer `TFRecord files` in `.config` during training if I have 20 files, I was expected to have only two (something like `pet_train.record` and `pet_eval.record` .???

The output with ignoring many examples:-

```
(py3-gpu) abdu@grasp-uoit:~/py3-gpu/models/research$ python object_detection/dataset_tools/create_pet_tf_record.py     --label_map_path=object_detection/data/pet_label_map.pbtxt     --data_dir=`pwd`     --output_dir=/home/abdu/py3-gpu/models/research/ttt/
/home/abdu/py3-gpu/models/research/object_detection/utils/dataset_util.py:75: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.
  if not xml:
WARNING:root:Invalid example: /home/abdu/py3-gpu/models/research/annotations/xmls/miniature_pinscher_14.xml, ignoring.
WARNING:root:Invalid example: /home/abdu/py3-gpu/models/research/annotations/xmls/leonberger_18.xml, ignoring.
WARNING:root:Could not find /home/abdu/py3-gpu/models/research/annotations/xmls/Egyptian_Mau_14.xml, ignoring example.
WARNING:root:Could not find /home/abdu/py3-gpu/models/research/annotations/xmls/saint_bernard_15.xml, ignoring example.
WARNING:root:Could not find /home/abdu/py3-gpu/models/research/annotations/xmls/Ragdoll_199.xml, ignoring example.
WARNING:root:Invalid example: /home/abdu/py3-gpu/models/research/annotations/xmls/Egyptian_Mau_162.xml, ignoring.
WARNING:root:Could not find /home/abdu/py3-gpu/models/research/annotations/xmls/Egyptian_Mau_186.xml, ignoring example.
WARNING:root:Could not find /home/abdu/py3-gpu/models/research/annotations/xmls/Bengal_175.xml, ignoring example.
WARNING:root:Could not find /home/abdu/py3-gpu/models/research/annotations/xmls/samoyed_10.xml, ignoring example.
WARNING:root:Could not find /home/abdu/py3-gpu/models/research/annotations/xmls/Egyptian_Mau_156.xml, ignoring example.
WARNING:root:Could not find /home/abdu/py3-gpu/models/research/annotations/xmls/Abyssinian_104.xml, ignoring example.
WARNING:root:Invalid example: /home/abdu/py3-gpu/models/research/annotations/xmls/saint_bernard_108.xml, ignoring.
WARNING:root:Invalid example: /home/abdu/py3-gpu/models/research/annotations/xmls/Egyptian_Mau_196.xml, ignoring.
WARNING:root:Invalid example: /home/abdu/py3-gpu/models/research/annotations/xmls/Egyptian_Mau_165.xml, ignoring.
WARNING:root:Could not find /home/abdu/py3-gpu/models/research/annotations/xmls/Bengal_111.xml, ignoring example.
```
",7,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-06-08 16:49:06,open,,,['stat:awaiting owner'],2019-01-03 05:34:34
912,tensorflow/models,models,4482,naoufelfrioui,confusion matrix for object_detection model in tensorflow,"I want to have a visual of confusion matrix in tensorboard


### System information
- **What is the top-level directory of the model you are using**:research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.8
- **Bazel version (if compiling from source)**:0.10.1
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:NA

",2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc""), NamedUser(login=""yhliang2018"")]",2018-06-07 13:23:51,open,,,['stat:contributions welcome'],2018-06-14 05:25:55
913,tensorflow/models,models,4480,Osalama1,random_horizontal_flip doesn't exist in func_arg_map ,"i got this error when  i trying to train  python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config
-using  windows 8.1.
 

- tensorflow 1.4.0rc1. 

- using cuda V 8.0. 

- using model Faster R-CNN.",3,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1""), NamedUser(login=""shlens"")]",2018-06-07 10:53:53,open,,,['stat:awaiting owner'],2018-06-19 15:38:50
914,tensorflow/models,models,4478,efeiefei,Official Transformer can not get bleu like T2T Transformer,"### System information
- **What is the top-level directory of the model you are using**: official/transformer
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6
- **Bazel version (if compiling from source)**: No
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: V100
- **Exact command to reproduce**: No


### Describe the problem
As official Transformer use embedding_softmax_sharding_weights, we use individual encoder_embedding, decdoer_embedding and softmax.

We use the same input data and params for t2t transfromer and this transformer.
However, we can't get Bleu similar to t2t, always lower than it by 0.5 bleu.
I think their implementation is almost the same
Where is it wrong?
",5,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-06-07 08:02:42,open,,,[],2018-06-10 09:00:47
915,tensorflow/models,models,4477,dori-reichmann,BugFix: python3 compatability,"Wrapped range() with a list, for python3 compatability.

See: https://github.com/tensorflow/models/issues/4455",3,,[],2018-06-07 07:30:12,open,,,['cla: yes'],2018-12-17 06:29:34
916,tensorflow/models,models,4470,twangnh,object detection API: no mean subtraction and std normalization,"Hi! I noticed that there is only simple remap to [-1, 1] manipulation:

```
  def preprocess(self, resized_inputs):

    return (2.0 / 255.0) * resized_inputs - 1.0
```

in the preprocess options, there is `SubtractChannelMean` option, but it is not used, why it not use the standard channel mean subtraction and std normalization? is it because there is batchnorm layers?",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-06 08:42:15,open,,,['stat:awaiting response'],2018-06-13 07:45:08
917,tensorflow/models,models,4469,TracesChen,AttributeError: 'Deeplab' object has no attribute 'module',"/home/ubuntu/anaconda3/envs/pytorch/bin/python /home/ubuntu/MaskTrack-master/training/train_offline.py --NoLabels 2 --lr 0.001 --wtDecay 0.001 --epochResume 0 --epochs 15 --batchSize 6
CUDA available
Traceback (most recent call last):
  File ""/home/ubuntu/MaskTrack-master/training/train_offline.py"", line 120, in <module>
    lr=base_lr, momentum=0.9, weight_decay=weight_decay)
  File ""/home/ubuntu/anaconda3/envs/pytorch/lib/python2.7/site-packages/torch/optim/sgd.py"", line 57, in __init__
    super(SGD, self).__init__(params, defaults)
  File ""/home/ubuntu/anaconda3/envs/pytorch/lib/python2.7/site-packages/torch/optim/optimizer.py"", line 39, in __init__
    self.add_param_group(param_group)
  File ""/home/ubuntu/anaconda3/envs/pytorch/lib/python2.7/site-packages/torch/optim/optimizer.py"", line 146, in add_param_group
    param_group['params'] = list(params)
  File ""/home/ubuntu/MaskTrack-master/training/utility_functions.py"", line 34, in get_1x_lr_params_NOscale
    b.append(model.module.Scale.conv1)
  File ""/home/ubuntu/anaconda3/envs/pytorch/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 398, in __getattr__
    type(self).__name__, name))
AttributeError: 'Deeplab' object has no attribute 'module'",2,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-06-06 07:18:08,open,,,[],2018-07-07 12:39:23
918,tensorflow/models,models,4467,a819721810,"Using pb inefficiently in ""object_detection_tutorial.ipynb""","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win7
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:tensorflow-gpu-1.8.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0/7.0.5
- **GPU model and memory**:GTX960
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
when i used many picture to detect my model by pb as ""research/object_detection/object_detection_tutorial.ipynb"" did,it was very slow.I found it will open Session and close once by detecting one picture.So, i think it was very slow in ""object_detection_tutorial.ipynb""

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",3,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-06-06 07:03:27,open,,,[],2018-06-09 18:46:29
919,tensorflow/models,models,4466,bob329,Update __init__.py,"there should use absolute path
",3,,[],2018-06-06 06:30:54,open,,,['cla: yes'],2018-06-07 11:20:09
920,tensorflow/models,models,4463,jiyongma,[deeplab] How to use your own training documents（.pbtxt +.ckpt+index）to  demonstrate with  deeplab_demo.ipynb,"the demo use   frozen_inference_graph.pb  ，I want to use graph.pbtxt  ，please tell me how to change this demo？thanks！

class DeepLabModel(object):
  """"""Class to load deeplab model and run inference.""""""

  INPUT_TENSOR_NAME = 'ImageTensor:0'
  OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'
  INPUT_SIZE = 400
  FROZEN_GRAPH_NAME = 'frozen_inference_graph'

  def __init__(self, tarball_path):
    """"""Creates and loads pretrained deeplab model.""""""
    self.graph = tf.Graph()

    graph_def = None
    # Extract frozen graph from tar archive.
    tar_file = tarfile.open(tarball_path)
    for tar_info in tar_file.getmembers():
      if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):
        file_handle = tar_file.extractfile(tar_info)
        #print(file_handle.read())
        graph_def = tf.GraphDef.FromString(file_handle.read())
        break

    tar_file.close()

    if graph_def is None:
      raise RuntimeError('Cannot find inference graph in tar archive.')

    with self.graph.as_default():
      tf.import_graph_def(graph_def, name='')

    self.sess = tf.Session(graph=self.graph)

  def run(self, image):
    """"""Runs inference on a single image.

    Args:
      image: A PIL.Image object, raw input image.

    Returns:
      resized_image: RGB image resized from original input image.
      seg_map: Segmentation map of `resized_image`.
    """"""
    width, height = image.size
    resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)
    target_size = (int(resize_ratio * width), int(resize_ratio * height))
    resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)
    batch_seg_map = self.sess.run(
        self.OUTPUT_TENSOR_NAME,
        feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})
    seg_map = batch_seg_map[0]
    return resized_image, seg_map


def create_pascal_label_colormap():
  """"""Creates a label colormap used in PASCAL VOC segmentation benchmark.

  Returns:
    A Colormap for visualizing segmentation results.
  """"""
  colormap = np.zeros((256, 3), dtype=int)
  ind = np.arange(256, dtype=int)

  for shift in reversed(range(8)):
    for channel in range(3):
      colormap[:, channel] |= ((ind >> channel) & 1) << shift
    ind >>= 3

  return colormap


def label_to_color_image(label):
  """"""Adds color defined by the dataset colormap to the label.

  Args:
    label: A 2D array with integer type, storing the segmentation label.

  Returns:
    result: A 2D array with floating type. The element of the array
      is the color indexed by the corresponding element in the input label
      to the PASCAL color map.

  Raises:
    ValueError: If label is not of rank 2 or its value is larger than color
      map maximum entry.
  """"""
  if label.ndim != 2:
    raise ValueError('Expect 2-D input label')

  colormap = create_pascal_label_colormap()

  if np.max(label) >= len(colormap):
    raise ValueError('label value too large.')

  return colormap[label]


def vis_segmentation(image, seg_map):
  """"""Visualizes input image, segmentation map and overlay view.""""""
  plt.figure(figsize=(15, 5))
  grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])

  plt.subplot(grid_spec[0])
  plt.imshow(image)
  plt.axis('off')
  plt.title('input image')

  plt.subplot(grid_spec[1])
  seg_image = label_to_color_image(seg_map).astype(np.uint8)
  plt.imshow(seg_image)
  plt.axis('off')
  plt.title('segmentation map')

  plt.subplot(grid_spec[2])
  plt.imshow(image)
  plt.imshow(seg_image, alpha=0.7)
  plt.axis('off')
  plt.title('segmentation overlay')

  unique_labels = np.unique(seg_map)
  ax = plt.subplot(grid_spec[3])
  plt.imshow(
      FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')
  ax.yaxis.tick_right()
  plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])
  plt.xticks([], [])
  ax.tick_params(width=0.0)
  plt.grid('off')
  plt.show()


LABEL_NAMES = np.asarray([
    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',
    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',
    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'
])
#LABEL_NAMES = np.asarray([
#    'background', 'fog'
#])    

FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)
FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-06-06 03:10:30,open,,,[],2018-06-11 19:02:52
921,tensorflow/models,models,4461,jiahaowork,Fixing code-doc inconsistency,The doc string should be consistent with the parameter names.,0,,[],2018-06-06 02:13:58,open,,,['cla: yes'],2018-06-06 02:14:00
922,tensorflow/models,models,4458,Abduoit,Could not train TFRecord pascal files with mask_rcnn ????,"**System information**

**What is the top-level directory of the model you are using: /home/jesse/gpu-py3/models/research/object_detection
Have I written custom code (NO):
OS Platform and Distribution (Linux Ubuntu 16.04):
TensorFlow installed from (binary[using pip3):
TensorFlow version (1.6.0):
Python version: (3.5.2)
Bazel version (N/A):
GCC/Compiler version (N/A):
CUDA/cuDNN version: CUDA 9.0/ CUDDN 7
GPU model and memory: Nvidia GTX 1080/ 8GB
Exact command to reproduce:** 
`python object_detection/train.py     --logtostderr     --pipeline_config_path=/home/jesse/gpu-py3/models/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config     --train_dir=/home/jesse/gpu-py3/models/research/object_detection/models/model/training_test/training/`

I get the following error when I use TFRecord files generated by `create_pascal_tf_record.py`, and trying to use `mask_rcnn_inception_v2_coco.config` as a checkpoint.

Any help please ???

```
(gpu-py3) jesse@jesse-System-Product-Name:~/gpu-py3/models/research$ python object_detection/train.py     --logtostderr     --pipeline_config_path=/home/jesse/gpu-py3/models/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config     --train_dir=/home/jesse/gpu-py3/models/research/object_detection/models/model/training_test/training/
/home/jesse/gpu-py3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-06-05 16:26:15.495165: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-06-05 16:26:15.588510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-06-05 16:26:15.588765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.86
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.38GiB
2018-06-05 16:26:15.588777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-06-05 16:26:15.737561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3245 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From /home/jesse/gpu-py3/models/research/object_detection/trainer.py:228: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
WARNING:tensorflow:From /home/jesse/gpu-py3/models/research/object_detection/core/box_predictor.py:396: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/jesse/gpu-py3/models/research/object_detection/core/losses.py:316: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See tf.nn.softmax_cross_entropy_with_logits_v2.

WARNING:tensorflow:From /home/jesse/gpu-py3/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1856: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/jesse/gpu-py3/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1963: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
WARNING:tensorflow:From /home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-06-05 16:26:22.456067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-06-05 16:26:22.456194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3958 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from /home/jesse/gpu-py3/models/research/object_detection/models/model/mask_rcnn_inception_v2_coco_train/mask_rcnn_inception_v2_coco_2018_01_28/model.ckpt
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path /home/jesse/gpu-py3/models/research/object_detection/models/model/training_test/training/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
2018-06-05 16:26:33.187348: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.06GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-05 16:26:33.425259: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-05 16:26:33.429808: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.19GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-05 16:26:33.436913: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.06GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-06-05 16:26:33.441532: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.83GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
INFO:tensorflow:Error reported to Coordinator: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_127, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_129, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_131)]]

Caused by op 'Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert', defined at:
  File ""object_detection/train.py"", line 200, in <module>
    tf.app.run()
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/train.py"", line 196, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/jesse/gpu-py3/models/research/object_detection/trainer.py"", line 246, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""/home/jesse/gpu-py3/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""/home/jesse/gpu-py3/models/research/object_detection/trainer.py"", line 181, in _create_losses
    losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
  File ""/home/jesse/gpu-py3/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1580, in loss
    groundtruth_masks_list,
  File ""/home/jesse/gpu-py3/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1813, in _loss_box_classifier
    groundtruth_boxlists, groundtruth_masks_list)
  File ""/home/jesse/gpu-py3/models/research/object_detection/core/target_assigner.py"", line 447, in batch_assign_targets
    anchors, gt_boxes, gt_class_targets, gt_weights)
  File ""/home/jesse/gpu-py3/models/research/object_detection/core/target_assigner.py"", line 151, in assign
    groundtruth_boxes.get())[:1])
  File ""/home/jesse/gpu-py3/models/research/object_detection/utils/shape_utils.py"", line 279, in assert_shape_equal
    return tf.assert_equal(shape_a, shape_b)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/check_ops.py"", line 402, in assert_equal
    return control_flow_ops.Assert(condition, data, summarize=summarize)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 169, in Assert
    return gen_logging_ops._assert(condition, data, summarize, name=""Assert"")
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 48, in _assert
    name=name)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_127, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_129, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_131)]]
Traceback (most recent call last):
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1361, in _do_call
    return fn(*args)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1340, in _run_fn
    target_list, status, run_metadata)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_127, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_129, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_131)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 495, in run
    self.run_loop()
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py"", line 1030, in run_loop
    self._sv.global_step])
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_127, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_129, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_131)]]

Caused by op 'Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert', defined at:
  File ""object_detection/train.py"", line 200, in <module>
    tf.app.run()
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/train.py"", line 196, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/jesse/gpu-py3/models/research/object_detection/trainer.py"", line 246, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""/home/jesse/gpu-py3/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""/home/jesse/gpu-py3/models/research/object_detection/trainer.py"", line 181, in _create_losses
    losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
  File ""/home/jesse/gpu-py3/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1580, in loss
    groundtruth_masks_list,
  File ""/home/jesse/gpu-py3/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1813, in _loss_box_classifier
    groundtruth_boxlists, groundtruth_masks_list)
  File ""/home/jesse/gpu-py3/models/research/object_detection/core/target_assigner.py"", line 447, in batch_assign_targets
    anchors, gt_boxes, gt_class_targets, gt_weights)
  File ""/home/jesse/gpu-py3/models/research/object_detection/core/target_assigner.py"", line 151, in assign
    groundtruth_boxes.get())[:1])
  File ""/home/jesse/gpu-py3/models/research/object_detection/utils/shape_utils.py"", line 279, in assert_shape_equal
    return tf.assert_equal(shape_a, shape_b)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/check_ops.py"", line 402, in assert_equal
    return control_flow_ops.Assert(condition, data, summarize=summarize)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 169, in Assert
    return gen_logging_ops._assert(condition, data, summarize, name=""Assert"")
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 48, in _assert
    name=name)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_127, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_129, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_131)]]

Traceback (most recent call last):
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1361, in _do_call
    return fn(*args)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1340, in _run_fn
    target_list, status, run_metadata)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_127, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_129, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_131)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py"", line 990, in managed_session
    yield sess
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 768, in train
    sess, train_op, global_step, train_step_kwargs)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_step
    run_metadata=run_metadata)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_127, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_129, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_131)]]

Caused by op 'Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert', defined at:
  File ""object_detection/train.py"", line 200, in <module>
    tf.app.run()
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/train.py"", line 196, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/jesse/gpu-py3/models/research/object_detection/trainer.py"", line 246, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""/home/jesse/gpu-py3/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""/home/jesse/gpu-py3/models/research/object_detection/trainer.py"", line 181, in _create_losses
    losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
  File ""/home/jesse/gpu-py3/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1580, in loss
    groundtruth_masks_list,
  File ""/home/jesse/gpu-py3/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1813, in _loss_box_classifier
    groundtruth_boxlists, groundtruth_masks_list)
  File ""/home/jesse/gpu-py3/models/research/object_detection/core/target_assigner.py"", line 447, in batch_assign_targets
    anchors, gt_boxes, gt_class_targets, gt_weights)
  File ""/home/jesse/gpu-py3/models/research/object_detection/core/target_assigner.py"", line 151, in assign
    groundtruth_boxes.get())[:1])
  File ""/home/jesse/gpu-py3/models/research/object_detection/utils/shape_utils.py"", line 279, in assert_shape_equal
    return tf.assert_equal(shape_a, shape_b)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/check_ops.py"", line 402, in assert_equal
    return control_flow_ops.Assert(condition, data, summarize=summarize)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 169, in Assert
    return gen_logging_ops._assert(condition, data, summarize, name=""Assert"")
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 48, in _assert
    name=name)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_127, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_129, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_131)]]


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/train.py"", line 200, in <module>
    tf.app.run()
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/train.py"", line 196, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/jesse/gpu-py3/models/research/object_detection/trainer.py"", line 370, in train
    saver=saver)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 783, in train
    ignore_live_threads=ignore_live_threads)
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py"", line 828, in stop
    ignore_live_threads=ignore_live_threads)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 495, in run
    self.run_loop()
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py"", line 1030, in run_loop
    self._sv.global_step])
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_127, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_129, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_131)]]

Caused by op 'Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert', defined at:
  File ""object_detection/train.py"", line 200, in <module>
    tf.app.run()
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/train.py"", line 196, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/jesse/gpu-py3/models/research/object_detection/trainer.py"", line 246, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""/home/jesse/gpu-py3/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""/home/jesse/gpu-py3/models/research/object_detection/trainer.py"", line 181, in _create_losses
    losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
  File ""/home/jesse/gpu-py3/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1580, in loss
    groundtruth_masks_list,
  File ""/home/jesse/gpu-py3/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1813, in _loss_box_classifier
    groundtruth_boxlists, groundtruth_masks_list)
  File ""/home/jesse/gpu-py3/models/research/object_detection/core/target_assigner.py"", line 447, in batch_assign_targets
    anchors, gt_boxes, gt_class_targets, gt_weights)
  File ""/home/jesse/gpu-py3/models/research/object_detection/core/target_assigner.py"", line 151, in assign
    groundtruth_boxes.get())[:1])
  File ""/home/jesse/gpu-py3/models/research/object_detection/utils/shape_utils.py"", line 279, in assert_shape_equal
    return tf.assert_equal(shape_a, shape_b)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/check_ops.py"", line 402, in assert_equal
    return control_flow_ops.Assert(condition, data, summarize=summarize)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 169, in Assert
    return gen_logging_ops._assert(condition, data, summarize, name=""Assert"")
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 48, in _assert
    name=name)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/home/jesse/gpu-py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_127, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_129, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_131)]]
```

`",17,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-06-05 23:02:22,open,,,['stat:awaiting owner'],2018-08-13 15:15:00
923,tensorflow/models,models,4456,svebert,How to transfer learning on ssd_mobilenet_v1_0.25_224,"### System information
- **What is the top-level directory of the model you are using**: ssd_mobilenet_v1_0.25_224
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072' 1.8.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.0.5
- **GPU model and memory**: Nvidia Quadro K620 2GB
- **Exact command to reproduce**:
py train.py --logtostderr --train_dir=training_0.25/ --pipeline_config_path=training_0.25/ssd_mobilenet_v1_hand.config

### Describe the problem
I managed successfully to do transfer training on ssd_mobilenet_v1 with the help of object detection api.
Config: (https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config)
model (pb and checkpoint) from:http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz

For performance reasons i like to train and test a smaller net. Now, I want to do the same with ssd_mobilenet_v1_0.25_224 from: http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_0.25_224.tgz
what do I need to change in the config file?
I tried changing the depth_multiplier to 0.25 and the height/width from 300 to 224.
I still get the error

> tensorflow.python.framework.errors_impl.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ssd_mobilenet_v1_0.25/model.ckpt: Not found: FindFirstFile failed for: ssd_mobilenet_v1_0.25 : The system cannot find the path specified.
> ; No such process

Any hints and suggestions are greatly appreciated!
### Source code / logs
> C:\Projects\tensorflow\models\research\object_detection>py train.py --logtostderr --train_dir=training_0.25/ --pipeline_config_path=training_0.25/ssd_mobilenet_v1_hand.config
> WARNING:tensorflow:From C:\Projects\tensorflow\models\research\object_detection\trainer.py:257: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
> Instructions for updating:
> Please switch to tf.train.create_global_step
> INFO:tensorflow:depth of additional conv before box predictor: 0
> INFO:tensorflow:depth of additional conv before box predictor: 0
> INFO:tensorflow:depth of additional conv before box predictor: 0
> INFO:tensorflow:depth of additional conv before box predictor: 0
> INFO:tensorflow:depth of additional conv before box predictor: 0
> INFO:tensorflow:depth of additional conv before box predictor: 0
> Traceback (most recent call last):
>   File ""train.py"", line 184, in <module>
>     tf.app.run()
>   File ""C:\Python3\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
>     _sys.exit(main(argv))
>   File ""train.py"", line 180, in main
>     graph_hook_fn=graph_rewriter_fn)
>   File ""C:\Projects\tensorflow\models\research\object_detection\trainer.py"", line 380, in train
>     var_map, train_config.fine_tune_checkpoint))
>   File ""C:\Projects\tensorflow\models\research\object_detection\utils\variables_helper.py"", line 126, in get_variables_available_in_checkpoint
>     ckpt_reader = tf.train.NewCheckpointReader(checkpoint_path)
>   File ""C:\Python3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 287, in NewCheckpointReader
>     return CheckpointReader(compat.as_bytes(filepattern), status)
>   File ""C:\Python3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ssd_mobilenet_v1_0.25/model.ckpt: Not found: FindFirstFile failed for: ssd_mobilenet_v1_0.25 : The system cannot find the path specified.
> ; No such process",4,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc""), NamedUser(login=""yhliang2018"")]",2018-06-05 16:16:31,open,,,['stat:awaiting tensorflower'],2018-06-14 05:16:06
924,tensorflow/models,models,4455,dori-reichmann,ValueError while Training With manual_stepping,"When training object detection with learning rate = manual_steeping, there is a ValueErrror:
(Python3.6)

ValueError: Tried to convert 't' to a tensor and failed. Error: Argument must be a dense tensor: range(0, 3) - got shape [3], but wanted [].

The error comes from this lines
```
# models/research/object_detection/utils/learning_schedules.py

warmup_steps = range(boundaries[0])

rate_index = tf.reduce_max(tf.where(tf.greater_equal(global_step, boundaries),
                                      range(num_boundaries),
                                      [0] * num_boundaries))
```

Very easy to fix, by making sure the range() returns a list:
```
# models/research/object_detection/utils/learning_schedules.py

warmup_steps = list(range(boundaries[0]))

rate_index = tf.reduce_max(tf.where(tf.greater_equal(global_step, boundaries),
                                      list(range(num_boundaries)),
                                      [0] * num_boundaries))
```

---  Edit: adding info:
What is the top-level directory of the model you are using: models/research/object-detection
OS Platform and Distribution = Ubuntu 16
TensorFlow installed from = conda tensorflow-gpu
TensorFlow version = 1.8
Bazel version = NA
CUDA/cuDNN version = 9.1 7.1.2
GPU model and memory = NA
Exact command to reproduce = you can just call:
`
import object_detection.utils.learning_schedules as  learning_schedules
learning_schedules.manual_stepping(3,[50],[0.2,0.3])
`

models/research/object-detection/utils/learning_schedules.manual_stepping",5,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-06-05 10:50:59,open,,,['stat:awaiting owner'],2018-06-07 15:45:26
925,tensorflow/models,models,4453,jiahaowork,Fixing wrong indices in _fixed_padding,The indices of kernel_size were wrong when computing the kernel_size_effective.,3,,[],2018-06-05 08:48:55,open,,,['cla: yes'],2018-06-05 09:06:23
926,tensorflow/models,models,4451,rgwt123,about the transformer,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Currently this project is an example of ende, which sharing the same vocabulary file. If I want to train two vocabulary files such as enzh, how can I change the code? Can you support it?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-05 02:46:44,open,,,[],2018-06-05 12:50:40
927,tensorflow/models,models,4439,ShawnDing1994,SSD mobilenet checkpoints are broken,"- **What is the top-level directory of the model you are using**:
research/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.6 (pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.6.0-cp27-none-linux_x86_64.whl)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA9.0 cuDNN7.0.5
- **GPU model and memory**:
1080Ti, 11GB
- **Exact command to reproduce**:
python object_detection/train.py \
    --logtostderr \
    --pipeline_config_path=/home/dingxiaohan/rda/ssd_mobilenet_v2_coco.config \
    --train_dir=p2_debug

python object_detection/eval.py \
    --logtostderr \
    --pipeline_config_path=/home/dingxiaohan/rda/ssd_mobilenet_v2_coco.config \
    --checkpoint_dir=p2_debug \
    --eval_dir=p2_debug_eval


### Describe the problem
I am trying to train the pre-trained SSD and faster rcnn models on COCO.
But when the training of SSD-mobilenet-v1 began, the loss was initially above 300 and decreased drastically. When the loss became stable (around 5) after a few minutes, the visualization of the detections made no sense, as the image is filled up with big boxes of all classes. After ten hours of training, the predictions started to make some sense. I think this suggests that the checkpoints do not work on COCO. I am sure that the checkpoints are loaded by the codes, as the parameters do not look like randomly initialized (I ran codes like [print(np.sum(sess.run(SOME_CONV_KERNEL_TENSORS)))] and got values with large magnitude (100 ~ 3000)).
SSD-mobilenet-v2 behaved similarly, where the initial loss was around 280.
The SSD-mobilenet models were downloaded from the model zoo (http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz, http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz).
Faster-rcnn-resnet101 works fine for me.
Only the path-related lines in the config files were modified.
Switching py27 to py36 made no difference.

",0,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-06-04 11:36:27,open,,,[],2018-06-18 17:20:51
928,tensorflow/models,models,4438,DharaBagadia,feature request : accuracy metrics in train.py/eval.py in object_detection module ,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:  object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 15.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:cudnn 7
- **GPU model and memory**: Tesla V100
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am doing training for my customized object using train.py available in object_detection module and want to add accuracy metrics to train.py and eval.py in object_detection module.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-06-04 09:01:34,open,,,[],2018-06-05 14:53:20
929,tensorflow/models,models,4433,gulingfengze,"InvalidArgumentError (see above for traceback): Reduction axis 1 is empty in shape [1,0]","- System information

What is the top-level directory of the model you are using:object_detection
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.8
Bazel version (if compiling from source):
CUDA/cuDNN version: 9.0
GPU model and memory:GTX 1080
Exact command to reproduce:

- Describe the problem

I am using the fasterrcnn-resnet50 training data to report an error:

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""train.py"", line 184, in 
tf.app.run()
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
_sys.exit(main(argv))
File ""train.py"", line 180, in main
graph_hook_fn=graph_rewriter_fn)
File ""/home/wangyutang/models-master/research/object_detection/trainer.py"", line 399, in train
saver=saver)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 784, in train
ignore_live_threads=ignore_live_threads)
File ""/home/wangyutang/anaconda3/lib/python3.6/contextlib.py"", line 99, in exit
self.gen.throw(type, value, traceback)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session
self.stop(close_summary_writer=close_summary_writer)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 828, in stop
ignore_live_threads=ignore_live_threads)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
six.reraise(*self._exc_info_to_raise)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/six.py"", line 693, in reraise
raise value
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
yield
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 495, in run
self.run_loop()
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 1030, in run_loop
self._sv.global_step])
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
run_metadata_ptr)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
feed_dict_tensor, options, run_metadata)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
run_metadata)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Reduction axis 1 is empty in shape [1,0]
[[Node: Loss/RPNLoss/Match/cond/ArgMax_1 = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, output_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Loss/RPNLoss/Match/cond/Shape_1/Switch:1, Loss/RPNLoss/Match/cond/ArgMax_1/dimension)]]
[[Node: Loss/RPNLoss/objectness_loss/_961 = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_3809_Loss/RPNLoss/objectness_loss"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]

Caused by op 'Loss/RPNLoss/Match/cond/ArgMax_1', defined at:
File ""train.py"", line 184, in 
tf.app.run()
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
_sys.exit(main(argv))
File ""train.py"", line 180, in main
graph_hook_fn=graph_rewriter_fn)
File ""/home/wangyutang/models-master/research/object_detection/trainer.py"", line 275, in train
clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
File ""/home/wangyutang/models-master/research/slim/deployment/model_deploy.py"", line 193, in create_clones
outputs = model_fn(args, **kwargs)
File ""/home/wangyutang/models-master/research/object_detection/trainer.py"", line 200, in _create_losses
losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
File ""/home/wangyutang/models-master/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1596, in loss
groundtruth_classes_with_background_list)
File ""/home/wangyutang/models-master/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1650, in _loss_rpn
groundtruth_boxlists, len(groundtruth_boxlists)[None])
File ""/home/wangyutang/models-master/research/object_detection/core/target_assigner.py"", line 447, in batch_assign_targets
anchors, gt_boxes, gt_class_targets, gt_weights)
File ""/home/wangyutang/models-master/research/object_detection/core/target_assigner.py"", line 162, in assign
match = self._matcher.match(match_quality_matrix, **params)
File ""/home/wangyutang/models-master/research/object_detection/core/matcher.py"", line 239, in match
return Match(self._match(similarity_matrix, **params),
File ""/home/wangyutang/models-master/research/object_detection/matchers/argmax_matcher.py"", line 190, in _match
_match_when_rows_are_non_empty, _match_when_rows_are_empty)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
return func(*args, **kwargs)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2063, in cond
orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1913, in BuildCondBranch
original_result = fn()
File ""/home/wangyutang/models-master/research/object_detection/matchers/argmax_matcher.py"", line 169, in _match_when_rows_are_non_empty
output_type=tf.int32)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
return func(*args, **kwargs)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 220, in argmax
return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 783, in arg_max
name=name)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
op_def=op_def)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
op_def=op_def)
File ""/home/wangyutang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in init
self._traceback = self._graph._extract_stack() # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Reduction axis 1 is empty in shape [1,0]
[[Node: Loss/RPNLoss/Match/cond/ArgMax_1 = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, output_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Loss/RPNLoss/Match/cond/Shape_1/Switch:1, Loss/RPNLoss/Match/cond/ArgMax_1/dimension)]]
[[Node: Loss/RPNLoss/objectness_loss/_961 = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_3809_Loss/RPNLoss/objectness_loss"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]

I need help for this error pls",10,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-06-03 09:15:27,open,,,['stat:awaiting owner'],2019-04-08 08:27:40
930,tensorflow/models,models,4432,gustavz,[object_detection] num_classes = 90 (IDs) for 80 COCO classes & How to train on selected classes,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes and No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: Source 
- **TensorFlow version (use command below)**: 1.7
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: 9 / 7
- **GPU model and memory**: 4GB GTX 1050
- **Exact command to reproduce**:

I have a general question i am wondering for quite a bit now and nobody could give me an answer sofar:

Why is the num_classes variable in all object_detection/sample/configs set to 90 for the COCO dataset, although it only contains 80 classes? Has it to do something with the COCO/Stuff classes?

IF I want to train a model from scratch on the standard COCO 80 Classes, do i need to set num_classes to 80? Or 90? 

The question is basically, what is the purpose of thise +10 classes?

**EDIT 1**: after closely comparing the the Coco Categories with the mscoco_label_map, it is clear that they both contain the same 80 classes but the label map tensorflow uses by default misses 10 class ids, seemingly random distributed.

Has anybody an idea why that is?

**EDIT 2**: which leads me to another question: which influence has the variable num_classes to the model dimensions? Where and to which value (probably some filters) does it get multiplied?

**EDIT 3**: Now i know that COCO initally had 91 classes but deleted 11 of them, so this explains the missing ids But also renews my big confusing question:

When training a new model you need to set the num_classes variable in the config. Default for COCO is 90, although there are only 80 classes in it. The label_map TF uses also contains 80 entries, but the IDs go up till 90, same as COCO obviously.
And here is the crux: the num_classes variable defines the spatial size of one dimenson of all convolutional filters, so if you set it to 90 but only use 80 classes you got 10 redundant dimensions.

The Problem is that the num_classes variable also sets the highest valid class_id. Therefore i dont know how i can train only a selected number of classes.
Lets say i want to train on the first ten class_ids and on the last (90th: toothbrush) then i still have to set the num_classes variable to 90 or the 90th id wont get added.

This is major confusing and i have no idea how to work around this problem.
It would be awesome if you have some deeper knowledge about this and can let me know!

Here is the link to some necessary files:
label_map_util where num_classes is used: https://github.com/tensorflow/models/blob/master/research/object_detection/utils/label_map_util.py
coco label_map with 90 ids but 80 classes: https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt
Faster RCNN MetaArch where num_classes is used for spatial size: https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-06-03 08:17:55,open,,,['stat:awaiting owner'],2019-04-07 07:55:41
931,tensorflow/models,models,4429,twangnh,object detection API how to do batch training with aspect ratio grouping,"seems the object detection API does not implement aspect ratio grouping for batching input images, is there any idea on this issue? do I have implement it myself?",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-06-02 02:55:15,open,,,"['models: research', 'stat:awaiting response']",2019-02-06 21:54:10
932,tensorflow/models,models,4423,GTimothee,Deeplab input and output node names,"### System information
- **What is the top-level directory of the model you are using**: tensorflow
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: git clone
- **TensorFlow version (use command below)**: r1.8
- **Bazel version (if compiling from source)**: 0.13.1 (last version)
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61
- **GPU model and memory**:  Tesla K80, 12G
- **Exact command to reproduce**: tensorflow/bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=saved_model.pb

### Describe the problem
Can someone include the input and output node names in the documentation please ? In particular, if it has any importance, I am using mobilenet backbone. (maybe the input tensor is different?).

I tried to use summarize_graph but it just don't work even with the last versions and so on. Building summarize_graph did work but as you can see below I cannot parse my .pb. If someone could just tell me the input and output node names it would save me some time. 

I thought it was ImageTensor:0 and SemanticPredictions:0 but as I am building my .pb with a homemade ""export_model.py"" without using the deprecated slim, the input and output nodes may have other names. 

PS: I need those names to build my client for tensorflow-serving 

Thank you very much for helping me.

### Source code / logs
`[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 7:1: Expected identifier, got: <
2018-06-01 15:39:50.659145: E tensorflow/tools/graph_transforms/summarize_graph_main.cc:320] Loading graph 'saved_model.pb' failed with Can't parse saved_model.pb as binary proto
         (both text and binary parsing failed for file saved_model.pb)
2018-06-01 15:39:50.659213: E tensorflow/tools/graph_transforms/summarize_graph_main.cc:322] usage: tensorflow/bazel-bin/tensorflow/tools/graph_transforms/summarize_graph
Flags:
        --in_graph=""""                           string  input graph file name
        --print_structure=false                 bool    whether to print the network connections of the graph
`",3,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-06-01 14:06:22,open,,,[],2018-12-19 08:15:09
933,tensorflow/models,models,4422,ds2268,TensorRT links for the models are not working,"### System information
- **What is the top-level directory of the model you are using**: research/tensorrt
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: centOS 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Bazel version (if compiling from source)** N/A:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Tesla K80
- **Exact command to reproduce**: N/A

### Describe the problem
The links provided for the Resnet models are not working: Permission denied.

Anonymous caller does not have storage.objects.get access to download.tensorflow.org/models/official/resnetv1_imagenet_savedmodel.tar.gz.


",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-06-01 13:49:24,open,,,[],2019-01-04 11:33:10
934,tensorflow/models,models,4421,twangnh,object detection API: is the coco dataset performance measured on the commonly used minival set?,"As stated in TF object detection model zoo:

> detector performance on subset of the COCO validation set or Open Images test split as measured by the dataset-specific mAP measure.

but do is this subset of COCO validation set the commonly used 5K minival set? ",7,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""pkulzc"")]",2018-06-01 12:24:50,open,,,['stat:awaiting owner'],2018-10-15 21:06:45
935,tensorflow/models,models,4418,gustavz,[object_detection] Using Adam Optimizer ,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes and No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: Source 
- **TensorFlow version (use command below)**: 1.7
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: 9 / 7
- **GPU model and memory**: 4GB GTX 1050
- **Exact command to reproduce**:

in the object_detection/protos only 3 optimizers are supported (Adam,Momentum,RMS-Prop)
As for my understanding Adam is the most advanced. If this is true why is it not used in any model of the model_zoo? All Faster RCNN and Mask RCNN Models use Momentum (which is, what i thought, outdated).

- So why not use Adam? Is there any disadvantage in using adam with those bigger models?
- Did anybody ever use Adam instead and check the results?
- If i want to use Adam to train a Mask RCNN Model, how should i set the LEarningRate? Constant? Scheduled? ExponentiallyDecayed? Because  as i understood Adam it adjusts LR by itself, but this implementation does not seem to do it.
- Why is it only possible to pass LearninRate (as defined in the protos)? Why not also the other Adam specific parameters?",3,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-06-01 08:49:46,open,,,['stat:awaiting owner'],2018-08-20 18:02:19
936,tensorflow/models,models,4409,kushagraagrawal,[deeplab][feature request] Checkpoint for MobileNet v2 (dw multiplier=0.75),"### System information
- **What is the top-level directory of the model you are using**:deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I wish to use the deeplab decoder with MobileNet v2 with depth multiplier=0.75. Can anyone provide a checkpoint for that? Or how do I train it on my own with the available checkpoints?

",3,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-05-30 10:49:40,open,,,[],2018-06-05 17:40:55
937,tensorflow/models,models,4408,peterroelants,Explicitly cast range function to list or tuple for Python 3 compatibility.,"Explicitly cast range function to list or tuple when the result is not used as an iterator. This makes the code Python 3 compatible.

This fixes an issue that I had running the Pet detector example with Python 3 and tensorflow 1.8. I tested the Pet detector example with tensorflow 1.8 and Python 3.6 locally and Python 3.5 on GC. This PR fixes the issues that I had.

Similar issue as #3465. Also related to the fixes suggested in #3705 and #3752.",0,,[],2018-05-30 10:43:47,open,,,['cla: yes'],2018-05-30 13:49:35
938,tensorflow/models,models,4407,pageedward,"how do u solve the problem when load checkpoint in transfer learning ssd mobile v2 ,","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
finetuning ssd mobile v2 ,when restoring  checkpoint ,there is a problem

### Source code / logs
ncodingPredictor/biases/ExponentialMovingAverage not found in checkpoint
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Key BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""/home/vsoon/liangpeijun/models-master/research/object_detection/train.py"", line 184, in <module>
    tf.app.run()
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/vsoon/liangpeijun/models-master/research/object_detection/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/opt/anaconda3/lib/python3.6/site-packages/object_detection-0.1-py3.6.egg/object_detection/trainer.py"", line 361, in train
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1311, in __init__
    self.build()
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1320, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1357, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 809, in _build_internal
    restore_sequentially, reshape)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 448, in _AddRestoreOps
    restore_sequentially)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 860, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1458, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op
    op_def=op_def)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Key BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""/home/vsoon/liangpeijun/models-master/research/object_detection/train.py"", line 184, in <module>
    tf.app.run()
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/vsoon/liangpeijun/models-master/research/object_detection/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/opt/anaconda3/lib/python3.6/site-packages/object_detection-0.1-py3.6.egg/object_detection/trainer.py"", line 361, in train
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1311, in __init__
    self.build()
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1320, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1357, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 809, in _build_internal
    restore_sequentially, reshape)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 448, in _AddRestoreOps
    restore_sequentially)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 860, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1458, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op
    op_def=op_def)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Traceback (most recent call last):
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1420, in _call_tf_sessionrun
    status, run_metadata)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Key BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/vsoon/liangpeijun/models-master/research/object_detection/train.py"", line 184, in <module>
    tf.app.run()
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/vsoon/liangpeijun/models-master/research/object_detection/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/opt/anaconda3/lib/python3.6/site-packages/object_detection-0.1-py3.6.egg/object_detection/trainer.py"", line 399, in train
    saver=saver)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 747, in train
    master, start_standard_services=False, config=session_config) as sess:
  File ""/opt/anaconda3/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 828, in stop
    ignore_live_threads=ignore_live_threads)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/opt/anaconda3/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 989, in managed_session
    start_standard_services=start_standard_services)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 726, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 275, in prepare_session
    config=config)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 207, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1775, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    run_metadata)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""/home/vsoon/liangpeijun/models-master/research/object_detection/train.py"", line 184, in <module>
    tf.app.run()
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/vsoon/liangpeijun/models-master/research/object_detection/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/opt/anaconda3/lib/python3.6/site-packages/object_detection-0.1-py3.6.egg/object_detection/trainer.py"", line 361, in train
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1311, in __init__
    self.build()
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1320, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1357, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 809, in _build_internal
    restore_sequentially, reshape)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 448, in _AddRestoreOps
    restore_sequentially)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 860, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1458, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op
    op_def=op_def)
  File ""/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
",14,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-05-30 10:00:04,open,,,[],2018-11-21 05:40:17
939,tensorflow/models,models,4406,gustavz,[object_detection] Mask RCNN Conv2D double inference time,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes and No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: Source 
- **TensorFlow version (use command below)**: 1.7
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: 9 / 7
- **GPU model and memory**: 4GB GTX 1050
- **Exact command to reproduce**:

I trained a mask_rcnn model with mobilenet_v1 as backbone (I took the inception_v2 config as templatet). 
Currently I achieve decent accuracy (around 22mAP compared to 25mAP of Inception) but inference time stayed the same, although the model is less than half the size.

Therefore i created json timeline files for both models timing each operation of the graph.
What jumps the eye is the fact that among other ops ""SecondStageBoxPredictor_1/Conv/Conv2D"" takes more then twice the time for the mobilenet version.

What could be the reason? I dont understand it from the technical and mathematical side.
Any hints, thoughts or explanations are highly appreciated.

[timeline_mask_rcnn_inceptionVSmobilenet.zip](https://github.com/tensorflow/models/files/2053230/timeline_mask_rcnn_inceptionVSmobilenet.zip)

![screenshot from 2018-05-30 11-13-39](https://user-images.githubusercontent.com/29252883/40711191-dd39e164-63fa-11e8-8a3b-0423ec513637.png)
![screenshot from 2018-05-30 11-13-25](https://user-images.githubusercontent.com/29252883/40711199-dfb12614-63fa-11e8-90b8-bdc02b1c44ae.png)

",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-05-30 09:16:20,open,,,[],2018-07-04 07:42:11
940,tensorflow/models,models,4398,Shiv1799," _MergeMessageField     raise tokenizer.ParseErrorPreviousToken('Expected ""%s"".' % (end_token,)) google.protobuf.text_format.ParseError: 140:16 : Expected ""}"".","### System information
- **What is the top-level directory of the model you are using**:Object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows 10
- **TensorFlow installed from (source or binary)**:Binary
- **TensorFlow version (use command below)**:1.8.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:latest
- **GPU model and memory**:NVIDIA GEFORCE
- **Exact command to reproduce**: python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config

(tensorflow1) C:\tensorflow1\models\research\object_detection>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\Ankit Sharma\Anaconda2\envs\tensorflow1\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 93, in main
    FLAGS.pipeline_config_path)
  File ""C:\tensorflow1\models\research\object_detection\utils\config_util.py"", line 94, in get_configs_from_pipeline_file
    text_format.Merge(proto_str, pipeline_config)
  File ""C:\Users\Ankit Sharma\Anaconda2\envs\tensorflow1\lib\site-packages\google\protobuf\text_format.py"", line 533, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\Users\Ankit Sharma\Anaconda2\envs\tensorflow1\lib\site-packages\google\protobuf\text_format.py"", line 587, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\Users\Ankit Sharma\Anaconda2\envs\tensorflow1\lib\site-packages\google\protobuf\text_format.py"", line 620, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\Users\Ankit Sharma\Anaconda2\envs\tensorflow1\lib\site-packages\google\protobuf\text_format.py"", line 635, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\Users\Ankit Sharma\Anaconda2\envs\tensorflow1\lib\site-packages\google\protobuf\text_format.py"", line 735, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\Ankit Sharma\Anaconda2\envs\tensorflow1\lib\site-packages\google\protobuf\text_format.py"", line 822, in _MergeMessageField
    raise tokenizer.ParseErrorPreviousToken('Expected ""%s"".' % (end_token,))
google.protobuf.text_format.ParseError: 140:16 : Expected ""}"".",0,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-05-29 14:50:58,open,,,['type:bug/performance'],2018-05-29 18:09:53
941,tensorflow/models,models,4397,wizyoung,Finish the uncompleted code comments.,Finish the uncompleted code comments of function `_configure_learning_rate`.,3,,[],2018-05-29 13:03:53,open,,,['cla: yes'],2018-05-29 13:07:25
942,tensorflow/models,models,4394,wangershi,Update README.md,"In '*_test.py', no function set the GPU device, default is 0. So if the GPU 0 is used by others and the memory is full, there will an exception like
'''failed to allocate 6.04G (6487007744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY'''
so there is a necessity to set the GPU device(and it's optional).",3,,[],2018-05-29 08:43:37,open,,,['cla: yes'],2018-05-29 11:46:48
943,tensorflow/models,models,4393,meghshyam,Finding probability of sentence from lm_1b output,"I am using lm_1b for predicting the probability of a sentence. But only four modes are available.
""eval"" and ""sample"" modes are not useful for this. Other two modes, Dump Embedding, and Dump lstm embedding, there is no documentation about how to use it.
I went to stack overflow, but issues regarding the same are not answered.
https://stackoverflow.com/questions/42553671/predicting-a-probability-of-a-sentence-using-tensorflow
https://stackoverflow.com/questions/47360704/extract-word-sentence-probabilities-from-lm-1b-trained-model
",6,"NamedUser(login=""OriolVinyals"")","[NamedUser(login=""OriolVinyals"")]",2018-05-29 07:10:15,open,,,[],2018-11-15 21:57:22
944,tensorflow/models,models,4392,tnkong,bug report: DEFINE_boolean not work fot tensorflow 1.8,"from tensorflow.python.platform.flags import DEFINE_float, DEFINE_string, DEFINE_integer, DEFINE_boolean, DEFINE_bool

this two functions DEFINE_boolean, DEFINE_bool is not working， when i try to run my code 
in a cmd windows. i can not set boolean parameters by function  DEFINE_boolean or DEFINE_bool.

python XXXX.py --stringTest hello --integerTest 500 --booleanTest False --floatTest 0.95

my code here:
DEFINE_string(""stringTest"", 'StringTest的测试数据', '默认测试数据是 StringTest的测试数据')
DEFINE_integer('integerTest', 20, '默认测试数据是20')
DEFINE_boolean('booleanTest', True, ""默认测试数据是True"")
DEFINE_float('floatTest', 0.65, ""默认测试数据是0.65"")

Flags = tf.app.flags.FLAGS

def main(_):
    str1 = Flags.stringTest
    print(str1, type(str1))
    integer1 = Flags.integerTest
    print(integer1, type(integer1))
    bool1 = Flags.booleanTest
    print(bool1, type(bool1))
    float1 = Flags.floatTest
    print(float1, type(float1))

if __name__ == '__main__':
    tf.app.run()",2,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-05-29 06:18:29,open,,"NamedUser(login=""ymodak"")",['stat:awaiting response'],2018-11-10 02:18:31
945,tensorflow/models,models,4391,Pelups,SSD+MobilenetV2 - Slow post processing ,"### System information
- **What is the top-level directory of the model you are using**: research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5
- **Bazel version (if compiling from source)**: 0.10.1
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
I'm benchmarking SSD+mobilenetV2 on my laptop using [benchmark model tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark). I've modified this benchmark file in order to log time spent on different parts of the model (Feature extractor, class predictor, box encoding predictor and post processor).
I wanted to compare results obtained with the pretrained model available in the Tensorflow detection model zoo, with the same model trained locally from a pretrained mobilenetV2 on Imagenet.
I'm only interesting in computation time, and I don't understand why the post-processing stepis slower with the locally trained model, than with the pretrained one.

### Source code / logs
Here are the result I obtained executing the benchmark script on a CPU.

| Model        | Feature extraction (ms)           | Class prediction (ms) |   Box prediction (ms)  | Post processing (ms) |
| ------------- |-------------| -----|------------| -----|
|Pretrained      | 175.9| 149.4| 17.7 | 26.3 | 
| Manually trained      | 170.3|   149.3| 16.2| 81.5 | 

As you can see, all the steps have a similar computation time for both models, except for the post processing step.
The config pipeline files are the same (I used the one included in the tar downloaded in the model zoo).
I tried to play with the post processing parameters (score threshold and iou threshold), but it's always slower than with the pretrained model.
I know that this step is relative to the NMS, but I don't know how to speed it up.

Can someone explain me that point, and how to speed up this post processing step ?

Thank you
",11,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-05-28 14:58:01,open,,,[],2018-11-21 03:39:00
946,tensorflow/models,models,4387,gustavz,[object detection] memory consumption mask rcnn,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes and No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: Source 
- **TensorFlow version (use command below)**: 1.7
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: 9 / 7
- **GPU model and memory**: 4GB GTX 1050
- **Exact command to reproduce**:

I trained mask r-cnn with mobilenet v1 as backbone. I get decent accuracy but the inference speed did not increase at all, although the model size decreased drastically.

My final goal is to deploy the model on a Nvidia Jetson Tx2, but even when training it on low res like 400x400 and use all working graph_transform tools to quantize the model, it is not deployable on the 8GB of shared memory of the Jetson, altough it runs on my 4gb GTX 1050 Laptop.i

What i like to know is what exactly defines the amount of memory mask rcnn models need.

while loading the model into the memory it crashes with the following Node: `ClipToWindow/Where` in `ClassNonMaxSupperession_1` and furthermoe says `WhereOP: COuld not launch ::DeviceSelect:: Flagged to copy indices out, status: too many resources requested for launch`

Any thoughts on that
",4,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-05-28 07:18:29,open,,,[],2018-07-02 14:33:53
947,tensorflow/models,models,4385,yh673025667,[deeplabv3] How to save my chectpoint by epochs but not every 20 minutes?,"
### Describe the problem
when i use deeplabv3, it seems that only save checkpoints every 20 minutes.<br/>
 How could i save my checkpoints every epoch? I just don't want save model by time.......

",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-05-28 05:53:16,open,,,[],2018-05-28 18:45:36
948,tensorflow/models,models,4383,colloidal-isotope,[Syntaxnet] Build errors while following readme on Mac,"- **What is the top-level directory of the model you are using**: models/research/syntaxnet
- **Have I written custom code**: No
- **OS Platform and Distribution**: MacOS High Sierra
- **TensorFlow installed from/version**: I'm installing syntaxnet from source, isn't it bundled with TF of required version?
- **Bazel version**: 0.11.1
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```
bazel test --linkopt=-headerpad_max_install_names \
    dragnn/... syntaxnet/... util/utf8/...
```

I'm following the https://github.com/tensorflow/models/blob/master/research/syntaxnet/README.md and got the exact versions of the dependencies listed (protobuf 3.3.0 in particular). I've also installed protobuf runtime and system includes (strange that there's nothing about that in readme). All python dependencies live in virtual env and I'm building from an env.

The problem is, that when building, I've got the following error
```
ERROR: /Users/screamer/dev/syntaxnet/models/research/syntaxnet/util/utf8/BUILD:35:1: C++ compilation of rule '//util/utf8:unicodetext_main' failed (Exit 1)
In file included from util/utf8/unicodetext_main.cc:26:
In file included from ./util/utf8/unicodetext.h:25:
In file included from ./syntaxnet/base.h:28:
In file included from external/org_tensorflow/tensorflow/core/lib/core/status.h:23:
bazel-out/darwin-opt/genfiles/external/org_tensorflow/tensorflow/core/lib/core/error_codes.pb.h:12:2: error: This file was generated by a newer version of protoc which is
#error This file was generated by a newer version of protoc which is
 ^
bazel-out/darwin-opt/genfiles/external/org_tensorflow/tensorflow/core/lib/core/error_codes.pb.h:13:2: error: incompatible with your Protocol Buffer headers.  Please update
#error incompatible with your Protocol Buffer headers.  Please update
 ^
bazel-out/darwin-opt/genfiles/external/org_tensorflow/tensorflow/core/lib/core/error_codes.pb.h:14:2: error: your headers.
#error your headers.
 ^
bazel-out/darwin-opt/genfiles/external/org_tensorflow/tensorflow/core/lib/core/error_codes.pb.h:39:46: error: no type named 'FieldMetadata' in namespace 'google::protobuf::internal'
  static const ::google::protobuf::internal::FieldMetadata field_metadata[];
               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
bazel-out/darwin-opt/genfiles/external/org_tensorflow/tensorflow/core/lib/core/error_codes.pb.h:40:46: error: no type named 'SerializationTable' in namespace 'google::protobuf::internal'
  static const ::google::protobuf::internal::SerializationTable serialization_table[];
```

With protobuf 3.5.1 it builds fine, but a bunch of test fail:
```
INFO: Build completed, 20 tests FAILED, 118 total actions
//dragnn/components/stateless:stateless_component_test                   PASSED in 0.2s
//dragnn/components/syntaxnet:syntaxnet_component_test                   PASSED in 0.3s
//dragnn/components/syntaxnet:syntaxnet_link_feature_extractor_test      PASSED in 0.3s
//dragnn/components/syntaxnet:syntaxnet_transition_state_test            PASSED in 0.2s
//dragnn/core:beam_test                                                  PASSED in 0.2s
//dragnn/core:compute_session_impl_test                                  PASSED in 0.3s
//dragnn/core:compute_session_pool_test                                  PASSED in 0.2s
//dragnn/core:dragnn_bulk_op_kernels_test                                PASSED in 0.2s
//dragnn/core:dragnn_op_kernels_test                                     PASSED in 0.3s
//dragnn/core:index_translator_test                                      PASSED in 0.2s
//dragnn/core:input_batch_cache_test                                     PASSED in 0.2s
//dragnn/core:resource_container_test                                    PASSED in 0.2s
//dragnn/io:sentence_input_batch_test                                    PASSED in 0.2s
//dragnn/mst:disjoint_set_forest_test                                    PASSED in 0.2s
//dragnn/mst:mst_solver_test                                             PASSED in 0.2s
//dragnn/mst:spanning_tree_iterator_test                                 PASSED in 0.3s
//dragnn/python:composite_optimizer_test                                 PASSED in 9.3s
//dragnn/python:digraph_ops_test                                         PASSED in 5.8s
//dragnn/python:evaluation_test                                          PASSED in 4.9s
//dragnn/python:render_parse_tree_graphviz_test                          PASSED in 5.4s
//dragnn/python:render_spec_with_graphviz_test                           PASSED in 5.4s
//dragnn/python:trainer_lib_test                                         PASSED in 5.3s
//dragnn/python:visualization_test                                       PASSED in 5.1s
//dragnn/tools/benchmarks:beam_benchmark                                 PASSED in 0.2s
//syntaxnet:arc_standard_transitions_test                                PASSED in 0.2s
//syntaxnet:binary_segment_state_test                                    PASSED in 0.2s
//syntaxnet:binary_segment_transitions_test                              PASSED in 0.2s
//syntaxnet:char_ngram_string_extractor_test                             PASSED in 0.2s
//syntaxnet:char_properties_test                                         PASSED in 0.2s
//syntaxnet:char_shift_transitions_test                                  PASSED in 0.2s
//syntaxnet:fml_parser_test                                              PASSED in 0.2s
//syntaxnet:generic_features_test                                        PASSED in 0.2s
//syntaxnet:head_label_transitions_test                                  PASSED in 0.3s
//syntaxnet:head_transitions_test                                        PASSED in 0.2s
//syntaxnet:label_transitions_test                                       PASSED in 0.2s
//syntaxnet:morphology_label_set_test                                    PASSED in 0.2s
//syntaxnet:once_transitions_test                                        PASSED in 0.2s
//syntaxnet:parser_features_test                                         PASSED in 0.2s
//syntaxnet:registry_test                                                PASSED in 0.2s
//syntaxnet:registry_test_with_duplicate                                 PASSED in 0.2s
//syntaxnet:segmenter_utils_test                                         PASSED in 0.2s
//syntaxnet:sentence_features_test                                       PASSED in 0.2s
//syntaxnet:shared_store_test                                            PASSED in 0.2s
//syntaxnet:tagger_transitions_test                                      PASSED in 0.2s
//syntaxnet:term_frequency_map_test                                      PASSED in 0.2s
//syntaxnet:whole_sentence_features_test                                 PASSED in 0.2s
//syntaxnet/util:check_test                                              PASSED in 4.6s
//syntaxnet/util:registry_test                                           PASSED in 4.6s
//syntaxnet/util:resources_test                                          PASSED in 4.5s
//util/utf8:unicodetext_unittest                                         PASSED in 0.2s
//dragnn/python:biaffine_units_test                                      FAILED in 4.6s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/biaffine_units_test/test.log
//dragnn/python:bulk_component_test                                      FAILED in 4.7s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/bulk_component_test/test.log
//dragnn/python:component_test                                           FAILED in 4.7s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/component_test/test.log
//dragnn/python:dragnn_model_saver_lib_test                              FAILED in 5.0s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/dragnn_model_saver_lib_test/test.log
//dragnn/python:lexicon_test                                             FAILED in 4.8s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/lexicon_test/test.log
//dragnn/python:mst_ops_test                                             FAILED in 4.7s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/mst_ops_test/test.log
//dragnn/python:mst_units_test                                           FAILED in 4.8s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/mst_units_test/test.log
//dragnn/python:network_units_test                                       FAILED in 5.2s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/network_units_test/test.log
//dragnn/python:runtime_support_test                                     FAILED in 5.1s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/runtime_support_test/test.log
//dragnn/python:sentence_io_test                                         FAILED in 5.2s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/sentence_io_test/test.log
//dragnn/python:spec_builder_test                                        FAILED in 5.2s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/spec_builder_test/test.log
//dragnn/python:transformer_units_test                                   FAILED in 5.1s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/transformer_units_test/test.log
//dragnn/tools:model_trainer_test                                        FAILED in 3.7s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/tools/model_trainer_test/test.log
//syntaxnet:beam_reader_ops_test                                         FAILED in 4.4s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/syntaxnet/beam_reader_ops_test/test.log
//syntaxnet:graph_builder_test                                           FAILED in 4.4s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/syntaxnet/graph_builder_test/test.log
//syntaxnet:lexicon_builder_test                                         FAILED in 4.4s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/syntaxnet/lexicon_builder_test/test.log
//syntaxnet:parser_trainer_test                                          FAILED in 4.3s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/syntaxnet/parser_trainer_test/test.log
//syntaxnet:reader_ops_test                                              FAILED in 4.4s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/syntaxnet/reader_ops_test/test.log
//syntaxnet:text_formats_test                                            FAILED in 3.9s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/syntaxnet/text_formats_test/test.log
//dragnn/python:graph_builder_test                                       FAILED in 5 out of 5 in 4.9s
  Stats over 5 runs: max = 4.9s, min = 1.9s, avg = 2.6s, dev = 1.2s
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/graph_builder_test/shard_4_of_5/test.log
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/graph_builder_test/shard_1_of_5/test.log
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/graph_builder_test/shard_2_of_5/test.log
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/graph_builder_test/shard_5_of_5/test.log
  /private/var/tmp/_bazel_screamer/18e2c8b2c8dff4a4063d43a04ecb656f/execroot/__main__/bazel-out/darwin-opt/testlogs/dragnn/python/graph_builder_test/shard_3_of_5/test.log

Executed 70 out of 70 tests: 50 tests pass and 20 fail locally.
There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.
```
Each of the test logs have the same contents:
```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
python(5945,0x7fff8df61380) malloc: *** error for object 0x10c7af7a0: pointer being freed was not allocated
*** set a breakpoint in malloc_error_break to debug
Abort trap: 6
```

I will appreciate any help and will be happy to provide any additional information, if necessary.",2,"NamedUser(login=""robieta"")","[NamedUser(login=""robieta"")]",2018-05-27 14:37:28,open,,,[],2018-05-28 18:45:27
949,tensorflow/models,models,4382,dakoner,Distributed training mode: the master node does training work but shouldn't,"I spent some time debugging distributed training mode and noticed something.  I'm not sure if it's intentional or not.  In particular, when you start distributed training by setting a TF_CONFIG, the master node ends up evaluating training data, computing loss, and optimizing is.  This is in addition to the worker nodes.  The master node appears to do dual duty (master coordination in addition to training).

I've worked in distributed computing for a while, and the convention is that the master node doesn't do training work- it's harder to configure the resource requests for a master that does dual duty, and there are subtle performance problems as the training and the master coordination tend to fight for limited CPU availability.  It also plays poorly with frameworks that handle distributed training (for example, kubeflow has a TFJob that makes it easy to set up distributed training on k8s; it assumes the ""master"" container doesn't need or want GPU.

Before going further, I'd like to ask: is the choice of doing training work on the master node intentional?  I am guessing it's just an oversight, and that the code for converting the TF_CONFIG to a computation needs to explicitly ensure that work is only assigned to worker nodes.

If the choice is intentional, then I'll have to make adjustments to my distributed training pipeline to include additional resources for the master node (more CPU, along with GPU), and spend time debugging performance problems.  If the choice is not intentional, then i think we can proceed to repair the script.",6,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-05-26 14:46:51,open,,,[],2018-06-02 07:31:25
950,tensorflow/models,models,4381,naoufelfrioui,how to create confusion matrix for object_detection model in tensorflow,I want to have a visual of confusion matrix in tensorboard. how can i do this,4,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-05-26 13:35:06,open,,,[],2018-06-14 14:54:51
951,tensorflow/models,models,4375,wenouyang,[deeplab] regarding Adding _LOGITS_SCOPE_NAME to exclude_list in utils/train_utils.py,"In this([issue 4231](https://github.com/tensorflow/models/issues/4231)), the author talks about making modification of the code by 

```
3. Adding _LOGITS_SCOPE_NAME to exclude_list in utils/train_utils.py
# _LOGITS_SCOPE_NAME = 'logits'
exclude_list = ['global_step', 'logits']
if not initialize_last_layer:
exclude_list.extend(last_layers)
```

Under what kind of scenario, we need to make this modification. What does this `logits` stand for?",1,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-05-25 22:56:45,open,,,['stat:awaiting response'],2018-05-27 03:26:40
952,tensorflow/models,models,4367,yh673025667,[deeplab] batch_norm_params {'decay': 0.9997} in model.py,"I have trained deeplab with fine_tune_batch_norm setting True.<br/>
when i set fine_tune_batch_norm **True**,the result after training is very bad.(with crop_size=513*513,batch_size = 15, training on 5 Titan xp),m_iou is about 0.004 <br/>
but when setting fine_tune_batch_norm **False**,the m_iou can be trained to 0.47 <br/>

So i have checked the source code, but i find **batch_norm_params {'decay': 0.9997}** in model.py . Is this ok? or something wrong with that?
",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-05-25 07:15:10,open,,,[],2018-05-29 12:59:59
953,tensorflow/models,models,4361,jinghuangintel,Intel-MKL: add inter/intra-op and performance tuning parameters specific to MKL,"In this PR, We added following parameters to improve the training and evaluation out-of-box experiences on CPU and make it easier for users to control the parallelism on CPU for object detection API.
1) inter_op and intra_op to control the parallelism.
2) performance tuning parameters specific to MKL (kmp_block_time and kmp_affinity) if MKL is enabled.",1,,[],2018-05-24 20:16:14,open,,,['cla: yes'],2018-05-29 23:52:38
954,tensorflow/models,models,4357,nithishdivakar,Update object_detection_tutorial.ipynb,"When number of objects are more than uint8's range, the earlier code crates problems. This is a fix.",3,,[],2018-05-24 10:51:05,open,,,['cla: yes'],2018-05-24 10:53:00
955,tensorflow/models,models,4355,shoma88,Very Slow inference speed of object detection models,"### System information
- **What is the top-level directory of the model you are using**: 
   models/research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
   no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
  Windows 7 64 bit
- **TensorFlow installed from (source or binary)**:
   ""native"" pip
- **TensorFlow version (use command below)**:
   Version 1.8.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
   CUDA® Toolkit 9.0; cuDNN v7.1
- **GPU model and memory**:
   GForce GTX 980 (4GB)
- **Exact command to reproduce**:
   ""object_detection_tutorial.py"" from ""models-master\research\object_detection""
   i added ""import time"" on the top and the following command to print the processing time:
   
      start_time = time.time()
      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})
      print(""--- %s seconds ---"" % (time.time() - start_time))



You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
The processing time is very high. Using the CPU i get 1.5 Second for each image. Using the GPU i get an higher time (1.7 Second). I get bad performances whit the standard model (dogs and people on the beach) and also whit a retrained model.
I have the same problem with the Cifar10 example. The performances during the train seems to be fine (7000 samples per sec) but the processing time on the eval.py code is huge (9 Seconds).
Do you have any suggestions about that? i can find a lot of benchmarks about the training time but not many about the evaluation time so i don't know how much time should i need to process a single image.

thanks,

Adriano.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",13,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-05-24 09:03:08,open,,,['stat:awaiting owner'],2019-04-01 10:56:21
956,tensorflow/models,models,4347,pucumt,Readme link incorrect(Quick Start: Jupyter notebook for off-the-shelf inference),"While practice the demos based on ""object_detection/README.md"".
I find the link of ""Quick Start: Jupyter notebook for off-the-shelf inference"" is to ""object_detection/object_detection_tutorial.ipynb"",
but the real link should be ""object_detection/g3doc/running_notebook.md""",2,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-05-23 08:07:33,open,,,['type:docs'],2018-09-25 19:03:53
957,tensorflow/models,models,4345,tonghe90,[deeplab v3+] training details,"It is said the size for calculating bn param is 16 (12 at least). What about the total batch size during the training process for voc trainaug and coco, separately.",4,,[],2018-05-23 06:07:24,open,,,[],2018-08-12 17:28:03
958,tensorflow/models,models,4342,grewe,Problem trying to run Object Detection API training on Google ML,"### System information
- **What is the top-level directory of the model you are using**:
NOTE I am running on Google ML by packaging object detection api + slim and starting a job.  Attempting to retrain the SSD_Mobile net model that I copied to my Google CLoud store and originally located at object_detection\ssd_mobilenet_v1_coco_2017_11_17\model.ckpt



- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO -but, have my own training TFRecord data and have altered the configuration file.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
It successfully trains on my local machine (Windows) the issue is with deployment to Google Cloud ML as a job to run using gcloud tools on 


- **TensorFlow installed from (source or binary)**:
Already part of Google Cloud ML

- **TensorFlow version (use command below)**:
have tried numerous version including 1.8 (not Google ML support 1.8 and this is the version used locally to make the TFRecord training files)

### NOTE:  trying to run training example (That trains locally ) on Google ML. Execute job request using gcloud tool. Followed instructions at https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_cloud.md.

COMMAND executed from tensorflow/models/research
gcloud ml-engine jobs submit training grewe_object_detection_6 --runtime-version 1.8 --job-dir=gs://BLAHBLAH-storage/Train --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz --module-name object_detection.train --region us-central1 --config object_detection/samples/cloud/cloud.yml -- --train_dir=gs://BLAHBLAH-storage/Train/models/train --pipeline_config_path=gs://BLAHBLAH-storage/Train/models/model/CLOUDssd_mobilenet_v1_userheading.config

NOTE: Do not understand the error or how to resolve. Please advise.




- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
Run gcloud

Followed instructions at https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_cloud.md.

COMMAND executed from tensorflow/models/research
`gcloud ml-engine jobs submit training grewe_object_detection_6 --runtime-version 1.8 --job-dir=gs://BLAHBLAH-storage/Train --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz --module-name object_detection.train --region us-central1 --config object_detection/samples/cloud/cloud.yml -- --train_dir=gs://BLAHBLAH-storage/Train/models/train --pipeline_config_path=gs://BLAHBLAH-storage/Train/models/model/CLOUDssd_mobilenet_v1_userheading.config`





### Describe the problem
See error below.  Have tried to alter the version of tensorflow used (note locally when run successfully using 1.8 so believe as that is what used to package TFRecord it should work on Google ML) --so tried to update the provided cloud.yaml (tried for version 1.2, 1.4, 1.6 and 1.8 and also tried updating the setup.py in models/research and nothing works.   

I tried last the following for my cloud.yaml file
`trainingInput:
  runtimeVersion: ""1.8""
  scaleTier: CUSTOM
  masterType: standard_gpu
  workerCount: 5
  workerType: standard_gpu
  parameterServerCount: 3
  parameterServerType: standard
`

I tried last the following for my setup.py


**_`""""""Setup script for object_detection.""""""
from setuptools import find_packages
from setuptools import setup

REQUIRED_PACKAGES = ['Pillow>=1.0', 'Matplotlib>=2.1', 'Cython>=0.28.1']

setup(
    name='object_detection',
    version='0.1',
    install_requires=REQUIRED_PACKAGES,
    include_package_data=True,
    packages=[p for p in find_packages() if p.startswith('object_detection')],
    description='Tensorflow Object Detection Library',
)`_**

### Source code / logs
This is the error from log on Google Cloud ML console
ERROR message:

The replica master 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): [...] File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__ return self.gen.next() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session self.stop(close_summary_writer=close_summary_writer) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 828, in stop ignore_live_threads=ignore_live_threads) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join six.reraise(*self._exc_info_to_raise) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 989, in managed_session start_standard_services=start_standard_services) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 726, in prepare_or_wait_for_session init_feed_dict=self._init_feed_dict, init_fn=self._init_fn) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 279, in prepare_session config=config) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 207, in _restore_checkpoint saver.restore(sess, ckpt.model_checkpoint_path) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1802, in restore {self.saver_def.filename_tensor_name: save_path}) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call raise type(e)(node_def, op, message) UnavailableError: OS Error The replica worker 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): [...] File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 828, in stop ignore_live_threads=ignore_live_threads) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join six.reraise(*self._exc_info_to_raise) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 989, in managed_session start_standard_services=start_standard_services) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 734, in prepare_or_wait_for_session max_wait_secs=max_wait_secs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 406, in wait_for_session sess) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 490, in _try_run_local_init_op sess.run(self._local_init_op) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call raise type(e)(node_def, op, message) UnavailableError: OS Error [[Node: init_ops/init_all_tables_S2 = _Recv[client_terminated=false, recv_device=""/job:master/replica:0/task:0/device:GPU:0"", send_device=""/job:worker/replica:0/task:0/device:CPU:0"", send_device_incarnation=6383848822399600260, tensor_name=""edge_29_init_ops/init_all_tables"", tensor_type=DT_FLOAT, _device=""/job:master/replica:0/task:0/device:GPU:0""]()]] The replica worker 1 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): [...] File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 747, in train master, start_standard_services=False, config=session_config) as sess: File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__ return self.gen.next() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session self.stop(close_summary_writer=close_summary_writer) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 828, in stop ignore_live_threads=ignore_live_threads) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join six.reraise(*self._exc_info_to_raise) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 989, in managed_session start_standard_services=start_standard_services) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 734, in prepare_or_wait_for_session max_wait_secs=max_wait_secs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 406, in wait_for_session sess) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 490, in _try_run_local_init_op sess.run(self._local_init_op) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call raise type(e)(node_def, op, message) UnavailableError: OS Error The replica worker 2 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): [...] File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 747, in train master, start_standard_services=False, config=session_config) as sess: File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__ return self.gen.next() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session self.stop(close_summary_writer=close_summary_writer) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 828, in stop ignore_live_threads=ignore_live_threads) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join six.reraise(*self._exc_info_to_raise) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 989, in managed_session start_standard_services=start_standard_services) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 734, in prepare_or_wait_for_session max_wait_secs=max_wait_secs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 406, in wait_for_session sess) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 490, in _try_run_local_init_op sess.run(self._local_init_op) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call raise type(e)(node_def, op, message) UnavailableError: OS Error The replica worker 4 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): [...] File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 747, in train master, start_standard_services=False, config=session_config) as sess: File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__ return self.gen.next() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session self.stop(close_summary_writer=close_summary_writer) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 828, in stop ignore_live_threads=ignore_live_threads) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join six.reraise(*self._exc_info_to_raise) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 989, in managed_session start_standard_services=start_standard_services) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 734, in prepare_or_wait_for_session max_wait_secs=max_wait_secs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 406, in wait_for_session sess) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 490, in _try_run_local_init_op sess.run(self._local_init_op) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call raise type(e)(node_def, op, message) UnavailableError: OS Error To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=36123659232&resource=ml_job%2Fjob_id%2Fgrewe_object_detection_8&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22grewe_object_detection_8%22
",4,,[],2018-05-22 23:17:23,open,,,[],2018-07-18 21:50:14
959,tensorflow/models,models,4341,parvizp,Script for quantizing ResNet50,I find the example scripts very useful for demonstrating end-to-end flows. Are you interested in contributions like this for quantizing models and converting to quantized TF-Lite models via TOCO?,1,,[],2018-05-22 22:22:31,open,,,['cla: yes'],2018-05-24 18:18:08
960,tensorflow/models,models,4339,austinmw,[Feature request] More object detection API tensorboard metrics,"I ran the demo notebook and then just recently ran a faster r-cnn model on my own data. In tensorboard I see an overall mAP and individual class mAPs, but I don't see any way to get precision-recall, ROC-AUC, F1-scores, or false positive rates. Is this currently not a feature in the object detection API?",6,,[],2018-05-22 17:25:07,open,,,[],2019-04-09 19:51:29
961,tensorflow/models,models,4338,FishermanZzhang,object_detection preprocess,"In FasterRCNNResnetV1FeatureExtractor, 
```
  def preprocess(self, resized_inputs):   
    channel_means = [123.68, 116.779, 103.939]
    return resized_inputs - [[channel_means]]
```
but the other is,
```
  def preprocess(self, resized_inputs):
    return (2.0 / 255.0) * resized_inputs - 1.0
```
why, and how it effect the mAP?",3,,[],2018-05-22 09:38:39,open,,,[],2018-06-14 15:01:33
962,tensorflow/models,models,4337,liangjianfans,"[deeplab] the size of [513,513] in train.py and eval.py","In deeplab, the` train.py` has the following setting

`flags.DEFINE_multi_integer('train_crop_size', [513, 513], image crop size [height,   width] during training.')`
  
In `eval.py`, it has the following setting

`flags.DEFINE_multi_integer('eval_crop_size', [513, 513],| 'Image crop size [height,   width] for evaluation.')`

Does this setting of [513,513] has to be modified for different data set? If the image set consists of images with different size, how to do that?",7,,[],2018-05-22 04:46:55,open,,,[],2019-03-18 08:27:28
963,tensorflow/models,models,4331,Karthik-Suresh93,"mAP in tensorboard are all 0 or close to 0, even though the detector is able to detect many objects well","Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.8
- **Bazel version (if compiling from source)**:-
- **CUDA/cuDNN version**:CUDA 9/ cuDNN 7
- **GPU model and memory**: Tesla K-80, (number:2). Each has a memory of 12GB
- **Exact command to reproduce**:tensorboard --logdir=eval/ 


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am training a faster rcnn model on a custom dataset using the object detection API. The training seems to be going well with losses currently:

INFO:tensorflow:global step 4766: loss = 2.8091 (2.764 sec/step)
INFO:tensorflow:global step 4767: loss = 1.6403 (2.768 sec/step)
INFO:tensorflow:global step 4767: loss = 1.6403 (2.768 sec/step)

However, when I try to visualize the metrics using `tensorboard --logdir=eval/`on tensorboard, I get mAPs in all categories either 0 or very close to 0 as shown below:

![map_tensorboard_0](https://user-images.githubusercontent.com/25709940/40321957-61ffc76c-5cf6-11e8-931a-217900ae9b3e.png)

When I look at the images tab in tensorboard, the detector seems to be finding decent boxes

![good_detection_4k](https://user-images.githubusercontent.com/25709940/40321998-88cdef9a-5cf6-11e8-8d70-bf7ecd80df84.png)

![night_time_4k](https://user-images.githubusercontent.com/25709940/40322005-9197ae54-5cf6-11e8-9691-70e932773ab1.png)

My configuration file is:


model {
  faster_rcnn {
    num_classes: 12
    image_resizer {
      keep_aspect_ratio_resizer {
        # Raw KITTI images have a resolution of 1242x375, if we wish to resize
        # them to have a height of 600 then their width should be
        # 1242/(375/600)=1987.2
        min_dimension: 1000
        max_dimension: 1600
      }
    }
    feature_extractor {
      type: 'faster_rcnn_resnet101'
      first_stage_features_stride: 16
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 16
        width_stride: 16
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 300
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 4
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.0001
          schedule {
            step: 500000
            learning_rate: .00001
          }
          schedule {
            step: 700000
            learning_rate: .000001
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""faster_rcnn_resnet101_kitti_2018_01_28/model.ckpt""
  from_detection_checkpoint: true
  num_steps: 800000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  label_map_path:""data/object-detection.pbtxt""
  tf_record_input_reader: {
    input_path: ""data/train_latest.record""
  }
}

eval_config: {
  use_moving_averages: false
  num_examples: 500
}

eval_input_reader: {
  label_map_path: ""data/object-detection.pbtxt""
  tf_record_input_reader: {
    input_path: ""data/val_latest.record""
  }
  }

Why are mAP values so bad? Please let me know if there is any other information I can provide.

Thanks and Regards,
Karthik



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",10,,[],2018-05-21 18:02:51,open,,,[],2018-07-31 09:05:06
964,tensorflow/models,models,4330,harshthaker,Ran out of GPU memory using Object Detection API,"### System information
- **What is the top-level directory of the model you are using**: object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: object_detection_tutorial notebook + custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.7
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: GTX 1080 16 GB
- **Exact command to reproduce**: NA


 ### Problem
I am running object_detection_tutorial notebook code for object detection. Later I have added custom code for cropping objects from the boxes on images. Size of an image from the data set is around 1000*800. 

After detecting and cropping more than 500 images, the program runs out of memory. What could be the possible cause ? Is there any way to release memory during execution ?

### Source code / logs
```
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image


sys.path.append("".."")

from utils import label_map_util
from utils import visualization_utils as vis_util


MODEL_NAME = 'ssd_mobilenet_v1_coco_11_06_2017'
MODEL_FILE = MODEL_NAME + '.tar.gz'
DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'
PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'
PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')
NUM_CLASSES = 90

opener = urllib.request.URLopener()
opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)
tar_file = tarfile.open(MODEL_FILE)
for file in tar_file.getmembers():
    file_name = os.path.basename(file.name)
    if 'frozen_inference_graph.pb' in file_name:
        tar_file.extract(file, os.getcwd())

detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)

TEST_DIR = 'path-to-dir'
TEST_IMAGE_PATHS = [os.path.join(TEST_DIR,f) for f in os.listdir(TEST_DIR) if f.endswith("".jpg"")]

with detection_graph.as_default():
  with tf.Session(graph=detection_graph) as sess:
    for image_path,i in zip(TEST_IMAGE_PATHS,range(0,99)):
        
        image = Image.open(image_path)
       
        image_np = load_image_into_numpy_array(image)
        image_np_expanded = np.expand_dims(image_np, axis=0)
        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
         boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
        scores = detection_graph.get_tensor_by_name('detection_scores:0')
        classes = detection_graph.get_tensor_by_name('detection_classes:0')
        num_detections = detection_graph.get_tensor_by_name('num_detections:0')
      
       
        (boxes, scores, classes, num_detections) = sess.run(
            [boxes, scores, classes, num_detections],
            feed_dict={image_tensor: image_np_expanded})
       
      
      fwrite= tf.write_file(path,cropped_image)
      sess.run(fwrite)
```



",7,,[],2018-05-21 16:15:54,open,,,[],2018-06-05 23:09:43
965,tensorflow/models,models,4327,kurita236,[object_detection] An error occurred when offline_eval_map_corloc was executed using pascal_voc_detection_metrics.,"### Describe the problem
An error occurred when offline_eval_map_corloc was executed using pascal_voc_detection_metrics.

It will succeeded with coco_detection_metrics.
It will failed with pascal_voc_detection_metrics, weighted_pascal_voc_detection_metrics and open_images_detection_metrics.

### System information
- **What is the top-level directory of the model you are using**:https://github.com/tensorflow/models/blob/master/research/object_detection/metrics/offline_eval_map_corloc.py
- **Have I written custom code**:No
- **OS Platform and Distribution**: Windows 7 64 bit
- **TensorFlow installed from (source or binary)**: pip install --upgrade tensorflow==1.8.0
- **TensorFlow version (use command below)**: b'v1.8.0-0-g93bc2e2072' 1.8.0
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**:

> python object_detection/inference/infer_detections.py ^
>  --input_tfrecord_paths=./coco_val.record ^
>  --output_tfrecord_path=./detections.tfrecord-00000-of-00001 ^
>  --inference_graph=./frozen_inference_graph.pb ^
>  --discard_image_pixels

> echo label_map_path: './object_detection/data/mscoco_label_map.pbtxt' > ./input_config.pbtxt
> echo tf_record_input_reader: { input_path: './detections.tfrecord-00000-of-00001' } >> ./input_config.pbtxt

> echo metrics_set: 'pascal_voc_detection_metrics' > ./eval_config.pbtxt

> python -m object_detection.metrics.offline_eval_map_corloc ^
  --eval_dir=. ^
  --eval_config_path=./eval_config.pbtxt ^
  --input_config_path=./input_config.pbtxt

### Logs
> INFO:tensorflow:Processing file: .\detections.tfrecord-00000-of-00001
> INFO:tensorflow:Processed 0 images...
> Traceback (most recent call last):
>   File ""C:\Users\user\Miniconda3\envs\tensorflow_detector\lib\runpy.py"", line 193, in _run_module_as_main
>     ""__main__"", mod_spec)
>   File ""C:\Users\user\Miniconda3\envs\tensorflow_detector\lib\runpy.py"", line 85, in _run_code
>     exec(code, run_globals)
>   File "".\models\research\object_detection\metrics\offline_eval_map_corloc.py"", line 173, in <module>
>     tf.app.run(main)
>   File ""C:\Users\user\Miniconda3\envs\tensorflow_detector\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
>     _sys.exit(main(argv))
>   File "".\models\research\object_detection\metrics\offline_eval_map_corloc.py"", line 166, in main
>     metrics = read_data_and_evaluate(input_config, eval_config)
>   File "".\models\research\object_detection\metrics\offline_eval_map_corloc.py"", line 124, in read_data_and_evaluate
>     decoded_dict)
>   File "".\models\research\object_detection\utils\object_detection_evaluation.py"", line 195, in add_single_ground_truth_image_info
>     (groundtruth_dict[standard_fields.InputDataFields.groundtruth_difficult]
> AttributeError: 'NoneType' object has no attribute 'size'",3,,[],2018-05-21 09:01:59,open,,,[],2019-02-05 17:34:17
966,tensorflow/models,models,4326,shijh1975,[deeplab]why deeplab v3+ have no decoder and ASSP when use mobilenet-v2 models,"If It is no good effects when add decoder and ASSP?
Can I add this coder to try?",26,,[],2018-05-21 06:49:44,open,,,[],2018-09-05 01:05:39
967,tensorflow/models,models,4325,grewe,Error training on Google ML,"When following instructions to run on Google ML get the following error when trying to train.   I have noticed a similar error posted at https://github.com/tensorflow/models/issues/2739 regarding dependencies and tried to update cloud.yaml (tried for version 1.2, 1.4, 1.6 and 1.8 and also tried updating the setup.py in models/research and nothing works. 

ERROR message:
The replica ps 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 52, in <module> from object_detection.builders import model_builder File ""/root/.local/lib/python2.7/site-packages/object_detection/builders/model_builder.py"", line 18, in <module> from object_detection.builders import box_coder_builder File ""/root/.local/lib/python2.7/site-packages/object_detection/builders/box_coder_builder.py"", line 21, in <module> from object_detection.protos import box_coder_pb2 File ""/root/.local/lib/python2.7/site-packages/object_detection/protos/box_coder_pb2.py"", line 28, in <module> dependencies=[object__detection_dot_protos_dot_faster__rcnn__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_keypoint__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_mean__stddev__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_square__box__coder__pb2.DESCRIPTOR,]) File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/descriptor.py"", line 829, in __new__ return _message.default_pool.AddSerializedFile(serialized_pb) TypeError: Couldn't build proto file into descriptor pool! Invalid proto descriptor for file ""object_detection/protos/box_coder.proto"": object_detection/protos/box_coder.proto: Import ""object_detection/protos/keypoint_box_coder.proto"" has not been loaded. object_detection.protos.BoxCoder.keypoint_box_coder: ""object_detection.protos.KeypointBoxCoder"" seems to be defined in ""keypoint_box_coder.proto"", which is not imported by ""object_detection/protos/box_coder.proto"". To use it here, please add the necessary import. The replica ps 1 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 52, in <module> from object_detection.builders import model_builder File ""/root/.local/lib/python2.7/site-packages/object_detection/builders/model_builder.py"", line 18, in <module> from object_detection.builders import box_coder_builder File ""/root/.local/lib/python2.7/site-packages/object_detection/builders/box_coder_builder.py"", line 21, in <module> from object_detection.protos import box_coder_pb2 File ""/root/.local/lib/python2.7/site-packages/object_detection/protos/box_coder_pb2.py"", line 28, in <module> dependencies=[object__detection_dot_protos_dot_faster__rcnn__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_keypoint__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_mean__stddev__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_square__box__coder__pb2.DESCRIPTOR,]) File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/descriptor.py"", line 829, in __new__ return _message.default_pool.AddSerializedFile(serialized_pb) TypeError: Couldn't build proto file into descriptor pool! Invalid proto descriptor for file ""object_detection/protos/box_coder.proto"": object_detection/protos/box_coder.proto: Import ""object_detection/protos/keypoint_box_coder.proto"" has not been loaded. object_detection.protos.BoxCoder.keypoint_box_coder: ""object_detection.protos.KeypointBoxCoder"" seems to be defined in ""keypoint_box_coder.proto"", which is not imported by ""object_detection/protos/box_coder.proto"". To use it here, please add the necessary import. The replica ps 2 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 52, in <module> from object_detection.builders import model_builder File ""/root/.local/lib/python2.7/site-packages/object_detection/builders/model_builder.py"", line 18, in <module> from object_detection.builders import box_coder_builder File ""/root/.local/lib/python2.7/site-packages/object_detection/builders/box_coder_builder.py"", line 21, in <module> from object_detection.protos import box_coder_pb2 File ""/root/.local/lib/python2.7/site-packages/object_detection/protos/box_coder_pb2.py"", line 28, in <module> dependencies=[object__detection_dot_protos_dot_faster__rcnn__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_keypoint__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_mean__stddev__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_square__box__coder__pb2.DESCRIPTOR,]) File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/descriptor.py"", line 829, in __new__ return _message.default_pool.AddSerializedFile(serialized_pb) TypeError: Couldn't build proto file into descriptor pool! Invalid proto descriptor for file ""object_detection/protos/box_coder.proto"": object_detection/protos/box_coder.proto: Import ""object_detection/protos/keypoint_box_coder.proto"" has not been loaded. object_detection.protos.BoxCoder.keypoint_box_coder: ""object_detection.protos.KeypointBoxCoder"" seems to be defined in ""keypoint_box_coder.proto"", which is not imported by ""object_detection/protos/box_coder.proto"". To use it here, please add the necessary import. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=36123659232&resource=ml_job%2Fjob_id%2Fgrewe_object_detection_5&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22grewe_object_detection_5%22",1,,[],2018-05-21 04:29:15,open,,,[],2018-05-21 04:48:11
968,tensorflow/models,models,4320,jytime,Gray padding when rand_crop,"------------------------
### System information
- **What is the top-level directory of the model you are using**:deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:1.7.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:CUDA Version 8.0.61
- **GPU model and memory**:two NVIDIA Corporation GM200 [GeForce GTX TITAN X]

### Describe the problem
Hi
When I am using deeplab v3 to train on SYNTHIA dataset (http://synthia-dataset.net/). This is a dataset with a similar format with cityscapes. It seems that some gray paddings are added to the input images. I am confused that is this specifically designed? Or did I do anything wrong? 
The size of SYNTHIA images is 1280x760. I set the train_crop_size as 513x513. The train_batch_size is 8 , the output_stride is 16 and fine_tune_batch_norm = True. 
I noticed the gray padding would occur in 10% images. As shown in tensorboard, the strange one is like:
![wechatimg146](https://user-images.githubusercontent.com/37647961/40285480-cd987558-5cdf-11e8-8aa8-926c5ad65647.jpeg)
While one good example is like:
![wechatimg145](https://user-images.githubusercontent.com/37647961/40285498-233f87a8-5ce0-11e8-84d0-edda1db1cc36.jpeg)
Hope for suggestions
",2,,[],2018-05-21 00:29:12,open,,,[],2018-05-22 14:30:35
969,tensorflow/models,models,4319,ProjectDent,Running same model on TF Models 1.8.0 is much slower than running on earlier release,"I previously followed [this modified version](https://medium.com/google-cloud/object-detection-tensorflow-and-google-cloud-platform-72e0a3f3bdd6) of the [Pets on Google Cloud tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md). The key difference is that this tutorial recommends running on commit a4944a57ad from June 15th 2017, when the TensorFlow Object Detection API was first added. This is to negate some issues they were running into on the most recent release, at their time of writing (and indeed, I ran into similar issues about a month ago, hence following their tutorial).

I'm training on the Faster-RCNN-Resnet101 model. I have 5 classes, and 5000 steps.

Training my model using this older commit takes about 20 minutes. I've optimised the accuracy/loss by improving my input data. Now that I'm satisfied with that, I decided to switch to TensorFlow Models 1.8.0, so that my repo is up to date.

Now, I'm training my model using the exact same setup, only on TensorFlow Models 1.8.0, but with TensorFlow 1.2 still set as my `runtime-version` (as per the official tutorial). It completes successfully. Rather than taking 20 minutes though, it takes 80 minutes. Earlier, each step would be logged as taking about 1s, and now they take about 5/6 seconds (assuming this is multi-threaded).

I'd like to be on the latest release of TensorFlow Models, but seem limited by the fact that it's so much slower.",2,,[],2018-05-20 12:24:11,open,,,['stat:awaiting response'],2018-06-05 09:47:42
970,tensorflow/models,models,4318,yzg050215,[slim-ResNet-V1]Question about the bottleneck in resnet.py,"System information

    What is the top-level directory of the model you are using:
    models/research/slim/

    Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
    No
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
    Linux Ubuntu 16.01
    TensorFlow installed from (source or binary):
    conda install
    TensorFlow version (use command below):
    1.0.0
    Bazel version (if compiling from source): N/A
    CUDA/cuDNN version:
    CUDA® Toolkit 8.0; cuDNN v7.1
    GPU model and memory:
    GForce GTX Titan X
    Exact command to reproduce: N/A

 when i reading the code of bottleneck in resnet.py in ""slim.nets"", there is a comment about ""When putting together two consecutive ResNet blocks that use this unit, one  should use stride = 2 in the last unit of the first block.""

However, according to the original description of  resnet, see: http://ethereon.github.io/netscope/#/gist/b21e2aae116dc1ac7b50, stride 2 is used in the first unit of the second block. ",1,,[],2018-05-20 08:51:42,open,,,['stat:awaiting response'],2018-05-27 07:53:10
971,tensorflow/models,models,4314,ProjectDent,"""UnavailableError: OS Error"" when running training on Google Cloud with TensorFlow 1.8","My model trains fine in 20 minutes with TensorFlow 1.2. I changed my cloud.yml file's `runtimeVersion` to TensorFlow 1.8, my setup.py file's `REQUIRED PACKAGES` to require `'Tensorflow>=1.8.0'`, and my submit training command's `runtime-version` to 1.8.

Now, training took about 80 minutes, before crashing at 4940 steps (60 short of my 5000 steps) I'd set in my training, with this error:

> The replica worker 2 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 184, in <module> tf.app.run() File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run _sys.exit(main(argv)) File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 180, in main graph_hook_fn=graph_rewriter_fn) File ""/root/.local/lib/python2.7/site-packages/object_detection/trainer.py"", line 399, in train saver=saver) File ""/root/.local/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 769, in train sess, train_op, global_step, train_step_kwargs) File ""/root/.local/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_step run_metadata=run_metadata) File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 900, in run run_metadata_ptr) File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1135, in _run feed_dict_tensor, options, run_metadata) File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run run_metadata) File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call raise type(e)(node_def, op, message) UnavailableError: OS Error",17,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-05-19 16:02:50,open,,,['stat:awaiting response'],2019-03-09 03:13:45
972,tensorflow/models,models,4307,Kmeliani,Train and Test loss on Tensorboard Losses/TotalLoss,"------------------------

### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  MacOS High Sierra 10.13.4
- **TensorFlow installed from (source or binary)**:  pip install
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0
- **CUDA/cuDNN version**: I am using CPU
- **CPU model and memory**: 2,2 Ghz Intel Core i7; 16 GB 1600 MHz DDR3


### Describe the problem
Hi !
I wish to display both train and eval losses in Tensorboard Losses/TotalLoss.

I display correctly classification_loss, localization_loss, PascalBoxes_PerformanceByCategory and PascalBoxes_Precision for eval.

How can i achieve that ?

Thank you in advance,
",1,,[],2018-05-18 15:45:25,open,,"NamedUser(login=""hgadig"")",['stat:awaiting response'],2018-12-05 21:55:51
973,tensorflow/models,models,4306,CasiaFan,[Bug Report]: returned shape of resize_to_range() function in preprocessor of object detection API,"I use this [function](https://github.com/tensorflow/models/blob/master/research/object_detection/core/preprocessor.py#L2125) for image resizing and padding during image preprocessing. Based on the document, the output shape should be like `[max_dim, max_dim, 3]` if I turn on `pad_to_max_dimension` parameter. But unfortunately it fails.  I check the source code and it seems that code like `new_size = tf.stack([max_dimension, max_dimension, 3])`  should be added in [this block](https://github.com/tensorflow/models/blob/master/research/object_detection/core/preprocessor.py#L2183), right?
",3,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-05-18 09:34:41,open,,,['stat:awaiting owner'],2018-10-08 12:44:33
974,tensorflow/models,models,4303,haichaoyu,[Deeplab] Why does evaluation performance depend on eval_crop_size heavily?,"Hello,

I finetuned Deeplabv3+ on ADE20K. During testing, mIOU varies with different eval_crop_size defined in eval.py. The mIOU difference between eval_crop_size=1601 and eval_crop_size=2801 (out_stride=8) can be up to 0.04. Does anyone have an idea about it? Thanks",1,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-05-17 23:37:52,open,,,['stat:awaiting response'],2018-05-18 07:00:10
975,tensorflow/models,models,4302,kramea,SSD VGG Config file for object detection,Has anyone written a config file for SSD VGGnet like the ones [here](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)?,2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-05-17 23:04:18,open,,,['stat:contributions welcome'],2018-06-06 18:14:33
976,tensorflow/models,models,4301,ywang370,TF records convert image to 4 dimension. ,"I build my own dataset from online images and want to convert them to tf-records files. All the image are jpeg with opencv checked dimension - make sure all the image has 3 dimension. 

I use the  models/research/slim/datasets/download_and_convert_flowers.py to convert my own dataset and double check the dimension on the decode jpeg function like the following:
  def read_image_dims(self, sess, image_data):
    image = self.decode_jpeg(sess, image_data)
    if not len(image.shape)==3:
            print(image.shape)
    return image.shape[0], image.shape[1]

  def decode_jpeg(self, sess, image_data):
    image = sess.run(self._decode_jpeg,
                     feed_dict={self._decode_jpeg_data: image_data})
    assert len(image.shape) == 3
    assert image.shape[2] == 3
    return image

However, no matter how I try to avoid the dimension issues when I read the tf records  I always find some files are with 4 dimension:   such as (1, 1143, 806, 3)

I got some warning when I convert the files:
2018-05-17 15:09:48.820251: W tensorflow/core/lib/png/png_io.cc:87] PNG warning: iCCP: known incorrect sRGB profile
2018-05-17 15:10:01.875096: W tensorflow/core/lib/png/png_io.cc:87] PNG warning: iCCP: known incorrect sRGB profile
2018-05-17 15:10:21.355789: W tensorflow/core/lib/png/png_io.cc:87] PNG warning: sBIT: invalid
2018-05-17 15:10:36.006651: W tensorflow/core/lib/png/png_io.cc:87] PNG warning: iCCP: known incorrect sRGB profile
2018-05-17 15:11:08.619677: W tensorflow/core/lib/png/png_io.cc:87] PNG warning: iCCP: known incorrect sRGB profile
Corrupt JPEG data: premature end of data segment
",1,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-05-17 22:11:57,open,,,['stat:awaiting response'],2018-05-30 20:49:54
977,tensorflow/models,models,4295,tanutarou,fix read mode of  build_ade20k_data.py,"related : https://github.com/tensorflow/models/pull/3741  
build_ade20k_data.py has a same problem.
The FastGFile(.., 'b') does not work for Python 3",3,,[],2018-05-17 14:59:08,open,,,['cla: yes'],2018-05-18 01:26:27
978,tensorflow/models,models,4288,clytieai,question about finetune batchnorm layer,"If i use 8 pieces of GPUs and each batchsize is 2, are the parametres of BN  finetuned by  using 16 images or just 2 images each time ?",5,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-05-17 13:27:42,open,,,['stat:awaiting owner'],2018-08-17 21:37:13
979,tensorflow/models,models,4285,a819721810,Multiple GPUs speed up training time for Faster R-CNN?,"
### System information
- **What is the top-level directory of the model you are using**:object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:tensorflow-gpu-1.7.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0/7.0.5
- **GPU model and memory**:3*1080Ti and 11GB
- **Exact command to reproduce**:


### Describe the problem
   I used to see https://www.tensorflow.org/performance/benchmarks,and i did test at my own computer,its speed ratio is the same as the above said.
  When i tried to use multiple to train real data by Faster R-CNN, How can I discriminate the acceleration multiplier?By time of loss convergence?
   I did test of ""faster_rcnn_inception_resnet_v2_atrous_coco.config"" .when i added multiple GPUs ,i also added batch_size.There are my two 1080ti GPUs training parameters:
![image](https://user-images.githubusercontent.com/10041362/40165830-907b55fc-59ef-11e8-8f4d-8fc0c8c97c38.png)

   This is my two 1080ti GPUs training result:
   
![image](https://user-images.githubusercontent.com/10041362/40165326-4dea27fa-59ee-11e8-88c3-8561594f842c.png)


   
   This is my one 1080ti GPU training result:
![image](https://user-images.githubusercontent.com/10041362/40165411-75a91c9c-59ee-11e8-88e5-bbb8dc0f121b.png)

   The GPU utilization will be about 70%, and one GPU utilization rate is higher than multiple GPUs utilization . One GPU about 80%, multiple GPUs about:60%~70% 
![image](https://user-images.githubusercontent.com/10041362/40165542-ddc5bc4a-59ee-11e8-8f75-f91e00aa9a8c.png)



   Then, i did other config ""faster_rcnn_resnet50_coco.config"" training:
   This is my three 1080ti GPUs training result:
![image](https://user-images.githubusercontent.com/10041362/40165992-ff7f1394-59ef-11e8-92bc-b284748b2229.png)

    This is my one 1080ti GPU training result:
![image](https://user-images.githubusercontent.com/10041362/40166038-23017668-59f0-11e8-9682-f360ab92e8cc.png)


    Multiple GPUs train Faster R-CNN that i cannot see big difference ,except it's a little smoother.How should I confirm its acceleration? Multiple GPUs also didn't increase sec/step.  I am sure that CPU,RAM,I/O don't be bottleneck for my training.
    Looking forward to your reply,thanks.

    
   

",13,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-05-17 08:43:47,open,,,[],2019-01-13 06:58:53
980,tensorflow/models,models,4280,surfreta,[deeplab] regarding the input for eval.py,"In `deeplab`, it includes `eval.py`, is that for prediction purposes? After reading the code, it seems that it uses the same input data path storing the generated `tfrecord `file as used by `train.py`. Is that true?",5,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-05-16 15:53:05,open,,,[],2018-05-27 03:26:13
981,tensorflow/models,models,4278,dailystudio,DeepLab with TensorFlow Mobile or TensorFlow Lite,"Hello @aquariusjay,

We just want to run this modal on Android. We have tried two approach TensorFlow Mobile and TensorFlow Lite.

With **TensorFlow Mobile**, we download the pre-trained modals with MobileNetV2:
mobilenetv2_coco_voc_trainaug
mobilenetv2_coco_voc_trainval
mobilenetv2_coco_cityscapes_trainfine

We can successfully load the modal, but when run the inference, we get the following error:
```
05-16 16:06:34.122 611-635/? E/AndroidRuntime: FATAL EXCEPTION: AsyncTask #1
    Process: com.orange.labs.colorme.dev, PID: 611
    java.lang.RuntimeException: An error occurred while executing doInBackground()
        at android.os.AsyncTask$3.done(AsyncTask.java:325)
        at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:354)
        at java.util.concurrent.FutureTask.setException(FutureTask.java:223)
        at java.util.concurrent.FutureTask.run(FutureTask.java:242)
        at android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:243)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1133)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:607)
        at java.lang.Thread.run(Thread.java:761)
     Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Slice' with these attrs.  Registered devices: [CPU], Registered kernels:
      device='CPU'; T in [DT_BOOL]
      device='CPU'; T in [DT_FLOAT]
      device='CPU'; T in [DT_INT32]
    
    	 [[Node: SemanticPredictions = Slice[Index=DT_INT32, T=DT_INT64](ArgMax, SemanticPredictions/begin, SemanticPredictions/size)]]
        at org.tensorflow.Session.run(Native Method)
        at org.tensorflow.Session.access$100(Session.java:48)
        at org.tensorflow.Session$Runner.runHelper(Session.java:298)
        at org.tensorflow.Session$Runner.runAndFetchMetadata(Session.java:260)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:220)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
        at com.dailystudio.deeplab.DeeplabV3.segment(DeeplabV3.java:104)
        at com.dailystudio.deeplab.DeeplabApplication$1.doInBackground(DeeplabApplication.java:46)
        at com.dailystudio.deeplab.DeeplabApplication$1.doInBackground(DeeplabApplication.java:22)
        at android.os.AsyncTask$2.call(AsyncTask.java:305)
        at java.util.concurrent.FutureTask.run(FutureTask.java:237)

```
I think this is caused by the output node ""SemanticPredictions"" call the operation Slice with INT64 data. This is not supported by TensorFlow Mobile yet.

With **TensorFlow Lite**, we use the following command to convert is to tflite format:
```
toco \
	--input_file=$(pwd)/model/frozen_inference_graph.pb \
	--input_format=TENSORFLOW_GRAPHDEF \
	--output_format=TFLITE \
	--output_file=$(pwd)/model/deeplabv3_mnv2_pascal_trainval.tflite \
	--inference_type=FLOAT \
	--input_type=QUANTIZED_UINT8 \
	--input_arrays=ImageTensor \
	--output_arrays=SemanticPredictions \
	--input_shapes=1,513,513,3 \
	--default_ranges_min=0 --default_ranges_max=255

```
We get the following warnings:
```
2018-05-16 16:19:29.429205: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:245] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.
2018-05-16 16:19:29.492371: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Equal
2018-05-16 16:19:29.492813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: LogicalAnd
2018-05-16 16:19:29.521856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 812 operators, 1241 arrays (0 quantized)
2018-05-16 16:19:29.550535: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 802 operators, 1222 arrays (0 quantized)
2018-05-16 16:19:29.582482: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 802 operators, 1222 arrays (0 quantized)
2018-05-16 16:19:29.610492: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 148 operators, 358 arrays (0 quantized)
2018-05-16 16:19:29.613697: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 148 operators, 358 arrays (0 quantized)
2018-05-16 16:19:29.616876: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 143 operators, 348 arrays (0 quantized)
2018-05-16 16:19:29.619981: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 4: 142 operators, 346 arrays (0 quantized)
2018-05-16 16:19:29.622935: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 5: 141 operators, 344 arrays (0 quantized)
2018-05-16 16:19:29.626348: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 141 operators, 344 arrays (0 quantized)
2018-05-16 16:19:29.629622: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 3158144 bytes, theoretical optimal value: 3158144 bytes.
2018-05-16 16:19:29.631849: F tensorflow/contrib/lite/toco/tflite/export.cc:315] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ExpandDims, Slice, Stack, TensorFlowShape.
./convert-lite.sh: line 11: 70835 Abort trap: 6           /Volumes/Workspace/tensorflow/workspace/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=$(pwd)/model/frozen_inference_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=$(pwd)/model/deeplabv3_mnv2_pascal_trainval.tflite --inference_type=FLOAT --input_type=QUANTIZED_UINT8 --input_arrays=ImageTensor --output_arrays=SemanticPredictions --input_shapes=1,513,513,3 --default_ranges_min=0 --default_ranges_max=255
```
The model could not be loaded successfully. I think it is caused the warning:
Here is a list of operators for which you will need custom implementations: ExpandDims, Slice, Stack, TensorFlowShape.

Is it possible to update node SemanticPredictions to use INT32 data type on Slice operation? Or do you have any suggestion on how to run it with TensorFlow lite?",17,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay""), NamedUser(login=""yhliang2018"")]",2018-05-16 08:35:01,open,,,"['stat:awaiting tensorflower', 'stat:community support']",2019-01-21 09:15:28
982,tensorflow/models,models,4277,xychu,Use `tf.estimator.train_and_evaluate` in cifar10,,0,,[],2018-05-16 03:51:06,open,,,['cla: yes'],2018-05-16 03:51:08
983,tensorflow/models,models,4276,fzou1,add warmup and horovod support (Inception V3 could reach to 77.6% …,"…test accuracy with 2K global batch size, 32 per CPU rank); add number of intra and inter threads to get better training performance on CPU",6,,[],2018-05-16 01:17:31,open,,,['cla: yes'],2018-08-16 17:53:47
984,tensorflow/models,models,4271,shamidreza,denoising autoencoder: scale tf placeholder not used,"The data corrupter scale tf placeholder is not used in the graph, rather the scale python variable is used. 
https://github.com/tensorflow/models/blob/461fc09474d8f532b9c0250dd54b885c537df99f/research/autoencoder/autoencoder_models/DenoisingAutoencoder.py#L16

should be changed to:
`self.hidden = self.transfer(tf.add(tf.matmul(self.x + self.scale * tf.random_normal((n_input,)),`

This change would allow users to modify model.training_scale during training iterations if desired.",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-05-15 18:30:19,open,,,['stat:awaiting response'],2018-05-16 01:19:24
985,tensorflow/models,models,4268,harshthaker,Can crop_to_bounding_box method be run on more than single image at once ?,"**System information**
What is the top-level directory of the model you are using: object_detection
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Added the code to crop detected objects in object_detection_tutorial
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.7.0
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A

**Problem**
I am trying to process cropping operation on a batch of images. As described in tensorflow documentation crop_to_bounding_box method takes following args:

image: 4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor of shape [height, width, channels].
offset_height: Vertical coordinate of the top-left corner of the result in the input.
offset_width: Horizontal coordinate of the top-left corner of the result in the input.
target_height: Height of the result.
target_width: Width of the result.

Here's the code glimpse:

(boxes, scores, classes, num_detections) = sess.run(
                    [boxes, scores, classes, num_detections],
                    feed_dict={image_tensor: images_batch})

cropped_image = tf.image.crop_to_bounding_box(image, ymin, xmin, ymax-ymin, xmax-xmin)

4-D tensor can be fed to crop method, but each image has its relevant boxes. So, offset_height, offset_width, target_height, target_width could also be arrays ?",3,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-05-15 13:54:55,open,,,['stat:awaiting owner'],2018-05-21 21:32:58
986,tensorflow/models,models,4266,Abhijit-2592,Object detection:Time for processing a batch of images at once is almost equal to processing them sequentially in order,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, just for inference
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: conda
- **TensorFlow version (use command below)**: 1.4.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: Nvidia GTX 1060 6GB
- **Exact command to reproduce**:
### Problem:
Theoretically, running inference on a batch of n images (n is the maximum number of images the GPU can accommodate) and 1 image should take almost the same time. I am trying to do batch inference using the faster_rcnn_inception_v2_coco's frozen_inference_graph. Visualizing the frozen inference graph shows that the input is of shape (None,None,None,3), thus I can safely process a batch of images simultaneously. 
![screenshot 15](https://user-images.githubusercontent.com/29851832/40047507-4ca0ce32-584d-11e8-88f6-c663338c0a4a.png). But when I tried processing a batch of 1 image vs a batch of 10 images, I didn't get the same inference time

The last block in the object detection tutorial Ipython Notebook is modified as follows and the graph used is faster_rcnn_inception_v2_coco.

```
import cv2
itr_num = 10
batch_size = 1
inference_times = []
with detection_graph.as_default():
    with tf.Session(graph=detection_graph) as sess:
        # Definite input and output Tensors for detection_graph
        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
        num_detections = detection_graph.get_tensor_by_name('num_detections:0')
        for im_num in range(itr_num):
            images = [cv2.cvtColor(cv2.imread(TEST_IMAGE_PATHS[1]),cv2.COLOR_BGR2RGB) for _ in range(batch_size)]
            images = np.array(images)
            print(images.shape)
            assert len(images) == batch_size, ""lists are not equal {}, {}"".format(len(images),batch_size)
            start_time = time.time()
            (boxes, scores, classes, num) = sess.run(
              [detection_boxes, detection_scores, detection_classes, num_detections],
              feed_dict={image_tensor: images})
            print(""The time taken for image set{} is {} secs"".format(im_num+1,time.time()-start_time))
            inference_times.append(time.time()-start_time)
            # Visualization of the results of a detection.
#             vis_util.visualize_boxes_and_labels_on_image_array(
#               image_np,
#               np.squeeze(boxes),
#               np.squeeze(classes).astype(np.int32),
#               np.squeeze(scores),
#               category_index,
#               use_normalized_coordinates=True,
#               line_thickness=8)
            
inference_times = np.array(inference_times[1:])
print(""average time for {} (images excluding the first image) is {}"".format(itr_num-1,inference_times.mean()))
```

### Output for batch_size=1

> (1, 900, 1352, 3)
> The time taken for image set1 is 2.029768228530884 secs
> (1, 900, 1352, 3)
> The time taken for image set2 is 0.12289047241210938 secs
> (1, 900, 1352, 3)
> The time taken for image set3 is 0.11852574348449707 secs
> (1, 900, 1352, 3)
> The time taken for image set4 is 0.12165331840515137 secs
> (1, 900, 1352, 3)
> The time taken for image set5 is 0.11874818801879883 secs
> (1, 900, 1352, 3)
> The time taken for image set6 is 0.11887550354003906 secs
> (1, 900, 1352, 3)
> The time taken for image set7 is 0.122528076171875 secs
> (1, 900, 1352, 3)
> The time taken for image set8 is 0.12240052223205566 secs
> (1, 900, 1352, 3)
> The time taken for image set9 is 0.1199343204498291 secs
> (1, 900, 1352, 3)
> The time taken for image set10 is 0.12351012229919434 secs
> average time for 9 (images excluding the first image) is 0.12106058332655165 

### Output for batch_size = 10:

> (10, 900, 1352, 3)
> The time taken for image set1 is 2.459228992462158 secs
> (10, 900, 1352, 3)
> The time taken for image set2 is 0.9496431350708008 secs
> (10, 900, 1352, 3)
> The time taken for image set3 is 0.9487781524658203 secs
> (10, 900, 1352, 3)
> The time taken for image set4 is 0.9454429149627686 secs
> (10, 900, 1352, 3)
> The time taken for image set5 is 0.9377844333648682 secs
> (10, 900, 1352, 3)
> The time taken for image set6 is 0.9478261470794678 secs
> (10, 900, 1352, 3)
> The time taken for image set7 is 0.9489026069641113 secs
> (10, 900, 1352, 3)
> The time taken for image set8 is 0.9412193298339844 secs
> (10, 900, 1352, 3)
> The time taken for image set9 is 1.0171809196472168 secs
> (10, 900, 1352, 3)
> The time taken for image set10 is 0.938474178314209 secs
> average time for 9 (images excluding the first image) is 0.9528606202867296

The time taken for a batch of 10 images processed together ~ 10 times the time taken to process a single image, Which shouldn't be the case. What am I missing here? Thanks

### PS: 
The time taken for inferring 1 image is almost twice as given in the model zoo bench mark (0.058).    ",8,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-05-15 09:21:28,open,,"NamedUser(login=""Abhijit-2592"")",['stat:awaiting owner'],2018-06-29 04:53:22
987,tensorflow/models,models,4260,89douner,when do I finish training?,"I'm training ssd_mobilenetV2 as below.
![2018-05-15 13-54-12](https://user-images.githubusercontent.com/31752297/40037275-79b56f64-5847-11e8-943d-3a37bf974179.png)
![2018-05-15 13-09-17](https://user-images.githubusercontent.com/31752297/40037276-7c07f412-5847-11e8-828c-20cb9f4bf8f2.png)

But, I don't know when I have to finish training. Usually, when do you finish training?
Please let me know!
",4,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-05-15 04:55:45,open,,,['stat:awaiting owner'],2018-05-16 03:26:36
988,tensorflow/models,models,4259,yryun,Training bug in mobilenet v1 extractor for Faster r-cnn,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
custom kitti format dataset (good performance in Inception, Resnet extractor)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 3.4
- **Bazel version (if compiling from source)**: No
- **GCC/Compiler version (if compiling from source)**: GCC 4.8.4
- **CUDA/cuDNN version**: 9, 7
- **GPU model and memory**: Tesla 40
- **Exact command to reproduce**:

### Describe the problem
It seems like training bug with Faster R-CNN Mobilenet.
I've been training Faster R-CNN with mobilenet feature extractor for 400k iteration, batch 8, learning rate 3e-03(mentioned in HuangMurphy_2017_Speed,accuracy trade-offs for modern convolutional object detectors). But it's mAP is ""zero"".  I'm using my own dataset and it's going well with InceptionV2, Resnet50, 101. mAP of InceptionV2, Resnet50, 101 is 0.7. So it's not about hyperparameter tuning problem.


### Source code / logs
here is loss of inception v2, which has good mAP.
![image](https://user-images.githubusercontent.com/39238559/40033921-76956e36-5835-11e8-966f-347359f9588d.png)

here is loss of mobilenet. Second stage loss is strangely low (loss is zero almost of time)
![image](https://user-images.githubusercontent.com/39238559/40033856-352ec6ae-5835-11e8-973e-44c2806cacea.png)

here is mAP of mobilenet. How can it be a zero?
![image](https://user-images.githubusercontent.com/39238559/40033951-b43a1142-5835-11e8-8166-c4315a83a17c.png)

here is my tensorboard distributions of mobilenet. Compared to Incepction V2, mobilenet has No change in second stage conv2d_12, conv2d_13 - moving_mean, moving_variance. I think it could be a clue of cause.
![image](https://user-images.githubusercontent.com/39238559/40034051-1937993e-5836-11e8-860c-e722bb71725b.png)

",10,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-05-15 02:57:25,open,,,['stat:awaiting owner'],2019-03-25 08:28:50
989,tensorflow/models,models,4253,yryun,New tutorial inference run time is too slower than old tutorial inference run time,"------------------------

### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: experimented in windows 10 and ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: 1.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9, 7
- **GPU model and memory**: 1050Ti, 4GB
- **Exact command to reproduce**:

### Describe the problem

I tried old and new version of sess.run in tutorial with two model(faster r-cnn, mask r-cnn with incept v2).
I check the fps of sess.run line. Old version take short time to run (4~7FPS), but New version take too long time to run (0.2FPS). It happened for both model. And GPU-UTIL percent is almost zero in new version. I think it's because of mask process in new version. 
But why too slow with faster r-cnn (only detection)?
Is there any solution for this?


### Source code / logs

Old version sess.run in tutorial with only detection
`(boxes, scores, classes) = sess.run([detection_boxes, detection_scores, detection_classes], feed_dict = {image_tensor: image_np_expanded})`

New version sess.run in tutorial with detection and mask
```
tensor_dict = {}
        for key in [
            'num_detections', 'detection_boxes', 'detection_scores',
            'detection_classes', 'detection_masks'
        ]:
            tensor_name = key + ':0'
            if tensor_name in all_tensor_names:
                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)
# ......
output_dict = sess.run(tensor_dict, feed_dict={image_tensor: np.expand_dims(image_np, 0)})
```
",1,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-05-14 01:27:17,open,,,['stat:awaiting owner'],2018-05-14 20:16:56
990,tensorflow/models,models,4252,stprior,#4251 use container optimized os rather than container-vm,"#4251 container-vm images no longer work - see https://cloud.google.com/container-optimized-os/docs/resources/faq
",0,,[],2018-05-13 22:57:49,open,,,['cla: yes'],2018-05-13 22:57:52
991,tensorflow/models,models,4249,Zzzbang,"Trying to run eval,ValueError: Metric not found: pascal_voc_metrics","
### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary(anaconda)
- **TensorFlow version (use command below)**:tensorflow1.6.0
- **CUDA/cuDNN version**:CUDA 8.0  cuDNN 7.0.5
- **GPU model and memory**:GTX860 2GB
- **Exact command to reproduce**:python eval.py  --pipeline_config_path=/home/zyf/faster/ssd_mobilenet_v1_raccoon.config  --checkpoint_dir=/home/zyf/faster/train --eval_dir=/home
/zyf/faster/eval


### Describe the problem

python eval.py  --pipeline_config_path=/home/zyf/faster/ssd_mobilenet_v1_raccoon.config  --checkpoint_dir=/home/zyf/faster/train --eval_dir=/home/zyf/faster/eval
/home/zyf/anaconda2/envs/tensorflow27/lib/python2.7/site-packages/matplotlib/__init__.py:1405: UserWarning: 
This call to matplotlib.use() has no effect because the backend has already
been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
Traceback (most recent call last):
  File ""eval.py"", line 142, in <module>
    tf.app.run()
  File ""/home/zyf/anaconda2/envs/tensorflow27/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""eval.py"", line 138, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/home/zyf/models/research/object_detection/evaluator.py"", line 218, in evaluate
    evaluator_list = get_evaluators(eval_config, categories)
  File ""/home/zyf/models/research/object_detection/evaluator.py"", line 113, in get_evaluators
    raise ValueError('Metric not found: {}'.format(eval_metric_fn_key))
ValueError: Metric not found: pascal_voc_metrics



Here part of my config file:
eval_config: {
  num_examples: 1
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  metrics_set:""pascal_voc_metrics"" 
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""/home/zyf/faster/data/eval.record""
  }
  label_map_path: ""/home/zyf/faster/data/raccoon_lable_map.pbtxt""
  shuffle: false
  num_readers: 1
}

",11,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-05-13 11:39:04,open,,,[],2019-03-16 18:47:44
992,tensorflow/models,models,4244,willSapgreen,Learning rate is close to zero but AP is still fluctuating.,"### System information
- **What is the top-level directory of the model you are using**:
Tensorflow Object Detection API

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No, but I adjust the 

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
Binary

- **TensorFlow version (use command below)**:
1.4.1

- **Bazel version (if compiling from source)**:
X

- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.0

- **GPU model and memory**:
GeForce GTX 1050, 2G memory

- **Exact command to reproduce**:
Run train.py

### Describe the problem
I use SSD-InceptionV2-Coco model to train with my own dataset ( two classes )
I intentionally decay the learning rate 50% every 2000 iteration, starting from 0.04.
My understanding is the model will stop learning when learning rate is close to zero. ( as shown below )
![learning-rate-close-to-zero](https://user-images.githubusercontent.com/6188375/39938754-d1c9b83c-5508-11e8-84d6-fb3888d0399b.png), 

and the bias/weight looks stop learning too.
![bias-weight](https://user-images.githubusercontent.com/6188375/39938794-ef2aed92-5508-11e8-9862-ffc646a4b67f.jpg)
( Another question is why bias learns at beginning but weight never learns )

However, the AP is still fluctuating. My evalulation datasetset contains 202 images, and I set shuffle to False.
As you can see that there is a pattern in the AP.
So I wonder if the AP is calculated based on part of evalulation dataset, not all 202 images in my case.
![learning-rate-close-to-zero-but-ap-flustrate](https://user-images.githubusercontent.com/6188375/39938857-2696b496-5509-11e8-9e5c-5e521d499bed.jpg)

### Source code / logs
Here is the config file
[pipeline.zip](https://github.com/tensorflow/models/files/1996341/pipeline.zip)


Thank you for precious time on my question.",1,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-05-11 18:05:47,open,,,['stat:awaiting tensorflower'],2018-09-22 00:22:40
993,tensorflow/models,models,4242,Zumbalamambo,Unable to train mask rcnn upon the pet dataset,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacBook Pro
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.8
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: No
- **GPU model and memory**: CPU. 8GB ram
- **Exact command to reproduce**:

Im trying to train the mask rcnn .

When Im trying to train after generating tf records, Im getting the following error,

> INFO:tensorflow:Error reported to Coordinator: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed)]]
> 
> Caused by op 'Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert', defined at:
>   File ""object_detection/train.py"", line 167, in <module>
>     tf.app.run()
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
>     _sys.exit(main(argv))
>   File ""object_detection/train.py"", line 163, in main
>     worker_job_name, is_chief, FLAGS.train_dir)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/trainer.py"", line 275, in train
>     clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
>   File ""/Users/rocky/Documents/research/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
>     outputs = model_fn(*args, **kwargs)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/trainer.py"", line 200, in _create_losses
>     losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1608, in loss
>     groundtruth_masks_list,
>   File ""/Users/rocky/Documents/research/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1851, in _loss_box_classifier
>     groundtruth_boxlists, groundtruth_masks_list)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/core/target_assigner.py"", line 447, in batch_assign_targets
>     anchors, gt_boxes, gt_class_targets, gt_weights)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/core/target_assigner.py"", line 151, in assign
>     groundtruth_boxes.get())[:1])
>   File ""/Users/rocky/Documents/research/models/research/object_detection/utils/shape_utils.py"", line 279, in assert_shape_equal
>     return tf.assert_equal(shape_a, shape_b)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 405, in assert_equal
>     return control_flow_ops.Assert(condition, data, summarize=summarize)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
>     return _add_should_use_warning(fn(*args, **kwargs))
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 172, in Assert
>     return gen_logging_ops._assert(condition, data, summarize, name=""Assert"")
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 51, in _assert
>     name=name)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
>     op_def=op_def)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
>     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
> 
> InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed)]]
> Traceback (most recent call last):
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
>     return fn(*args)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
>     options, feed_dict, fetch_list, target_list, run_metadata)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
>     run_metadata)
> tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed)]]
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
>     yield
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 495, in run
>     self.run_loop()
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 1030, in run_loop
>     self._sv.global_step])
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
>     run_metadata_ptr)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
>     feed_dict_tensor, options, run_metadata)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
>     run_metadata)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed)]]
> 
> Caused by op 'Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert', defined at:
>   File ""object_detection/train.py"", line 167, in <module>
>     tf.app.run()
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
>     _sys.exit(main(argv))
>   File ""object_detection/train.py"", line 163, in main
>     worker_job_name, is_chief, FLAGS.train_dir)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/trainer.py"", line 275, in train
>     clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
>   File ""/Users/rocky/Documents/research/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
>     outputs = model_fn(*args, **kwargs)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/trainer.py"", line 200, in _create_losses
>     losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1608, in loss
>     groundtruth_masks_list,
>   File ""/Users/rocky/Documents/research/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1851, in _loss_box_classifier
>     groundtruth_boxlists, groundtruth_masks_list)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/core/target_assigner.py"", line 447, in batch_assign_targets
>     anchors, gt_boxes, gt_class_targets, gt_weights)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/core/target_assigner.py"", line 151, in assign
>     groundtruth_boxes.get())[:1])
>   File ""/Users/rocky/Documents/research/models/research/object_detection/utils/shape_utils.py"", line 279, in assert_shape_equal
>     return tf.assert_equal(shape_a, shape_b)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 405, in assert_equal
>     return control_flow_ops.Assert(condition, data, summarize=summarize)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
>     return _add_should_use_warning(fn(*args, **kwargs))
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 172, in Assert
>     return gen_logging_ops._assert(condition, data, summarize, name=""Assert"")
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 51, in _assert
>     name=name)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
>     op_def=op_def)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
>     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
> 
> InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed)]]
> 
> Traceback (most recent call last):
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
>     return fn(*args)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
>     options, feed_dict, fetch_list, target_list, run_metadata)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
>     run_metadata)
> tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed)]]
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 990, in managed_session
>     yield sess
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 769, in train
>     sess, train_op, global_step, train_step_kwargs)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_step
>     run_metadata=run_metadata)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
>     run_metadata_ptr)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
>     feed_dict_tensor, options, run_metadata)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
>     run_metadata)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed)]]
> 
> Caused by op 'Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert', defined at:
>   File ""object_detection/train.py"", line 167, in <module>
>     tf.app.run()
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
>     _sys.exit(main(argv))
>   File ""object_detection/train.py"", line 163, in main
>     worker_job_name, is_chief, FLAGS.train_dir)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/trainer.py"", line 275, in train
>     clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
>   File ""/Users/rocky/Documents/research/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
>     outputs = model_fn(*args, **kwargs)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/trainer.py"", line 200, in _create_losses
>     losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1608, in loss
>     groundtruth_masks_list,
>   File ""/Users/rocky/Documents/research/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1851, in _loss_box_classifier
>     groundtruth_boxlists, groundtruth_masks_list)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/core/target_assigner.py"", line 447, in batch_assign_targets
>     anchors, gt_boxes, gt_class_targets, gt_weights)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/core/target_assigner.py"", line 151, in assign
>     groundtruth_boxes.get())[:1])
>   File ""/Users/rocky/Documents/research/models/research/object_detection/utils/shape_utils.py"", line 279, in assert_shape_equal
>     return tf.assert_equal(shape_a, shape_b)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 405, in assert_equal
>     return control_flow_ops.Assert(condition, data, summarize=summarize)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
>     return _add_should_use_warning(fn(*args, **kwargs))
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 172, in Assert
>     return gen_logging_ops._assert(condition, data, summarize, name=""Assert"")
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 51, in _assert
>     name=name)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
>     op_def=op_def)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
>     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
> 
> InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed)]]
> 
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""object_detection/train.py"", line 167, in <module>
>     tf.app.run()
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
>     _sys.exit(main(argv))
>   File ""object_detection/train.py"", line 163, in main
>     worker_job_name, is_chief, FLAGS.train_dir)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/trainer.py"", line 399, in train
>     saver=saver)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 784, in train
>     ignore_live_threads=ignore_live_threads)
>   File ""/anaconda3/envs/jungle/lib/python3.6/contextlib.py"", line 99, in __exit__
>     self.gen.throw(type, value, traceback)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session
>     self.stop(close_summary_writer=close_summary_writer)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 828, in stop
>     ignore_live_threads=ignore_live_threads)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
>     six.reraise(*self._exc_info_to_raise)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/six.py"", line 693, in reraise
>     raise value
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
>     yield
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 495, in run
>     self.run_loop()
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 1030, in run_loop
>     self._sv.global_step])
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
>     run_metadata_ptr)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
>     feed_dict_tensor, options, run_metadata)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
>     run_metadata)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed)]]
> 
> Caused by op 'Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert', defined at:
>   File ""object_detection/train.py"", line 167, in <module>
>     tf.app.run()
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
>     _sys.exit(main(argv))
>   File ""object_detection/train.py"", line 163, in main
>     worker_job_name, is_chief, FLAGS.train_dir)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/trainer.py"", line 275, in train
>     clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
>   File ""/Users/rocky/Documents/research/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
>     outputs = model_fn(*args, **kwargs)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/trainer.py"", line 200, in _create_losses
>     losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1608, in loss
>     groundtruth_masks_list,
>   File ""/Users/rocky/Documents/research/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1851, in _loss_box_classifier
>     groundtruth_boxlists, groundtruth_masks_list)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/core/target_assigner.py"", line 447, in batch_assign_targets
>     anchors, gt_boxes, gt_class_targets, gt_weights)
>   File ""/Users/rocky/Documents/research/models/research/object_detection/core/target_assigner.py"", line 151, in assign
>     groundtruth_boxes.get())[:1])
>   File ""/Users/rocky/Documents/research/models/research/object_detection/utils/shape_utils.py"", line 279, in assert_shape_equal
>     return tf.assert_equal(shape_a, shape_b)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 405, in assert_equal
>     return control_flow_ops.Assert(condition, data, summarize=summarize)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
>     return _add_should_use_warning(fn(*args, **kwargs))
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 172, in Assert
>     return gen_logging_ops._assert(condition, data, summarize, name=""Assert"")
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 51, in _assert
>     name=name)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
>     op_def=op_def)
>   File ""/anaconda3/envs/jungle/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
>     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
> 
> InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed)]]
> 
> 
> 
> 
",6,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-05-11 14:35:06,open,,,[],2018-07-13 13:46:01
994,tensorflow/models,models,4237,Zumbalamambo,Training mask rcnn,I have generated the mask for training mask rcnn but Im not able to train it as it requires bounding box. Has anybody succeeded in training the mask rcnn?,4,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-05-11 06:47:38,open,,,['stat:awaiting owner'],2018-07-13 14:35:00
995,tensorflow/models,models,4226,aashish-0393,"[object_detection, instance_segmentation] How to train mask_rcnn_inception_resnet_v2_atrous_coco instance segmentation on my own dataset","hi ,
please help me with training my own dataset on **mask_rcnn_inception_resnet_v2_atrous_coco** model.
I have refered to https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/instance_segmentation.md ; but I can't clearly understand the steps. 
Do we have to give the Bounding box coordinates of the object along with the mask.png file?

How to convert the mask data to tfRecord files (for instance segmentation).?

Can anyone suggest the labelling tool used for bounding box as well as mask.png file!!

tools like LabelBox, labelme, labelimg gives either bounding box coordinated or mask.png file or the polygon coordinates for the object. 
please help",5,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-05-10 11:52:48,open,,,['stat:awaiting tensorflower'],2018-09-07 06:00:36
996,tensorflow/models,models,4225,Bahramudin,Error when running model_builder_test.py,"### System information
- **What is the top-level directory of the model you are using**: object-detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Bazel version (if compiling from source)**: --
- **CUDA/cuDNN version**: 9.0, 7.1.3
- **GPU model and memory**: Tesla P40, totalMemory: 22.38GiB freeMemory: 22.21GiB
- **Exact command to reproduce**:

I have installed Tensor-flow successfully, and when I try to install Object-Detection-API in issuing below command I got error:
Command:
`python3 object_detection/builders/model_builder_test.py`
Error:
```
ction/builders/model_builder_test.py
Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 21, in <module>
    from object_detection.builders import model_builder
  File ""/home/adil/workspace/models/research/object_detection/builders/model_builder.py"", line 17, in <module>
    from object_detection.builders import anchor_generator_builder
  File ""/home/adil/workspace/models/research/object_detection/builders/anchor_generator_builder.py"", line 21, in <module>
    from object_detection.protos import anchor_generator_pb2
  File ""/home/adil/workspace/models/research/object_detection/protos/anchor_generator_pb2.py"", line 15, in <module>
    from object_detection.protos import grid_anchor_generator_pb2 as object__detection_dot_protos_dot_grid__anchor__generator__pb2
  File ""/home/adil/workspace/models/research/object_detection/protos/grid_anchor_generator_pb2.py"", line 22, in <module>
    serialized_pb=_b('\n3object_detection/protos/grid_anchor_generator.proto\x12\x17object_detection.protos\""\xcd\x01\n\x13GridAnchorGenerator\x12\x13\n\x06height\x18\x01 \x01(\x05:\x03\x32\x35\x36\x12\x12\n\x05width\x18\x02 \x01(\x05:\x03\x32\x35\x36\x12\x19\n\rheight_stride\x18\x03 \x01(\x05:\x02\x31\x36\x12\x18\n\x0cwidth_stride\x18\x04 \x01(\x05:\x02\x31\x36\x12\x18\n\rheight_offset\x18\x05 \x01(\x05:\x01\x30\x12\x17\n\x0cwidth_offset\x18\x06 \x01(\x05:\x01\x30\x12\x0e\n\x06scales\x18\x07 \x03(\x02\x12\x15\n\raspect_ratios\x18\x08 \x03(\x02')
TypeError: __new__() got an unexpected keyword argument 'serialized_options'
```

**Note:** I have installed protobuf version 2.6 , 3.5.1, 3.4.0 **All not worked?**
",8,"NamedUser(login=""bitfort"")","[NamedUser(login=""bitfort"")]",2018-05-10 07:34:55,open,,,[],2018-10-01 10:27:46
997,tensorflow/models,models,4224,xiaoyongzhu,Fix bugs in object detection docs,"According to code here:
https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/ssd_meta_arch.py#L780

The interface that DetectionModels should implement to load a checkpoint into the Tensorflow graph should be `restore_map` rather than `restore`.",3,,[],2018-05-10 03:29:29,open,,,['cla: yes'],2018-05-10 03:32:06
998,tensorflow/models,models,4219,jamesben6688,unexpected objection detection result of mAP: -1,"## System information

- What is the top-level directory of the model you are using: object-detection
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.5.0
- Bazel version (if compiling from source): --
- CUDA/cuDNN version: 9.0, 7.0.5
- GPU model and memory: GTX 1080 Ti, totalMemory: 10.91GiB freeMemory: 10.75GiB
- Exact command to reproduce:

I followed the [evaluation steps](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/oid_inference_and_evaluation.md) with [pretrained faster-rcnn](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) on coco 2017 object detection dataset. However, some results seem strange, especially there are -1.000s. Aayone met this before?

     Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.375
     Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.554
     Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.421
     Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.375
     Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
     Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.307
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.432
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.437
     Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.437
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
     Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000` ",6,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-05-09 15:00:54,open,,,[],2018-05-11 18:15:17
999,tensorflow/models,models,4218,a819721810,Add error:about creating voc tfrecord when image size don't match xml size ,"when i incised big picture to small picture , i forgot to change xml “size parameters”. As follow:
![image](https://user-images.githubusercontent.com/10041362/39810764-dc1d1ac8-53b8-11e8-93eb-2156332e3981.png)
xml size: width=4963  height=3509 ,image size: width=1024  height=1024

but it can generate tfreord,as follow:
![image](https://user-images.githubusercontent.com/10041362/39810808-0d780402-53b9-11e8-902c-92a5980d329b.png)

it worked!

I used the tfreord to train my model and  it did't report error , but  using tensorboard to find out that some loss was abnormal. As follow:
![image](https://user-images.githubusercontent.com/10041362/39810985-cc813cce-53b9-11e8-93c0-e8e610c0943e.png)

After a few hours, i found error that xml size is wrong and using  software “labelImg” cannot find error. As follow:
 
![image](https://user-images.githubusercontent.com/10041362/39811171-6b900d4a-53ba-11e8-8284-dbfc39eaee83.png)

But tensorflow/models did't check out it. 

After i changed code , it can check out.As follow:
![image](https://user-images.githubusercontent.com/10041362/39811477-68a3a0dc-53bb-11e8-8259-68280a9e65a4.png)

Wishing that it can help other people to avoid this error. ",0,,[],2018-05-09 11:04:52,open,,,['cla: yes'],2018-06-06 06:41:16
1000,tensorflow/models,models,4217,tacchan7412,zip consumed and not computing eps/delta,"https://github.com/tensorflow/models/blob/4b8fe70416fe4826a3bad622e56780a7c2eb330c/research/differential_privacy/privacy_accountant/tf/accountant.py#L282

Hi.
I was using GaussianMomentsAccountant,get_privacy_spent() and noticed that though the func accepts list of eps (or delta) as a target_eps (which is one of the arguments), delta (or eps) was calculated only for the first element of the list.
I found that log_moments_with_order is zipped, so after it is consumed at _compute_delta (or _compute_eps) for the first time, log_moments_with_order will have length of 0, which causes not computing other delta (or eps) inside _compute_delta (or _compute_eps).

You can see zip consuming with the below simple codes.
```
>>> a=[1,2,3]
>>> b=[4,5,6]
>>> c=zip(a,b)
>>> for d, e in c:
...     print(d, e)
...
1 4
2 5
3 6
>>> for d, e in c:
...     print(d, e)
...
>>>
```

It is easy to solve this problem, just create log_moments_with_order every time before it is passed. I would be happy to create a new pull request to fix it! Soon after you recognize this problem, I will work on to it!",2,"NamedUser(login=""ilyamironov"")","[NamedUser(login=""ilyamironov"")]",2018-05-09 10:19:14,open,,,['stat:awaiting owner'],2018-05-11 19:05:52
1001,tensorflow/models,models,4208,jatinmandav,Update object_detection_tutorial.ipynb,Corrected Matplotlib warning during imports.,3,,[],2018-05-08 21:15:41,open,,,['cla: yes'],2018-05-08 21:20:05
1002,tensorflow/models,models,4204,flavea,[deeplab] training dataset without pretrained models,"
### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 1.6.0
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: NVIDIA GeForce GTX 1060 6GB
- **Exact command to reproduce**:

### Describe the problem
Hello, I am currently trying to train an aerial/satellite dataset ([Volodymyr Mnih's dataset](https://www.cs.toronto.edu/~vmnih/data)) for building/road detection using deeplab as a research. I am wondering if it's possible to train the data without pre-trained models, because it seemed that the pre-trained models available are not suited for aerial/satellite images. I tried to train the data without using any pre-trained model, setting tf_initial_checkpoint as not required, but when I test the training result, nothing is really detected .  After reading around the issues and the documentation, I have set initialize_last_layer and last_layers_contain_logits_only to false to ignore the classes from the pre-trained model. I got better results, but the result is very inaccurate.

I am still checking if it's a problem with the dataset or if it's because the pre-trained model is incompatible with this dataset. But I am still wondering if I set the flags right and is it possible to use this code without any pre-trained model?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

flags: python train.py \ --logtostderr \ --training_number_of_steps=1000 \ --train_split=""train"" \--model_variant=""xception_65"" \ --atrous_rates=6 \ --atrous_rates=12 \ --atrous_rates=18 \ --output_stride=16 \ --decoder_output_stride=4 \ --train_crop_size=513 \ --train_crop_size=513 \ --train_batch_size=2 \ --resize_factor=16 \ --fine_tune_batch_norm=False \ --dataset=""mnih"" \ --initialize_last_layer=False \ --last_layers_contain_logits_only=True \ --train_logdir=""datasets/mnih_buildings/exp/train_on_train_set/train/"" \ --dataset_dir=""datasets/mnih_buildings/tfrecord/"" \ --tf_initial_checkpoint=""datasets/pascal_voc_seg/init_models/deeplabv3_pascal_train_aug/model.ckpt"" 
<img src=""https://i.imgur.com/aRAAgxs.png"">

flags: python train.py \ --logtostderr \ --training_number_of_steps=1000 \ --train_split=""train"" \--model_variant=""xception_65"" \ --atrous_rates=6 \ --atrous_rates=12 \ --atrous_rates=18 \ --output_stride=16 \ --decoder_output_stride=4 \ --train_crop_size=513 \ --train_crop_size=513 \ --train_batch_size=2 \ --resize_factor=16 \ --fine_tune_batch_norm=False \ --dataset=""mnih"" \ --initialize_last_layer=False \ --last_layers_contain_logits_only=False \ --train_logdir=""datasets/mnih_buildings/exp/train_on_train_set/train/"" \ --dataset_dir=""datasets/mnih_buildings/tfrecord/"" \ --tf_initial_checkpoint=""datasets/pascal_voc_seg/init_models/deeplabv3_pascal_train_aug/model.ckpt"" 
<img src=""https://i.imgur.com/QIo0IKA.png"">


flags: python train.py \ --logtostderr \ --training_number_of_steps=1000 \ --train_split=""train"" \--model_variant=""xception_65"" \ --atrous_rates=6 \ --atrous_rates=12 \ --atrous_rates=18 \ --output_stride=16 \ --decoder_output_stride=4 \ --train_crop_size=513 \ --train_crop_size=513 \ --train_batch_size=2 \ --resize_factor=16 \ --fine_tune_batch_norm=False \ --dataset=""mnih"" \ --initialize_last_layer=False \ --last_layers_contain_logits_only=False \ --train_logdir=""datasets/mnih_buildings/exp/train_on_train_set/train/"" \ --dataset_dir=""datasets/mnih_buildings/tfrecord/""
<img src=""https://i.imgur.com/JrxB3NT.png"">

",6,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-05-08 11:43:11,open,,,[],2018-12-13 09:38:56
1003,tensorflow/models,models,4203,RomRoc,assertion failed: [`predictions` out of bound] in Deeplab eval.py with ADE20K,"- **What is the top-level directory of the model you are using**:
/content

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 17.10 in Google Colab (env: Python2 with GPU)

- **TensorFlow installed from (source or binary)**:
standard Tensorflow in Google Colab

- **TensorFlow version (use command below)**:
('unknown', '1.7.0')

- **Bazel version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
Cuda 8.0

- **GPU model and memory**:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.111                Driver Version: 384.111                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   29C    P8    26W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
- **Exact command to reproduce**:
In Google Colab:

Cell1:
```
%cd
!git clone https://github.com/tensorflow/models.git /content/models
```

Cell2:
```
%cd models/research/deeplab/datasets
!sh ./download_and_convert_ade20k.sh

```
Cell3:
```
%cd /content/models/research
%env PYTHONPATH=/env/python/:/content/models/research/:/content/models/research/slim
%env WORK_DIR=/content/models/research/deeplab

# Set up the working directories.
%env INIT_FOLDER=/content/models/research/deeplab/datasets/ADE20K/init_models
%env TRAIN_LOGDIR=/content/models/research/deeplab/datasets/ADE20K/exp/train_on_trainval_set/train
%env EVAL_LOGDIR=/content/models/research/deeplab/datasets/ADE20K/exp/train_on_trainval_set/eval
%env EXPORT_DIR=/content/models/research/deeplab/datasets/ADE20K/exp/train_on_trainval_set/export
!mkdir -p ""${INIT_FOLDER}""
!mkdir -p ""${TRAIN_LOGDIR}""
!mkdir -p ""${EVAL_LOGDIR}""
!mkdir -p ""${EXPORT_DIR}""

# Copy locally the trained checkpoint as the initial checkpoint.
%env TF_INIT_ROOT=http://download.tensorflow.org/models
%env TF_INIT_CKPT=deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz
%cd /content/models/research/deeplab/datasets/ADE20K/init_models
!wget -nd -c ""${TF_INIT_ROOT}/${TF_INIT_CKPT}""
!tar -xf ""${TF_INIT_CKPT}""
%cd ""/content/models/research/""

%env ADE20K_DATASET=/content/models/research/deeplab/datasets/ADE20K/tfrecord

print('START train.py')
%env NUM_ITERATIONS=1000
!python ""${WORK_DIR}""/train.py \
  --logtostderr \
  --training_number_of_steps=""${NUM_ITERATIONS}"" \
  --train_split=""train"" \
  --model_variant=""mobilenet_v2"" \
  --train_crop_size=513 \
  --train_crop_size=513 \
  --train_batch_size=4 \
  --min_resize_value=350 \
  --max_resize_value=500 \
  --resize_factor=16 \
  --fine_tune_batch_norm=False \
  --dataset=""ade20k"" \
  --initialize_last_layer=False \
  --last_layers_contain_logits_only=True \
  --tf_initial_checkpoint=""${INIT_FOLDER}/deeplabv3_mnv2_pascal_train_aug/model.ckpt-30000"" \
  --train_logdir=""${TRAIN_LOGDIR}"" \
  --dataset_dir=""${ADE20K_DATASET}""


print('START eval.py')
!python ""${WORK_DIR}""/eval.py \
    --logtostderr \
    --eval_split=""val"" \
    --model_variant=""mobilenet_v2"" \
    --eval_crop_size=2113 \
    --eval_crop_size=2113 \
    --dataset=""ade20k"" \
    --checkpoint_dir=${TRAIN_LOGDIR} \
    --eval_logdir=${EVAL_LOGDIR} \
    --dataset_dir=${ADE20K_DATASET}
```

### Describe the problem
I try to train and evaluate deeplab model with ADE20K dataset in Google Colab.
I use as initial checkpoint mobilenetv2_coco_voc_trainaug, but I get the same error if I use xception_coco_voc_trainaug.
I see even others here #3730 has the same problem.
Can you help please?

### Source code / logs
I get error in evaluation step:
```
InvalidArgumentError (see above for traceback): assertion failed: [`predictions` out of bound] [Condition x < y did not hold element-wise:] [x (mean_iou/confusion_matrix/control_dependency_1:0) = ] [0 3 3...] [y (mean_iou/ToInt64_2:0) = ] [150]
```
",4,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-05-08 11:07:36,open,,,[],2018-07-26 02:09:50
1004,tensorflow/models,models,4193,eewindfly,fix losses proto hard example minor typo,"Fix document typo about default lossType of hardExampleMiner
I send out this PR because I would like to double confirm the default lossType is ""BOTH"" instead of just classification loss.",0,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-05-07 15:42:52,open,,,['cla: yes'],2018-05-18 03:52:14
1005,tensorflow/models,models,4191,fengsky401,Mismatch between checkpoint and model when I restored Nasnet,"There is no bug in the codes. The code is as below.
The length of var_list is 3117,while the length of variables_to_restore is 1547.
I don't know what causes the difference.
checkpoint  is nasnet-a_large downloaded from https://storage.googleapis.com/download.tensorflow.org/models/nasnet-a_large_04_10_2017.tar.gz

 
```
import tensorflow as tf
from tensorflow.contrib.slim.python.slim.nets import inception_resnet_v2
from tensorflow.contrib.framework.python.framework import checkpoint_utils
from tensorflow.contrib.slim.python.slim.nets.nasnet import nasnet

slim = tf.contrib.slim
checkpoint_file=""./model_weight_nasnet/model.ckpt""
model_dir=""./model_weight_nasnet/""
cnt=0

#checkpoint_file=tf.train.latest_checkpoint(""./model_weight/"")
var_list = checkpoint_utils.list_variables(checkpoint_file)
#
for v in var_list:
    print(""No %d:%s"" %(cnt,v))
    cnt+=1


NUM_CLASS=1001

image = tf.placeholder(tf.float32, shape=[331, 331, 3])


image_reshape = tf.reshape(image, [-1, 331, 331, 3])
with slim.arg_scope(nasnet.nasnet_large_arg_scope()):

    logits,_ = nasnet.build_nasnet_large(image_reshape, num_classes=NUM_CLASS, is_training=False)

    global_step = tf.Variable(0, name='global_step',dtype=tf.int64, trainable=False)

    variables_to_restore = slim.get_variables_to_restore()

    cnt = 0
    for i in variables_to_restore:
        print(""No %d:%s"" % (cnt, v))
        cnt+=1

    saver = tf.train.Saver(variables_to_restore)


    init_op = tf.initialize_all_variables()

variable_list=[]


saver2 = tf.train.Saver(tf.global_variables())

sess=tf.Session()
sess.run(init_op)
sess.run(variables_to_restore)

saver.restore(sess, checkpoint_file)

saver2.save(sess, model_dir + ""/nasnet_initial.ckpt"", global_step=0)



```
",2,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel""), NamedUser(login=""hgadig"")]",2018-05-07 10:00:10,open,,"NamedUser(login=""hgadig"")",['stat:awaiting response'],2018-11-10 02:17:47
1006,tensorflow/models,models,4189,AliceDinh,PATH_TO_BE_CONFIGURED,"What is `PATH_TO_BE_CONFIGURED` in the following command:
`sed -i ""s|PATH_TO_BE_CONFIGURED|""${YOUR_GCS_BUCKET}""/data|g"" object_detection/samples/configs/faster_rcnn_resnet101_pets.config`
In the tutorial at the address:
https://cloud.google.com/blog/big-data/2017/06/training-an-object-detector-using-cloud-machine-learning-engine
`The PATH_TO_BE_CONFIGURED strings need to be changed so they point to the dataset files and fine-tune checkpoint you’ve uploaded to your Cloud Storage bucket.`
So, it is my local machine or my bucket:
`sed -i ""s|/home/alicedinh/models/research/pet_train.record|""gs://alicedinh""/data|g"" object_detection/samples/configs/faster_rcnn_resnet101_pets.config	`
OR
`sed -i ""s|gs://alicedinh/data/pet_train.record|""gs://alicedinh""/data|g"" object_detection/samples/configs/faster_rcnn_resnet101_pets.config	`



",6,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""yhliang2018"")]",2018-05-07 09:19:44,open,,,['stat:awaiting tensorflower'],2018-08-29 09:04:08
1007,tensorflow/models,models,4177,John3-16,tensorflow.python.framework.errors_impl.UnknownError: Failed to rename:,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
D:\GitRepro\UIUC\CS498AML\Week12\cifar10\part2a>
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
It is example code modified for our class assignment
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 Pro 1709
- **TensorFlow installed from (source or binary)**:
Python Package install
- **TensorFlow version (use command below)**:
1.7.0
- **Bazel version (if compiling from source)**:  n/a
- **CUDA/cuDNN version**:
CUDA 8.0/cuDNN64_7 which v7.04 
- **GPU model and memory**:
2 GPUs GTX 1080ti with 11 GB for each
- **Exact command to reproduce**:
Traceback (most recent call last):
  File ""cifar10_train.py"", line 145, in <module>
    tf.app.run()
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""cifar10_train.py"", line 141, in main
    train()
  File ""cifar10_train.py"", line 133, in train
    mon_sess.run(train_op)
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 546, in run
    run_metadata=run_metadata)
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1022, in run
    run_metadata=run_metadata)
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1113, in run
    raise six.reraise(*original_exc_info)
  File ""c:\users\john3\venv\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1098, in run
    return self._sess.run(*args, **kwargs)
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1178, in run
    run_metadata=run_metadata))
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py"", line 458, in after_run
    self._save(run_context.session, global_step)
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py"", line 474, in _save
    self._get_saver().save(session, self._save_path, global_step=step)
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\training\saver.py"", line 1686, in save
    save_relative_paths=self._save_relative_paths)
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\training\saver.py"", line 1041, in _update_checkpoint_state
    text_format.MessageToString(ckpt))
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 431, in atomic_write_string_to_file
    rename(temp_pathname, filename, overwrite)
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 410, in rename
    compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)
  File ""c:\users\john3\venv\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: D:/GitRepro/UIUC/CS498AML/Week12/cifar10/data/cifar10_train_part2a\checkpoint.tmp7d34a7365c60428f8bb597559422ebcf to: D:/GitRepro/UIUC/CS498AML/Week12/cifar10/data/cifar10_train_part2a\checkpoint : Access is denied.
; Input/output error

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. 
I have been training model updates all day long with runs about 25 to 45 minutes.  Several times it has generated this same error each time at different points.  In the stack trace above it was the 2nd time in a row on the same model which is running through 65k steps.  I have had runs complete with no issue even up to 134k steps. While the training  is running I have it mapped to 1 GPU and have cifar10_eval.py running and mapped to another GPU.  While those are running I also have TensorBoard running so that I see the progress in the scalar graphs.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

[part2A.zip](https://github.com/tensorflow/models/files/1976273/part2A.zip)


",6,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-05-05 03:02:18,open,,,[],2018-11-20 13:23:17
1008,tensorflow/models,models,4173,gustavz,[object_detection] newest commits mess up protos and Box Predictor among other Errors,"when i try to export trained mask_rcnn model since the newest commits i face errors like this:
`AttributeError: 'MaskRCNNBoxPredictor' object has no attribute 'share_box_across_classes'`
also i run into errors when i do protobuf compilation and many other stuff. what happened?
```
object_detection/protos/losses.proto:67:79: Field number 5 has already been used in ""object_detection.protos.ClassificationLoss"" by field ""weighted_logits_softmax"".
object_detection/protos/faster_rcnn.proto: Import ""object_detection/protos/losses.proto"" was not found or had errors.
object_detection/protos/faster_rcnn.proto:126:12: ""HardExampleMiner"" is not defined.
object_detection/protos/faster_rcnn.proto:133:12: ""ClassificationLoss"" is not defined.
```
",4,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-05-04 10:43:10,open,,,[],2018-12-01 05:48:57
1009,tensorflow/models,models,4168,ernstgoyer,object-detection-api with 16-bit images,"System information
Have I written custom code: Yes
OS Platform and Distribution: Ubuntu 17.10
TensorFlow installed from: binary
TensorFlow version: 1.5 - 1.8
Bazel version N/A
CUDA/cuDNN version 9.0
GPU model and memory: GTX 1080 TI
Exact command to reproduce: N/A

i want to use tensorflow's object detection api with 16 bit satellite-images. i created a tfrecord file with 16bit pngs than using the train.py from object detection the images are converted to 8bit - not what i wanted

so i changed in tensorflow/python/ops following files 
gen_image_ops.py in the function decode_png() the dytpe to uint16 
image_ops.impl.py the function decode_image to jucst return _png() this seems to work

but i want to ask if there is a smarter way to use 16bit images 
thanks for help",2,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-05-04 06:34:23,open,,,['stat:awaiting owner'],2018-05-04 17:15:01
1010,tensorflow/models,models,4164,magick2,Object detection Slow processing video,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
Object detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, just to insert a video instead of using the webcam
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 64 bits (Last version)
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.8.0-0-g93bc2e2072' 1.8.0
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA: cuda_9.0.176
cuDNN: cudnn-9.0
- **GPU model and memory**:
MSI Geforce GTX 1070 8gb
- **Exact command to reproduce**:
N/A
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I'm running the sample code that comes with the Object Detection model, I made a modification to read a video instead of a webcam the problem is that the window opens and plays the video but in extreme slow (really is very slow , does not exceed 1 fps I think) and when you run it does not use any PC resources (I clarify it in case they ask if the PC saturates and that's why the video is wrong)

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
The code I use is in a .zip file
PS: I'm sorry for my English


[object_detection_tutorial.zip](https://github.com/tensorflow/models/files/1972843/object_detection_tutorial.zip)
",12,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-05-03 22:27:50,open,,,[],2018-05-17 03:00:21
1011,tensorflow/models,models,4156,zbsean,ssd_mobilenet_v2_coco train is error,"
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint",7,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-05-03 02:37:07,open,,,"['models: research', 'stat:awaiting response']",2019-02-01 22:19:42
1012,tensorflow/models,models,4139,k-w-w,decode byte text to prevent error in python3,Resolving issue #4036 ,0,,[],2018-05-01 18:34:57,open,,,['cla: yes'],2018-05-10 19:05:36
1013,tensorflow/models,models,4130,chellary,Wrong attention while decode,"seq2seq.attention_decoder argument initial_state_attention is true while decoding, This calculates ""attns"" as per the initial_state passed and, that is from encoder hidden states. This is not the behavior while training. This lead to predicting the first word more influence with the last hidden state of Encoder. 
As per training ""attns"" for first time-step is zeros, so first time-step attention decoder also should be zeros.
",3,"NamedUser(login=""peterjliu"")","[NamedUser(login=""peterjliu"")]",2018-04-30 12:34:08,open,,,['stat:awaiting owner'],2018-07-23 08:09:24
1014,tensorflow/models,models,4129,HutzelFutzel,Faster R-CNN change IoU threshold - nothing happens,"Hi, 
when I am changing the IoU threshold of the Faster-RNN network from 0.7 to 0.2 or 0.9, nothing happens to the AP or mAP. They are still exactly the same curves. 
I thought that this is the threshold at which an anchor is labeled positive or negative, isn't it? In my opinion the mAP should increase with a threshold of 0.2 instead of 0.7? Am I doing something wrong?",4,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-04-30 11:09:30,open,,,[],2018-05-10 15:58:41
1015,tensorflow/models,models,4124,ProjectDent,"""ImportError: No module named nasnet"" when following basic installation instructions","I'm following the basic [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md).

I installed TensorFlow using pip, as suggested, and installed all required components. I cloned the `models` repo, at the latest commit on `master`.

Then I followed the steps of, from the `research` directory, running:
`protoc object_detection/protos/*.proto --python_out=.`
`export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim`

When I run the next line, to test that installation was successful:
`python object_detection/builders/model_builder_test.py`

I get this error:

> WARNING:tensorflow:From /Users/Andrew/environments/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
> Instructions for updating:
> Use the retry module or similar alternatives.
> Traceback (most recent call last):
>   File ""object_detection/builders/model_builder_test.py"", line 21, in <module>
>     from object_detection.builders import model_builder
>   File ""/Users/Andrew/Code/tf-update/research/object_detection/builders/model_builder.py"", line 32, in <module>
>     from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
>   File ""/Users/Andrew/Code/tf-update/research/object_detection/models/faster_rcnn_nas_feature_extractor.py"", line 26, in <module>
>     from nets.nasnet import nasnet
> ImportError: No module named nasnet

Going back to commit `a4944a57ad` - the introduction of the `object_detection` features on master - I can run this without any issue.",1,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-04-29 10:02:23,open,,,['stat:awaiting response'],2018-04-29 18:39:41
1016,tensorflow/models,models,4116,gustavz,[object_detection] Feature: Resume  training from last checkpoint,"This is about the object_detection repo:

As far as i know, when training breaks because of any kind of error and you want to continue training from the last saved checkpoint you need to manually adress the checkpoint from where you want to resume training in your models config file.

So now my question: Is there a way to let the model find the last saved checkpoint and continue from it automatically? If not I think it would be a nice feature to add in train protos and should not be too hard to implement.

Maybe something like this (taken from matterport):
```
def find_last(self):
        """"""Finds the last checkpoint file of the last trained model in the
        model directory.
        Returns:
            log_dir: The directory where events and weights are saved
            checkpoint_path: the path to the last checkpoint file
        """"""
        # Get directory names. Each directory corresponds to a model
        dir_names = next(os.walk(self.model_dir))[1]
        key = self.config.NAME.lower()
        dir_names = filter(lambda f: f.startswith(key), dir_names)
        dir_names = sorted(dir_names)
        if not dir_names:
            return None, None
        # Pick last directory
        dir_name = os.path.join(self.model_dir, dir_names[-1])
        # Find the last checkpoint
        checkpoints = next(os.walk(dir_name))[2]
        checkpoints = filter(lambda f: f.startswith(""mask_rcnn""), checkpoints)
        checkpoints = sorted(checkpoints)
        if not checkpoints:
            return dir_name, None
        checkpoint = os.path.join(dir_name, checkpoints[-1])
return dir_name, checkpoint
```",12,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-04-28 08:54:56,open,,,"['stat:awaiting response', 'type:feature']",2018-11-06 02:24:39
1017,tensorflow/models,models,4115,esmanchik,Pull request,I have a legacy boxes-only dataset without masks and here are the fixes to let script process it as before.,2,,[],2018-04-28 06:28:02,open,,,['cla: yes'],2018-04-28 06:39:40
1018,tensorflow/models,models,4112,willSapgreen,Is ssd_inception_v2_coco.config used originally in Google internally?,"### System information
- **What is the top-level directory of the model you are using**:
object_detection

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, but it is not related to this report.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
1.7.0

- **Bazel version (if compiling from source)**:
X

- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.0

- **GPU model and memory**:
GeForce GTX 1050, 2G memory

### Describe the problem
Up to the origin/master,
the functions used in classification_loss and localization_loss are both DEPRECATED.
https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_inception_v2_coco.config
https://github.com/tensorflow/models/blob/3f78f4cfd21c786c62bf321c07830071027ebb5e/research/object_detection/protos/losses.proto
So I wonder if the config file is not up-to-date.
Thank you.

### Exact command to reproduce
N/A",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-04-27 18:44:18,open,,,[],2018-05-09 19:40:11
1019,tensorflow/models,models,4107,123liluky,[SSD object_detection] print network structure,"Hello, I used config file revised from ssd_mobilenet_v1_pets.config to train my images successfully. How can I print the network structure which describes layer name, filter nums, filter size, input size and output size for each layer? 
Thank you.

",2,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-04-27 08:12:52,open,,,['stat:awaiting response'],2018-04-27 22:30:51
1020,tensorflow/models,models,4104,YangSong1997,ValueError: The first layer in a Sequential model must get an `input_shape` argument.,"When running the officials/mnist/mnist.py there is an error message:

Traceback (most recent call last):
  File ""mnist.py"", line 261, in <module>
    main(argv=sys.argv)
  File ""mnist.py"", line 225, in main
    mnist_classifier.train(input_fn=train_input_fn, hooks=train_hooks)
  File ""/Users/yangsong/tensorflow/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 355, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/Users/yangsong/tensorflow/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 824, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/Users/yangsong/tensorflow/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 805, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""mnist.py"", line 89, in model_fn
    model = create_model(params['data_format'])
  File ""mnist.py"", line 83, in create_model
    l.Dense(10)
  File ""/Users/yangsong/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/sequential.py"", line 120, in __init__
    self.add(layer)
  File ""/Users/yangsong/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/sequential.py"", line 163, in add
    raise ValueError('The first layer in a '
ValueError: The first layer in a Sequential model must get an `input_shape` argument.







Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",7,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-04-27 03:23:23,open,,"NamedUser(login=""YangSong1997"")",[],2018-06-14 22:45:38
1021,tensorflow/models,models,4095,Abhijit-2592,[Object detection] Feature Request: Running the API  when you have access to limited GPU,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: conda
- **TensorFlow version (use command below)**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

This is not a bug but a feature request. The official documentation recommends starting both the training and evaluation scripts in parallel to monitor the training and model convergance. This however works properly only when you have 2 or more GPUs. If you have only one GPU eg a GTX 1060 or 1080, The above recommended way will create horrible drag in training. This is because, when the evaluation script runs it will try to hog the GPU thereby creating a training bottleneck. Faster RCNN with considerably large image sizes example (1500 x 1500 pixels)  are prone to this problem .There are two ways to go about this problem: 

1. Run training in GPU and run evaluation in CPU. Even this solution does not guarantee that your training will work without freezing if you have RAM less than 16GB or when you are using a heavy model

2. Run only training script for specific iterations and then run the evaluation script, to check if the model has converged. I know this is a round about way but, I couldn't think of any better solution. But the evaluation script will run the evaluation on the recent checkpoint and not all the checkpoints. 

I have added both of these features in this pull request #4038. ",3,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""pkulzc""), NamedUser(login=""yhliang2018"")]",2018-04-26 04:07:11,open,,,['stat:awaiting owner'],2018-05-10 17:31:18
1022,tensorflow/models,models,4086,abidmalikwaterloo,ImageNet Data processing,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Redhat
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
9/6.6
- **GPU model and memory**:
K20 / 5.5 GB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am trying to prepare the imagenet data for the tf_cnn_benchmarks. I am using the""download_and_preprocess_imagenet.sh"" script . I worked but when it prepare for TFRECORD format I got processing errors with tf.



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

./download_and_preprocess_newimagenet.sh /home/amalik/NEWIMAGENETDATA/
Organizing the validation data into sub-directories.
Extracting bounding box information from XML.
Finished downloading and preprocessing the ImageNet data.
Saving results to /home/amalik/NEWIMAGENETDATA
Successfully read 615299 bounding boxes across 544546 images.
Determining list of input files and labels from /home/amalik/NEWIMAGENETDATA/raw-data/validation/.
Finished finding files in 100 of 1000 classes.
Finished finding files in 200 of 1000 classes.
Finished finding files in 300 of 1000 classes.
Finished finding files in 400 of 1000 classes.
Finished finding files in 500 of 1000 classes.
Finished finding files in 600 of 1000 classes.
Finished finding files in 700 of 1000 classes.
Finished finding files in 800 of 1000 classes.
Finished finding files in 900 of 1000 classes.
Finished finding files in 1000 of 1000 classes.
Found 50000 JPEG files across 1000 labels inside /home/amalik/NEWIMAGENETDATA/raw-data/validation/.
Found 0 images with bboxes out of 50000 images
Launching 8 threads for spacings: [[0, 6250], [6250, 12500], [12500, 18750], [18750, 25000], [25000, 31250], [31250, 37500], [37500, 43750], [43750, 50000]]
2018-04-25 11:32:43.064265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-04-25 11:32:43.064342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-04-25 11:32:45.273683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-25 11:32:45.273756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-04-25 11:32:45.273775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-04-25 11:32:45.274055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5285 MB memory) -> physical GPU (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)

Exception in thread Thread-6:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)
Exception in thread Thread-7:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)

Exception in thread Thread-5:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)


Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)
Exception in thread Thread-3:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)

Exception in thread Thread-4:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)


Exception in thread Thread-8:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)

2018-04-25 11:32:46.426148: Finished writing all 50000 images in data set.
Determining list of input files and labels from /home/amalik/NEWIMAGENETDATA/raw-data/train/.
Finished finding files in 100 of 1000 classes.
Finished finding files in 200 of 1000 classes.
Finished finding files in 300 of 1000 classes.
Finished finding files in 400 of 1000 classes.
Finished finding files in 500 of 1000 classes.
Finished finding files in 600 of 1000 classes.
Finished finding files in 700 of 1000 classes.
Finished finding files in 800 of 1000 classes.
Finished finding files in 900 of 1000 classes.
Finished finding files in 1000 of 1000 classes.
Found 1281167 JPEG files across 1000 labels inside /home/amalik/NEWIMAGENETDATA/raw-data/train/.
Found 544546 images with bboxes out of 1281167 images
Launching 8 threads for spacings: [[0, 160145], [160145, 320291], [320291, 480437], [480437, 640583], [640583, 800729], [800729, 960875], [960875, 1121021], [1121021, 1281167]]
2018-04-25 11:34:48.594990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-04-25 11:34:48.595083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-25 11:34:48.595103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-04-25 11:34:48.595116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-04-25 11:34:48.595422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5285 MB memory) -> physical GPU (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
Exception in thread Thread-13:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)
Exception in thread Thread-12:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)


Exception in thread Thread-14:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)

Exception in thread Thread-11:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)
Exception in thread Thread-10:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)


Exception in thread Thread-9:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)

Exception in thread Thread-15:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)

Exception in thread Thread-16:
Traceback (most recent call last):
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/amalik/tenENV/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 395, in _process_image_files_batch
    height, width)
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 214, in _convert_to_example
    'image/colorspace': _bytes_feature(colorspace),
  File ""/home/amalik/DATA/models/research/inception/inception/data/build_imagenet_data.py"", line 175, in _bytes_feature
    value = six.binary_type(value, encoding='utf-8')
TypeError: str() takes at most 1 argument (2 given)

2018-04-25 11:34:49.610442: Finished writing all 1281167 images in data set.








",3,"NamedUser(login=""yhliang2018"")","[NamedUser(login=""yhliang2018"")]",2018-04-25 15:52:32,open,,,['stat:community support'],2018-06-14 05:33:11
1023,tensorflow/models,models,4079,tanguofu,fix the miss suffix 'py' of /build_imagenet_data.py,fix the miss suffix 'py' of /build_imagenet_data.py,0,,[],2018-04-25 07:11:41,open,,,['cla: yes'],2018-04-25 07:11:44
1024,tensorflow/models,models,4078,bysowhat,"SSD,FASTER RCNN....: Eager execution example! Debug Tools","This project helps you debug object_detection(SSD,FASTER...) using tensorflow eager excution model!
https://github.com/bysowhat/object_detection_debug.git
If you have any question please contact me.

**The top-level directory of the project:**
models\research\object_detection
**OS Platform and Distribution:**
WIN10
**TensorFlow installed from:**
binary
**TensorFlow version：1.7.0**
Bazel version：
**CUDA/cuDNN version:**
CUDA: release 9.0
cuDNN: CUDNN_MAJOR 7
**GPU model and memory:**
GeForce GTX 1080 Ti, 11G memory
**Exact command to reproduce：**",2,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-04-25 03:39:18,open,,,[],2018-04-26 12:50:36
1025,tensorflow/models,models,4072,jashshopin,Problems while performing online prediction using CloudML Engine and Tensorflow Object Detection API,"We are following the guidelines mentioned [here](https://cloud.google.com/blog/big-data/2017/09/performing-prediction-with-tensorflow-object-detection-models-on-google-cloud-machine-learning-engine) to perform prediction for our already trained model on Cloud ML Engine.

However after successfully running the steps word for word till deploying the model for serving we ran into the following error while running online prediction:

`{""error"": ""Prediction failed: Error during model execution: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\""NodeDef mentions attr 'identical_element_shapes' not in Op<name=TensorArrayV3; signature=size:int32 -> handle:resource, flow:float; attr=dtype:type; attr=element_shape:shape,default=<unknown>; attr=dynamic_size:bool,default=false; attr=clear_after_read:bool,default=true; attr=tensor_array_name:string,default=\""\""; is_stateful=true>; NodeDef: map/TensorArray = TensorArrayV3[_output_shapes=[[2], []], clear_after_read=true, dtype=DT_STRING, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name=\""\"", _device=\""/job:localhost/replica:0/task:0/device:CPU:0\""](map/TensorArrayUnstack/strided_slice). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n\t [[Node: map/TensorArray = TensorArrayV3[_output_shapes=[[2], []], clear_after_read=true, dtype=DT_STRING, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name=\""\"", _device=\""/job:localhost/replica:0/task:0/device:CPU:0\""](map/TensorArrayUnstack/str...TRUNCATED\"")""`

We are using runtime-version=1.4 for creating a version on Cloud ML Engine since runtime-version=1.2 was giving us an `Nonmax Suppression Error` 
The graph was also exported locally using tensorflow version > 1.4. 
The model was trained using runtime version 1.2 as given in the documentation.


We want to deploy a model as soon as possible, so any help will be appreciated.

",1,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""yhliang2018"")]",2018-04-24 17:09:52,open,,,['stat:awaiting tensorflower'],2018-06-14 05:29:43
1026,tensorflow/models,models,4068,pgrandinetti,Tf-Slim: Load output of slim training as Tf Graph,"Following along your documentation at `models/research/slim` till the usage of `freeze_graph.py` and I can't get how to load into memory the frozen model.

What I have done:

1) Followed your readme using inception_v3 and fine-tuned a model with 1 label
2) Got previous model in `./checkpoints/inception_v3.cpkt` and the new model in `./inception_v3/model.cpkt-155`
3) Exported the graph *exactly* as in the readme:
4) Frozen the graph *exactly* as in the readme
5) Moved the frozen graph (by default in `/tmp/`) to working directory
6) Used the functions in object_detection_tutorial.ipynb to load the graph in memory (the `run_inference_for_single_image` function)

While executing (6) I get

```
KeyError: ""The name 'image_tensor:0' refers to a Tensor which does not exist. The operation, 'image_tensor', does not exist in the graph.""
```

which, by the way, is found in several other threads, but without a clear solution imho.

Do you have any suggestion on how to solve this? How to load the output of a training done with `slim` and use it (for example in a API)?
Thanks!
",1,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-04-24 12:50:14,open,,,[],2018-04-25 15:58:27
1027,tensorflow/models,models,4067,haichaoyu,[Deeplab] Label index does not match with code for dataset ADE20k?,"Hello,

I tried to fine-tune Deeplab v3+ on ADE20k. I just found that in ADE20k, ignore_label is set to 0, while in the code the ignore label is set to 255. Directly setting ignore_label to 0 in code might not work because the label index counts from 0.

Just want to confirm that this ""problem"" exists. Please correct me if I a wrong.

Thanks.",7,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-04-24 08:31:09,open,,,[],2019-01-23 21:01:23
1028,tensorflow/models,models,4066,clemkoa,batch_norm_trainable field in ssd mobilenet v2 coco,"### System information
- **What is the top-level directory of the model you are using**: /models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  MacOS 10.13
- **TensorFlow installed from (source or binary)**:  source
- **TensorFlow version (use command below)**: ('v1.5.1-0-g6a1ec9deeb', '1.5.1')
- **Bazel version (if compiling from source)**: 0.11.1-homebrew
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:


### Describe the problem
The field batch_norm_trainable is still present in the config sample for ssd_mobilenet_v2_coco (https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config). I think it is deprecated and it throws an error when trying to train:

```
google.protobuf.text_format.ParseError: 109:7 : Message type ""object_detection.protos.SsdFeatureExtractor"" has no field named ""batch_norm_trainable"".
```
It seems to be referenced in this stack overflow question:

https://stackoverflow.com/questions/49880939/tf-object-detection-api-detection-model-retraining-object-detection-protos-ssd

Sorry if this has already been asked before",5,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-04-24 07:55:21,open,,,[],2018-09-02 22:22:44
1029,tensorflow/models,models,4058,ProjectDent,"""AttributeError: 'module' object has no attribute 'data'"" termination error when running Pets on Google Cloud tutorial","I'm following the [""Distributed Training on the Oxford-IIIT Pets Dataset on Google Cloud"" tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md).

Following the step of starting training, about 8 minutes later, I get a termination, with this error from the logs:

> Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""main"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 167, in tf.app.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run _sys.exit(main(_sys.argv[:1] + flags_passthrough)) File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 163, in main worker_job_name, is_chief, FLAGS.train_dir) File ""/root/.local/lib/python2.7/site-packages/object_detection/trainer.py"", line 264, in train train_config.prefetch_queue_capacity, data_augmentation_options) File ""/root/.local/lib/python2.7/site-packages/object_detection/trainer.py"", line 59, in create_input_queue tensor_dict = create_tensor_dict_fn() File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 120, in get_next dataset_builder.build(config)).get_next() File ""/root/.local/lib/python2.7/site-packages/object_detection/builders/dataset_builder.py"", line 164, in build functools.partial(tf.data.TFRecordDataset, buffer_size=8 * 1000 * 1000), AttributeError: 'module' object has no attribute 'data' The replica worker 0 exited with a non-zero status of 1.

Googling this, I can't find any specific results, other than suggestions that the version of Python may be out of date. The tutorial gives the TensorFlow runtime version as 1.2, of which the options are 1.2, 1.4, 1.5 and 1.6. Changing to any of these other versions, I run into different errors, and I assume that isn't the correct solution.",15,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-04-23 15:56:11,open,,,[],2018-05-30 08:59:11
1030,tensorflow/models,models,4051,ahyunSeo,build_imagenet_data.py Found 0 images with bboxes...,"I'm working on preprocessing ImageNet dataset.
Mainly I'm working on models/research/slim/datasets directory of the repository.

I had my own download so I removed DOWNLOAD part in download_and_convert_imagenet.sh 
even after bazel build the script failed at running python files,
so I ran python scripts **one-by one**
succesfully ran **preprocess_imagenet_validation_data.py** (It only organizes the subdir)
and ran **process_bounding_boxes.py**,
I obtained proper bounding box csv with this message (also this msg are the correct one according to the comment)

**Wrote 615299 bounding boxes from 544546 annotated images.**

But I ran **build_imagenet_data.py** I got 

**Found 0 images with bboxes out of 2500000 images**

at the point of processing train set it detects N images with bbox but
at the point of processing validation set it detects 0 bboxes.
I manually checked csv file and it only contains entry of n000*****_*****.JPEG,
and **because of the basename of my validation set ILSVRC2012_val_00000***, It detects None.**
Is this normal?

I'm starting this issue because I got almost-zero evaluation accuracy running this repo,
https://github.com/mfigurnov/sact

TF ver : v1.7.0-3-g024aecf414 1.7.0
CUDA 9.0 cudnn 7.x w/ NVIDIA Titan XP (12GB) * 2

Summary of the question
1. Does process_bounding_boxes.py supposed to process only training set?
2. Does build_imagenet_data.py supposed to find bboxes for training set only?",4,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-04-22 07:59:05,open,,,[],2018-04-26 06:17:21
1031,tensorflow/models,models,4039,Harry-Zhi,[Deeplab] Can we provide only COCO pretrained checkpoints? How well does Deeplab work for indoor scenes?,"Hi,

In DeepLab model Zoo, models trained on PASCAL VOC 2012 and Cityscapes are provided. 

I am just wondering **if you could provide the COCO only pre-trained weights**, since I want to fine-tune on my own data, where COCO pre-trained model may be a better starting point rather than VOC and cityscape pre-trained ones because COCO has more general classes and objects.

By the way, **have you tested the Deeplab segmentation using indoor datasets such as NYUv2 or Sun3D datasets? Does Deeplab work still well on indoor scenes**?

Best,
Shuaifeng
 ",6,"NamedUser(login=""YknZhu"")","[NamedUser(login=""YknZhu""), NamedUser(login=""aquariusjay"")]",2018-04-20 12:25:09,open,,,[],2018-04-25 19:16:47
1032,tensorflow/models,models,4038,Abhijit-2592,Object detection: Run evalutation in CPU and run evaluation on all checkpoints,"This is a small additional feature which will be helpful  for people with **single GPU (gtx-1060, gtx-1080 etc)** or **limited GPU memory.**

If above is the case, when both the Training and evaluation scripts are started together (official recommended way), training might hang or might cause Resource Exhaust Error! (Especially with Faster RCNN architecture). In most cases the user experiences training freeze because, The evaluation script will take over the hardware, creating a training bottleneck.There are two ways to go about this problem.

1. Run training in GPU and run evaluation in CPU. (even this solution does not guarantee that your training will work without freezing if you have RAM less than 16GB or you use a heavy model).

2. Run only training script for specific iterations and then run the evaluation script, to check if the model has converged. I know this is a round about way but, I couldn't think of any better solution .Before this commit, evaluation script runs evaluation only on the latest checkpoint thus, you won't be able to evaluate other checkpoints. This commit also addresses this problem by optionally making it to run on all the checkpoints

Both of the above cases can be set using boolean via terminal while calling eval.py script",15,,[],2018-04-20 08:51:28,open,,,['cla: yes'],2019-01-12 07:44:13
1033,tensorflow/models,models,4036,abslamp,"Textsum error: a bytes-like object is required, not 'str'","--------------------
### System information
- **What is the top-level directory of the model you are using**: Textsum
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7
- **Bazel version (if compiling from source)**: 0.12.0(binary)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: seq2seq_attention --mode=train --article_key=article --abstract_key=abstract --data_path=D:/TS/data/training-* --vocab_path=D:/TS/data/vocab --log_root=D:/TS/textsum/log_root --train_dir=D:/TS/textsum/log_root/train

### Source code / logs
Simply used the toy data provided with Textsum and renamed it to training-0. The example training code (changed dir to use in cmd) caused many of this error:

Exception in thread Thread-xxx:
Traceback (most recent call last):
  File ""C:\Users\xxx\AppData\Local\Programs\Python\Python36\lib\threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""C:\Users\xxx\AppData\Local\Programs\Python\Python36\lib\threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""\\?\C:\Users\xxx\AppData\Local\Temp\Bazel.runfiles_0br8l4_4\runfiles\__main__\textsum\batch_reader.py"", line 139, in _FillInputQueue
    data.ToSentences(article, include_token=False)]
  File ""\\?\C:\Users\xxx\AppData\Local\Temp\Bazel.runfiles_0br8l4_4\runfiles\__main__\textsum\data.py"", line 215, in ToSentences
    return [s for s in s_gen]
  File ""\\?\C:\Users\xxx\AppData\Local\Temp\Bazel.runfiles_0br8l4_4\runfiles\__main__\textsum\data.py"", line 215, in <listcomp>
    return [s for s in s_gen]
  File ""\\?\C:\Users\xxx\AppData\Local\Temp\Bazel.runfiles_0br8l4_4\runfiles\__main__\textsum\data.py"", line 189, in SnippetGen
    start_p = text.index(start_tok, cur)
**TypeError: a bytes-like object is required, not 'str'**
",10,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu""), NamedUser(login=""peterjliu""), NamedUser(login=""yhliang2018"")]",2018-04-20 01:29:35,open,,,['stat:awaiting owner'],2018-12-28 08:45:18
1034,tensorflow/models,models,4035,Amey-D,Update CLOUD.md,"The image family `gci-stable` in `google-containers` project has long been deprecated.
```
$ gcloud compute images list --project google-containers --no-standard-images
NAME                    PROJECT            FAMILY        DEPRECATED  STATUS
container-vm-v20170214  google-containers  container-vm              READY
```
The preferred way to use COS images is via `cos-stable` image family in `cos-cloud` project.",3,,[],2018-04-19 22:21:39,open,,,['cla: yes'],2018-04-27 18:26:08
1035,tensorflow/models,models,4029,fengdongfjg,Update mobilenet_v1.py,"modify depth_multiplier param in slim.separable_conv2d in mobilenet_v1_base function, change depth_multiplier=1 to depth_multiplier",1,,[],2018-04-19 10:49:35,open,,,['cla: no'],2018-04-19 10:49:38
1036,tensorflow/models,models,4019,Andreas71,Evaluation results,"Hello,

I trained my model using my own training dataset but when i try to run the evaluation it only displays the test images with the detections in the tensorboard, but nothing about the MaP. How can i display the MaP or any other accuracy result?

Thanks in advance",9,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""k-w-w"")]",2018-04-18 16:17:22,open,,,[],2018-04-26 11:54:05
1037,tensorflow/models,models,4015,zeynali,i get very bad result with ssd_mobilev1_coco on my own datatset,"System information
```

    What is the top-level directory of the model you are using: models/research/object_detection
    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
    TensorFlow installed from (source or binary): binary
    TensorFlow version (use command below): 1.5
    Bazel version (if compiling from source): /
    CUDA/cuDNN version: 9.0 / 7.0.5
    GPU model and memory: GTX 1080, 8GB
    RAM : 16G
```

Hi , I've trained the ssd_mobilev1_coco on my own dataset that have only one class the total dataset for training is 200k samples and for testing 35k , and i go through 450k step with size 608*608 and batch_size 20 , and my train mAP is 85 but test is 35 why ?????? ",3,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-04-18 12:59:47,open,,,[],2018-05-07 01:39:38
1038,tensorflow/models,models,4010,tt7533,Relationship between Squeeze_2 and detection_boxes after SecondStagePostprocessor,"- **models/research/object_detection/**
- specifically using ""faster_rcnn_resnet101_coco"" from the model zoo.

I have a trained frozen graph model _frozen_inference_graph.pb_ that I can load into _detection_graph_.   In order to investigate the relationship between the proposed bounding boxes at different stages of the detection, I plot the contents of:
1)  _Squeeze_2_ 
2) _detection_boxes_  (which is the output of **SecondStagePostprocessor** given _Squeeze_2_ tensor.)

As a code snippet, I can extract both tensors by running:
```
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
_preBoxes = detection_graph.get_tensor_by_name('Squeeze_2:0')

boxes, preBoxes = sess.run([detection_boxes,  _preBoxes],  {image_tensor: image})
```

Using this, I plot the bounding boxes 
```
for _ in range(boxes.shape[0]):
    ymin, xmin, ymax, xmax = boxes[_]
    ax.add_patch(Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin))

for _ in range(preBoxes.shape[0]):
    ymin, xmin, ymax, xmax = preBoxes[_]
    ax.add_patch(Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin))
    
plt.imshow(image)
```

One would expect that both sets of boxes should be pretty similar to each other since _detection_boxes_ is simply the output of _Squeeze_2_ after applying **SecondStagePostprocessor**

While the plot of _detection_boxes_ looks very good, the boxes coming out of _Squeeze_2_ are all over the place with **no apparent relationship** with the ones coming from _detection_boxes_.   Clearly the boxes of _Squeeze_2_ are free to extend out the image itself and should be clipped.  Nevertheless, this is not enough to explain the discrepancy.
 
If this is because both boxes are stored in a different format than  `ymin, xmin, ymax, xmax`, I cannot find anywhere a documentation for it.  They do seem to accumulate in the top left corner of the image...

Otherwise, could you please explain the reason for such a wide discrepancy?

As an example, you can see in black the boxes from _detection_boxes_ and in blue the boxes from _Squeeze_2_ in this image

![screen shot 2018-04-18 at 11 37 52](https://user-images.githubusercontent.com/26541376/38921197-cdd5044c-42fd-11e8-9640-d19e177a0a24.png)
",0,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-18 08:25:22,open,,,[],2018-04-18 20:20:16
1039,tensorflow/models,models,4006,rudevel,research/object_detection/dataset_tools/create_pet_tf_record.py obviously mixes up the file names,"Please **READ BEFORE** you go and watch the error- message- 
As the **error- message** is **totally misleading**:

**_TL;DR;: Swap the if-else branches 285-288_**

In **lines 285 - 288** the scripts applies names _""_with_masks_""_ if **FLAGS.faces_only**.

But according to the description (AND my mask training _finally_ working) this is exactly the switch that causes NO masks to be generated. 

Additional flaws: 
- With PNG the file sizes are NOT significantly larger as the parameter description says (another confusion point)
- Folders do not work as the parameters suggest (only pwd)- but that is (partly) covered by the pets- howto (and symlinks to images and annotations did work)
- With this minor problem I get misleading error messages like the following:

...
assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ]
...

While this is for sure a result of the wrong input it has most probably already - and will for sure - cause many others to be confused and search for hours.",0,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-17 21:23:44,open,,,[],2018-04-18 07:51:11
1040,tensorflow/models,models,3996,gustavz,[Object Detection] Mobilenet as Backbone for Mask R-CNN,"Is it possible to create a config file with Mobilenet V1 or V2 as  Backbone /Feature extractor to train a Mask R-CNN Model?

Something like this:

```
model {
  faster_rcnn {
    num_classes: 90
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 800
        max_dimension: 1365
      }
    }
    number_of_stages: 3
    feature_extractor {
type: 'ssd_mobilenet_v2'
...
```",34,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-04-17 08:27:03,open,,,['type:feature'],2019-04-05 04:03:30
1041,tensorflow/models,models,3994,airmak,"Trying to run lm_1b; ParseError: 1:1 : Expected identifier or number, got <","### System information

Running Python 3.6.4 on Windows

### Describe the problem

I'm trying to run lm_1b on sample mode, by inputting: 

`$ bazel-bin/lm_1b/lm_1b_eval --mode sample --prefix ""I love that I""  --pbtxt data/vocab-2016-09-10.txt --vocab_file data/vocab-2016-09-10.txt --ckpt 'data/ckpt-*'`

But I get the error:

`google.protobuf.text_format.ParseError: 1:1 : Expected identifier or number, got <.
`
Any help would really be appreciated

### Source code / logs
```
Recovering graph.
Traceback (most recent call last):
  File ""\\?\C:\Users\snmsa\AppData\Local\Temp\Bazel.runfiles_9sq54ngc\runfiles\__main__\lm_1b\lm_1b_eval.py"", line 308, in <module>
    tf.app.run()
  File ""C:\Users\snmsa\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""\\?\C:\Users\snmsa\AppData\Local\Temp\Bazel.runfiles_9sq54ngc\runfiles\__main__\lm_1b\lm_1b_eval.py"", line 298, in main
    _SampleModel(FLAGS.prefix, vocab)
  File ""\\?\C:\Users\snmsa\AppData\Local\Temp\Bazel.runfiles_9sq54ngc\runfiles\__main__\lm_1b\lm_1b_eval.py"", line 174, in _SampleModel
    sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)
  File ""\\?\C:\Users\snmsa\AppData\Local\Temp\Bazel.runfiles_9sq54ngc\runfiles\__main__\lm_1b\lm_1b_eval.py"", line 89, in _LoadModel
    text_format.Merge(s, gd)
  File ""C:\Users\snmsa\Anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 533, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\Users\snmsa\Anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 587, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\Users\snmsa\Anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 620, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\Users\snmsa\Anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 635, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\Users\snmsa\Anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 679, in _MergeField
    name = tokenizer.ConsumeIdentifierOrNumber()
  File ""C:\Users\snmsa\Anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1152, in ConsumeIdentifierOrNumber
    raise self.ParseError('Expected identifier or number, got %s.' % result)
google.protobuf.text_format.ParseError: 1:1 : Expected identifier or number, got <.
```",1,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-17 01:43:59,open,,,[],2019-04-01 16:26:35
1042,tensorflow/models,models,3990,RodSernaPerez,Error when executing the parser trainer,"I am trying to train Syntaxnet on a new Corpus. I have edited the context.pbtxt so it points to the route of the corpus:

input {
  name: 'training-corpus'
  record_format: 'conll-sentence'
  Part {
    file_pattern: '/root/models/syntaxnet/corpus/ud-treebanks-v2.1/UD_English/en-ud-train.conllu'
  }
}
input {
  name: 'tuning-corpus'
  record_format: 'conll-sentence'
  Part {
    file_pattern: '/root/models/syntaxnet/corpus/ud-treebanks-v2.1/UD_English/en-ud-dev.conllu'
  }
}
input {
  name: 'test-corpus'
  record_format: 'conll-sentence'
  Part {
    file_pattern: '/root/models/syntaxnet/corpus/ud-treebanks-v2.1/UD_English/en-ud-test.conllu'
  }
}

But when I try to execute the parser trainer with

bazel-bin/syntaxnet/parser_trainer \
  --task_context=/root/models/syntaxnet/syntaxnet/context.pbtxt \
  --arg_prefix=brain_pos \ 
  --compute_lexicon \     
  --graph_builder=greedy \  
  --training_corpus=training-corpus \  
  --tuning_corpus=tuning-corpus \
  --output_path=models \ 
  --batch_size=32 \       
  --decay_steps=3600 \
  --hidden_layer_sizes=128 \
  --learning_rate=0.08 \
  --momentum=0.9 \
  --seed=0 \
  --params=128-0.08-3600-0.9-0 

I get the following error:

root@1f26ab651d1f:~/models/syntaxnet# bazel-bin/syntaxnet/parser_trainer \
>   --task_context=/root/models/syntaxnet/syntaxnet/context.pbtxt \
>   --arg_prefix=brain_pos \
Traceback (most recent call last):
  File ""/root/models/syntaxnet/bazel-bin/syntaxnet/parser_trainer.runfiles/syntaxnet/parser_trainer.py"", line 303, in <module>
    tf.app.run()
  File ""/root/models/syntaxnet/bazel-bin/syntaxnet/parser_trainer.runfiles/external/tf/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/root/models/syntaxnet/bazel-bin/syntaxnet/parser_trainer.runfiles/syntaxnet/parser_trainer.py"", line 263, in main
    RewriteContext()
  File ""/root/models/syntaxnet/bazel-bin/syntaxnet/parser_trainer.runfiles/syntaxnet/parser_trainer.py"", line 98, in RewriteContext
    text_format.Merge(fin.read(), context)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 309, in Merge
    return MergeLines(text.split('\n'), message, allow_unknown_extension)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 346, in MergeLines
    _ParseOrMerge(lines, message, True, allow_unknown_extension)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 371, in _ParseOrMerge
    allow_unknown_extension)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 473, in _MergeField
    allow_unknown_extension)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py"", line 443, in _MergeField
    message_descriptor.full_name, name))
google.protobuf.text_format.ParseError: 232:1 : Message type ""syntaxnet.TaskInput"" has no field named ""input"".

How can I fix it?
",0,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-16 11:01:09,open,,,[],2018-04-16 20:12:17
1043,tensorflow/models,models,3987,bhack,Triplet loss tutorial,"It could be nice to have an official tutorial here with the highl level API (Dataset/Estimator) on Triplet loss similar to https://omoindrot.github.io/triplet-loss.

/cc @omoindrot",1,,[],2018-04-15 11:55:09,open,,,['stat:awaiting tensorflower'],2018-05-09 22:16:28
1044,tensorflow/models,models,3986,gautam1858,Update distributions.py,"posterior post_zs should start from 0

Example
```
>>> a = [1,5,23,45]
>>> a[1:]
[5, 23, 45]
>>> a[0:]
[1, 5, 23, 45]
```",0,,[],2018-04-15 09:21:53,open,,,['cla: yes'],2018-07-17 16:40:44
1045,tensorflow/models,models,3985,gautam1858,Update lfdas.py,Adding self to datasets and factors_dim,0,,[],2018-04-15 09:06:58,open,,,['cla: yes'],2018-04-15 09:07:00
1046,tensorflow/models,models,3984,gravitywp,Train dragnn parser standalone,"I was trying to train a dragon parser without a link to tagger。
here is my network structure.
![image](https://user-images.githubusercontent.com/8477504/38776514-8d990cdc-40ca-11e8-8074-d881c7d1d21c.png)
here are my lookahead fml parameters
![image](https://user-images.githubusercontent.com/8477504/38776526-d4953502-40ca-11e8-8cef-07266391a317.png)

I'd like to have the tag feature in lookahead component, but it seems ""input.token.tagger"" always is ""UNKNOWN"", and I'm using UD training data.
![image](https://user-images.githubusercontent.com/8477504/38776553-42a62ec0-40cb-11e8-9ab8-c2cd5578db3f.png)

any suggestion will be greatly appreciated.
and sorry for my poor English.",0,"NamedUser(login=""calberti"")","[NamedUser(login=""calberti"")]",2018-04-15 08:45:37,open,,,[],2018-04-16 18:00:08
1047,tensorflow/models,models,3983,jiyongma,[deeplab]NotFoundError (see above for traceback): Key aspp1_depthwise/BatchNorm/beta not found in checkpoint,"when I run 
python deeplab/eval.py \
    --logtostderr \
    --eval_split=""val"" \
    --model_variant=""xception_65"" \
    --atrous_rates=6 \
    --atrous_rates=12 \
    --atrous_rates=18 \
    --output_stride=16 \
    --decoder_output_stride=4 \
    --eval_crop_size=513\
    --eval_crop_size=513 \
    --dataset=""ade20k"" \
    --checkpoint_dir=""./deeplab/datasets/ADE20K/exp/train_on_train_set/train"" \
    --eval_logdir=""./deeplab/datasets/ADE20K/exp/train_on_train_set/eval"" \
    --dataset_dir=""./deeplab/datasets/ADE20K/tfrecord""


NotFoundError (see above for traceback): Key aspp1_depthwise/BatchNorm/beta not found in checkpoint
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_299 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_306_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
",3,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel"")]",2018-04-15 08:41:41,open,,,"['models: research', 'stat:awaiting response']",2019-02-06 21:40:41
1048,tensorflow/models,models,3980,zeynali,problem in the ssd_mobilenetv2 training,"when i run to trained the ssd mobile v2 in the terminal i get such warning : 

```
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/RMSProp] is not available in checkpoint
WARNING:root:Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/RMSProp_1] is not available in checkpoint
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:736: Supervi
```",4,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-14 12:53:03,open,,,[],2018-04-25 11:54:24
1049,tensorflow/models,models,3971,opencici2006,Add interface to pass the number of threads for inter/intra-op parall…,"…elism

Signed-off-by: shaohua <shaohua.zhang@intel.com>",4,,[],2018-04-13 10:18:39,open,,,['cla: yes'],2018-05-02 01:07:06
1050,tensorflow/models,models,3970,opencici2006,Add interface to pass the number of threads for inter/intra-op parallelism,Signed-off-by: shaohua <shaohua.zhang@intel.com>,4,,[],2018-04-13 10:03:46,open,,"NamedUser(login=""opencici2006"")",['cla: yes'],2018-05-28 11:00:48
1051,tensorflow/models,models,3967,prakhar-agarwal,[Feature Request] Graph embedding methods in Tensorflow/models,"Hi,

I would like to implement graph embedding models starting with Deepwalk followed by metapath2vec in tensorflow/models. Let me know if someone is already working on it.",0,"NamedUser(login=""k-w-w"")","[NamedUser(login=""k-w-w"")]",2018-04-13 04:58:30,open,,,['stat:community support'],2018-04-17 17:52:53
1052,tensorflow/models,models,3962,imaxpayne,[Learned_optimizer] Error when running metarun.py,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: ~/models/research/learned_optimizer
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:6.0
- **GPU model and memory**: Titan xp
- **Exact command to reproduce**: python  metarun.py



### Describe the problem
I was trying to run the training script 'metarun.py' but encountered errors shown as below

### Source code / logs
Traceback (most recent call last):
  File ""metarun.py"", line 394, in <module>
    tf.app.run()
  File ""/home/rvl224/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""metarun.py"", line 388, in main
    callbacks=[])
  File ""/home/rvl224/models/research/metaopt.py"", line 184, in train_optimizer
    train_output = opt.train(problem, dataset)
  File ""/home/rvl224/models/research/learned_optimizer/optimizer/trainable_optimizer.py"", line 347, in train
    swap_memory=True, shape_invariants=invariants)
  File ""/home/rvl224/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2816, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/home/rvl224/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2640, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/home/rvl224/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2597, in _BuildLoop
    nest.assert_same_structure(list(packed_vars_for_body), list(body_result))
  File ""/home/rvl224/anaconda3/lib/python3.5/site-packages/tensorflow/python/util/nest.py"", line 222, in assert_same_structure
    % (len_nest1, nest1, len_nest2, nest2))
ValueError: The two structures don't have the same number of elements.

First structure (120 elements): [<tf.Tensor 'while/Identity:0' shape=() dtype=int32>, <tf.Tensor 'while/Identity_1:0' shape=() dtype=float32>, [<tf.Tensor 'while/Identity_2:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_3:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_4:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_5:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_6:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_7:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_8:0' shape=(12, 1) dtype=float32>], [<tf.Tensor 'while/Identity_9:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_10:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_11:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_12:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_13:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_14:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_15:0' shape=(12, 1) dtype=float32>], [[<tf.Tensor 'while/Identity_16:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_17:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_18:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_19:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_20:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_21:0' shape=(1, 20) dtype=float32>, <tf.Tensor 'while/Identity_22:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_23:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_24:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_25:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_26:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_27:0' shape=(11, 10) dtype=float32>, <tf.Tensor 'while/Identity_28:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/Identity_29:0' shape=(11, 1) dtype=float32>], [<tf.Tensor 'while/Identity_30:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_31:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_32:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_33:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_34:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_35:0' shape=(1, 20) dtype=float32>, <tf.Tensor 'while/Identity_36:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_37:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_38:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_39:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_40:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_41:0' shape=(3, 10) dtype=float32>, <tf.Tensor 'while/Identity_42:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/Identity_43:0' shape=(3, 1) dtype=float32>], [<tf.Tensor 'while/Identity_44:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_45:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_46:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_47:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_48:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_49:0' shape=(1, 20) dtype=float32>, <tf.Tensor 'while/Identity_50:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_51:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_52:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_53:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_54:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_55:0' shape=(9, 10) dtype=float32>, <tf.Tensor 'while/Identity_56:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/Identity_57:0' shape=(9, 1) dtype=float32>], [<tf.Tensor 'while/Identity_58:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_59:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_60:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_61:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_62:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_63:0' shape=(1, 20) dtype=float32>, <tf.Tensor 'while/Identity_64:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_65:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_66:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_67:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_68:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_69:0' shape=(7, 10) dtype=float32>, <tf.Tensor 'while/Identity_70:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/Identity_71:0' shape=(7, 1) dtype=float32>], [<tf.Tensor 'while/Identity_72:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_73:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_74:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_75:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_76:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_77:0' shape=(1, 20) dtype=float32>, <tf.Tensor 'while/Identity_78:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_79:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_80:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_81:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_82:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_83:0' shape=(5, 10) dtype=float32>, <tf.Tensor 'while/Identity_84:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/Identity_85:0' shape=(5, 1) dtype=float32>], [<tf.Tensor 'while/Identity_86:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_87:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_88:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_89:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_90:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_91:0' shape=(1, 20) dtype=float32>, <tf.Tensor 'while/Identity_92:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_93:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_94:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_95:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_96:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_97:0' shape=(13, 10) dtype=float32>, <tf.Tensor 'while/Identity_98:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/Identity_99:0' shape=(13, 1) dtype=float32>], [<tf.Tensor 'while/Identity_100:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_101:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_102:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_103:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_104:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_105:0' shape=(1, 20) dtype=float32>, <tf.Tensor 'while/Identity_106:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_107:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_108:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_109:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_110:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_111:0' shape=(12, 10) dtype=float32>, <tf.Tensor 'while/Identity_112:0' shape=(12, 1) dtype=float32>, <tf.Tensor 'while/Identity_113:0' shape=(12, 1) dtype=float32>]], [<tf.Tensor 'while/Identity_114:0' shape=(1, 20) dtype=float32>], <tf.Tensor 'while/Identity_115:0' shape=(?,) dtype=float32>, <tf.Tensor 'while/Identity_116:0' shape=() dtype=float32>, <tf.Tensor 'while/Identity_117:0' shape=<unknown> dtype=float32>, <tf.Tensor 'while/Identity_118:0' shape=<unknown> dtype=int32>, <tf.Tensor 'while/Identity_119:0' shape=<unknown> dtype=int32>]

Second structure (23 elements): [<tf.Tensor 'while/add_21:0' shape=() dtype=int32>, <tf.Tensor 'while/Add:0' shape=() dtype=float32>, [<tf.Tensor 'while/LOL/PerTensor/sub_15:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_1/sub_15:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_2/sub_15:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_3/sub_15:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_4/sub_15:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_5/sub_15:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_6/sub_15:0' shape=(12, 1) dtype=float32>], [<tf.Tensor 'while/LOL/PerTensor/sub_15:0' shape=(11, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_1/sub_15:0' shape=(3, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_2/sub_15:0' shape=(9, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_3/sub_15:0' shape=(7, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_4/sub_15:0' shape=(5, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_5/sub_15:0' shape=(13, 1) dtype=float32>, <tf.Tensor 'while/LOL/PerTensor_6/sub_15:0' shape=(12, 1) dtype=float32>], <map object at 0x7f0a9b8f4be0>, [<tf.Tensor 'while/LOL/Layer2_RNN/BiasGRUCell/add:0' shape=(1, 20) dtype=float32>], <tf.Tensor 'while/concat:0' shape=(?,) dtype=float32>, <tf.Tensor 'while/Identity_116:0' shape=() dtype=float32>, <tf.Tensor 'while/Identity_117:0' shape=<unknown> dtype=float32>, <tf.Tensor 'while/Identity_118:0' shape=<unknown> dtype=int32>, <tf.Tensor 'while/Identity_119:0' shape=<unknown> dtype=int32>]
",2,"NamedUser(login=""nirum"")","[NamedUser(login=""nirum"")]",2018-04-12 11:02:08,open,,,['stat:awaiting tensorflower'],2018-05-09 22:16:06
1053,tensorflow/models,models,3958,kushagraagrawal,[deeplab] [feature request] Code for calculating runtime of MobileNet v2 and Xception,"Hi, could someone guide with how to find runtime of the models provided? I wanted to run these models on my PC and find out the same. The documentation suggests using tfprof but I was unable to figure out how to use the same. 

Thanks a lot!
",0,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-12 06:04:52,open,,,[],2018-04-12 13:51:31
1054,tensorflow/models,models,3956,bitfort,Adding flag to set random seeds.,This adds flags to set the random seed for python and tensorflow. This is intended to make timing benchmarks (more) repeatable. ,1,,[],2018-04-11 22:29:31,open,,,['cla: no'],2018-04-12 04:43:16
1055,tensorflow/models,models,3953,hsm207,[deeplab] What is the purpose of separable_conv2d_same?,"I am trying to understand the following comments in the `separable_conv2d_same` function:

```
Note that

     net = separable_conv2d_same(inputs, num_outputs, 3,
       depth_multiplier=1, stride=stride)

  is equivalent to

     net = slim.separable_conv2d(inputs, num_outputs, 3,
       depth_multiplier=1, stride=1, padding='SAME')
     net = resnet_utils.subsample(net, factor=stride)

  whereas

     net = slim.separable_conv2d(inputs, num_outputs, 3, stride=stride,
       depth_multiplier=1, padding='SAME')

  is different when the input's height or width is even, which is why we add the
  current function.
```
I don't understand the point about the `slim.separable_conv2d` giving different results when the input's height or width is even, so I wrote the following code:

```
print('When input\'s height and width is odd:')
x = tf.random_uniform((1, 5, 5, 1), 1, 10, tf.int32)
x = tf.cast(x, tf.float32)

stride = 2
y1 = slim.separable_conv2d(x, 2, 3, depth_multiplier=1, stride=1, padding='same')
y1 = resnet_utils.subsample(y1, factor=stride)
print(y1.shape)

y2 = separable_conv2d_same(x, 2, 3, depth_multiplier=1, stride=stride, scope='xxx')
print(y2.shape)

y3 = slim.separable_conv2d(x, 2, 3, stride=stride, depth_multiplier=1, padding='same')
print(y3.shape)

print('When input\'s height is even and width is odd:')
x = tf.random_uniform((1, 6, 5, 1), 1, 10, tf.int32)
x = tf.cast(x, tf.float32)

stride = 2
y1 = slim.separable_conv2d(x, 2, 3, depth_multiplier=1, stride=1, padding='same')
y1 = resnet_utils.subsample(y1, factor=stride)
print(y1.shape)

y2 = separable_conv2d_same(x, 2, 3, depth_multiplier=1, stride=stride, scope='xxx')
print(y2.shape)

y3 = slim.separable_conv2d(x, 2, 3, stride=stride, depth_multiplier=1, padding='same')
print(y3.shape)
```

The output from all 6 print statements is the same: (1, 3, 3, 2). So, it looks to me that the `separable_conv2d_same`  function does the same thing as `slim.separable_conv2d` in all cases.

Please help me understand the message the comment is meant to convey.

The following is the entire code to reproduce the output I got:

```
import tensorflow.contrib.eager as tfe
import tensorflow as tf
import tensorflow.contrib.slim as slim
from tensorflow.contrib.slim.nets import resnet_utils


def fixed_padding(inputs, kernel_size, rate=1):
    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)
    pad_total = kernel_size_effective - 1
    pad_beg = pad_total // 2
    pad_end = pad_total - pad_beg
    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],
                                    [pad_beg, pad_end], [0, 0]])
    return padded_inputs


def separable_conv2d_same(inputs,
                          num_outputs,
                          kernel_size,
                          depth_multiplier,
                          stride,
                          rate=1,
                          use_explicit_padding=True,
                          regularize_depthwise=False,
                          scope=None,
                          **kwargs):
    def _separable_conv2d(padding):
        """"""Wrapper for separable conv2d.""""""
        return slim.separable_conv2d(inputs,
                                     num_outputs,
                                     kernel_size,
                                     depth_multiplier=depth_multiplier,
                                     stride=stride,
                                     rate=rate,
                                     padding=padding,
                                     scope=scope,
                                     **kwargs)

    def _split_separable_conv2d(padding):
        """"""Splits separable conv2d into depthwise and pointwise conv2d.""""""
        outputs = slim.separable_conv2d(inputs,
                                        None,
                                        kernel_size,
                                        depth_multiplier=depth_multiplier,
                                        stride=stride,
                                        rate=rate,
                                        padding=padding,
                                        scope=scope + '_depthwise',
                                        **kwargs)
        return slim.conv2d(outputs,
                           num_outputs,
                           1,
                           scope=scope + '_pointwise',
                           **kwargs)

    if stride == 1 or not use_explicit_padding:
        if regularize_depthwise:
            outputs = _separable_conv2d(padding='SAME')
        else:
            outputs = _split_separable_conv2d(padding='SAME')
    else:
        inputs = fixed_padding(inputs, kernel_size, rate)
        if regularize_depthwise:
            outputs = _separable_conv2d(padding='VALID')
        else:
            outputs = _split_separable_conv2d(padding='VALID')
    return outputs


tfe.enable_eager_execution()

print('When input\'s height and width is odd:')
x = tf.random_uniform((1, 5, 5, 1), 1, 10, tf.int32)
x = tf.cast(x, tf.float32)

stride = 2
y1 = slim.separable_conv2d(x, 2, 3, depth_multiplier=1, stride=1, padding='same')
y1 = resnet_utils.subsample(y1, factor=stride)
print(y1.shape)

y2 = separable_conv2d_same(x, 2, 3, depth_multiplier=1, stride=stride, scope='xxx')
print(y2.shape)

y3 = slim.separable_conv2d(x, 2, 3, stride=stride, depth_multiplier=1, padding='same')
print(y3.shape)

print('When input\'s height is even and width is odd:')
x = tf.random_uniform((1, 6, 5, 1), 1, 10, tf.int32)
x = tf.cast(x, tf.float32)

stride = 2
y1 = slim.separable_conv2d(x, 2, 3, depth_multiplier=1, stride=1, padding='same')
y1 = resnet_utils.subsample(y1, factor=stride)
print(y1.shape)

y2 = separable_conv2d_same(x, 2, 3, depth_multiplier=1, stride=stride, scope='xxx')
print(y2.shape)

y3 = slim.separable_conv2d(x, 2, 3, stride=stride, depth_multiplier=1, padding='same')
print(y3.shape)

```",5,"NamedUser(login=""YknZhu"")","[NamedUser(login=""YknZhu""), NamedUser(login=""aquariusjay"")]",2018-04-11 12:09:54,open,,,[],2018-08-14 04:30:46
1056,tensorflow/models,models,3952,orgicus,use current directory as default --model_dir ,"Running the [Recurrent Neural Networks for Drawing Classification](https://www.tensorflow.org/tutorials/recurrent_quickdraw) tutorial with the suggested command:
```bash
python train_model.py \
    --training_data=rnn_tutorial_data/training.tfrecord-?????-of-????? \
    --eval_data=rnn_tutorial_data/eval.tfrecord-?????-of-????? \
    --classes_file=rnn_tutorial_data/training.tfrecord.classes
```

currently produces a ValueError crash without explicitly specifying the `--model_dir`

e.g.
```
Traceback (most recent call last):
  File ""train_model.py"", line 388, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Volumes/GP_2T/tensorflow_tutorials/tf_src/venv/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train_model.py"", line 297, in main
    save_summary_steps=100))
  File ""/Volumes/GP_2T/tensorflow_tutorials/tf_src/venv/lib/python2.7/site-packages/tensorflow/python/estimator/run_config.py"", line 450, in __init__
    compat_internal.path_to_str(model_dir))
  File ""/Volumes/GP_2T/tensorflow_tutorials/tf_src/venv/lib/python2.7/site-packages/tensorflow/python/estimator/run_config.py"", line 754, in _get_model_dir
    raise ValueError('model_dir should be non-empty.')
ValueError: model_dir should be non-empty.
```

One option is to update the tutorial to mention the model directory **must** be specified, otherwise a quick fix is to default to the current directory

P.S. I agree with @angersson 's [comment](https://github.com/tensorflow/models/issues/3478#issuecomment-368979610) which is why I've started a [stackoverflow thread](https://stackoverflow.com/questions/49774035/how-to-classify-a-quickdraw-doodle-using-tensorflows-sketch-rnn-tutorial) voicing my confusion over the tutorial",5,,[],2018-04-11 11:43:16,open,,,['cla: no'],2018-04-24 17:55:51
1057,tensorflow/models,models,3950,zBabar,Show and Tell/ Im2txt: For a new dataset when run the inference it generates same caption for each,I have tried to run im2txt with respect to another dataset which has almost 7k images... after training for about 100000 steps.. when I run inference it gives same set of captions for each image... I have changed the image but still I gives same... Any clue where the problem could be.... but log prob are change for each image.... I can see that generated caption has some relevance with some image in dataset... but why generates same caption for each image along with original image.,1,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-11 10:24:05,open,,,[],2019-03-04 12:41:43
1058,tensorflow/models,models,3947,dseuss,Fix bug exporting FasterRCNN with only one stage,Fixes issue #1916,4,,[],2018-04-11 04:55:28,open,,,['cla: yes'],2019-04-05 13:02:17
1059,tensorflow/models,models,3945,classicsong,Distributed training of slim examples use only one node,"I deployed the slim example on my distributed GPU environment for distributed training (with 2 nodes, each has 4 P40 GPU). When using async training mode, I found out that only one node has GPU workload an other node's GPU is idle.

TensorFlow Version: 1.6.0
TensorFlow Model Version: 1.8.0
cuda version: nvidia-docker with cuda9.1 and cudnn7
os: ubuntu 16.04
python 2.7.0

Distributed Env config:
ps: ""A:2222"", ""B:2222""
worker: ""A:2223"", ""B:2223""
PS use CPU only and worker use GPU with num_clones=4

The solution is to modify the clone_device(self, clone_index) function in deployment/model_deploy.py
to assign task id with device assignment.

    --- a/research/slim/deployment/model_deploy.py
    +++ b/research/slim/deployment/model_deploy.py
    @@ -593,6 +593,9 @@ class DeploymentConfig(object):
           device += '/device:CPU:0'
         else:
           device += '/device:GPU:%d' % clone_index
    +
    +    if self._num_ps_tasks > 0:
    +      device = '%s/task:%d' % (device, self._replica_id) 
         return device
 
       def clone_scope(self, clone_index):

",9,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-04-11 02:16:15,open,,,[],2019-02-16 07:22:51
1060,tensorflow/models,models,3940,khcy82dyc,[deeplab] [feature request] update FAQ for 'train the model on other datasets',"Would be really great to see a step by step guide line on how to use deeplabV3 to train on own datasets, including:


format of the image annotation (e.g. one folder contains the original images, the other folder contains their multi color image mask), 

parameter settings for tf_initial_checkpoint, initialize_last_layer, fine_tune_batch_norm. when to set False/True for these values. Also, I wonder if the parameters should be set differently when training on a small datasets, e.g. 100 images with 10 classes?

I notice these two issues are not solved and I've encountered both: please google search for ""deeplab CUDA_ERROR_LAUNCH_FAILED"" and ""deeplab Tensor had NaN values"". Adjusting the above parameter helps thus it would be really useful to know how to use them.

Could you please provide the version of Cuda, cudnn, tensorflow, python that you have successfully tested?  

Also, which initial checkpoint to use for different segmentation task, is it the deeplabv3_xception? but the recently update on the ade20k mentioned deeplabv3_pascal_aug. it's a bit confusing.

finally, the reason to remove colormap in the ground truth annotations for VOC but not ade20k

Thanks a lot!",3,"NamedUser(login=""YknZhu"")","[NamedUser(login=""YknZhu""), NamedUser(login=""aquariusjay"")]",2018-04-10 15:16:14,open,,,[],2018-04-29 11:08:31
1061,tensorflow/models,models,3936,adarvit,Running the Evaluation Job is not working?,"hey all, i am using the code from g3doc for running locally in order to do evaluation job

but i am getting the following warning and then the cmd is seems to be frozen:

![capture](https://user-images.githubusercontent.com/33596858/38556661-1a77b78a-3cd3-11e8-9450-a6b39aa04949.PNG)

is it ok? am i missing something?",2,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-10 12:25:18,open,,,[],2018-04-12 14:04:55
1062,tensorflow/models,models,3934,AmemiyaYuko,How could I apply focal loss on Faster RCNN models?,"While I was trying to apply focal loss on Faster RCNN, I got error message like this:

google.protobuf.text_format.ParseError: 28:5 : Message type ""object_detection.protos.FasterRcnn"" has no field named ""loss"".

It appears I'm unable to change loss function for Faster RCNN models. In this case, how could I apply focal loss on Faster RCNN models if I really need to? Thank you.",3,"NamedUser(login=""pkulzc"")","[NamedUser(login=""pkulzc"")]",2018-04-10 11:36:31,open,,,['stat:contributions welcome'],2018-06-22 09:11:22
1063,tensorflow/models,models,3930,gustavz,[deeplab] extract boundingboxes from segmenation map,"i wonder if there is already an implementation or an util to extract boundingboxes from the segmentation masks produced by deeplab's model?

I wanted to investigate the seg_maps that are the ouput of the tf session when running deeplabv3_mnv2 (https://github.com/GustavZ/realtime_segmenation), but printing the map results in a kind of reduced/simplified array in the form of
```
[[0 0 0 ... 0 0 0]
[0 0 0 ... 0 0 0]
[0 0 0 ... 0 0 0]
...
[0 0 0 ... 0 0 0]
[0 0 0 ... 0 0 0]
[0 0 0 ... 0 0 0]]
```
So i can't really figure out how to extract bounding boxes from this. But in my understanding it should be possible to create a box around each connected object segment in the mask, right?

Has anybody experience with it, or give me a hint on how to proceed?",9,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-04-10 08:19:17,open,,,"['stat:contributions welcome', 'type:feature']",2018-05-14 14:13:25
1064,tensorflow/models,models,3927,ashley915,evaluation not finishing...help,"I'm having trouble evaluating my trained oxford pet data. 
I put my command in terminal like this:
python3 object_detection/eval.py \
    --logtostderr \
    --pipeline_config_path=/home/cjonrnd/models/research/object_detection/models/model/train_again/pipeline.config \
    --checkpoint_dir=/home/cjonrnd/models/research/object_detection/models/model/train_again/checkpoint \
    --eval_dir=/home/cjonrnd/models/research/object_detection/models/model/eval_again

and this runs with no problem, but stops after printing: See tf.nn.softmax_cross_entropy_with_logits_v2.
the terminal just sits there for like half an hour and stops running without any error message. 


I'm attaching a screenshot of my error. I'm using Ubuntu 16.04.3, Python 3.5, Tensorflow_gpu-1.5, and I am running all this locally. Please help me out.
Thank you!


![screenshot from 2018-04-10 15-26-56](https://user-images.githubusercontent.com/35290370/38539566-b42e8ba6-3cd3-11e8-8911-d3fc76fbac25.png)
",10,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-04-10 06:27:42,open,,,['stat:awaiting owner'],2018-05-22 16:42:11
1065,tensorflow/models,models,3923,yenchenlin,Remove redundant operation in spatial transformer,"Performing `grid = tf.expand_dims(grid, 0)` right before `grid = tf.reshape(grid, [-1])` seems redundant.",0,,[],2018-04-10 03:12:48,open,,,['cla: yes'],2018-04-10 03:12:51
1066,tensorflow/models,models,3919,bhack,[deeplab]High level api,"What about including an highlevel api version using tf.keras and model_to_estimator? @bonlime Started a [keras version](https://github.com/bonlime/keras-deeplab-v3-plus) but I don't know if he is interested to contribute it upstream

",2,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-09 23:05:09,open,,,[],2018-04-10 13:58:50
1067,tensorflow/models,models,3913,sjwhhhi,Cannot train the mask-rcnn models,"I want to train a mask-rcnn models by my personal dataset. I use create_pascal_tf_record.py to make it in tf-format. However, I cannot train it with this error.

> 2018-04-09 13:52:34.408287: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ****************************************************************************************************
> 2018-04-09 13:52:34.408300: W tensorflow/core/framework/op_kernel.cc:1198] Resource exhausted: OOM when allocating tensor with shape[4,160,56,67] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
> Traceback (most recent call last):
>   File ""train.py"", line 167, in <module>
>     tf.app.run()
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 124, in run
>     _sys.exit(main(argv))
>   File ""train.py"", line 163, in main
>     worker_job_name, is_chief, FLAGS.train_dir)
>   File ""/home/sjw/models/object_detection/trainer.py"", line 370, in train
>     saver=saver)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 782, in train
>     ignore_live_threads=ignore_live_threads)
>   File ""/usr/lib/python2.7/contextlib.py"", line 35, in __exit__
>     self.gen.throw(type, value, traceback)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 998, in managed_session
>     self.stop(close_summary_writer=close_summary_writer)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 826, in stop
>     ignore_live_threads=ignore_live_threads)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 387, in join
>     six.reraise(*self._exc_info_to_raise)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 295, in stop_on_exception
>     yield
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 492, in run
>     self.run_loop()
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 1028, in run_loop
>     self._sv.global_step])
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 895, in run
>     run_metadata_ptr)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1128, in _run
>     feed_dict_tensor, options, run_metadata)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1344, in _do_run
>     options, run_metadata)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1363, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_2119, Loss/BoxClassifierLoss/assert_equal_1/Assert/Assert/data_0, Loss/BoxClassifierLoss/assert_equal_1/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_2121, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/BoxClassifierLoss/ones_1/shape/_129)]]
> 
> Caused by op u'Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert', defined at:
>   File ""train.py"", line 167, in <module>
>     tf.app.run()
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 124, in run
>     _sys.exit(main(argv))
>   File ""train.py"", line 163, in main
>     worker_job_name, is_chief, FLAGS.train_dir)
>   File ""/home/sjw/models/object_detection/trainer.py"", line 246, in train
>     clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
>   File ""/home/sjw/models/slim/deployment/model_deploy.py"", line 193, in create_clones
>     outputs = model_fn(*args, **kwargs)
>   File ""/home/sjw/models/object_detection/trainer.py"", line 181, in _create_losses
>     losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
>   File ""/home/sjw/models/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1580, in loss
>     groundtruth_masks_list,
>   File ""/home/sjw/models/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1813, in _loss_box_classifier
>     groundtruth_boxlists, groundtruth_masks_list)
>   File ""/home/sjw/models/object_detection/core/target_assigner.py"", line 447, in batch_assign_targets
>     anchors, gt_boxes, gt_class_targets, gt_weights)
>   File ""/home/sjw/models/object_detection/core/target_assigner.py"", line 151, in assign
>     groundtruth_boxes.get())[:1])
>   File ""/home/sjw/models/object_detection/utils/shape_utils.py"", line 279, in assert_shape_equal
>     return tf.assert_equal(shape_a, shape_b)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/check_ops.py"", line 392, in assert_equal
>     return control_flow_ops.Assert(condition, data, summarize=summarize)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
>     return _add_should_use_warning(fn(*args, **kwargs))
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 169, in Assert
>     condition, data, summarize, name=""Assert"")
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py"", line 48, in _assert
>     name=name)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
>     op_def=op_def)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
>     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
> 
> InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
> 	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_2119, Loss/BoxClassifierLoss/assert_equal_1/Assert/Assert/data_0, Loss/BoxClassifierLoss/assert_equal_1/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_2121, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/BoxClassifierLoss/ones_1/shape/_129)]]
> 

And my tensorflow-gpu vesion is 1.5 in Ubuntu16.
Could anyone help me? Thanks.",30,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-04-09 05:05:12,open,,,['stat:awaiting tensorflower'],2018-11-06 01:03:28
1068,tensorflow/models,models,3904,eyaler,using im2txt for image search,"how can i use the im2txt code to find the closest images given a description? (image search task in tables 4,5 in paper)?

@cshallue",0,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-07 09:36:58,open,,,[],2018-04-07 19:41:01
1069,tensorflow/models,models,3902,fera0013,"Missing step in the ""Inference and evaluation on the Open Images dataset"" tutorial","### System information
NA

### Describe the problem
The [Inference and Evaluation tutorial][1] explains, how the [infer_detections.py script][2] can be used to do inference and evaluation on a trained model. What is missing however is, how you get to the images with the detection boxes, shown at the top of the tutorial. I am guessing you can use [the visualization_utils.py script][3]  for this. But how do you do that based on the output from the `infer_detections.py` script?  That step is clearly missing. 


  [1]: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/oid_inference_and_evaluation.md
  [2]: https://github.com/tensorflow/models/blob/master/research/object_detection/inference/infer_detections.py
  [3]: https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py

### Source code / logs
NA
",0,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""jch1"")]",2018-04-07 06:48:54,open,,,[],2018-04-09 20:01:56
1070,tensorflow/models,models,3900,twtygqyy,Quantized model works on X86 platform but not on ARM,"Hi, I trained a model on X86 server with GPU, and I met a strong problem as follows:

X86, original model, both GPU/CPU mode:  work
X86, quantized model, both GPU/CPU mode:  work
ARM64, original model, CPU mode:  work
ARM64, quantized model, CPU mode:  **not work** (code can run but always no output)

Does anybody know the reason why quantized model gives no result on ARM platform? 
Thanks in advance.",6,"NamedUser(login=""karmel"")","[NamedUser(login=""karmel""), NamedUser(login=""bitfort"")]",2018-04-06 20:32:28,open,,,['type:build/install'],2018-04-11 17:07:35
1071,tensorflow/models,models,3894,HutzelFutzel,Faster RCNN - Which approach is implemented? ,"Hi,
which approach from the Faster RCNN Paper is implemented in the object detection? Alternating training, Approximate joint training or Non-approximate joint training?

",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""jch1"")]",2018-04-06 13:20:41,open,,,[],2018-05-16 18:05:18
1072,tensorflow/models,models,3892,gustavz,[deeplab] no image resizing in preprocessing?,"Why do the images need to be manually resized before Inference?
Why is the resizing not automatically added like in the object detection models?

I guess there is a special reason and i would really like to know it! Thanks!",2,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-06 08:09:29,open,,,[],2018-05-14 14:14:55
1073,tensorflow/models,models,3889,stephenbalaban,Fix typo in README,"Had variables for `master`, `worker` in the pyobject being `json.dumps`'d instead of strings.",1,,[],2018-04-06 02:57:54,open,,,['cla: no'],2018-04-06 02:57:58
1074,tensorflow/models,models,3884,dextroza,cannot get the same mAP for SSD MobileNet as provided in official table,"### System information
- **What is the top-level directory of the model you are using**: models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6
- **Bazel version (if compiling from source)**: /
- **CUDA/cuDNN version**: 9.0 / 7.0.5
- **GPU model and memory**: GTX 1080 Ti, 11GB
- **Exact command to reproduce**: 

python object_detection/eval.py \
        --logtostderr \
        --checkpoint_dir=ssd_mobilenet_v1_coco_2017_11_17 \
        --eval_dir=$eval_dir \
        --pipeline_config_path=object_detection/samples/configs/ssd_mobilenet_v1_coco.config

### Describe the problem
First bug, I cannot even evaluate model ssd_mobilenet_v1_coco_2017_11_17 without adding ""metrics_set: coco_detection_metrics"" in eval_config{} in object_detection/samples/configs/ssd_mobilenet_v1_coco.config

More important,  I got mAP: 26, not 21 as in the offical table. Also for ssd_mobilenet_v2_coco_2018_03_29 I got 25, not 22 as in the official table.
Evaluation was on: COCO  val_2017 (tfRecords are created by provided script ./object_detection/dataset_tools/download_and_preprocess_mscoco.sh )

Link on the official table: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md

### Source code / logs
here is my output: 

INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Restoring parameters from tiris/ssd_mobilenet_v1_coco_2017_11_17/model.ckpt
INFO:tensorflow:Restoring parameters from tiris/ssd_mobilenet_v1_coco_2017_11_17/model.ckpt
creating index...
index created!
INFO:tensorflow:Loading and preparing annotation results...
INFO:tensorflow:Loading and preparing annotation results...
INFO:tensorflow:DONE (t=0.39s)
INFO:tensorflow:DONE (t=0.39s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=71.17s).
Accumulating evaluation results...
DONE (t=11.80s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.419
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.279
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.016
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.362
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.042
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.643



",10,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-04-05 14:28:46,open,,,['stat:awaiting tensorflower'],2018-08-30 12:00:50
1075,tensorflow/models,models,3882,gustavz,[deeplab] Running mobilenetV2 on Jetson Tx2,Did someone manage to run deeplab's mobilenetV2 on the jetson tx2?,18,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-05 11:04:56,open,,,[],2019-03-29 10:46:35
1076,tensorflow/models,models,3880,lientv,Cogmap: Cannot import name 'get_path_ids',"I have install dependencies in both virtualenv and anaconda but not linux built-in python.
When I run `sh scripts/script_test_pretrained_models.sh `, both `virtualenv` and conda virtual environment give this error:
```
 File ""/home/lien/work/cognitive_mapping_and_planning/datasets/nav_env.py"", line 47, in <module>
    import src.graph_utils as gu
  File ""/home/lien/work/cognitive_mapping_and_planning/src/graph_utils.py"", line 23, in <module>
    from datasets.nav_env import get_path_ids
```

I believe it is a dependency hell.
How did you guys escape this error?
",2,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-05 04:02:17,open,,,[],2018-04-10 12:47:22
1077,tensorflow/models,models,3877,ritaxiaotian,"ssd_mobilenet for object detection on GPU ""INFO:tensorflow:depth of additional conv before box predictor: 0"" and ""INFO:tensorflow:global_step/sec: 0""","### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I'm using the tensorflow object detection API
**OS Platform and Distribution : Linux Ubuntu 16.04
**TensorFlow installed from : pip install
TensorFlow version (use command below): 1.6
Python version: python3
CUDA/cuDNN version: Cuda 9.0
GPU model and memory: GTX 1060
Exact command to reproduce: python3 train.py --logtos
tderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config

### Describe the problem

I was trying to use Tensorflow object detection API for my own images. 

I've successfully used the API for grey-scale images following the procedure : https://www.youtube.com/watch?v=COlbP62-B-U&list=PLQVvvaa0QuDcNK5GeCQnxYnSSaar2tpku

Then, I changed the images to larger RGB images (341*341), around 150k per image. I encountered the following errors. And I can't get anything in tensorboard.

I also found that, when I first started to run the model, Pwr Usage for GPU is aroung 30w/120w. However, after several minutes, it becomes 9w/120w. It seems that no model is running on the GPU.

But the GPU memory usage is: 5877MiB /  6072MiB 

Was the error caused by running out of memory? 

Thanks!

### Error messages:
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/trainer.py:228: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
WARNING:tensorflow:From /home/xiao/.local/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-04-04 13:02:45.271420: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-04-04 13:02:45.606312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-04-04 13:02:45.607148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7465
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.50GiB
2018-04-04 13:02:45.607207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ssd_mobilenet_v1_coco_2017_11_17/model.ckpt
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0",7,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-04-04 18:31:21,open,,,['stat:awaiting owner'],2019-01-11 08:24:16
1078,tensorflow/models,models,3876,aicaffeinelife,Training SSD Inception network with COCO-Text API,"
### System information
- **Top level directory: ~/models/research/object-detection**:
- **A custom code similar to found [here](https://github.com/offbye/tensorflow_object_detection_create_coco_tfrecord/blob/master/README.md)**
- **Linux Ubuntu 16.04**
- **TensorFlow install: From pip wheel**
- **1.5.0**
- **CUDA: 9.0 / CuDNN: 7.0**
- **GPU: Gtx1080(8GB)**
- **Python:3.5.4**

### Description of the problem
I created my tfrecords using the `coco-text` API and since I'm detecting only one class I just set the label to 1. When I run `python object_detection/train.py --logtostderr --pipeline_config_path object_detection/ssd_inception_v2_cocotxt.config --out_dir log_dir` I get the following error: 
```
shape[0] = [1,46] vs. shape[1] = [1,23]
	 [[Node: concat = ConcatV2[N=4, T=DT_FLOAT, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ExpandDims, ExpandDims_1, ExpandDims_2, ExpandDims_3, Equal_4/y)]]
INFO:tensorflow:Caught OutOfRangeError. Stopping Training.

```

I have set my `batch_size` to 1 as well. The relevant portion of my code that's creating the tfrecord is: 
```
 bboxes = img_data['bboxes']
    xmins = []
    xmaxs = []
    ymins = []
    ymaxs = []
    
    # the coco format is [left,top,width,height]

    for bbox in bboxes:
        xmins.append(bbox[0])
        xmaxs.append(bbox[0] + bbox[2]) # 
        ymins.append(bbox[1])
        ymins.append(bbox[1] + bbox[3])

    example =  tf.train.Example(features=tf.train.Features(feature={
        'image/height':dataset_util.int64_feature(img_data['height']),
        'image/width':dataset_util.int64_feature(img_data['width']),
        'image/object/bbox/xmin':dataset_util.float_list_feature(xmins),
        'image/object/bbox/xmax':dataset_util.float_list_feature(xmaxs),
        'image/object/bbox/ymin':dataset_util.float_list_feature(ymins),
        'image/object/bbox/ymax':dataset_util.float_list_feature(ymaxs),
        'image/object/class/label':dataset_util.int64_list_feature(img_data['labels']),
        'image/object/class/text':dataset_util.bytes_list_feature(img_data['text']),
        'image/encoded':dataset_util.bytes_feature(img_data['data']),
        'image/format':dataset_util.bytes_feature('jpeg'.encode('utf-8')) 

    }))

    return example

```
Another interesting aspect is that `ConcatOp` fails with size mismatch of different sizes each time (size[0] and size[1]) are different each time. I am at a loss to where to localize this error to even begin debugging. So any help will be appreciated. ",2,,[],2018-04-04 17:50:28,open,,,[],2018-04-11 16:48:35
1079,tensorflow/models,models,3875,zccoder,"How to run it with older version of tf, Cuda and cudnn?","I have tried to use docker to run such high version of TF, CUDA and cudnn, but I failed. It occurred below:

```
2018-04-04 15:36:02.320888: E tensorflow/stream_executor/cuda/cuda_dnn.cc:403] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2018-04-04 15:36:02.321177: E tensorflow/stream_executor/cuda/cuda_dnn.cc:411] possibly insufficient driver version: 384.90.0
2018-04-04 15:36:02.321264: F tensorflow/core/kernels/conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) 
Aborted (core dumped)
```

I have tried many methods trying to solve it. It seems that  I could not degrade cudnn in my own coumputer to make this program run in the docker.

So how can I run this program without upgrading my version?",0,"NamedUser(login=""nealwu"")","[NamedUser(login=""nealwu"")]",2018-04-04 15:51:37,open,,,[],2018-04-05 00:14:11
1080,tensorflow/models,models,3873,ZER-0-NE,"TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [-1, None]. Consider casting elements to a supported type.","python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"":
b'unknown' 1.5.0

System information
**Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
**OS Platform and Distribution (e.g., Linux Ubuntu 16.04**): Windows 7 x64
**Python version**: 3.5.2 
**Exact command to reproduce**: python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_nas_coco.config

### Describe the problem
I am trying to train my own dataset using faster_rcnn_nas_coco model. I get this unknown error. Other issues which I think were similar could not help: [#11974](https://github.com/tensorflow/tensorflow/issues/11974)

### Source code / logs
**Here is the traceback of the error I get:**

C:\Users\User\Desktop\Abhishek_Singh\models-master\research>python train.py --lo
gtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_nas_
coco.config
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From C:\Users\User\Desktop\Abhishek_Singh\models-master\resea
rch\object_detection\trainer.py:228: create_global_step (from tensorflow.contrib
.framework.python.ops.variables) is deprecated and will be removed in a future v
ersion.
Instructions for updating:
Please switch to tf.train.create_global_step
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
2018-04-04 18:41:53.436438: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
5\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: AVX AVX2
2018-04-04 18:41:53.904439: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
5\tensorflow\core\common_runtime\gpu\gpu_device.cc:1105] Found device 0 with pro
perties:
name: Quadro K620 major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:02:00.0
totalMemory: 2.00GiB freeMemory: 1.80GiB
2018-04-04 18:41:53.920039: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
5\tensorflow\core\common_runtime\gpu\gpu_device.cc:1195] Creating TensorFlow dev
ice (/device:GPU:0) -> (device: 0, name: Quadro K620, pci bus id: 0000:02:00.0,
compute capability: 5.0)
INFO:tensorflow:A GPU is available on the machine, consider using NCHW data form
at for increased speed on GPU.
INFO:tensorflow:depth of additional conv before box predictor: 0
WARNING:tensorflow:From C:\Users\User\Desktop\Abhishek_Singh\models-master\resea
rch\object_detection\core\box_predictor.py:396: calling reduce_mean (from tensor
flow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a
future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From C:\Users\User\Desktop\Abhishek_Singh\models-master\resea
rch\object_detection\core\losses.py:316: softmax_cross_entropy_with_logits (from
 tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future ver
sion.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See tf.nn.softmax_cross_entropy_with_logits_v2.

Traceback (most recent call last):
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\framework\tensor_util.py"", line 498, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\framework\tensor_util.py"", line 498, in <listcomp>
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\util\compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got -1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 167, in <module>
    tf.app.run()
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\platform\app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""train.py"", line 163, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""C:\Users\User\Desktop\Abhishek_Singh\models-master\research\object_detec
tion\trainer.py"", line 246, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""C:\Users\User\Desktop\Abhishek_Singh\models-master\research\slim\deploym
ent\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""C:\Users\User\Desktop\Abhishek_Singh\models-master\research\object_detec
tion\trainer.py"", line 181, in _create_losses
    losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
  File ""C:\Users\User\Desktop\Abhishek_Singh\models-master\research\object_detec
tion\meta_architectures\faster_rcnn_meta_arch.py"", line 1580, in loss
    groundtruth_masks_list,
  File ""C:\Users\User\Desktop\Abhishek_Singh\models-master\research\object_detec
tion\meta_architectures\faster_rcnn_meta_arch.py"", line 1773, in _loss_box_class
ifier
    weights=batch_cls_weights),
  File ""C:\Users\User\Desktop\Abhishek_Singh\models-master\research\object_detec
tion\core\losses.py"", line 73, in __call__
    return self._compute_loss(prediction_tensor, target_tensor, **params)
  File ""C:\Users\User\Desktop\Abhishek_Singh\models-master\research\object_detec
tion\core\losses.py"", line 315, in _compute_loss
    labels=tf.reshape(target_tensor, [-1, num_classes]),
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\ops\gen_array_ops.py"", line 5184, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\framework\op_def_library.py"", line 513, in _apply_op_helper
    raise err
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\framework\op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\framework\ops.py"", line 1022, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\framework\constant_op.py"", line 233, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\framework\constant_op.py"", line 212, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""C:\Program Files (x86)\Python\Python35\lib\site-packages\tensorflow\pyth
on\framework\tensor_util.py"", line 502, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents:
[-1, None]. Consider casting elements to a supported type.

**Here is my config file:**
model {
faster_rcnn {
num_classes: 12
image_resizer {
fixed_shape_resizer {
height: 300
width: 300
}
}
feature_extractor {
type: 'faster_rcnn_nas'
}
first_stage_anchor_generator {
grid_anchor_generator {
scales: [0.25, 0.5, 1.0, 2.0]
aspect_ratios: [0.5, 1.0, 2.0]
height_stride: 16
width_stride: 16
}
}
first_stage_box_predictor_conv_hyperparams {
op: CONV
regularizer {
l2_regularizer {
weight: 0.0
}
}
initializer {
truncated_normal_initializer {
stddev: 0.01
}
}
}
first_stage_nms_score_threshold: 0.0
first_stage_nms_iou_threshold: 0.7
first_stage_max_proposals: 300
first_stage_localization_loss_weight: 2.0
first_stage_objectness_loss_weight: 1.0
initial_crop_size: 17
maxpool_kernel_size: 1
maxpool_stride: 1
second_stage_box_predictor {
mask_rcnn_box_predictor {
use_dropout: false
dropout_keep_probability: 1.0
fc_hyperparams {
op: FC
regularizer {
l2_regularizer {
weight: 0.0
}
}
initializer {
variance_scaling_initializer {
factor: 1.0
uniform: true
mode: FAN_AVG
}
}
}
}
}
second_stage_post_processing {
batch_non_max_suppression {
score_threshold: 0.0
iou_threshold: 0.6
max_detections_per_class: 100
max_total_detections: 100
}
score_converter: SOFTMAX
}
second_stage_localization_loss_weight: 2.0
second_stage_classification_loss_weight: 1.0
second_stage_batch_size: 1
}
}

train_config: {
batch_size: 1
optimizer {
momentum_optimizer: {
learning_rate: {
manual_step_learning_rate {
initial_learning_rate: 0.0003
schedule {
step: 0
learning_rate: .0003
}
schedule {
step: 900000
learning_rate: .00003
}
schedule {
step: 1200000
learning_rate: .000003
}
}
}
momentum_optimizer_value: 0.9
}
use_moving_average: false
}
gradient_clipping_by_norm: 10.0
fine_tune_checkpoint: faster_rcnn_nas_coco_2018_01_28/model.ckpt""
from_detection_checkpoint: true
num_steps: 200000
data_augmentation_options {
random_horizontal_flip {
}
}
}

train_input_reader: {
tf_record_input_reader {
input_path: ""data/train.record""
}
label_map_path: ""data/CFPS.pbtxt""
}

eval_config: {
metrics_set: ""pascal_voc_metrics""
num_examples: 8000
max_evals: 10}

eval_input_reader: {
tf_record_input_reader {
input_path: ""data/test.record""
}
label_map_path: ""training/CFPS.pbtxt""
shuffle: false
num_readers: 1
num_epochs: 1
}
",6,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""k-w-w"")]",2018-04-04 13:26:22,open,,,[],2018-07-20 18:25:57
1081,tensorflow/models,models,3866,oneTimePad,MobileNetV2 SSD Inference,"I trained SSD with the MobileNetV2 and achieved a loss of around .8. However, when I run inference on it, all of the scores it outputs are very low. All are less .006. This is even true when it's ran on images from the training set. 

Has anyone else experienced this issue?",4,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-04 04:09:32,open,,,[],2018-06-19 15:26:42
1082,tensorflow/models,models,3856,radu-diaconescu13,Faster RCNN resnet50 inference time has 20%-30% variance,"### System information
- **What is the top-level directory of the model you are using**:
models/research/object_detection/models
- **Exact command to reproduce**
N/A just used train.py script
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
    No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    Linux Ubuntu 16.04.3 LTS (Xenial Xerus)
- **TensorFlow installed from (source or binary)**:
  Binary - pip install tensorflow-gpu
- **TensorFlow version (use command below)**:
 v1.7.0-3-g024aecf414 1.7.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA release 9.0, V9.0.176
cuDNN V7.0.5
- **GPU model and memory**:
NVIDIA GTX 1080 Ti 11 GB

### Describe the problem
I have trained a Faster RCNN with resnet 50 as the feature extractor using the tensorflow object detection API. I trained it with the train.py script included in API. Afterwards I exported the graph of the model using export_inference_graph.py script also included in the API.
Using this frozen graph I tried to find out the inference time of the model as an average of multiple runs like in the code below.
The problem is when running the graph on the gpu I get a very high variance,i.e. average time is 242 ms  and the standard deviation is 55ms (23%).
I  have included below a plot of inference time vs iteration step.
![faster_resnet50_time_series](https://user-images.githubusercontent.com/32652089/38255121-a8625746-3763-11e8-9098-be9d35235b68.png)

I also have tried to run the model exclusively on the CPU and while the inference time is much bigger,it is also much stabler. I also have included a plot of this.
![resnet50_time_series_cpu](https://user-images.githubusercontent.com/32652089/38254818-dc267ea0-3762-11e8-86a4-5e333bad11d0.png)

I also tried running it on another PC,also with NVIDIA GTX 1080 Ti 11 GB , CUDA 9,cuDNN 7 and Ubuntu 16.04 and this is the plot.
![inference_time_series](https://user-images.githubusercontent.com/32652089/38255255-02b283f6-3764-11e8-8296-9c6df8c1be24.png)

Any help would be appreciated.
Thank you

### Source code / logs
`
def load_inference_graph(inference_graph_path):
    od_graph = tf.Graph()
    with od_graph.as_default():
        od_graph_def = tf.GraphDef()
        with tf.gfile.GFile(inference_graph_path, mode='rb') as fid:
            serialized_graph = fid.read()
            od_graph_def.ParseFromString(serialized_graph)
            tf.import_graph_def(od_graph_def, name='')
    return od_graph


def placeholder(vis=False):
    inference_graph = load_inference_graph(inference_graph_path)
    config = tf.ConfigProto()
    times = []

    with tf.Session(config=config, graph=inference_graph) as sess:
        sess.run(tf.initialize_all_variables())

        image_tensor = inference_graph.get_tensor_by_name('image_tensor:0')
        boxes = inference_graph.get_tensor_by_name('detection_boxes:0')
        scores = inference_graph.get_tensor_by_name('detection_scores:0')
        classes = inference_graph.get_tensor_by_name('detection_classes:0')
        num_detections = inference_graph.get_tensor_by_name('num_detections:0')
 
        iterations = 10000

        input_image = np.ones((1, 720, 1280, 3)).astype(np.uint8)
                for index in range((iterations)):

                     start = time.time()
                     sess.run([boxes, scores, classes, num_detections],feed_dict={image_tensor: input_image})
                     stop = time.time()

                    # tensorflow needs some time to warm up
                    if index > 10:
                         times.append(stop - start)
                         print(""index %d: %f"" % (index, stop - start))

                   avg_time = np.sum(times) / len(times)
                   print('Standard deviation is %f ms' % np.std(times, ddof=1))
                   print('Median value is %f ms' % np.median(times))
                   print('AVG TIME IS %f ms' % avg_time)
`

",1,,[],2018-04-03 14:30:54,open,,"NamedUser(login=""hgadig"")",['stat:awaiting response'],2018-12-05 21:42:47
1083,tensorflow/models,models,3848,jwc1,Update README.md,"Hi, I rewrote some parts of the description to make it more clear and concise. Hope this helps.",2,,[],2018-04-03 04:37:31,open,,,['cla: no'],2018-04-03 05:16:14
1084,tensorflow/models,models,3842,weichencoder,cannot find <path_to_tensorflow>/models/research/ path,"I was installing something with the  ""models/research/object_detection/g3doc/installation.md"" guide. but I can't find <path_to_tensorflow>/models/research/ path.

and what's the relationship between ""tensorflow/models""and ""tensorflow/tensorflow""? how should I install model? Can I install ""model"" with pip? Or I should copy the ""model"" files to the ""tensorflow"" folder?

and 
- \# From tensorflow/models/research/
- protoc object_detection/protos/*.proto --python_out=.

shows ""-bash: protoc: command not found"".",1,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2018-04-02 17:06:37,open,,,[],2018-06-18 03:22:49
1085,tensorflow/models,models,3837,mxmxlwlw,object detection fail when I train mask_rcnn_resnet_101_pets,"Here's the error info:
```
InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [1]
	 [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Loss/BoxClassifierLoss/assert_equal_2/All/_153, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_155, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/packed/_145)]]
	 [[Node: FirstStageFeatureExtractor/resnet_v1_101/block3/unit_16/bottleneck_v1/conv2/BatchNorm/gamma/read/_919 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_2186_...gamma/read"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
```

run with
```
python object_detection/train.py \
    --logtostderr \
    --train_dir=experiment/mask_rcnn_resnet101_pets/train \
    --pipeline_config_path=experiment/mask_rcnn_resnet101_pets/mask_rcnn_resnet101_pets.config
```
pipeline script is 
```
# Mask R-CNN with Resnet-101 (v1) configured for the Oxford-IIIT Pet Dataset.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for ""PATH_TO_BE_CONFIGURED"" to find the fields that
# should be configured.

model {
  faster_rcnn {
    num_classes: 37
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
      }
    }
    number_of_stages: 3
    feature_extractor {
      type: 'faster_rcnn_resnet101'
      first_stage_features_stride: 16
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 16
        width_stride: 16
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        predict_instance_masks: true
        conv_hyperparams {
          op: CONV
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.01
            }
          }
        }
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 300
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 1
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.0007
          schedule {
            step: 15000
            learning_rate: 0.00007
          }
          schedule {
            step: 30000
            learning_rate: 0.000007
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""experiment/mask_rcnn_resnet101_pets/pretrain/model.ckpt""
  from_detection_checkpoint: true
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""experiment/mask_rcnn_resnet101_pets/data/pet_train.record""
  }
  label_map_path: ""experiment/data/pet_label_map.pbtxt""
  load_instance_masks: true
}

eval_config: {
  num_examples: 2000
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""experiment/mask_rcnn_resnet101_pets/data/pet_val.record""
  }
  label_map_path: ""experiment/data/pet_label_map.pbtxt""
  load_instance_masks: true
  shuffle: false
  num_readers: 1
}
```
pretrain model is faster_rcnn_resnet101_coco_11_06_2017

",9,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""jch1"")]",2018-04-02 11:14:33,open,,,[],2018-07-27 09:57:39
1086,tensorflow/models,models,3827,flavea,[deeplab] ValueError: No data files found in val-* during eval and vis,"### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install --ignore-installed --upgrade tensorflow 
- **TensorFlow version (use command below)**: 1.7.0
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: - 
- **Exact command to reproduce**:

Hello, I am currently trying to train pascal voc2012 dataset using the deeplab codes provided here, I have no problem in training the data using train.py but when I tried to do eval and vis, I run into error: ValueError: No data files found in val-*

I am running the codes from D:\ProgramData\Anaconda3\Lib\site-packages\deeplab
I have tried changing the directory where I executed the codes, but I haven't succeeded doing any eval or vis. I am using Python 3.6

the training flags:

`python train.py \ --logtostderr \ --training_number_of_steps=10 \ --train_split=""train"" \ --model_variant=""xception_65"" \ --atrous_rates=6 \ --atrous_rates=12 \ --atrous_rates=18 \ --output_stride=16 \ --decoder_output_stride=4 \ --train_crop_size=513 \ --train_crop_size=513 \ --train_batch_size=1 \ --dataset=""pascal_voc_seg"" \ --tf_initial_checkpoint=""datasets/pascal_voc_seg/init_models/deeplabv3_pascal_train_aug/model.ckpt"" \ --train_logdir=""datasets/pascal_voc_seg/exp/train_on_trainval_set/train/"" \ --dataset_dir=""datasets/pascal_voc_seg/tfrecord/""`

the eval flags:

`python eval.py \ --logtostderr \ --eval_split=""val"" \ --model_variant=""xception_65"" \ --atrous_rates=6 \ --atrous_rates=12 \ --atrous_rates=18 \ --output_stride=16 \ --decoder_output_stride=4 \ --eval_crop_size=513 \ --eval_crop_size=513 \ --dataset=""pascal_voc_seg"" \  --checkpoint_dir= ""datasets/pascal_voc_seg/exp/train_on_trainval_set/train"" \ --eval_logdir= ""datasets/pascal_voc_seg/exp/train_on_trainval_set/eval"" \ --dataset_dir= ""datasets/pascal_voc_seg/tfrecord""`

stack trace:

`(base) D:\ProgramData\Anaconda3\Lib\site-packages\deeplab>python eval.py \ --logtostderr \ --eval_split=""val"" \ --model_variant=""xception_65"" \ --atrous_rates=6 \ --atrous_rates=12 \ --atrous_rates=18 \ --output_stride=16 \ --decoder_output_stride=4 \ --eval_crop_size=513 \ --eval_crop_size=513 \ --dataset=""pascal_voc_seg"" \  --checkpoint_dir= ""datasets/pascal_voc_seg/exp/train_on_trainval_set/train"" \ --eval_logdir= ""datasets/pascal_voc_seg/exp/train_on_trainval_set/eval"" \ --dataset_dir= ""datasets/pascal_voc_seg/tfrecord""
D:\ProgramData\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
WARNING:tensorflow:From D:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
INFO:tensorflow:Evaluating on val set
Traceback (most recent call last):
  File ""eval.py"", line 175, in <module>
    tf.app.run()
  File ""D:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""eval.py"", line 103, in main
    model_variant=FLAGS.model_variant)
  File ""D:\ProgramData\Anaconda3\lib\site-packages\deeplab\utils\input_generator.py"", line 121, in get
    shuffle=is_training)
  File ""D:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\slim\python\slim\data\dataset_data_provider.py"", line 96, in __init__
    scope=scope)
  File ""D:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\slim\python\slim\data\parallel_reader.py"", line 238, in parallel_read
    data_files = get_data_files(data_sources)
  File ""D:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\slim\python\slim\data\parallel_reader.py"", line 311, in get_data_files
    raise ValueError('No data files found in %s' % (data_sources,))
ValueError: No data files found in val-*`

perhaps anyone can give me a clue on what I might did wrong? Thank you in advance.",6,,[],2018-03-31 18:03:41,open,,"NamedUser(login=""flavea"")",[],2019-02-15 17:38:34
1087,tensorflow/models,models,3826,marksandler2,Adds license text,,0,,[],2018-03-31 17:17:55,open,,,['cla: yes'],2018-03-31 17:17:58
1088,tensorflow/models,models,3823,deepearthgo,unable to read mobilenet_v2 checkpoint(Top1-Imagenet 74.9),"What is the version of TF for read the mobilenet_v2 checkpoint 

My version is TF 1.4.1

However, Unable to read the mobilenet_v2 checkpoint
",16,,[],2018-03-31 06:22:38,open,,,[],2019-04-09 09:09:08
1089,tensorflow/models,models,3805,eewindfly,Remove not used flags in training script of object detection API,"It took me some time to figure out environment variable of
TF_CONFIG is used for distributed training instead of Flags variables.
This PR removes these unused flags, so that it would not make someone confused again.",5,,[],2018-03-29 15:31:12,open,,,['cla: yes'],2018-05-01 23:12:16
1090,tensorflow/models,models,3800,surfound,model/resrarch/astronet     Typeerror:buffer is too small for requested array,"when I excute this command 
bazel-bin/astronet/data/generate_input_records --input_tce_csv_file=${TCE_CSV_FILE} --kepler_data_dir=${KEPLER_DATA_DIR} --output_dir=${TFRECORD_DIR} --num_worker_processes=5

It normal work for a period of time.
INFO:tensorflow:PoolWorker-3: Wrote 1573 items in shard train-00002-of-00008
INFO:tensorflow:PoolWorker-2: Processed 1430/1573 items in shard train-00005-of-00008
INFO:tensorflow:PoolWorker-5: Processed 380/1574 items in shard test-00000-of-00001
INFO:tensorflow:PoolWorker-1: Processed 1540/1573 items in shard train-00000-of-00008
INFO:tensorflow:PoolWorker-4: Processed 1570/1574 items in shard train-00003-of-00008
INFO:tensorflow:PoolWorker-2: Processed 1440/1573 items in shard train-00005-of-00008
INFO:tensorflow:PoolWorker-5: Processed 390/1574 items in shard test-00000-of-00001
INFO:tensorflow:PoolWorker-1: Processed 1550/1573 items in shard train-00000-of-00008
INFO:tensorflow:PoolWorker-4: Wrote 1574 items in shard train-00003-of-00008
INFO:tensorflow:PoolWorker-2: Processed 1450/1573 items in shard train-00005-of-00008
INFO:tensorflow:PoolWorker-1: Processed 1560/1573 items in shard train-00000-of-00008
INFO:tensorflow:PoolWorker-5: Processed 400/1574 items in shard test-00000-of-00001
INFO:tensorflow:PoolWorker-2: Processed 1460/1573 items in shard train-00005-of-00008
INFO:tensorflow:PoolWorker-5: Processed 410/1574 items in shard test-00000-of-00001
INFO:tensorflow:PoolWorker-1: Processed 1570/1573 items in shard train-00000-of-00008
INFO:tensorflow:PoolWorker-2: Processed 1470/1573 items in shard train-00005-of-00008
INFO:tensorflow:PoolWorker-1: Wrote 1573 items in shard train-00000-of-00008

Then a error occurred.
Traceback (most recent call last):
  File ""/home/stuf/astronet/bazel-bin/astronet/data/generate_input_records.runfiles/__main__/astronet/data/generate_input_records.py"", line 301, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""/home/stuf/astronet/bazel-bin/astronet/data/generate_input_records.runfiles/__main__/astronet/data/generate_input_records.py"", line 293, in main
    async_result.get()
  File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get
    raise self._value
TypeError: buffer is too small for requested array

The previous steps are all normal.Only this...",8,,[],2018-03-29 01:45:56,open,,,[],2018-10-14 01:46:36
1091,tensorflow/models,models,3799,nobodykid,CIFAR DCGAN training stuck at 10th iteration output,"### System information
- **What is the top-level directory of the model you are using**: The models repo is located at my notebook dir, `/home/<username>/notebooks/models
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: installed from Anaconda
- **TensorFlow version (use command below)**:1.4.0
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: CUDA 8.0
- **GPU model and memory**: Tesla K80
- **Exact command to reproduce**: `./launch_job.sh ""unconditional"" ""/home/<username>/notebooks/models""` on directory `/models/research/gan/cifar`

### Describe the problem
I tried running the CIFAR examples by executing the shell script in the `gan/cifar` dir, but every time it get stuck at `Starting train step : 10` (sometimes 11 or 12). I suspect this might be similar to #3366, but I haven't look up into the process manager

### Source code / logs
![dcgan tfgan](https://user-images.githubusercontent.com/12664445/38063299-8fd6fbe0-3322-11e8-9f82-54b3108dfb4c.png)
",0,,[],2018-03-29 00:28:38,open,,,[],2018-03-29 00:28:38
1092,tensorflow/models,models,3792,bhack,[deeplab] F-boundary metrics,"What you think to add [F-boundary](https://github.com/fperazzi/davis/blob/master/python/lib/davis/measures/f_boundary.py) metric?
Other then adding here in the segmentation API or deeplab probably it is better to add it on [TF](https://www.tensorflow.org/api_docs/python/tf/metrics).

What do you think? Do you have a better metric related to the boundaries?

/cc @YknZhu



",1,,[],2018-03-28 12:48:56,open,,,[],2018-03-28 12:50:42
1093,tensorflow/models,models,3778,Sharma96,How to increase the fps rate in the object_detection_tutorial.ipynb file?,"The object_detection_tutorial.ipynb file runs but the fps(frame per second) is slow,is there any change with which the fps rate becomes moderate?
Thank you for your time.",9,,[],2018-03-27 19:30:33,open,,,[],2019-03-07 12:20:49
1094,tensorflow/models,models,3769,toyow,Deeplab：a question about the “global average pooling” operation,"![clipboard](https://user-images.githubusercontent.com/37426295/37971537-c3d52c20-3208-11e8-85a3-ce29c2c82e77.png)

After reading the paper “Rethinking Atrous Convolution for Semantic Image Segmentation”,I'm confused by the “global average pooling” operation within the ASPP。

The original text is described as follows：
“Specifically, we apply global average pooling on the last feature map of the model, feed the resulting image-level features to a 1 × 1 convolution with 256 filters (and batch normalization [38]), and then bilinearly upsample the feature to the desired spatial dimension.”

I guess the ”global average pooling” means  turn the multilayered feature map(256 layers?) into one-dimensional vector(i.e image-level features ),the feature points  of this vector are average of the feature map.
After feeding the resulting image-level features to a 1 × 1 convolution with 256 filters,do we still get a new one-dimensional vector?And how can we bilinearly upsample a one-dimensional vector to a two-dimensional feature map?

I hope to get the answer, thank you!



",4,,[],2018-03-27 14:18:56,open,,,"['models: research', 'stat:awaiting response']",2019-02-01 22:24:06
1095,tensorflow/models,models,3757,Jana-Wessels,tensorflow.python.framework.errors_impl.UnavailableError: Stream removed ERROR,"Please go to Stack Overflow for help and support:


### System information
- **What is the top-level directory of the model you are using**:
/models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.5
- **GPU model and memory**:
CPU
- **Exact command to reproduce**:
/home/jana/google-cloud-sdk/bin/gcloud ml-engine jobs submit training `whoami`_object_detection_`date +%s` \
    --runtime-version 1.5\
    --job-dir=gs://pets_jana/train \
    --packages /home/jana/models/research/dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz \
    --module-name object_detection.train \
    --region us-central1 \
    --config /home/jana/models/research/object_detection/samples/cloud/cloud.yml \
    -- \
    --train_dir=gs://pets_jana/train \
    --pipeline_config_path=gs://pets_jana/models/model/faster_rcnn_resnet101_pets_online.config



### Describe the problem
When submitting training job it gets to step 434 and the throws the error below (tensorflow.python.framework.errors_impl.UnavailableError: Stream removed) then continues to step 697 and fails. I don't even know where to start with this error. At the moment I am sitting with over 20 failed training runs with error fixing trying to get the pet data classification tutorial to work and I don't know where next. If anyone has a clue please help.

### Source code / logs
The replica worker 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): [...] File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__ c_api.TF_GetCode(self.status.status)) tensorflow.python.framework.errors_impl.UnavailableError: Stream removed During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main ""__main__"", mod_spec) File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code exec(code, run_globals) File ""/root/.local/lib/python3.5/site-packages/object_detection/train.py"", line 167, in <module> tf.app.run() File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 124, in run _sys.exit(main(argv)) File ""/root/.local/lib/python3.5/site-packages/object_detection/train.py"", line 163, in main worker_job_name, is_chief, FLAGS.train_dir) File ""/root/.local/lib/python3.5/site-packages/object_detection/trainer.py"", line 362, in train saver=saver) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 767, in train sess, train_op, global_step, train_step_kwargs) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_step run_metadata=run_metadata) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 895, in run run_metadata_ptr) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1128, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1344, in _do_run options, run_metadata) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1363, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.UnavailableError: Stream removed The replica worker 1 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): [...] File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__ c_api.TF_GetCode(self.status.status)) tensorflow.python.framework.errors_impl.UnavailableError: Stream removed During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main ""__main__"", mod_spec) File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code exec(code, run_globals) File ""/root/.local/lib/python3.5/site-packages/object_detection/train.py"", line 167, in <module> tf.app.run() File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 124, in run _sys.exit(main(argv)) File ""/root/.local/lib/python3.5/site-packages/object_detection/train.py"", line 163, in main worker_job_name, is_chief, FLAGS.train_dir) File ""/root/.local/lib/python3.5/site-packages/object_detection/trainer.py"", line 362, in train saver=saver) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 767, in train sess, train_op, global_step, train_step_kwargs) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_step run_metadata=run_metadata) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 895, in run run_metadata_ptr) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1128, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1344, in _do_run options, run_metadata) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1363, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.UnavailableError: Stream removed To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=827587257014&resource=ml_job%2Fjob_id%2Fjana_object_detection_1522091457&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22jana_object_detection_1522091457%22
",14,,[],2018-03-26 19:44:46,open,,,[],2018-05-09 12:04:19
1096,tensorflow/models,models,3752,ashiqks,"ValueError: Tried to convert 't' to a tensor and failed. Error: Argument must be  a dense tensor: range(0, 3) - got shape [3], but wanted [].","### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: 
binary
- **TensorFlow version (use command below)**: 1.6.0
- **Bazel version (if compiling from source)**: No
- **CUDA/cuDNN version**: No
- **GPU model and memory**: No
- **Exact command to reproduce**: python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28.config

While running with the pre-trained object_detection every model with Anaconda python 3.6.4 below given error is shown with the new version Tensorflow 1.6.0

`Traceback (most recent call last):
  File ""train.py"", line 167, in <module>
    tf.app.run()
  File ""C:\Users\MAT\Anaconda3\lib\site-packages\tensorflow\python\platform\app.
py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 163, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""C:\Users\MAT\models-master\research\object_detection\trainer.py"", line 2
55, in train
    train_config.optimizer)
  File ""C:\Users\MAT\models-master\research\object_detection\builders\optimizer_
builder.py"", line 50, in build
    learning_rate = _create_learning_rate(config.learning_rate)
  File ""C:\Users\MAT\models-master\research\object_detection\builders\optimizer_
builder.py"", line 109, in _create_learning_rate
    learning_rate_sequence, config.warmup)
  File ""C:\Users\MAT\models-master\research\object_detection\utils\learning_sche
dules.py"", line 169, in manual_stepping
    [0] * num_boundaries))
  File ""C:\Users\MAT\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops
.py"", line 2619, in where
    return gen_math_ops._select(condition=condition, x=x, y=y, name=name)
  File ""C:\Users\MAT\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_math_
ops.py"", line 4503, in _select
    ""Select"", condition=condition, t=x, e=y, name=name)
  File ""C:\Users\MAT\Anaconda3\lib\site-packages\tensorflow\python\framework\op_
def_library.py"", line 528, in _apply_op_helper
    (input_name, err))
ValueError: Tried to convert 't' to a tensor and failed. Error: Argument must be
 a dense tensor: range(0, 3) - got shape [3], but wanted [].`",12,,[],2018-03-26 14:09:49,open,,,[],2018-05-09 08:25:04
1097,tensorflow/models,models,3746,zeynali,Train ssd_mobilenet_v2_coco,"Hi,
How we train the SSD_mobilenet_v2_coco ? i don't find it's respective check point. and why this is not possible for focal loss ?",3,,[],2018-03-26 07:11:33,open,,,[],2018-04-10 09:07:51
1098,tensorflow/models,models,3741,grayskripko,Update build_voc2012_data.py,"The original FastGFile(..., 'b') does not work for Python 3",0,,[],2018-03-25 12:54:02,open,,,['cla: yes'],2018-03-26 16:27:57
1099,tensorflow/models,models,3740,grayskripko,Update build_data.py,"I've got a lot of type errors here when was launching https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/pascal.md in this line
https://github.com/tensorflow/models/blob/master/research/deeplab/datasets/build_voc2012_data.py#L128",0,,[],2018-03-25 12:50:47,open,,,['cla: yes'],2018-03-26 20:33:33
1100,tensorflow/models,models,3739,kirk86,deeplab doesn't predict correctly the segmentation masks,"Everything seems to be working properly training/evaluation etc. except from the fact that deeplab doesn't predict the segmentation masks.

Example:
![001278_image](https://user-images.githubusercontent.com/2902390/37874610-2613500a-302a-11e8-9c78-e21cf9353231.png)

![001278_prediction](https://user-images.githubusercontent.com/2902390/37874614-321cc548-302a-11e8-83c5-2d902bbca865.png)

The original images in the dataset are either colored like the above one or black and white, but all the masks are black and white.
",43,,[],2018-03-25 11:44:52,open,,,[],2019-04-05 12:19:11
1101,tensorflow/models,models,3730,walkerlala,[deeplab] Training deeplab model with ADE20K dataset,"### System information
- **What is the top-level directory of the model you are using**: deeplab
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.6.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7.0.4
- **GPU model and memory**: 1080Ti * 2 , 10Gb * 2
- **Exact command to reproduce**: 

### Describe the problem
This is a feature request. I am trying to train the deeplab model with the [ADE20K dataset](http://groups.csail.mit.edu/vision/datasets/ADE20K/) (see [this presentation](http://presentations.cocodataset.org/Places17-GMRI.pdf)). I have finished the data format conversion and ""successfully"" train the model on a small subset of ADE20K. Below is the  modification to file `research/deeplab/datasets/segmentation_dataset.py` which is used to extract segmentation data.

```
diff --git a/research/deeplab/datasets/segmentation_dataset.py b/research/deeplab/datasets/segmentation_dataset.py
index a777252..8648fb2 100644
--- a/research/deeplab/datasets/segmentation_dataset.py
+++ b/research/deeplab/datasets/segmentation_dataset.py
@@ -85,10 +85,22 @@ _PASCAL_VOC_SEG_INFORMATION = DatasetDescriptor(
     ignore_label=255,
 )
 
+_ADE20K_INFORMATION = DatasetDescriptor(
+    splits_to_sizes = {
+        'train': 40,
+        'val': 5,
+    },
+    # TODO temporarily change it to 21 otherwise dimension mismatch
+    num_classes=21,
+    ignore_label=255,
+)
+
 
 _DATASETS_INFORMATION = {
     'cityscapes': _CITYSCAPES_INFORMATION,
     'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
+    'ade20k': _ADE20K_INFORMATION,
 }
 
 # Default file pattern of TFRecord of TensorFlow Example.
```

The problem is, in the ADE20K dataset there are 150 classes, which is different from that in the VOC or cityspace dataset. That brings problem w.r.t the checkpoint file. Currently there are only pretrained model on the VOC and cityspace dataset. So we have two choices here:

  1. Do not use the checkpoint file. In this case, there is an error:
```
absl.flags._exceptions.IllegalFlagValueError: flag --tf_initial_checkpoint=None: Flag --tf_initial_checkpoint must be specified.
```

  2. set num_classes=21 to use those two provided checkpoint files

Are there any alternatives to these?

If anyone have any workable solution for the ADE20K dataset it would be really appreciated.",69,,[],2018-03-24 12:51:41,open,,,[],2019-03-14 13:29:54
1102,tensorflow/models,models,3722,kirk86,some thoughts making code more readable,"Hi and thank you for providing these models for easy access. I've been going through the code base and in IMHO I believe that there is a lot of unnecessary clutter that prevents us from fully understanding the codebase, especially when it comes to more complicated models than the average case. My two cents for whatever might that be worth it or not (depending on perspective) is the following. The use of `tf.slim` complicates things unnecessarily (and now with Eager mode, Experimenter, interactive sessions, whatever have you, all in one, even so much worse) prevents us from understanding the baseline `tf` code. Another thing that I find unnecessary is the use of `tf.flags`. I can't think of any reason in which that could not have been easily replaced by `Argparse` python library. Except if I'm missing sth here? If `tf.flags` doesn't provide any benefit over `Argparse` then what's the reason of its existence in the first place?

Have I written custom code N/A
OS Platform and Distribution N/A
TensorFlow installed from N/A
TensorFlow version N/A
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A",1,,[],2018-03-23 19:05:17,open,,,['stat:awaiting response'],2018-04-25 08:17:33
1103,tensorflow/models,models,3721,anishCFC,No module named entropy_coder,"
 File ""./core/entropy_coder_train.py"", line 26, in <module>
    from entropy_coder.all_models import all_models
ImportError: No module named 'entropy_coder'

If any one has trained model of entropy coder please post it on github


",1,,[],2018-03-23 18:16:11,open,,,[],2018-03-26 13:55:22
1104,tensorflow/models,models,3714,donghun9808,1080ti 4way GPU Parallel Setup Issue,"
### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10 Pro
- **TensorFlow version (use command below)**: tensorflow-gpu 1.6.0
- **CUDA/cuDNN version**: cuda_9.0.176_win10 // cudnn-9.0-windows10-x64-v7
- **GPU model and memory**: ZOTAC GeForce GTX 1080 Ti Blower (11Gb) * 4 // Samsung DDR4-2400 16Gb * 8 (128Gb)

- **CPU** : intel XEON E5-2620 v4 * 2



Images : 10Mb(3840*2748) data set : 1874 (training set : 1234 / test : 640)

`batch_size: 4` in `faster_rcnn_resnet101_coco.config` and `num_clones` in `train.py` is `4`. 
[https://github.com/tensorflow/models/blob/master/research/deeplab/train.py#L38](url)

There are four GPUs, and the compute_0 of all four gpus is at the highest rate of 18%.
Memory is also 128Gb, but only 30Gb is used.  The learning speed is 1.9 seconds to 2.5 seconds per step.

On other PCs, `batch_size: 1, num_clones:1`, a graphics card with 1060 6Gb gpu and a memory with 32Gb of PC will have compute_0 up to 65% and memory up to 30Gb. The learning speed is 0.5 to 0.6 seconds per step.


1080ti gpu 4way pc is much higher, but I do not know why learning is slow.

I also wonder why you use only 30Gb of memory.

Please let me know if it is wrong to parallelize the gpu.


```
# Faster R-CNN with Resnet-101 (v1) configuration for MSCOCO Dataset.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for ""PATH_TO_BE_CONFIGURED"" to find the fields that
# should be configured.

model {
  faster_rcnn {
    num_classes: 90
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
      }
    }
    feature_extractor {
      type: 'faster_rcnn_resnet101'
      first_stage_features_stride: 16
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 16
        width_stride: 16
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 300
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 4
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.0003
          schedule {
            step: 0
            learning_rate: .0003
          }
          schedule {
            step: 900000
            learning_rate: .00003
          }
          schedule {
            step: 1200000
            learning_rate: .000003
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""faster_rcnn_resnet101_coco_2017_11_08/model.ckpt""
  from_detection_checkpoint: true
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  num_steps: 300000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""data/train.record""
  }
  label_map_path: ""data/toda.pbtxt""
}

eval_config: {
  num_examples: 8000
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""data/test.record""
  }
  label_map_path: ""data/toda.pbtxt""
  shuffle: false
  num_readers: 1
  num_epochs: 1
}`
```",5,,[],2018-03-23 09:00:21,open,,,[],2018-06-21 08:20:26
1105,tensorflow/models,models,3706,eilifsolberg,fp16 support in the Object Detection API,"### Featuere request: fp16/mixed precision support for training
  - Is fp16/mixed precision support on the roadmap for training networks using the Object Detection API?
  - If not, do you see any issues that needs to be resolved? It seems like you would either need to have two sets of pretrained models, our some automatic conversions between them.

### System information
  - What is the top-level directory of the model you are using: N/A
  - Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
  - OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A
  - TensorFlow installed from (source or binary): binary
  - TensorFlow version (use command below): 1.5.0
  - Bazel version (if compiling from source): N/A
  - CUDA/cuDNN version: CUDA 9.1/cuDNN v7.1.3
  - GPU model and memory: Nvidia Titan V, 12GB
  - Exact command to reproduce: N/A
",2,,[],2018-03-22 20:35:32,open,,,['stat:awaiting response'],2018-06-25 19:24:18
1106,tensorflow/models,models,3697,shartoo,object detect api fasterrcnn OOM,"Here is my basic information

### system summary

+ windows: 10/16GB RAM
+ GPU :  GeForce GTX 1080 Ti
+ cuda : 8.0 
+ cudnn:  7.1
+ tensorflow-gpu:  1.5
+ python:  3.5

### data summary

+ number of samples: 10000 (training and validation percent are 70% and 30% respectively) 
+ size of train.records: 744MB
+ size of val.records:  334MB
+ image (jpg)
  + shape: $640\times 480$ (resized by opencv)
  + size: 35kb to 1275kb

### configuration of training

I'm using `faster_rcnn_inception_resnet_v2.config` copied from obeject detection sample config files. Here is the detail 

```
model {
  faster_rcnn {
    num_classes: 1
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 600
        max_dimension: 1024
      }
    }
    feature_extractor {
      type: 'faster_rcnn_inception_resnet_v2'
      first_stage_features_stride: 8
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 8
        width_stride: 8
      }
    }
    first_stage_atrous_rate: 2
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.6
    # modify from 300 to  600
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 17
    maxpool_kernel_size: 1
    maxpool_stride: 1
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.7
		iou_threshold: 0.3
        max_detections_per_class: 10 
        max_total_detections: 40
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 4
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.0003
          schedule {
            step: 0
            learning_rate: .0003
          }
          schedule {
            step: 900000
            learning_rate: .00003
          }
          schedule {
            step: 1200000
            learning_rate: .000003
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: """"
  from_detection_checkpoint: true
  num_steps: 100000
  data_augmentation_options {
    random_horizontal_flip {
    }
	}
}

train_input_reader: {
  tf_record_input_reader {
    input_path: """"
  }
  label_map_path: """"
}

eval_config: {
  num_examples: 1000
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: """"
  }
  label_map_path: """"
  shuffle: false
  num_readers: 1
}
```
as you can read ,the `batch_size=4`(decrease from 64 to 4 while made no difference)


### error log 

Here is the error log
```
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From D:\workspace\compet\ipcr3\object_detection\trainer.py:176: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From D:\workspace\compet\ipcr3\object_detection\builders\optimizer_builder.py:105: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
C:\Python35\lib\site-packages\tensorflow\python\ops\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
2018-03-22 15:03:47.552158: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2018-03-22 15:03:47.890510: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.00GiB freeMemory: 9.10GiB
2018-03-22 15:03:47.890825: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path D:/workspace/compet/ipcr3/data/ICPR3part1/tf_ckpt\model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
2018-03-22 15:05:22.078875: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 124.51MiB.  Current allocation summary follows.
2018-03-22 15:05:22.079202: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\bfc_allocator.cc:627] Bin (256): 	Total Chunks: 730, Chunks in use: 666. 182.5KiB allocated for chunks. 166.5KiB in use in bin. 36.1KiB client-requested in use in bin.
......
2018-03-22 15:05:23.036772: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\bfc_allocator.cc:683] Sum Total of in-use chunks: 8.52GiB
2018-03-22 15:05:23.036933: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\bfc_allocator.cc:685] Stats: 
Limit:                  9280555582
InUse:                  9147851008
MaxInUse:               9226589952
NumAllocs:                    5341
MaxAllocSize:           1278345216

2018-03-22 15:05:23.037782: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\bfc_allocator.cc:277] ****************************************************************************************************
2018-03-22 15:05:23.038097: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\framework\op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[16,38,50,384]
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.ResourceExhaustedError'>, OOM when allocating tensor with shape[4,75,100,1088]
	 [[Node: FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_8/add = Add[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_7/Relu, FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_8/mul)]]
	 [[Node: gradients/FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_16/Branch_0/Conv2d_1x1/BatchNorm/FusedBatchNorm_grad/tuple/control_dependency_2/_5985 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_21725_gradients/FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_16/Branch_0/Conv2d_1x1/BatchNorm/FusedBatchNorm_grad/tuple/control_dependency_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/Repeat_1/block17_8/add', defined at:

```

### other information 

I can train on other dataset using the same configuration well,while failed on the dataset described above whatever parameters changes made. Can somebody pull me out?Thank you !

 





",8,,[],2018-03-22 08:01:12,open,,,[],2018-10-01 17:28:07
1107,tensorflow/models,models,3688,Bidski,LossTensor is inf or nan while training ssd_mobilenet_v1_coco model in my own dataset,"I am having issues similar to #1881 and #1907.

Using google object_detection api and the latest tensorflow master repo built with CUDA 9.1 on Linux Mint 18.2 (bases on Ubuntu Xenial).

**Have I written custom code:** No, but custom dataset
**OS Platform and Distribution:** Linux Mint 18.2 (based on Ubuntu 16.04)
**TensorFlow installed from:** Tensorflow built and installed from github master
**TensorFlow version:** 1.8.0-rc0-cp35-cp35m-linux_x86_64
**Bazel version:** 0.12.0
**CUDA/cuDNN version:** CUDA 9.1, cuDNN 7.1
**GPU model and memory:** GTX1080Ti 11GB
**Exact command to reproduce:** cd tensorflow/models/research && python3 object_detection/train.py --logtostderr --pipeline_config_path=/path/to/pipeline_config.pbtxt --train_dir=/path/to/train/folder

### Describe the problem
I am trying to fine-tune the ssd mobilenet v1 coco model using my own dataset. I am using the default config file that is provided in the object detection repository. 

From the very first global step of training I receive the ""LossTensor is inf or nan. : Tensor had NaN values"" error. Things I have tried:
- Ensuring that all bounding box coordinates are inside of the image boundary. 
- Removing all bounding boxes that are smaller than 20 pixels in either width or height.
- Removing all bounding boxes that are smaller than 15% of the image width of height.
- Increasing batch size
- Decreasing learning rate
- Removing data augmentation from the config file

From what I can tell, these are all of the things that were suggested in #1881 and #1907, none of these have worked for me.

### Source code / logs
```
INFO:tensorflow:Restoring parameters from /media/bidski/Portable/imagetagger/tf_objapi/models/ssd_mobilenet_v1_coco_2017_11_17/model.ckpt
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path /media/bidski/Portable/imagetagger/tf_objapi/models/ssd_mobilenet/train/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:global step 1: loss = 31.4505 (12.442 sec/step)
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](AddN/_4851)]]

Caused by op 'CheckNumerics', defined at:
  File ""object_detection/train.py"", line 167, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/train.py"", line 163, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/bidski/Projects/models/research/object_detection/trainer.py"", line 288, in train
    total_loss = tf.check_numerics(total_loss, 'LossTensor is inf or nan.')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 734, in check_numerics
    ""CheckNumerics"", tensor=tensor, message=message, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3303, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1669, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](AddN/_4851)]]

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1328, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1313, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1421, in _call_tf_sessionrun
    status, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](AddN/_4851)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/train.py"", line 167, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/train.py"", line 163, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/bidski/Projects/models/research/object_detection/trainer.py"", line 360, in train
    saver=saver)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 769, in train
    sess, train_op, global_step, train_step_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_step
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 906, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1141, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1322, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1341, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](AddN/_4851)]]

Caused by op 'CheckNumerics', defined at:
  File ""object_detection/train.py"", line 167, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""object_detection/train.py"", line 163, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/bidski/Projects/models/research/object_detection/trainer.py"", line 288, in train
    total_loss = tf.check_numerics(total_loss, 'LossTensor is inf or nan.')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 734, in check_numerics
    ""CheckNumerics"", tensor=tensor, message=message, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3303, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1669, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](AddN/_4851)]]
```
",9,,[],2018-03-22 00:53:58,open,,,[],2018-08-16 05:24:58
1108,tensorflow/models,models,3677,shipengai,[deeplab] How much loss to stop train,"I'm using 8 cards to train on cityscape in order to the reproduce the article result.
I don't know when to stop train on the segmentation.
I find that when train segment, the lr is usually smaller than classify.
Can anyone share me the train log on cityscape dataset?and  the super-parameters?",6,,[],2018-03-21 03:46:37,open,,,[],2018-05-18 19:45:57
1109,tensorflow/models,models,3664,bachma,Update losses_test.py,"testEasyExamplesProduceSmallLossComparedToSigmoidXEntropy failed because of rounding error in https://github.com/tensorflow/models/blob/875fcb3b417cc74852454472d660996401fb13f7/research/object_detection/core/losses_test.py#L249-L250

Furthermore, with this fix, the logit values are symmetric",1,,[],2018-03-20 13:20:33,open,,,['cla: no'],2018-03-26 16:37:18
1110,tensorflow/models,models,3648,LightRayGo, deeplab v3+ deeplab_demo.ipynb crash  in windows 10 python 3.6," 

------------------------

### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows 10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.6.0
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:1080



### Describe the problem
I have setup follow https://www.tensorflow.org/get_started/os_setup.html 
with windows and testing the installation https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/installation.md
`python deeplab/model_test.py`  is ok.

```
D:\DevelopEnv\models\research>python deeplab/model_test.py
e:\ProgramData\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-03-19 21:30:28.612390: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-03-19 21:30:29.020673: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1212] Found device 0 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.873
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.59GiB
2018-03-19 21:30:29.161326: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1212] Found device 1 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.873
pciBusID: 0000:02:00.0
totalMemory: 8.00GiB freeMemory: 6.59GiB
2018-03-19 21:30:29.173972: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1227] Device peer to peer matrix
2018-03-19 21:30:29.177940: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1233] DMA: 0 1
2018-03-19 21:30:29.185130: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1243] 0:   Y N
2018-03-19 21:30:29.188656: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1243] 1:   N Y
2018-03-19 21:30:29.194402: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0, 1
2018-03-19 21:30:30.179120: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2457 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-03-19 21:30:30.278496: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2457 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
e:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\tensor_util.py:560: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead
  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)
deeplab/model_test.py:81: DeprecationWarning: Please use assertEqual instead.
  self.assertEquals(len(scales_to_logits), expected_num_logits[i])
2018-03-19 21:30:33.124090: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0, 1
2018-03-19 21:30:33.132516: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2457 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-03-19 21:30:33.142920: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2457 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
e:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\util\tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  if d.decorator_argspec is not None), _inspect.getargspec(target))
.2018-03-19 21:30:38.066761: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0, 1
2018-03-19 21:30:38.071123: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2457 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-03-19 21:30:38.081243: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2457 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
....
----------------------------------------------------------------------
Ran 5 tests in 13.553s

OK
```

but when I run deeplab_demo.ipynb in Jupyter Notebook the python crash in
`    resized_im, seg_map = model.run(orignal_im)`
I have modify the code to support python3
`import StringIO  ` ---->
`import io`
All print change to print()
I just test "" Run on sample images"" not ""Run on internet images""
The model was download ""deeplab_model.tar.gz""
",5,,[],2018-03-19 13:36:57,open,,,[],2018-03-20 03:21:23
1111,tensorflow/models,models,3647,rameshjesswani,"Server terminated abruptly (error code: 14, error message: '', log file: '/home/ramesh/.cache/bazel/_bazel_dlr/d2c1d92543abd40408507a006043a91a/server/jvm.out')","
------------------------

### System information
- **What is the top-level directory of the model you are using**: syntaxnet
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Running TensorFlow script
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Virtual machine Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  ('v1.6.0-0-gd2e24b6039', '1.6.0')
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: Running on CPU
- **GPU model and memory**:
- **Exact command to reproduce**: ```bazel test ...```
- **Virtual Machine RAM**: 4 GB
### Describe the problem
I am trying to setup Google syntaxnet by following manual installation steps from this link: ```https://github.com/tensorflow/models/tree/master/research/syntaxnet```
I have installed all the required dependencies, after that I follow these commands

```
git clone --recursive https://github.com/tensorflow/models.git
  cd models/research/syntaxnet/tensorflow
  ./configure
  cd ..
  bazel test ...
```
When I execute ```bazel test...```, terminal shows:
```
INFO: Loading package: @org_tensorflow//third_party/fft2d

Server terminated abruptly (error code: 14, error message: '', log file: '/home/dlr/.cache/bazel/_bazel_dlr/d2c1d92543abd40408507a006043a91a/server/jvm.out')
```

 **TensorFlow configuration**
```
(tensor) dlr@ubuntu:~/github/models/research/syntaxnet/tensorflow$ ./configure 
You have bazel 0.5.4 installed.
Please specify the location of python. [Default is /home/dlr/anaconda2/bin/python]: 
Found possible Python library paths:
/home/dlr/anaconda2/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is /home/dlr/anaconda2/lib/python2.7/site-packages
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: Y
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]: N
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [y/N]: N
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: N
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: N
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: N
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL support? [y/N]: N
No OpenCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with MPI support? [y/N]: N
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Add ""--config=mkl"" to your bazel command to build with MKL support.
Please note that MKL on MacOS or windows is still not supported.
If you would like to use a local MKL instead of downloading, please set the environment variable ""TF_MKL_ROOT"" every time before build.
Configuration finished


```
",2,,[],2018-03-19 10:22:54,open,,,[],2018-07-31 11:18:19
1112,tensorflow/models,models,3638,toyow,"deeplab ：Running the train/eval/vis jobs , found a mistake","
https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/pascal.md

# Running the train/eval/vis jobs

python deeplab/train.py \
    --logtostderr \
    --training_number_of_steps=30000 \
    --train_split=""train"" \
    --model_variant=""xception_65"" \
    --atrous_rates=6 \
    --atrous_rates=12 \
    --atrous_rates=18 \
    --output_stride=16 \
    --decoder_output_stride=4 \
    --train_crop_size=513 \
    --train_crop_size=513 \
    --train_batch_size=1 \
    --dataset=""pascal_voc_seg"" \
    --tf_initial_checkpoints=${PATH_TO_INITIAL_CHECKPOINT} \
    --train_logdir=${PATH_TO_TRAIN_DIR} \
    --dataset_dir=${PATH_TO_DATASET}




--tf_initial_checkpoints,  need to remove ""s"".
There is no ""s"" in source code:

flags.DEFINE_string('tf_initial_checkpoint', None,
                    'The initial checkpoint in tensorflow format.')",1,"NamedUser(login=""aquariusjay"")","[NamedUser(login=""aquariusjay"")]",2018-03-18 02:30:23,open,,,[],2018-03-18 21:40:34
1113,tensorflow/models,models,3636,kritchie,Fixed typo in box_list_ops,,0,,[],2018-03-18 00:03:34,open,,,['cla: yes'],2018-03-18 00:03:36
1114,tensorflow/models,models,3635,brettkoonce,models/docs: minor spelling tweaks,,0,,[],2018-03-17 18:09:18,open,,,['cla: yes'],2019-01-02 18:38:12
1115,tensorflow/models,models,3625,scotthuang1989,train slim:  GPU utilization decrease when Increase GPU number,"
### System information
- **What is the top-level directory of the model you are using**: research/slim
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

 I found out that my GPU utilization is decreasing when GPU increase:

1. ~ 40% when I run with 4 GPU,
2.  ~50 when I run with 2 GPU
3. ~ 90% when I train it on 1 GPU.

I notice that GPU_0 consume almost double GPU memory compared to other GPU. what could be the problem? ",0,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-03-16 08:36:18,open,,,[],2018-09-25 17:15:07
1116,tensorflow/models,models,3618,marksandler2,PiperOrigin-RevId: 189040463,"PiperOrigin-RevId: 189059229

PiperOrigin-RevId: 189214402

Removes dependency on contextlib2 for mobilenet",0,,[],2018-03-15 19:26:48,open,,,['cla: yes'],2018-03-15 19:26:50
1117,tensorflow/models,models,3617,stevenpclark,Fix for box color assignment in visualization_utils.py,"In research/object_detection/utils/visualization_utils.py, boxes with identical locations are grouped together within visualize_boxes_and_labels_on_image_array(). However, since we are typically processing in order of decreasing score, the color of the boxes and labels are overwritten by lower-scoring classes within a group. As a result, the lowest-scoring class color is what ends up showing through (BAD). This commit fixes the issue.",0,,[],2018-03-15 17:25:26,open,,,['cla: yes'],2018-03-15 17:25:29
1118,tensorflow/models,models,3615,HutzelFutzel,PixelDA Bazel Server terminated abruptly,"
### System information
- **What is the top-level directory of the model you are using**: 
~/models/research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4.1
- **Bazel version (if compiling from source)**:
0.11.1
- **CUDA/cuDNN version**:
8.0 / 7.1
- **GPU model and memory**:
2x Geforce GTX 1060 (2x 6 GB), 32 GB RAM


### Describe the problem
I am running the PixelDA models from https://github.com/tensorflow/models/tree/master/research/domain_adaptation using the bazel run commands. But I can either train a model OR evaluate. Not both at the same time. Even if I do both processes via ssh with another computer (so I excluded GPU issues)

When I have a running training process, and then start the eval process, the training terminates with this error message
`Server terminated abruptly (error code: 14, error message: '', log file: '/home/.../.cache/bazel/_bazel_.../c96d77bc006e939d39d6eba4227a082c/server/jvm.out')`

How can I train and eval at the same time?


",1,,[],2018-03-15 12:30:38,open,,,['stat:awaiting response'],2018-04-25 06:56:54
1119,tensorflow/models,models,3610,ahmed-18,Tensorflow Object Detection Api Instance Segmentation takes up entire RAM (>32 GB),"### System information
- **What is the top-level directory of the model you are using**: mask_rcnn_resnet50_atrous_coco
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: Cuda 9.0 and CuDNN 7.0
- **GPU model and memory**: Gtx 1080 (8 GB)
- **Exact command to reproduce**: python object_detection/train.py --logtostderr --pipeline_config_path=/mask_rcnn_resent50_atrous_coco.config --train_dir=/train  


### Describe the problem
I'm trying to train instance segmentation model using Tensorflow Object Detection API (Mask RCNN) and have followed the instructions [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/instance_segmentation.md).

I'm using a pretrained mask_rcnn_resnet50_atrous_coco to initialize weights and adapted [this](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config) sample config file for the said model. I've created my tfrecord files with masks for training and evaluation sets as per [create_coco_tf_record.py](https://github.com/tensorflow/models/blob/master/research/object_detection/dataset_tools/create_coco_tf_record.py). I'm able to run the training script successfully but the problem is, apart from GPU memory, it takes up around 45GB of my RAM. Apart from this everything runs fine and I'm able to finish training upto 10k steps after which it decides it needs more RAM and takes up around 60GB which crashes my system. Same thing happens when I run the evaluation script after training.
I'm not sure why tensorflow needs so much RAM when I'm running the model on GPU. I have only **1 foreground class** and around **500 training samples** with up to **50** objects/masks per image. 

**Note:** I posted the issue on stackoverflow first and posting here because I didn't get any answer. [Here's](https://stackoverflow.com/questions/49261007/tensorflow-object-detection-api-instance-segmentation-takes-up-entire-ram-32-gb) the question I asked.

### Source code / logs
Here's my pipeline config file:
```
`# Mask R-CNN with Resnet-50 (v1), Atrous version
# Configured for MSCOCO Dataset.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for ""PATH_TO_BE_CONFIGURED"" to find the fields that
# should be configured.

model {
  faster_rcnn {
    num_classes: 1
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 300
        max_dimension: 400
      }
    }
    number_of_stages: 3
    feature_extractor {
      type: 'faster_rcnn_resnet50'
      first_stage_features_stride: 8
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 8
        width_stride: 8
      }
    }
    first_stage_atrous_rate: 2
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: true
        dropout_keep_probability: 0.5
        predict_instance_masks: true
        mask_height: 33
        mask_width: 33
        mask_prediction_conv_depth: 0
        mask_prediction_num_conv_layers: 4
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
        conv_hyperparams {
          op: CONV
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.01
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 300
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
    second_stage_mask_prediction_loss_weight: 4.0
    second_stage_batch_size: 4
  }
}

train_config: {
  batch_size: 1
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.0003
          schedule {
            step: 0
            learning_rate: .0003
          }
          schedule {
            step: 900000
            learning_rate: .00003
          }
          schedule {
            step: 1200000
            learning_rate: .000003
          }
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""/media/ahmed/1A6E52446E5218B9/Projects/TF/MaskRCNN/pretrained_models/mask_rcnn_resnet50_atrous_coco_2018_01_28/model.ckpt""
  from_detection_checkpoint: true
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  num_steps: 50000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""/media/ahmed/1A6E52446E5218B9/Projects/TF/MaskRCNN/train_mask.record""
  }
  label_map_path: ""/media/ahmed/1A6E52446E5218B9/Projects/TF/label_map.pbtxt""
  load_instance_masks: true
  mask_type: PNG_MASKS
}

eval_config: {
  num_examples: 200
  num_visualizations : 200
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""/media/ahmed/1A6E52446E5218B9/Projects/TF/MaskRCNN/val_mask.record""
  }
  label_map_path: ""/media/ahmed/1A6E52446E5218B9/Projects/TF/label_map.pbtxt""
  load_instance_masks: true
  mask_type: PNG_MASKS
  shuffle: false
  num_readers: 1
}`
![tf1](https://user-images.githubusercontent.com/19409195/37453939-12c38b9a-285b-11e8-9c96-a0ad81a4bc6d.png)
![tf3](https://user-images.githubusercontent.com/19409195/37453941-12f32eae-285b-11e8-9ef8-2105e957b250.png)


```",2,,[],2018-03-15 09:19:53,open,,,[],2018-07-12 11:31:41
1120,tensorflow/models,models,3605,jrabary,feature_map_generator in Object Detection API and Python 3 compatibility,"The function `fpn_top_down_feature_maps` is not working with python 3 and cause the following error 
```
line 225, in fpn_top_down_feature_maps
    reversed(zip(output_feature_map_keys, output_feature_maps_list)))
TypeError: 'zip' object is not reversible
```

The workaround is to change `zip(...)` into `list(zip(...))`",1,,[],2018-03-15 07:42:22,open,,,[],2018-03-15 09:11:16
1121,tensorflow/models,models,3600,Viehzeug,Export for Inception Resnet Atrous model,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no, only custom data
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04, macOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6.0 (tested back to 1.3.0)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: 1080 GTX Ti
- **Exact command to reproduce**: python object_detection/export_inference_graph.py --input_type image_tensor --pipeline_config_path faster_rcnn_inception_resnet_v2_atrous_coco.config --trained_checkpoint_prefix ""..."" --output_directory ../../

### Describe the problem
I trained a custom object detection mechanism based on the provided faster_rcnn_inception_resnet_v2_atrous_coco model from the zoo (I used the provided pipleine.conf, but custom data). Training and Eval work fine. When I try to extract the graph I run into the following exception:

`NotFoundError (see above for traceback): Tensor name ""Conv/biases"" not found in checkpoint files ../../train/model.ckpt-899.index
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
	 [[Node: save/RestoreV2/_299 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_306_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]`

I tried this with two OSes, different versions of TF (and corresponding checkout of the object detection library), but this is persistent in all including the newest version. I looked into the code and I assume the difference is in how the graph is set up in eval (which works) versus exporter (which doesn't), but I'm not an expert on the codebase so this is only a guess. (Either that or the value is never saved, which I don't think because training and evaluation work well.)

### Source code / logs
Attached you can find the full [traceback.txt](https://github.com/tensorflow/models/files/1813334/traceback.txt).",1,,[],2018-03-14 22:22:59,open,,,[],2018-05-06 15:44:44
1122,tensorflow/models,models,3590,WillLiGitHub,mobilenet batch norm effect single image's prediction,"I finetune mobilenet with retrained ckpt file, I feed batch images to model when training , the train result is good, but when i feed single image to trained model, the result is bad, single images' predict distribution similar to each other. I guess the moving mean and moving variable(batch norm parameters) change when i feed the model with single image!
",1,,[],2018-03-14 09:02:40,open,,,[],2018-04-10 09:47:34
1123,tensorflow/models,models,3589,riokt,slim_example_decoder object has no attribute LookupTensor,"I got this error I don't know how to fix it. I use tensorflow 1.4.1 because my cuda is version 8. Maybe you could give me other alternative to solve this (beside change tensorflow to the newest)

![image](https://user-images.githubusercontent.com/12220831/37391378-94a739ae-279d-11e8-9310-003d4f082f71.png)

",6,,[],2018-03-14 08:36:49,open,,,[],2018-03-19 12:50:27
1124,tensorflow/models,models,3588,scotthuang1989,How do I use multiple GPU on 1 server to train model slim,"### System information
- **What is the top-level directory of the model you are using**: research/slim
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no

I have 4 GPU on 1 server, I want use them all to train ` models/research/slim`.  My understanding is that. 
I need create 1 replica, 4 clones. So I use following commands.

> python train_image_classifier.py --train_dir=${TRAIN_DIR} --dataset_name=flowers --dataset_split_name=train --dataset_dir=${DATASET_DIR}  --model_name=inception_v3 --num_clones=4

But, the after I run this command, tensorflow allocate memory on all 4 GPUs, and only 1 GPU have significant usage(90%), other 3 are near 0.

I don't find any documentation on how to use this functionality. can someone help me?",10,,[],2018-03-14 02:07:16,open,,,[],2019-02-20 12:12:06
1125,tensorflow/models,models,3570,JoshuaPiinRueyPan,Build ImageNet TFRecord,"When building the TFRecord of ILSVRC, I get the following error:
```
file: research/inception/inception/data/build_imagenet_data.py
line: 175: six.binary_type TypeError: str() takes at most 1 argument
```
This probably due to the incompatible of python2 and python3 even if the author uses the six module.
In Python3: 
`class str(object='', encoding='utf-8', ...)
`
however, In Python2:
`class str(object='')`
i.e. It only take one argument.

I fix this bug by catching the exception and assign the string without encoding.",1,,[],2018-03-12 11:45:47,open,,,['cla: yes'],2018-03-12 18:57:25
1126,tensorflow/models,models,3566,cauldnz,Typo.,,1,,[],2018-03-11 09:47:39,open,,,['cla: no'],2018-03-11 09:47:43
1127,tensorflow/models,models,3563,StevenHickson,Allow user to specify allow_growth in training script.,Added allow_gpu_growth flag in training script to allow reducing gpu memory usage if requested,1,,[],2018-03-11 00:22:10,open,,,['cla: yes'],2018-05-18 00:59:48
1128,tensorflow/models,models,3562,sstolpovskiy,from_detection_checkpoint missed in faster_rcnn_inception_resnet_v2_atrous_oid.config,"Hi, 

It should be from_detection_checkpoint: true
inside https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid.config

Without it model can't load checkpoint.",6,,[],2018-03-10 20:50:02,open,,,[],2018-04-10 11:13:32
1129,tensorflow/models,models,3559,aayushARM,TFSlim: Exporting NASNet-A large inference graph always produces a graph with input batch_size=1?,"### System information
- **What is the top-level directory of the model you are using**: 
[https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet](url)
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5
- **Bazel version (if compiling from source)**:  0.9.0
- **CUDA/cuDNN version**: N.A.
- **GPU model and memory**: N.A.
- **Exact command to reproduce**:
`python export_inference_graph.py --alsologtostderr --model_name=nasnet_large --batch_size=32 --output_file=./nasnet-large.pb`

### Describe the problem
After exporting inference graph using above command, irrespective of batch_size arg, the graph produced always has batch_size=1. I've tried 3-4 times, and even tried hard-coding batch size in the input placeholder in export_inference_graph.py:

```
placeholder = tf.placeholder(name='input', dtype=tf.float32,
                                             shape=[32,  #FLAGS.batch_size,
			                     image_size,
                                             image_size, 3]) 
```
But due to some strange reason, the placeholder produced is always of shape [1,331,331,3]. Either there's a problem, or I am doing something terribly silly. Below is the tensorboard visualization of exported graph:

![graph_large_attrs_key _too_large_attrs limit_attr_size 1024 run](https://user-images.githubusercontent.com/20727454/37241919-15fd9d78-2487-11e8-9618-8424387292c3.png)

I assure that the visualization is of the latest and the only log event in directory. Also, to verify, giving input of any other size produces shape mismatch error as expected.",0,,[],2018-03-10 11:52:51,open,,,[],2018-03-10 12:05:32
1130,tensorflow/models,models,3554,rohitkharanghar,Object Detection: an unexpected keyword argument 'groundtruth_is_difficult_list',"
### System information
- **Have I written custom code - No**
- **OS Platform and Distribution-Ubuntu 16.04)**
- **TensorFlow installed from binary**:
- **TensorFlow version **:1.4.1
- **CUDA/cuDNN version**:8.0/6.0
- **GPU model and memory**:NVIDIA Corporation Device 1b80 and 8GB
- **Exact command to reproduce**:python eval.py --logtostderr --checkpoint_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet101_coco.config --eval_dir=evaluation_dir/

I am getting following error:
`WARNING:root:image 0 does not have groundtruth difficult flag specified
WARNING:root:The following classes have no ground truth examples: [3 4 7]
/data/rsk/models1/models/research/object_detection/utils/metrics.py:144: RuntimeWarning: invalid value encountered in true_divide
  num_images_correctly_detected_per_class / num_gt_imgs_per_class)
Traceback (most recent call last):
  File ""eval.py"", line 137, in <module>
    tf.app.run()
  File ""/data/rsk/anaconda3/envs/tfod/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""eval.py"", line 133, in main
    FLAGS.checkpoint_dir, FLAGS.eval_dir)
  File ""/data/rsk/models1/models/research/object_detection/evaluator.py"", line 210, in evaluate
    save_graph_dir=(eval_dir if eval_config.save_graph else ''))
  File ""/data/rsk/models1/models/research/object_detection/eval_util.py"", line 381, in repeated_checkpoint_run
    save_graph_dir)
  File ""/data/rsk/models1/models/research/object_detection/eval_util.py"", line 271, in _run_checkpoint_once
    image_id=batch, detections_dict=result_dict)
  File ""/data/rsk/models1/models/research/object_detection/utils/object_detection_evaluation.py"", line 250, in add_single_detected_image_info
    detected_masks=detection_masks)
  File ""/data/rsk/models1/models/research/object_detection/utils/object_detection_evaluation.py"", line 622, in add_single_detected_image_info
    groundtruth_masks=groundtruth_masks))
TypeError: compute_object_detection_metrics() got an unexpected keyword argument 'groundtruth_is_difficult_list'`
Help please.",2,,[],2018-03-09 18:08:49,open,,,[],2018-03-19 12:23:41
1131,tensorflow/models,models,3549,jkravanja,Read prefetcher capacity from eval_input_reader field of the configuration file.,The prefetcher capacity was hard coded. New commit enables reading this value from the configuration file.,1,,[],2018-03-08 15:51:30,open,,,['cla: no'],2018-03-08 15:51:32
1132,tensorflow/models,models,3547,RFXMed,Trained model ...,"Hi,

Where can i download trained model ""frozen_inference_graph.pb"" of pets ...

Thanks

RFXMed
",6,,[],2018-03-08 14:35:17,open,,,[],2018-03-14 09:20:57
1133,tensorflow/models,models,3546,chansekar,Unable to run Mask RCNN Resnet 101,"Hi,

I was trying to train 'mask_rcnn_resnet101', with the below command

python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/mask_rcnn_resnet101_atrous_coco.config

However i get bug which states

Message type ""object_detection.protos.FasterRcnn"" has no field named ""number_of_stages"".

Not sure how to go about, can anyone let me know how to go about?

Regards
Sekar
",10,,[],2018-03-08 10:56:47,open,,,[],2018-05-08 05:02:43
1134,tensorflow/models,models,3543,jkravanja,eval.py evaluates same image multiple times,"While evaluating a Mask RCNN model, i noticed that same images get evaluated multiple times. I counted the number of times images repeated and it was the same as 'num_readers' variable set in the configuration file. This was my initial configuration:

    eval_config: {
      eval_interval_secs: 300
      num_examples: 500
      max_evals: 10
      num_visualizations: 500
      visualization_export_dir: ""PATH/eval_detections/""
    }
    eval_input_reader: {
      tf_record_input_reader {
        input_path: ""PATH/eval.record""
      }
      label_map_path: ""PATH/labelmap.pbtxt""
      load_instance_masks: true
      mask_type: PNG_MASKS
      shuffle: false
      num_readers: 6
      queue_capacity: 10
      prefetch_size: 4
      min_after_dequeue: 4
    }

Visualizations (images with bounding boxes) were saved to ""PATH/eval_detections/"". However, i found out that there were 6 instances of each image (which is exactly num_readers). After setting num_readers: 1, i get the correct output (one visualization per image)

Should the readers not read different files, but do this in parallel, instead of 6 readers reading the same image 6 times?",1,,[],2018-03-07 17:08:32,open,,"NamedUser(login=""hgadig"")",['stat:awaiting response'],2018-12-05 21:43:43
1135,tensorflow/models,models,3541,LaurentBerder,Add data to existing dataset,"I am trying to train a model for object detection of a class that does not exist in standard datasets, so I created my own, labeled it and converted the whole thing to TF Record.

However, my different models (based on config files from the sample models in [/samples/configs](https://github.com/tensorflow/models/tree/fd7b6887fb294e90356d5664724083d1f61671ef/research/object_detection/samples/configs)) return many false positives, and I believe it would be best to enrich my dataset with standard ones (such as COCO).

To do that, it'd be ideal if there was a function such as `tf.add_data(old_tf_record, new_tf_record)` in order to combine both and result a TF Record with additional classes.

This is related to #1809, where @humayun wants to add new classes to COCO.",8,,[],2018-03-07 15:34:57,open,,,[],2018-04-10 06:44:27
1136,tensorflow/models,models,3538,wanghuite,"I can't train, how to code with the problem, I have tried many times.","Traceback (most recent call last):
  File ""train.py"", line 167, in <module>
    tf.app.run()
  File ""C:\Users\as\AppData\Local\conda\conda\envs\kate\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""train.py"", line 163, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""D:\as\models\research\object_detection\trainer.py"", line 235, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""D:\as\models\research\object_detection\trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""train.py"", line 120, in get_next
    dataset_builder.build(config)).get_next()
  File ""D:\as\models\research\object_detection\builders\dataset_builder.py"", line 138, in build
    label_map_proto_file=label_map_proto_file)
  File ""D:\as\models\research\object_detection\data_decoders\tf_example_decoder.py"", line 110, in __init__
    dct_method=dct_method),
TypeError: __init__() got an unexpected keyword argument 'dct_method'",1,,[],2018-03-07 10:03:24,open,,,[],2018-03-08 03:14:51
1137,tensorflow/models,models,3537,ddurgaprasad,Smaple Request :Preparing data and TFRecords for Semantic Segmentation,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Instance Segmentation [workflow ](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/instance_segmentation.md)is unclear on preparing data . 
[Examples ](https://github.com/tensorflow/models/blob/master/research/object_detection/dataset_tools/)on creation of TFRecords either has Image-Lable or Image-Mask combination. 
AFAIK  TFRecords are generated with
 - Images in jpg or png format
 - Labels in PASCAL VOC XML format
 - Image Masks in png format 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Is it alright to add 'image/object/mask'  to the TFRecord generation snippet?
Notice that [Convert raw PASCAL dataset to TFRecord for object_detection](https://github.com/tensorflow/models/blob/master/research/object_detection/dataset_tools/create_pascal_tf_record.py)  uses annotations directory. [Convert the Oxford pet dataset to TFRecord for object_detection](https://github.com/tensorflow/models/blob/master/research/object_detection/dataset_tools/create_pet_tf_record.py) does not use annotations but uses mask directory. Is there an example which combines both ?

example = tf.train.Example(features=tf.train.Features(feature={
      'image/height': dataset_util.int64_feature(height),
      'image/width': dataset_util.int64_feature(width),
      'image/filename': dataset_util.bytes_feature(
          data['filename'].encode('utf8')),
      'image/source_id': dataset_util.bytes_feature(
          data['filename'].encode('utf8')),
      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),
      'image/encoded': dataset_util.bytes_feature(encoded_jpg),
      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),
      'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),
      'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),
      'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),
      'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),
      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
      'image/object/class/label': dataset_util.int64_list_feature(classes),
      'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),
      'image/object/truncated': dataset_util.int64_list_feature(truncated),
      'image/object/view': dataset_util.bytes_list_feature(poses),
  }))",0,,[],2018-03-07 04:57:56,open,,,[],2018-03-07 04:57:56
1138,tensorflow/models,models,3532,zeynali,Add NasMobile to SSD detector,Please Add NasMobile to SSD.,1,,[],2018-03-06 16:06:01,open,,"NamedUser(login=""hgadig"")",['stat:awaiting response'],2018-12-05 21:46:26
1139,tensorflow/models,models,3531,89douner,object_detection_tutorial test runtime,"### System information
- **What is the top-level directory of the model you are using**: ssd_inception_v2_coco_2017_11_17
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: installing on Python and Pip 
(https://www.tensorflow.org/versions/r1.5/install/install_linux#InstallingNativePip)
- **TensorFlow version (use command below)**: 1.5.0
- **Bazel version (if compiling from source)**: there is no bazel command in my computer, i just install tensorflow 1.5.0 through Python and Pip method as i mentioned above
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**: GeForce GTX 1070
- **Exact command to reproduce**: ??

###You can collect some of this information using our environment capture script:

2018-03-06 22:58:39.700094: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-06 22:58:39.816687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-03-06 22:58:39.816965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.48GiB
2018-03-06 22:58:39.816981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

### Describe the problem
1. Test runtime too long.
 - I executed object_detection_tutorial after changing 'ipynb' into 'py' file. As you can see below code, 
It took about 3 second to detect one image. Also, It took about 130 second to detect 130 images, 253 second 400 frames. But, I think that, at least, it take 200 frames 10 second since this model has real-time performance. 

2. Possible cause I expected...
1) GPU doesn't work properly.
--> if in this case, you can give me answer through above environment capture, system specification.

2) Wrong measurement test runtime method (you can see the method below picture)
--> Please, let me know how to exactly measure detection time.  

Please answer me detail.. I wanna give your team help to upgrade Google Object Detection models !

### Source code / logs

 with detection_graph.as_default():  

    with tf.Session(graph=detection_graph) as sess:
        start = time.time()
        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
        # Each box represents a part of the image where a particular object was detected.
        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
        # Each score represent how level of confidence for each of the objects.
        # Score is shown on the result image, together with the class label.
        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
        num_detections = detection_graph.get_tensor_by_name('num_detections:0')
       
        for image_path in TEST_IMAGE_PATHS:
            image = Image.open(image_path)
            image_np = load_image_into_numpy_array(image)
            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
            image_np_expanded = np.expand_dims(image_np, axis=0)
            # Actual detection.
            (boxes, scores, classes, num) = sess.run(
                [detection_boxes, detection_scores, detection_classes, num_detections],
                feed_dict={image_tensor: image_np_expanded})

            # Visualization of the results of a detection.
            vis_util.visualize_boxes_and_labels_on_image_array(
                image_np,
                np.squeeze(boxes),
                np.squeeze(classes).astype(np.int32),
                np.squeeze(scores),
                category_index,
                use_normalized_coordinates=True,
                line_thickness=2)

        print(""--- %s seconds ---"" % (time.time() - start))

        '''
        except for 'start = time.time()' and 'print(""--- %s seconds ---"" % (time.time() - start))' code,
        It's the same as 'object_detection_tutorial.ipynb' code. I used  ssd_inception_v2_coco_2017_11_17
        ''' 


################################################################################

![2018-03-06 22-41-56](https://user-images.githubusercontent.com/31752297/37038025-019df14a-2197-11e8-9db7-8cbb79e04fe7.png)

#################################################################################

P.S  
If you need more information, please tell me. Thanks!",5,,[],2018-03-06 14:36:10,open,,,[],2018-04-09 07:41:23
1140,tensorflow/models,models,3525,gfphoenix78,fix bug when user specifies input_shape,"* fix bug when user specifies input_shape, such as '-1,256,256,3', which program interprets -1 as parameter name not value
* update arg message of input_shape",0,,[],2018-03-06 10:16:27,open,,,['cla: yes'],2018-03-06 10:16:29
1141,tensorflow/models,models,3523,prp20,None.de file not found,"when i run the nmt module for GNMT with 4 layers i get this error
""tensorflow.python.framework.errors_impl.NotFoundError: None.de; No such file or directory""
finding it very hard to solve. my tensorflow version is 1.5.0 and also have install tf-nightly 1.7.0.dev20180304. 
please provide suggestions in this matter.
thanks in advance
",1,,[],2018-03-06 06:07:47,open,,,[],2018-03-15 03:22:16
1142,tensorflow/models,models,3522,wanghuite,"I can't train,and many errors occurs when I test the  tf_example_decoder_test.py and dataset_builder_test.py","PS D:\models\research\object_detection\builders> python dataset_builder_test.py
C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-03-06 08:30:53.477280: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
EEEE..
======================================================================
ERROR: test_build_tf_record_input_reader (__main__.DatasetBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1269, in _ConsumeSingleByteString
    result = text_encoding.CUnescape(text[1:-1])
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_encoding.py"", line 105, in CUnescape
    .decode('unicode_escape')
UnicodeDecodeError: 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""dataset_builder_test.py"", line 92, in test_build_tf_record_input_reader
    text_format.Merge(input_reader_text_proto, input_reader_proto)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 525, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 579, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 612, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 627, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 727, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 815, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 727, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 866, in _MergeScalarField
    value = tokenizer.ConsumeString()
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1229, in ConsumeString
    the_bytes = self.ConsumeByteString()
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1244, in ConsumeByteString
    the_list = [self._ConsumeSingleByteString()]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1271, in _ConsumeSingleByteString
    raise self.ParseError(str(e))
google.protobuf.text_format.ParseError: 5:21 : 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

======================================================================
ERROR: test_build_tf_record_input_reader_and_load_instance_masks (__main__.DatasetBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1269, in _ConsumeSingleByteString
    result = text_encoding.CUnescape(text[1:-1])
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_encoding.py"", line 105, in CUnescape
    .decode('unicode_escape')
UnicodeDecodeError: 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""dataset_builder_test.py"", line 125, in test_build_tf_record_input_reader_and_load_instance_masks
    text_format.Merge(input_reader_text_proto, input_reader_proto)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 525, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 579, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 612, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 627, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 727, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 815, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 727, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 866, in _MergeScalarField
    value = tokenizer.ConsumeString()
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1229, in ConsumeString
    the_bytes = self.ConsumeByteString()
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1244, in ConsumeByteString
    the_list = [self._ConsumeSingleByteString()]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1271, in _ConsumeSingleByteString
    raise self.ParseError(str(e))
google.protobuf.text_format.ParseError: 6:21 : 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

======================================================================
ERROR: test_build_tf_record_input_reader_with_batch_size_two (__main__.DatasetBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1269, in _ConsumeSingleByteString
    result = text_encoding.CUnescape(text[1:-1])
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_encoding.py"", line 105, in CUnescape
    .decode('unicode_escape')
UnicodeDecodeError: 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""dataset_builder_test.py"", line 148, in test_build_tf_record_input_reader_with_batch_size_two
    text_format.Merge(input_reader_text_proto, input_reader_proto)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 525, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 579, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 612, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 627, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 727, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 815, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 727, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 866, in _MergeScalarField
    value = tokenizer.ConsumeString()
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1229, in ConsumeString
    the_bytes = self.ConsumeByteString()
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1244, in ConsumeByteString
    the_list = [self._ConsumeSingleByteString()]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1271, in _ConsumeSingleByteString
    raise self.ParseError(str(e))
google.protobuf.text_format.ParseError: 5:21 : 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

======================================================================
ERROR: test_build_tf_record_input_reader_with_batch_size_two_and_masks (__main__.DatasetBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1269, in _ConsumeSingleByteString
    result = text_encoding.CUnescape(text[1:-1])
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_encoding.py"", line 105, in CUnescape
    .decode('unicode_escape')
UnicodeDecodeError: 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""dataset_builder_test.py"", line 196, in test_build_tf_record_input_reader_with_batch_size_two_and_masks
    text_format.Merge(input_reader_text_proto, input_reader_proto)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 525, in Merge
    descriptor_pool=descriptor_pool)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 579, in MergeLines
    return parser.MergeLines(lines, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 612, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 627, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 727, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 815, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 727, in _MergeField
    merger(tokenizer, message, field)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 866, in _MergeScalarField
    value = tokenizer.ConsumeString()
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1229, in ConsumeString
    the_bytes = self.ConsumeByteString()
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1244, in ConsumeByteString
    the_list = [self._ConsumeSingleByteString()]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\text_format.py"", line 1271, in _ConsumeSingleByteString
    raise self.ParseError(str(e))
google.protobuf.text_format.ParseError: 6:21 : 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape

----------------------------------------------------------------------
Ran 6 tests in 0.380s

FAILED (errors=4)

PS D:\models\research\object_detection\data_decoders> python tf_example_decoder_test.py
C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-03-05 23:32:53.845870: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
EEEEEEEEEEEEEEEEEE.
======================================================================
ERROR: testDecodeBoundingBox (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 183, in testDecodeBoundingBox
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeDefaultGroundtruthWeights (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 214, in testDecodeDefaultGroundtruthWeights
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeEmptyPngInstanceMasks (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 158, in testDecodeEmptyPngInstanceMasks
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeImageKeyAndFilename (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 86, in testDecodeImageKeyAndFilename
    'image/key/sha256': self._BytesFeature('abc'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'abc' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeInstanceSegmentation (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 518, in testDecodeInstanceSegmentation
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeJpegImage (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 66, in testDecodeJpegImage
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeObjectArea (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 385, in testDecodeObjectArea
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeObjectDifficult (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 429, in testDecodeObjectDifficult
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeObjectGroupOf (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 453, in testDecodeObjectGroupOf
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeObjectIsCrowd (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 406, in testDecodeObjectIsCrowd
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeObjectLabel (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 239, in testDecodeObjectLabel
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeObjectLabelNoText (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 262, in testDecodeObjectLabelNoText
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeObjectLabelUnrecognizedName (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 305, in testDecodeObjectLabelUnrecognizedName
    self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeObjectLabelWithMapping (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 347, in testDecodeObjectLabelWithMapping
    self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodeObjectWeight (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 477, in testDecodeObjectWeight
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodePngImage (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 105, in testDecodePngImage
    'image/format': self._BytesFeature('png'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'png' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testDecodePngInstanceMasks (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 135, in testDecodePngInstanceMasks
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

======================================================================
ERROR: testInstancesNotAvailableByDefault (__main__.TfExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_example_decoder_test.py"", line 569, in testInstancesNotAvailableByDefault
    'image/format': self._BytesFeature('jpeg'),
  File ""tf_example_decoder_test.py"", line 58, in _BytesFeature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 510, in init
    copy.extend(field_value)
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""C:\Users\42911\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\type_checkers.py"", line 109, in CheckValue
    raise TypeError(message)
TypeError: 'jpeg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)

----------------------------------------------------------------------
Ran 19 tests in 0.113s

FAILED (errors=18)

",1,,[],2018-03-06 00:32:40,open,,,[],2018-03-06 03:27:23
1143,tensorflow/models,models,3515,achao2013,Does NasNet-A has a train code?,"hi, i just want to ask if there is a train code of NasNet-A.
@sguada ",2,,[],2018-03-05 03:45:56,open,,,[],2018-06-27 05:23:12
1144,tensorflow/models,models,3492,apachemycat,should give the python code to generate example.npy file ,"I found no python code to generate example.npy file ,so users have difficult to test there custom hand write files. ",4,,[],2018-02-28 13:31:54,open,,,[],2018-04-06 07:49:46
1145,tensorflow/models,models,3490,maxwang7,NOT FOR MERGING: Fixes to pet detector tutorial,"I went through the pet detector tutorial and made some fixes that got the training portion running. I haven't figured out how to get the evaluation code (there's an open issue for pycocotools) or the ipython notebook running. I don't have context on whether the fixes I made here were the correct ones, but they did get the program to the point where it was working for me.

I hope this contribution helps improve the quality of the tutorials in the future, thank you for maintaining this project!",4,,[],2018-02-28 08:18:10,open,,,['cla: no'],2018-09-06 06:47:52
1146,tensorflow/models,models,3480,MicrocontrollersAndMore,removed dct_method=dct_method from line 110 as that causes an error w…,"…hen running train.py

In this issue:

https://github.com/tensorflow/models/issues/3476

I've found that the change in this file from

    fields.InputDataFields.image: slim_example_decoder.Image(
        image_key='image/encoded', format_key='image/format', channels=3),

to:

    fields.InputDataFields.image:
        slim_example_decoder.Image(
            image_key='image/encoded',
            format_key='image/format',
            channels=3,
            dct_method=dct_method),

causes an error when running https://github.com/tensorflow/models/blob/master/research/object_detection/train.py

I think this should be changed back as above.  I should mention in issue 3476 another poster verified experiencing the same concern and verified the same fix, thanks!",4,,[],2018-02-27 09:02:39,open,,,['cla: yes'],2018-02-27 09:09:10
1147,tensorflow/models,models,3473,dizcology,Generating adversarial images with TensorFlow,"This is intended as the reference implementation for a blogpost.

DO NOT MERGE.",1,,[],2018-02-26 22:05:22,open,,,['cla: yes'],2018-04-23 23:16:16
1148,tensorflow/models,models,3468,gariepyalex,Set device of pre- and post-processing to CPU,Fix #3270 for the SSD architecture.,12,,[],2018-02-26 14:42:14,open,,,['cla: yes'],2018-05-30 18:38:19
1149,tensorflow/models,models,3466,nguyeho7,resnet_v1_50() got an unexpected keyword argument 'store_non_strided_activations',"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Solus Linux 3
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0-dev20180225 (nightly build)
- **Bazel version (if compiling from source)**: 9.0
- **CUDA/cuDNN version**:
- **GPU model and memory**: Nvidia Geforce 1080ti
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When I try to train the retina net model (resnet50, SSD meta architecture, focal loss), I get the following error:

  File ""train.py"", line 167, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 163, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/root/gv/models/research/object_detection/trainer.py"", line 246, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""/root/gv/models/research/slim/deployment/model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""/root/gv/models/research/object_detection/trainer.py"", line 179, in _create_losses
    prediction_dict = detection_model.predict(images, true_image_shapes)
  File ""/root/gv/models/research/object_detection/meta_architectures/ssd_meta_arch.py"", line 343, in predict
    preprocessed_inputs)
  File ""/root/gv/models/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py"", line 130, in extract_features
    scope=scope)
TypeError: resnet_v1_50() got an unexpected keyword argument 'store_non_strided_activations'

I did not change any code apart from the configuration file, where I set the feature extractor to be ssd_resnet_v1_fpn                                                                                                                                                 
",1,,[],2018-02-26 12:15:54,open,,,[],2018-04-06 07:47:26
1150,tensorflow/models,models,3465,DominikAuras,object_detection: python3 compatibility: range creates iterator,"In Python 3, `range(...)` creates an iterator. This leads to a failure here:
https://github.com/tensorflow/models/blob/4d1f67cf1a0c6f4cccdd2e3e46ce036c0a734e94/research/object_detection/utils/learning_schedules.py#L153
The op `tf.constant` correctly complains, that it received an iterator, not a list of integers.

Possible fix: use `list(range(...))` or adapt `tf.constant` to accept iterators.",2,,[],2018-02-26 10:56:41,open,,"NamedUser(login=""hgadig"")","['models: research', 'stat:awaiting response']",2019-02-06 22:15:25
1151,tensorflow/models,models,3457,AdamMiltonBarker,Slim Inception TF 1.5.0 / 1.4.0 / 1.3.0,"Hi guys should everything be working as expected with retraining Inception v3 on 1.5.0 / 1.4.0 / 1.3.0 ? On both CPU installation and GPU installations I am receiving errors and cannot retrain. 

https://github.com/tensorflow/models/tree/master/research/slim

TF 1.5.0 Windows  GPU GTX1050ti
TF 1.4.0 Ubuntu LTS CPU
TF 1.3.0 (Raspian Stretch) 

Errors include:

```
2018-02-25 12:45:12.633598: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1105] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.31GiB
2018-02-25 12:45:12.640729: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Cannot assign a device for operation 'gradients/aux_loss/xentropy_grad/LogSoftmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
         [[Node: gradients/aux_loss/xentropy_grad/LogSoftmax = LogSoftmax[T=DT_FLOAT, _device=""/device:GPU:0""](aux_loss/xentropy/Reshape)]]

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'gradients/aux_loss/xentropy_grad/LogSoftmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
         [[Node: gradients/aux_loss/xentropy_grad/LogSoftmax = LogSoftmax[T=DT_FLOAT, _device=""/device:GPU:0""](aux_loss/xentropy/Reshape)]]
```

Similar errors on CPU. TIA",2,,[],2018-02-25 11:57:40,open,,,[],2018-05-15 04:58:24
1152,tensorflow/models,models,3454,JoyMengjiaoZhao,Update generate_cifar10_tfrecords.py,"In case unable encoding.
I use the python3.5",4,,[],2018-02-24 22:46:53,open,,"NamedUser(login=""JoyMengjiaoZhao"")",['cla: no'],2018-02-24 23:00:03
1153,tensorflow/models,models,3445,noahstier,add learning rate to summaries,Learning rate is a handy thing to visualize. Is there a cleaner way to do this than `tf.get_default_graph` with a hard-coded tensor name?,2,,[],2018-02-23 22:25:37,open,,,['cla: no'],2018-02-23 22:25:40
1154,tensorflow/models,models,3442,JeroenDelcour,explicitely create list from range() for python3 compatibility,"Fixes the error below when running `train.py` using python3 with the [default rcnn_inception_v2_coco mask-rcnn config file](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config) linked in the [instance segmentation readme](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/instance_segmentation.md).

```
python object_detection/train.py     --logtostderr     --pipeline_config_path=""../../../mask_rcnn_inception_v2_coco.config"" --train_dir=""../../../train/""
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From /home/delcourj/BAM_Telecom_grond/tensorflow/models/research/object_detection/trainer.py:228: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
WARNING:tensorflow:From /home/delcourj/BAM_Telecom_grond/tensorflow/models/research/object_detection/core/box_predictor.py:391: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/delcourj/BAM_Telecom_grond/tensorflow/models/research/object_detection/core/losses.py:306: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See tf.nn.softmax_cross_entropy_with_logits_v2.

WARNING:tensorflow:From /home/delcourj/BAM_Telecom_grond/tensorflow/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1851: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
Traceback (most recent call last):
  File ""object_detection/train.py"", line 167, in <module>
    tf.app.run()
  File ""/home/delcourj/tfenv/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""object_detection/train.py"", line 163, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/delcourj/BAM_Telecom_grond/tensorflow/models/research/object_detection/trainer.py"", line 255, in train
    train_config.optimizer)
  File ""/home/delcourj/BAM_Telecom_grond/tensorflow/models/research/object_detection/builders/optimizer_builder.py"", line 50, in build
    learning_rate = _create_learning_rate(config.learning_rate)
  File ""/home/delcourj/BAM_Telecom_grond/tensorflow/models/research/object_detection/builders/optimizer_builder.py"", line 108, in _create_learning_rate
    learning_rate_sequence)
  File ""/home/delcourj/BAM_Telecom_grond/tensorflow/models/research/object_detection/utils/learning_schedules.py"", line 153, in manual_stepping
    tf.constant(range(num_boundaries), dtype=tf.int32),
  File ""/home/delcourj/tfenv/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 212, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/delcourj/tfenv/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 413, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/home/delcourj/tfenv/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 328, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got range(0, 3) of type 'range' instead.
```",6,,[],2018-02-23 15:26:11,open,,,['cla: yes'],2018-03-19 13:36:53
1155,tensorflow/models,models,3441,itsergiu,Jupyter Notebook iris_data.ipynb for iris_data.py,"The purpose of  application iris_data.ipynb is to provide Tensorflow iris_data.py tested step by step code in format of Jupyter Notebook .
This may help you to speed up your tests and understanding.

Source:
https://www.tensorflow.org/get_started/get_started_for_beginners
https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py

Solution:
https://github.com/itsergiu/Tensorflow/blob/master/iris_data/iris_data.ipynb


",2,,[],2018-02-23 13:17:10,open,,,[],2018-04-06 07:49:37
1156,tensorflow/models,models,3440,FarooqKhan,Fix get_input_fn() for PREDICT mode,"During predict we should not return None tensor, doing so will cause an exception in dataset.map()
Also while extracting data from the dataset iterator we should not expect labels since they do not exist in the dataset",2,,[],2018-02-23 11:27:36,open,,,['cla: yes'],2018-03-01 08:56:41
1157,tensorflow/models,models,3433,harahu,Mismatch between code and docs,"[train_input_fn()](https://github.com/tensorflow/models/blob/d903d5edbc0dc540ebf7971a7fec8b5d0dc903cb/samples/core/get_started/iris_data.py#L30-L39) in iris_data.py is used as the basis for the [Datasets quickstart tutorial](https://www.tensorflow.org/get_started/datasets_quickstart). At the moment there is a mismatch between the code in the tutorial and the actual code in the models repo. The tutorial is based around returning an Iterator, while the function now directly returns a Dataset. The tutorial could probably do with an update, but I am not sure how to report this properly.",2,"NamedUser(login=""MarkDaoust"")","[NamedUser(login=""MarkDaoust"")]",2018-02-22 22:15:23,open,,,[],2018-04-06 07:47:17
1158,tensorflow/models,models,3432,noahstier,models/research/object_detection requires Tensorflow 1.5,"### System information
- **What is the top-level directory of the model you are using**:
models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary: ""conda install tensorflow-gpu=1.4""
- **TensorFlow version (use command below)**:
1.4
- **CUDA/cuDNN version**:
cuda 8.0-3, cudnn 7.05
- **GPU model and memory**:
Nvidia GTX 1080
- **Exact command to reproduce**:
from the example in the object_detection readme:
python object_detection/train.py \
    --logtostderr \
    --pipeline_config_path=${PATH_TO_YOUR_PIPELINE_CONFIG} \
    --train_dir=${PATH_TO_TRAIN_DIR}

### Describe the problem

A recent commit to models/research/object_detection introduced a dependency on the Tensorflow 1.5 release. However, the models/research README claims ""Currently, the models are compatible with TensorFlow 1.0 or later"". The most recent tensorflow release available through Anaconda is 1.4, so this change forces me to compile from source, use a different package manager, or find a conda package outside the official repo, which are not horrible options but still inconvenient.

Here is a link to the commit diff. It calls tf.contrib.data.parallel_interleave which is not present in Tensorflow 1.4. 
https://github.com/tensorflow/models/commit/8f93258365b78d64d0643571a1a2e4d7b22df74d#diff-9e49d577701a72acf6f96e468190b191R129

If the 1.5 dependency is deemed acceptable, then I think it should be indicated either in the models/research readme or the models/research/object_detection readme.",15,,[],2018-02-22 21:56:22,open,,,"['stat:awaiting tensorflower', 'type:docs']",2018-08-01 14:25:18
1159,tensorflow/models,models,3426,zorancupic,MobileNet TF-Slim freezing graph / input and output nodes ,"### System Information:

Tensorflow Version: 'v1.4.0-rc1-11-g130a514', '1.4.0'
Also tested this with all versions from 1.0.0 to 1.4.0
OS: Ubuntu 16.04
CUDA: CUDA 8.0

### Problem
I retrained the in TF-Slim implemented MobileNet on a custom dataset. I'm using this implementation of the Mobile Net in TF-Slim : https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py


The retraining worked fine, but now I want to freeze and optimize the graph, so that I can deploy the model to an app. The problem now is, that I really can't figure out which are the input and output node names. I already inspected the graph with the summarize graph script, but I just found out that the node  `MobilenetV1/Predictions/Reshape_1` could be a possible output node.

[In this, in the TF-Slim Model Repository, provided script](https://github.com/tensorflow/models/blob/master/research/slim/scripts/export_mobilenet.sh) for exporting the MobileNet, there is `input` and `MobilenetV1/Predictions/Reshape_1` provided as input and output node. But if I'm using `input ` as input node in for example `label_image.py` there I'm getting this error: `""The name 'import/input' refers to an Operation not in the graph."" `

What worked for running `python label_image.py --graph=""./train_dir/frozen_model.pb"" --input_layer=""fifo_queue_Dequeue"" --output_layer=""MobilenetV1/Predictions/Reshape_1"" --input_width=224 --input_height=224 --labels=""../dataset/labels.txt"" --image=""two.jpg""` is using `fifo_queue_Dequeue` as input node, but then I'm getting this error:

> [[17  1 20  0  6  2  4 12 24  9  3 10 25 22 11 15 14 19 16  8  7  5 23 13
  18 21]
 [ 1 17 20  0  6  2  4 12 24  9  3 25 10 22 15 11 19 14 16  8  7  5 23 13
  18 21]
 [17  1 20  0  6  2  4 12 24  9  3 25 10 22 15 11 19 14 16  8  7  5 23 13
  18 21]
 [17  1 20  0  6  2  4 12  9 24  3 25 10 22 15 11 14 19 16  8  7  5 23 13
  18 21]
 [ 1 17 20  0  6  2  4 12 24  9  3 25 10 22 15 11 16 14 19  8  7  5 23 13
  18 21]]
['0:Aloe Vera', '1:Dracanea marginata', '2:Haworthia limifolia', '3:Persea gratissima', '4:Sansevieria trifasciata', '5:Tradescantia fluminensis Tricolor', '6:Tradescantia pallida', '7:calathea lancifolia', '8:calathea orbifolia', '9:chlorophytum comosum vittatum', '10:crassula ovata', '11:dracaea reflexa', '12:echeveria elegans', '13:euphorbia triangularis', '14:euphorbia trigona rubra', '15:fatsia japonica', '16:ficus elastica', '17:ficus lyrata', '18:maranta leuconeura erythroneura', '19:monstera deliciosa', '20:oxalis triangularis', '21:phlebodium pseudoaureum', '22:pilea peperomioides', '23:senecio kleiniiformis', '24:senecio rowleyanus', '25:tradescantia zebrina']
Traceback (most recent call last):
  File ""label_image.py"", line 134, in <module>
    print(labels[i], results[i])
TypeError: only integer scalar arrays can be converted to a scalar index

Here I'm printing my labels and the top 5 results, which have to be scalars instead of vectors. I think the problem is here the given output node.
For labeling I'm using this [python script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py).



",10,,[],2018-02-22 12:30:18,open,,,[],2018-11-10 16:10:18
1160,tensorflow/models,models,3424,FarooqKhan,Possible None check required in Dataset.map(),"**System information**

- **What is the top-level directory of the model you are using**: models/tutorials/rnn/quickdraw/train_model.py
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution**: MacOS High Sierra 10.13.3
- **TensorFlow installed from (source or binary)**: pip, virtualenv
- **TensorFlow version (use command below)**:1.5.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**:

**Describe the problem**

Am trying to do a prediction for this tutorial and building on the existing code in train_model.py.

To do this I am trying to reuse the `get_input_fn()` function for prediction. I think the inner function `_parse_tfexample_fn()` seems to be coded for use during prediction as well as it has few if conditions checking for mode == PREDICT

I am invoking it as follows:

```
predict_results = estimator.predict(input_fn=get_input_fn(
          mode=tf.estimator.ModeKeys.PREDICT,
          tfrecord_pattern=FLAGS.predict_temp_file,
          batch_size=FLAGS.batch_size))

```
`_parse_tfexample_fn()` returns a tuple of 'parsed_features, labels' and labels is None when `mode == PREDICT`
However this then causes causes a exception as below which is due to the labels being None i think.

```
File ""/Users/farooq/.virtualenvs/tensor1.0/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 486, in add_to_graph
    self._create_definition_if_needed()
  File ""/Users/farooq/.virtualenvs/tensor1.0/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 321, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/Users/farooq/.virtualenvs/tensor1.0/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 338, in _create_definition_if_needed_impl
    outputs = self._func(*inputs)
  File ""/Users/farooq/.virtualenvs/tensor1.0/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1579, in tf_map_func
    ret, [t.get_shape() for t in nest.flatten(ret)])
  File ""/Users/farooq/.virtualenvs/tensor1.0/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1579, in <listcomp>
    ret, [t.get_shape() for t in nest.flatten(ret)])
AttributeError: 'NoneType' object has no attribute 'get_shape'
```
Maybe the `dataset.map()` should check for None value before invoking `get_shape()` on it?",2,,[],2018-02-22 10:00:51,open,,,['type:support'],2018-04-06 07:49:28
1161,tensorflow/models,models,3423,byungjae89,object_detection preprocessor.py random_image_scale TypeError and bug,"### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: Titan X
- **Exact command to reproduce**: use random_image_scale preprocessing with Mask R-CNN

### Describe the problem
When I tried to use 'random_image_scale' preprocess option with Mask R-CNN, I encountered type error and code errors.

1. TypeError
[preprocessor.py#L768](https://github.com/tensorflow/models/blob/master/research/object_detection/core/preprocessor.py#L768)
```
masks: (optional) rank 3 float32 tensor containing masks with
      size [height, width, num_masks]. The value is set to None if there are no masks.
```

mask tensor shape is `[num_instances, height, width]`, not `[height, width, num_masks]`

2. Code error (mask existence check, mask resize)
[preprocessor.py#L803](https://github.com/tensorflow/models/blob/master/research/object_detection/core/preprocessor.py#L803)
```
    if masks:
      masks = tf.image.resize_nearest_neighbor(
          masks, [image_newysize, image_newxsize], align_corners=True)
      result.append(masks)
```

2.1. Mask existence check
if we use masks which means it is tf.Tensor type rather than None type, `if masks:` raise error
```
raise TypeError(""Using a `tf.Tensor` as a Python `bool` is not allowed. ""
TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use Tensorflow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.
```
This error can be fixed followed by: `if masks is not None:`

2.2. Mask resize
Also, resize function in code `tf.image.resize_nearest_neighbor(
          masks, [image_newysize, image_newxsize], align_corners=True)` is not working since the masks tensor shape is `[num_instances, height, width]`, but `tf.image.resize_nearest_neighbor` require the shape `[batch, height, width, channels]`. Therefore, error raises:
```
ValueError: Shape must be rank 4 but is rank 3 for 'RandomImageScale/ResizeNearestNeighbor' (op: 'ResizeNearestNeighbor') with input shapes: [?,?,?], [2].
```

My temporal fixed code is:
```
if masks is not None:
  masks = tf.image.resize_nearest_neighbor(tf.expand_dims(masks, -1), [image_newysize, image_newxsize], align_corners=True)
  masks = tf.squeeze(masks, -1)
  result.append(masks)
```
",1,,[],2018-02-22 08:34:34,open,,,['stat:awaiting tensorflower'],2018-02-22 23:21:15
1162,tensorflow/models,models,3419,xav12358,Object_detection  in windows 10,"Hello,

I try to learn using object detection.

### System information
- **What is the top-level directory of the model you are using**: 
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: from python
- **TensorFlow version (use command below)**: 1.4
- **CUDA/cuDNN version**: No
- **GPU model and memory**: NO
- **Exact command to reproduce**: python object_detection/train.py  --logtostderr --pipeline_config_path=/c/Users/Xavier/Desktop/developpement/Network/raccoon_dataset/training/ssd_mobilenet_v1_pets.config --train_dir=/c/Users/Xavier/Desktop/developpement/Network/raccoon_dataset/training/images/

I get that error:
```
Traceback (most recent call last):
  File ""object_detection/train.py"", line 50, in <module>
    from object_detection import trainer
  File ""C:\Users\Xavier\AppData\Local\Programs\Python\Python36\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\trainer.py"", line 33, in <module>
    from deployment import model_deploy
ModuleNotFoundError: No module named 'deployment'

```

",5,,[],2018-02-21 18:12:55,open,,,[],2018-04-06 07:49:20
1163,tensorflow/models,models,3412,DominikAuras,object_detection trainer.py does not use train_config.load_all_detection_checkpoint_vars,"There might a be bug `trainer.py`. In

https://github.com/tensorflow/models/blob/af6527c975a65a834d71adfde1041333b6fb8dc2/research/object_detection/trainer.py#L270-L271

the parameter `load_all_detection_checkpoint_vars`, which is defined here

https://github.com/tensorflow/models/blob/af6527c975a65a834d71adfde1041333b6fb8dc2/research/object_detection/protos/train.proto#L41

is not honored.

What I did: finetune mobilenet_ssd based on weights from your website. Then, use the last checkpoint to experiment with quantization.  So, I created a new train config, then referenced the checkpoint as ""fine_tune_checkpoint"". The initial loss is quite high, without quantization enabled! It appears that all the BoxPredictor scopes are not restored. Indeed, after checking the code, this is expected. Only the FeatureExtractor scope gets restored. The additional flag mentioned above needs to be used, I believe. When I changed the aforementioned code line in trainer.py, it started working as expecting. From Step 1 on in the new training (in a different train dir btw.), training continued exactly from the last training state.",5,,[],2018-02-20 07:23:05,open,,,['stat:awaiting tensorflower'],2018-03-12 17:39:02
1164,tensorflow/models,models,3404,Shuolongbj,Minor fixes to README.md for NASNet for TF Slim.,Minor fixes to README.md for NASNet for TF Slim.,1,,[],2018-02-17 15:06:45,open,,,['cla: yes'],2018-02-17 15:14:45
1165,tensorflow/models,models,3387,bkj,DELF: Training procedure,"Are the DELF authors able to give a little more detail about how they train their model?  Any insight into things like

    - cross-entropy loss and/or accuracy curves during fine-tuning training and/or attention training
    - number of epochs of training; number of GPUs; wall clock time
    - learning rates; how layers are frozen/unfrozen
    - how/whether hyperparameters were tuned on a validation set

would be super helpful.  Any specific pointers to other projects (maybe in this repo?) that used a roughly similar procedure would be helpful as well.

EDIT: Also, can you verify that both the fine-tuning and attention models were trained on [this dataset](http://sites.skoltech.ru/compvision/projects/neuralcodes/), rather than the Google-Landmarks dataset introduced in your paper.

Thanks
Ben

cc @andrefaraujo ",67,,[],2018-02-15 01:07:02,open,,,['stat:awaiting response'],2019-03-26 06:58:34
1166,tensorflow/models,models,3384,rogercw,Detection precision is worse after fine tune with pre-trained model,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, but not much
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: UbuntuMATE 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: V8.0.61
- **GPU model and memory**: GeForce GTX 1080 Ti / 10.91GiB
- **Exact command to reproduce**: 

### Describe the problem
I downloaded the pre-trained model of ssd-inception_v2_coco from detection model zoo, and set the ""fine_tune_checkpoint"" in ""ssd_inception_v2_coco.config"" to it. I use script ""train.py"" following the instruction in tutorial to train the network. After 200000 steps of training, ""export_inference_graph.py"" is used to export the model. However, the detection precision is getting worse (not just slightly different) with the re-trained model in the evaluation, comparing with directly using pre-trained model. May I know whether it is expected or a bug? If it is not a bug, how could we re-train a model to have better or at least similar detection precision as pre-trained model? Thanks in advance.",5,,[],2018-02-14 19:19:29,open,,,['stat:awaiting tensorflower'],2018-07-24 16:35:10
1167,tensorflow/models,models,3368,jerowe,mask_rcnn configs missing some values,"### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

In the instructions here for training with mask_rcnn - 
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/instance_segmentation.md

There are instructions for updating a faster_rcnn config. In this instructions are two steps, load_instance_masks: true and mask_type: 1/2 (1 for numeric, 2 for png).

These configuration variables are not in the mask_rcnn configs provided in the instructions, and it would be nice if they were. ;-)
 
```

train_input_reader: {
  tf_record_input_reader {
    input_path: ""PATH_TO_BE_CONFIGURED/mscoco_train.record""
  }
  label_map_path: ""PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt""
  load_instance_masks: true
  mask_type: 2
}
```
Edit again - for instance this one does not have the load_instance_masks field.

https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config",3,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-02-12 09:35:55,open,,,['type:docs'],2018-04-06 07:47:04
1168,tensorflow/models,models,3366,VanitarNordic,GPU is detected but training starts on the CPU,"Hi,

I have installed the tensorflow-gpu 1.5 or 1.6.rc0 in accompany with Cuda-9.0 and CuDNN-7.0.5
When I start training using `train.py`, it detects the GPU, but it starts the training on the CPU and CPU load is 100%. The GPU memory gets filled and its core clocks increases but it does not show any consistent load on the cores.

```
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.759
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 4.97GiB
2018-02-12 11:23:48.533753: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1308] Adding visible gpu devices: 0
2018-02-12 11:23:57.838951: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4742 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
```
",51,,[],2018-02-12 08:02:41,open,,,[],2019-03-21 17:02:30
1169,tensorflow/models,models,3365,VanitarNordic,Background only images support,"Hi,

Does the object detection model support ""background only"" images in training?",13,,[],2018-02-12 07:51:33,open,,,[],2019-03-06 00:19:46
1170,tensorflow/models,models,3364,baimukashev,Object Detection API error with 16-bit images (PNG),"
### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: 
- **Exact command to reproduce**:


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Hi, 
I have an error with 16-bit images of grayscale (channel = 1). In the object_detection_tutorial file jupyter notebook I have the following error: 
ValueError: Cannot feed value of shape (1, 480, 640, 1) for Tensor 'image_tensor:0', which has shape '(?, ?, ?, 3)' .

What are steps required in feeding 16-bit 1 channel images for pre-trained models?  

### Source code / logs
",1,,[],2018-02-12 04:45:04,open,,,['stat:awaiting tensorflower'],2018-02-13 19:46:32
1171,tensorflow/models,models,3363,brendanlundy,Calculate the small size only if necessary in preprocessor,Only calculate the small size if the large size is too big for the max_dimension.,1,,[],2018-02-12 02:25:27,open,,,['cla: yes'],2018-02-12 02:35:39
1172,tensorflow/models,models,3358,AshutoshSancheti,  ValueError: var_list cannot be empty,"I used the tf.contrib.slim model of Inception v4, modified it to classify 120 classes instead of 1001.
I have used the pretrained weights and initialized my new layers randomly.
I am new to tensorflow. Please tell me whether the method I have followed to change the model is correct?

These are the changes done by me.
```
net = slim.flatten(net, scope='PreLogitsFlatten')
end_points['PreLogitsFlatten'] = net
 """"""
  # 1536
logits = slim.fully_connected(net, num_classes, activation_fn=None,scope='Logits')
""""""
net = slim.fully_connected(net, 1001, activation_fn=tf.nn.relu, scope='fc_1c', weights_initializer = tf.truncated_normal_initializer(stddev=0.01), weights_regularizer=slim.l2_regularizer(0.0005))
net = slim.dropout(net, keep_prob = 0.6, scope = 'Dropout_1c')
logits = slim.fully_connected(net, num_classes,activation_fn = None,scope='Logits')
end_points['Logits'] = logits
end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')
```
------------------------

### System information
- OS : Windows 10 
- TensorFlow installed from binary and version is 1.4
- CUDA version is 8.0, cuDNN version is 6.0
- GPU: Nvidia GeForce 920M 2.0 GiB
- the ckpt file is in the 'checkpoints' folder of the current directory

Here is the error
```
 File ""dog_classify.py"", line 53, in <module>
    pretrained_weights = slim.assign_from_checkpoint_fn(ckpt_dir, slim.get_model_variables('InceptionV4'))
  File ""C:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\variables.py"", line 643, in assign_from_checkpoint_fn
    raise ValueError('var_list cannot be empty')
ValueError: var_list cannot be empty
```
Here is the necessary part of the python script I am using for training
```
with tf.device('/device:GPU:1'):
    with tf.Graph().as_default():
        tf.logging.set_verbosity(tf.logging.INFO)
        with slim.arg_scope(inception_blocks_v4.inception_v4_arg_scope()):
            X_input = tf.placeholder(tf.float32, shape = [None, image_size, image_size, 3])
            Y_label = tf.placeholder(tf.float32, shape = [None, num_classes])
            pretrained_weights = slim.assign_from_checkpoint_fn(ckpt_dir, slim.get_model_variables('InceptionV4'))
            with tf.Session() as sess:
                pretrained_weights(sess)
            logits, end_points = inception_blocks_v4.inception_v4(inputs = X_input, num_classes = 120, is_training = True)
            predictions = tf.nn.softmax(logits)
            loss = tf.losses.mean_squared_error(labels = Y_label, predictions = predictions)
            total_loss = tf.losses.get_total_loss()

            # Specify the optimizer and create the train op:
            optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)
            train_op = slim.learning.create_train_op(total_loss, optimizer) 
            
            #Generating batch
            images, labels = load_batch(32)
            # Run the training inside a session.
            final_loss = slim.learning.train(train_op,logdir = new_ckpt_dir, number_of_steps = iterations, save_summaries_secs=5,log_every_n_steps=50)(feed_dict = {X_input:images , Y_label: labels})
  
print (""Finished training. Last batch loss:"", final_loss)
print (""Checkpoint saved in %s"" % new_ckpt_dir)
```",5,,[],2018-02-10 13:34:02,open,,,[],2018-05-15 07:50:44
1173,tensorflow/models,models,3355,erfannoury,Bug in finding smallest side when preprocessing,"### System information
- **What is the top-level directory of the model you are using**: `models/research/slim/preprocessing`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.5, Windows 10 x64
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: `v1.4.0-19-ga52c8d9 1.4.1` (On Ubuntu), `unknown 1.4.0` (On Windows)
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 8.0.61/6.0.21 (On Ubuntu), No GPU on Windows
- **GPU model and memory**: Titan X (Pascal), 12GB
- **Exact command to reproduce**: 
```python
>>> sess.run(vgg_preprocessing._smallest_size_at_least(height, width, smallest_side),
             feed_dict={height: 1080, width: 1920, smallest_side: 224})
223, 398
```

### Describe the problem
When using the `vgg_preprocessing.preprocess_for_eval`, and by giving the image height value mentioned above, the given output is actually smaller than the `smallest_side` which happens due to precision loss when storing the result of dividing `smallest_side` by `height` in a `float32`.

This bug can also be easily reproduced using Numpy
```python
>>> a = np.float32(224.0 / 1080.0)
>>> (a * 1080.0).astype(np.int32)
223
```
I fixed this by changing [these lines](https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/vgg_preprocessing.py#L249-L251) to all use `to_double`.",1,,[],2018-02-09 22:05:14,open,,,['stat:awaiting tensorflower'],2018-02-23 08:47:01
1174,tensorflow/models,models,3351,amirjamez,Feature Request: eval_image_classifier.py to accept a frozen (.pb) file,"Is there a way to use `eval_image_classifier.py` with a `frozen.pb` file directly? I know `label_image` does the inference with .pb, but I need to have standardized comparison using the whole test batch of imagenet. This feature will be useful for many I guess.  Thanks.

- **OS Platform and Distribution: Linux Ubuntu 14.04/16.04
- **TensorFlow installed from (source or binary)**: Source v1.3
- **Bazel version (if compiling from source)**: bazel release 0.5.4
- **CUDA/cuDNN version**:  v8
- **GPU model and memory**: NVIDIA gtx1060
- **Exact command to reproduce**:

`python eval_image_classifier.py --alsologtostderr --checkpoint_path=FROZEN.pb --dataset_dir=$IMAGENET --dataset_name=imagenet --dataset_split_name=validation --model_name=inception_v3`

...
`2018-02-09 12:24:29.714748: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: Unable to open table file FROZEN.pb: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?`",2,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-02-09 17:26:41,open,,,[],2018-04-06 07:46:59
1175,tensorflow/models,models,3350,LaurentBerder,ImportError: cannot import name 'preprocessor_pb2',"------------------------

### System information
- **What is the top-level directory of the model you are using**: Models/research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: python train.py --h


### Describe the problem
Hi,

I'm trying out the Object_detection model, but can't launch train.py.
I get error:

```
research\object_detection\builders\preprocessor_builder.py"", line 24, in <module>
    from protos import preprocessor_pb2
ImportError: cannot import name 'preprocessor_pb2'
```

Indeed, there is no file called `preprocessor_pb2 `in the folder ""protos""

### Source code / logs
```
Traceback (most recent call last):
  File ""Models\research\object_detection\train.py"", line 49, in <module>
    import trainer
  File ""G:\My Drive\CMA-CGM\Reconnaissance_Container\Scripts\Models\research\object_detection\trainer.py"", line 27, in <module>
    from builders import preprocessor_builder
  File ""G:\My Drive\CMA-CGM\Reconnaissance_Container\Scripts\Models\research\object_detection\builders\preprocessor_builder.py"", line 24, in <module>
    from protos import preprocessor_pb2
ImportError: cannot import name 'preprocessor_pb2'
```",4,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-02-09 11:42:11,open,,,[],2018-06-21 09:44:54
1176,tensorflow/models,models,3348,teng88,Request for pre-trained model of NASNet-A (7 @ 1920),Can we release the pre-trained model for NASNet-A (7 @ 1920)? I think this config is very competitive compared to inception-v3 and inception-resnet v2 which have already widely used.,7,"NamedUser(login=""BarretZoph"")","[NamedUser(login=""BarretZoph"")]",2018-02-09 06:15:27,open,,,[],2018-06-02 17:42:22
1177,tensorflow/models,models,3346,Johnson145,Slim models: How to know model-specific std and mean values,"### Describe the problem
I'm wondering how to know which mean and std values should be used for running inference with a specific net.

For instance, [this](https://github.com/tensorflow/models/tree/master/research/slim#run-label-image-in-c) suggests that the slim inception-v3 model requires `input_mean=0` and `input_std=255` to be used. However, the ""old non-slim"" inception-v3 model used `input_mean=128` and `input_std=128` ([source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py)). 

I see that the first one normalizes all inputs from [0, 255] to [0, 1], while the latter maps all values to [-1, 1]. Of course, both choices can make sense as long as the same choice was already used during the training process. However, I don't understand where to find this choice in the training script. Can someone point me to the correct slim code part that implies the given values?

PS:
This may be a question for Stack Overflow as well. However, as these values are required to successfully deploy a net, I think this info should be added to the docs, too.

### System information
(not relevant here)
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
N/A
- **TensorFlow installed from (source or binary)**:
N/A
- **TensorFlow version (use command below)**:
N/A
- **Python version**: 
N/A
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A",4,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-02-08 19:30:32,open,,,[],2018-12-18 16:35:24
1178,tensorflow/models,models,3323,vs-zhehangd,"[object_detection] scope names of ""slim.conv2d"" calls.","I am trying to parallelize object detection inference on multiple GPUs using pretrained checkpoints from the model zoo. I parallelize the model by doing like this:
```python
with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):
  with tf.name_scope('dev1'):
    with tf.device('gpu:0'):
      preprocessed_inputs   = model.preprocess(input1)
      output_tensors        = model.predict(preprocessed_inputs)
      postprocessed_tensors = model.postprocess(output_tensors)
  
  with tf.name_scope('dev2'):    
    with tf.device('gpu:1'):
      preprocessed_inputs   = model.preprocess(input2)
      output_tensors        = model.predict(preprocessed_inputs)
      postprocessed_tensors = model.postprocess(output_tensors)
```
During checkpoint restoration, I got the following error:
```
NotFoundError (see above for traceback): Key Conv_1/biases not found in checkpoint
         [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]
         [[Node: save/RestoreV2_2/_1 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1820_save/RestoreV2_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]
```
This is because some variables are not properly shared. I managed to identify the error source, it is in `FasterRCNNMetaArch._extract_rpn_feature_maps` ( `meta_architectures/faster_rcnn_meta_arch.py`) 
```python
rpn_box_predictor_features = slim.conv2d(
    rpn_features_to_crop,
    self._first_stage_box_predictor_depth,
    kernel_size=[kernel_size, kernel_size],
    rate=self._first_stage_atrous_rate,
    activation_fn=tf.nn.relu6)
```
The scope name is not explicitly given, as a result we will get `Conv_1` instead of `Conv` in the second run. By adding the scope name to the function call
```python
rpn_box_predictor_features = slim.conv2d(
    rpn_features_to_crop,
    self._first_stage_box_predictor_depth,
    kernel_size=[kernel_size, kernel_size],
    rate=self._first_stage_atrous_rate,
    activation_fn=tf.nn.relu6),
    scope='Conv')
```
the problem is solved and fully compatible with the original one. I did not investigate the other architectures, but I casually ran `grep` to get all `slim.conv2d` calls and found there were a few other calls in `core/box_predictor.py` that missed the scope names as well. Maybe someone would like to test and fix all this kind of problems at once? 
",2,,[],2018-02-05 19:54:13,open,,,['stat:contributions welcome'],2018-02-26 17:43:22
1179,tensorflow/models,models,3320,alansberman,Slim nets import name error despite updated PythonPath,"
I'm currently trying to run an implementation of the Creative Adversarial Network (CAN), a Generative Adversarial Network for fine art generation (https://github.com/mlberkeley/Creative-Adversarial-Networks).

### System information:
OS: Windows 10 Education 64bit
 GPU: NVIDIDA GeForce GTX 1060 6GB
Tensorflow-GPU 1.5.0 (installed via pip)
 CUDA 9.0
Python 3.6.0

### Problem:
I got the seemingly common ""ImportError: No module named nets"" error (see https://github.com/tensorflow/models/issues/1842), and implemented the fix that thread (and multiple others) recommended: updating my PYTHONPATH a la:

export PYTHONPATH=$PYTHONPATH:mylocaldir:mylocaldir/slim

(mylocaldir is a placeholder). I used System>Edit the system environment variables, as I am on Windows. I have done this separately using both the directory where I have installed tensorflow (\~/tensorflow/contrib/slim), and using the /slim directory in the CAN repo itself (\~/Creative-Adversarial-Network/slim), but both give me the following error:

### Stack Trace
`Traceback (most recent call last):`
`File ""main.py"", line 6, in <module>`
`from model import DCGAN`
`File ""C:\Users\alan\Desktop\CAN\Creative-Adversarial-Networks\model.py"", line 10, in <module>`
`from slim.nets import nets_factory`
`File ""C:\Users\alan\Desktop\CAN\Creative-Adversarial-Networks\slim\nets\nets_factory.py"", line 25, in <module>`
`from nets import cifarnet`
`ImportError: cannot import name 'cifarnet'`

Interestingly, the line directly above the import of cifarnet _does not fail_:
`from nets import alexnet`

Why does that line not fail? What should I do next? ",0,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2018-02-04 10:23:58,open,,,[],2018-02-06 04:59:35
1180,tensorflow/models,models,3316,civilman628,"Randomly crash for GPU OOM error during training for object detection, though use small batch size.","------------------------

### System information
- **What is the top-level directory of the model you are using**:
/models/research/object_detection/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
 Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4 GPU
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA 8.0  cuDNN:6
- **GPU model and memory**:
GeForce GTX TITAN X, totalMemory: 11.92GiB freeMemory: 11.80GiB

- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh


== cat /etc/issue ===============================================
Linux scopephotos 4.4.0-109-generic #132-Ubuntu SMP Tue Jan 9 19:52:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux scopephotos 4.4.0-109-generic #132-Ubuntu SMP Tue Jan 9 19:52:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.0.post1)
tensorflow-gpu (1.4.1)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Feb  2 17:05:15 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.111                Driver Version: 384.111                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 00000000:05:00.0  On |                  N/A |
| 22%   55C    P0    86W / 250W |    123MiB / 12205MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX TIT...  Off  | 00000000:06:00.0 Off |                  N/A |
| 22%   56C    P0    91W / 250W |      2MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX TIT...  Off  | 00000000:09:00.0 Off |                  N/A |
| 22%   48C    P0    86W / 250W |      2MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX TIT...  Off  | 00000000:0A:00.0 Off |                  N/A |
| 22%   37C    P2    70W / 250W |  11747MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1477      G   /usr/lib/xorg/Xorg                           119MiB |
|    0     15876      G   /usr/bin/nvidia-settings                       0MiB |
|    3      6844      C   python                                     11735MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/lib/python2.7/dist-packages/torch/lib/libcudart-5d6d23a3.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
'v1.4.0-19-ga52c8d9', '1.4.1'

### Describe the problem
training object detection model: ssd_mobilenet_v1   meet OOM error of GPU, I reduce the number of batch size, but useless. Large batch size (default 24 ) will crash early, after about 300 steps. Small batch size=16 will crash later, like 600 steps.  

### Source code / logs

WARNING:tensorflow:From ../object_detection/trainer.py:210: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
2018-02-02 16:59:44.774125: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-02 16:59:45.101562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:06:00.0
totalMemory: 11.92GiB freeMemory: 11.80GiB
2018-02-02 16:59:45.101595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0, compute capability: 5.2)
INFO:tensorflow:Restoring parameters from ./fashion_checkpoint/model.ckpt-0
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path ./fashion_checkpoint/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:Recording summary at step 0.
**INFO:tensorflow:global step 1: loss = 35.1448 (15.144 sec/step)
INFO:tensorflow:global step 2: loss = 34.5482 (2.441 sec/step)**
2018-02-02 17:01:02.690697: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.98GiB.  Current allocation summary follows.
2018-02-02 17:01:02.690759: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (256): 	Total Chunks: 915, Chunks in use: 913. 228.8KiB allocated for chunks. 228.2KiB in use in bin. 10.3KiB client-requested in use in bin.
2018-02-02 17:01:02.690771: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (512): 	Total Chunks: 24, Chunks in use: 14. 12.0KiB allocated for chunks. 7.0KiB in use in bin. 7.0KiB client-requested in use in bin.
2018-02-02 17:01:02.690779: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (1024): 	Total Chunks: 18, Chunks in use: 17. 19.0KiB allocated for chunks. 17.8KiB in use in bin. 17.3KiB client-requested in use in bin.
2018-02-02 17:01:02.690788: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (2048): 	Total Chunks: 31, Chunks in use: 30. 66.0KiB allocated for chunks. 63.5KiB in use in bin. 63.2KiB client-requested in use in bin.
2018-02-02 17:01:02.690795: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (4096): 	Total Chunks: 394, Chunks in use: 393. 2.85MiB allocated for chunks. 2.85MiB in use in bin. 2.84MiB client-requested in use in bin.
2018-02-02 17:01:02.690803: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (8192): 	Total Chunks: 9, Chunks in use: 9. 87.5KiB allocated for chunks. 87.5KiB in use in bin. 83.5KiB client-requested in use in bin.
2018-02-02 17:01:02.690811: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (16384): 	Total Chunks: 22, Chunks in use: 22. 471.8KiB allocated for chunks. 471.8KiB in use in bin. 467.9KiB client-requested in use in bin.
 
...
...
...


2018-02-02 17:01:03.001726: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 3145728 totalling 3.00MiB
2018-02-02 17:01:03.001731: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 4194304 totalling 8.00MiB
2018-02-02 17:01:03.001738: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 4718592 totalling 9.00MiB
2018-02-02 17:01:03.001744: I tensorflow/core/common_runtime/bfc_allocator.cc:683] Sum Total of in-use chunks: 60.28MiB
2018-02-02 17:01:03.001753: I tensorflow/core/common_runtime/bfc_allocator.cc:685] Stats: 
Limit:                 12041302836
InUse:                    63212032
MaxInUse:               8206210816
NumAllocs:                   19700
MaxAllocSize:           4294967296

2018-02-02 17:01:03.001834: W tensorflow/core/common_runtime/bfc_allocator.cc:277] *___________________________________________________________________________________________________
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, Dst tensor is not initialized.
	 [[Node: prefetch_queue_Dequeue/_4113 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_838_prefetch_queue_Dequeue"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
Traceback (most recent call last):
  **File ""train_fashion.py"", line 171, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""train_fashion.py"", line 167, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""../object_detection/trainer.py"", line 333, in train
    saver=saver)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 763, in train
    sess, train_op, global_step, train_step_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_step
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
	 [[Node: prefetch_queue_Dequeue/_4113 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_838_prefetch_queue_Dequeue"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
scopeserver@scopephotos:~/RaidDisk/DeepLearning/mwang/models/research/object_detection$** 


",9,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-02-03 01:15:12,open,,,[],2018-08-30 23:13:32
1181,tensorflow/models,models,3313,rs1024,7x1 and 1x7 filters mixed up in Inception v4,"I noticed several places where the filter order is mixed up in Inception v4. For example, in the Inception-B block, branch 2 filters are in the order of: 1x1, 7x1, 1x7, 7x1, 1x7; however, in the paper, the order is described as: 1x1, 1x7, 7x1, 1x7, 7x1. There are a few other occurrences throughout the model.

Is there any reason for this?",2,,[],2018-02-02 23:39:18,open,,,['stat:awaiting tensorflower'],2018-02-28 11:13:59
1182,tensorflow/models,models,3307,chengmengli06,tensorflow model behaves differently in c++ and python,"I report a bug about SavedModel, which does not work good when load in c++, it concens two api, tf.gather_nd and tf.Print. Here are the details: 
https://stackoverflow.com/questions/48578706/tensorflow-model-behaves-differently-in-c-and-python",9,,[],2018-02-02 11:28:42,open,,,[],2018-04-06 07:48:49
1183,tensorflow/models,models,3298,civilman628,please provide inception v3 model check point file for SSD and Faster RCNN,"@tombstone   @nealwu   Could you please provide the pre-trained **base model**: **Inception v3** check point file for SSD and Faster RCNN?

Now I can see the published model ""faster_rcnn_inception_v2_coco"" and ""ssd_inception_v2_coco"", which contain the base model for Inception v2.

the inception v3 model check point file in the link below is very out of date. it does not have **.meta** and **.index** files.

https://github.com/tensorflow/models/tree/master/research/slim

Thanks




",4,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2018-02-01 23:00:53,open,,,[],2018-07-24 08:14:17
1184,tensorflow/models,models,3291,krystynak,Error running Oxford Pets Tutorial on Google Cloud ML Engine,"Followed directions to run this tutorial on google cloud ML:
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md

I followed all the directions.
Set up fine on Mac and uploaded files to GCP.  Started training and evaluation jobs as per directions.  After starting TF, job terminates with a python error - missing the matplotlib.pyplot module on the GCP side.

s-replica-1
Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 51, in <module> from object_detection.builders import model_builder File ""/root/.local/lib/python2.7/site-packages/object_detection/builders/model_builder.py"", line 29, in <module> from object_detection.meta_architectures import ssd_meta_arch File ""/root/.local/lib/python2.7/site-packages/object_detection/meta_architectures/ssd_meta_arch.py"", line 31, in <module> from object_detection.utils import visualization_utils File ""/root/.local/lib/python2.7/site-packages/object_detection/utils/visualization_utils.py"", line 24, in <module> import matplotlib.pyplot as plt ImportError: No module named matplotlib.pyplot",10,,[],2018-02-01 00:25:34,open,,,['stat:awaiting tensorflower'],2018-06-27 22:21:24
1185,tensorflow/models,models,3277,raingo,Fix visualize,new numpy does not allow float values for the reshape dimensions.,1,,[],2018-01-30 20:54:22,open,,,['cla: yes'],2018-01-30 20:54:24
1186,tensorflow/models,models,3274,kibergus,Do not demand filenames to have continous numeration,Current code in create_kitti_tf_record.py requires input files to have names in a form of 000123.txt with continuous numeration. It is easy to leverage this restriction and have more semantically meaningful code.,1,,[],2018-01-30 10:45:19,open,,,['cla: yes'],2018-01-30 10:45:23
1187,tensorflow/models,models,3271,kibergus,There is no DEFINE_list in public tensorflow,Here is a patch to deal with missing DEFINE_list #3239 ,5,,[],2018-01-30 08:53:35,open,,,['cla: no'],2018-01-30 09:21:02
1188,tensorflow/models,models,3270,wkelongws,Slow inference speed of object detection models and a hack as solution,"### System information
- **What is the top-level directory of the model you are using**:
models/research/object_detection/
- **Have I written custom code**:
No custom code for reproducing the bug. I have written custom code for diagnosing.
- **OS Platform and Distribution**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Anaconda conda-forge channel
- **TensorFlow version**:
b'unknown' 1.4.1 (output from `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`)
- **CUDA/cuDNN version**:
CUDA 8.0/cuDNN 6.0
- **GPU model and memory**:
1 TITAN X (Pascal)  12189MiB
- **Exact command to reproduce**:
Run the provided [object detection demo](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb) (ssd_mobilenet_v1_coco_2017_11_17 model) with a small modification in the last cell to record the inference speed:
```
    i = 0
    for _ in range(10):
      image_path = TEST_IMAGE_PATHS[1]
      i += 1
      image = Image.open(image_path)
      # the array based representation of the image will be used later in order to prepare the
      # result image with boxes and labels on it.
      image_np = load_image_into_numpy_array(image)
      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
      image_np_expanded = np.expand_dims(image_np, axis=0)
      # Actual detection.
      options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
      run_metadata = tf.RunMetadata()
      start_time = time.time()
      (boxes, scores, classes, num) = sess.run(
          [detection_boxes, detection_scores, detection_classes, num_detections],
          feed_dict={image_tensor: image_np_expanded})
      print('Iteration %d: %.3f sec'%(i, time.time()-start_time))
```
The results show that the inference speed is much shower than the reported inference speed, 30ms, in the [model zoo page](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md):
```
Iteration 1: 2.212 sec
Iteration 2: 0.069 sec
Iteration 3: 0.076 sec
Iteration 4: 0.068 sec
Iteration 5: 0.072 sec
Iteration 6: 0.072 sec
Iteration 7: 0.071 sec
Iteration 8: 0.079 sec
Iteration 9: 0.085 sec
Iteration 10: 0.071 sec
```

### Describe the problem
**Summary:**
By directly running the provided [object detection demo](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb), the observed inference speed of object detection models in the model zoo is much slower than the reported inference speed. With some hack, a higher inference speed than the reported speed can be achieved. After some diagnostics, it is highly likely that the slow inference speed is caused by:
* **tf.where and other post-processing operations are running anomaly slow on GPU; or**
* **The frozen inference graph is lack of the ability to optimize the GPU/CPU assignment.** 

**proof of the hypothesis: tf.where and other post-processing operations are running anomaly slow on GPU**

By outputting trace file, we can diagnose the running time of each node in details.
To output the trace file, modify the last cell of [object detection demo](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb) as:
```
from tensorflow.python.client import timeline
with detection_graph.as_default():
  with tf.Session(graph=detection_graph) as sess:
    # Definite input and output Tensors for detection_graph
    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
    # Each box represents a part of the image where a particular object was detected.
    detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
    # Each score represent how level of confidence for each of the objects.
    # Score is shown on the result image, together with the class label.
    detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
    detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
    num_detections = detection_graph.get_tensor_by_name('num_detections:0')
    i = 0
    for _ in range(10):
      image_path = TEST_IMAGE_PATHS[1]
      i += 1
      image = Image.open(image_path)
      # the array based representation of the image will be used later in order to prepare the
      # result image with boxes and labels on it.
      image_np = load_image_into_numpy_array(image)
      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
      image_np_expanded = np.expand_dims(image_np, axis=0)
      # Actual detection.
      options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
      run_metadata = tf.RunMetadata()
      start_time = time.time()   
      (boxes, scores, classes, num) = sess.run(\
      [detection_boxes, detection_scores, detection_classes, num_detections], \
      feed_dict={image_tensor: image_np_expanded}, \
      options=options, run_metadata=run_metadata)    
      print('Iteration %d: %.3f sec'%(i, time.time()-start_time))
      # Visualization of the results of a detection.
      vis_util.visualize_boxes_and_labels_on_image_array(
        image_np,
        np.squeeze(boxes),
        np.squeeze(classes).astype(np.int32),
        np.squeeze(scores),
        category_index,
        use_normalized_coordinates=True,
        line_thickness=8)
        
    plt.figure(figsize=IMAGE_SIZE)
    plt.imshow(image_np)
    
    fetched_timeline = timeline.Timeline(run_metadata.step_stats)
    chrome_trace = fetched_timeline.generate_chrome_trace_format()
    with open('Experiment_1.json' , 'w') as f:
      f.write(chrome_trace)
``` 
The output json file has been included in the .zip file in the **source code** section below.
Visualizing the json file in chrome://tracing/ gives:

![experiment1](https://user-images.githubusercontent.com/14045078/35551422-dae50440-0543-11e8-896f-62bc33bcf0af.png)

The CNN related operations end at ~13ms and the rest post-processing operations take about 133ms. We have noticed that adding the trace function will further slow down the inference speed. But it is shows clearly that the post-processing operations (post CNN) run very slowly on GPU.

As a comparison, one can run the [object detection demo](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb) with GPU disabled, and profile the running trace using the same method. To disable GPU, add `os.environ['CUDA_VISIBLE_DEVICES'] = ''` in the first row of the last cell.

The output json file has been included in the .zip file in the **source code** section below.
Visualizing this  json file in chrome://tracing/ gives: 

![experiment_2](https://user-images.githubusercontent.com/14045078/35581507-d329b978-05a0-11e8-808e-dc78232e284d.png)

By running everything on CPU, the CNN operations end at roughly 63ms and the rest post-processing operations only takes about 15ms on CPU which is significantly faster than the time they take when running on GPU.

**proof of the hypothesis: The frozen inference graph is lack of the ability to optimized the GPU/CPU assignment**

We add some hack trying to see can we achieve a higher inference speed. The hack is manually assigning the CNN related nodes on GPU and the rest nodes on CPU. The idea is using GPU to accelerate only CNN operations and leave the post-processing operations on CPU.

The source code has been included in the .zip file in the **source code** section below.

With this hack, we are able to observe a higher inference speed than the reported speed. 
```
Iteration 1: 1.021 sec
Iteration 2: 0.027 sec
Iteration 3: 0.026 sec
Iteration 4: 0.027 sec
Iteration 5: 0.026 sec
Iteration 6: 0.026 sec
Iteration 7: 0.026 sec
Iteration 8: 0.031 sec
Iteration 9: 0.031 sec
Iteration 10: 0.026 sec
```
**To verify the hypothesis, here are some questions we need from the tensorflow team:**

1. Are the numbers of inference speed reported on the detection model zoo page tested on the frozen inference graphs or original graphs?

2. Are the slow tf.where and other post-processing operations supposed to run on GPU or CPU? Is the slow running speed on GPU normal?

3. Is there a device assigning function to optimize the GPU/CPU use in the original tensorflow graphs? Is that function missing in the frozen inference graphs?

### Source code / logs

[tensorflowissue.zip](https://github.com/tensorflow/models/files/1678729/tensorflowissue.zip)

",76,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-01-30 06:33:46,open,,,[],2019-01-14 06:24:22
1189,tensorflow/models,models,3268,FightForCS,tfgan dcgan.py width of generated picture must be power of 2?,"Hi, I was working on tfgan, I want to generate images of size 48*48, but the discriminator function in dcgan.py says dimensions must be power of two
```
Raises:
    ValueError: If the input image shape is not 4-dimensional, if the spatial
      dimensions aren't defined at graph construction time, if the spatial
      dimensions aren't square, or if the spatial dimensions aren't a power of
      two.
```
As far as I know, GANs does not have such a constraint, anyone can help? Is there another function that I can call?",5,"NamedUser(login=""joel-shor"")","[NamedUser(login=""joel-shor"")]",2018-01-30 02:33:17,open,,,[],2018-04-06 07:48:45
1190,tensorflow/models,models,3261,ronhandler,Update iris_data.py,"Corrected ""Sentosa"" to ""Setosa""",3,,[],2018-01-27 17:47:58,open,,,['cla: no'],2018-01-27 17:51:18
1191,tensorflow/models,models,3255,oneTimePad,Classification API,"This is a feature request. 

I made a [Classification API](https://github.com/oneTimePad/classification) modeled after the Object Detection API. 

Features it supports:
- multi-task learning, which allows for the classification of multiple attributes of an image 
- supports model configuration via protobufs.
- easily defining new models following a template
- evaluation while training
- evaluation
- graph freezing
- inference on frozen graph

Just wondering if there was any interest in integrating something along the lines of this into models? It probably still needs a lot of work to meet the style and documentation requirements of this repo.",1,,[],2018-01-26 16:14:11,open,,,['stat:awaiting tensorflower'],2018-01-30 18:54:21
1192,tensorflow/models,models,3249,DaneSpiritGOD,Chinese word compatible problem when I use the Object Detection API of reading the config file,"### System information
== cat /etc/issue ===============================================
Linux myName 4.13.0-31-generic #34~16.04.1-Ubuntu SMP Fri Jan 19 17:11:01 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

== uname -a =====================================================
Linux myName 4.13.0-31-generic #34~16.04.1-Ubuntu SMP Fri Jan 19 17:11:01 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.1)
tensorflow-gpu (1.4.1)
tensorflow-tensorboard (0.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64
DYLD_LIBRARY_PATH is unset

### Describe the problem
**#Problem 1:**
In ""Object Detection API""--""train.py"":
`1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file`
`can be specified by --pipeline_config_path.`

`Example usage:`
    `./train \`
        `--logtostderr \`
        `--train_dir=path/to/train_dir \`
        `--pipeline_config_path=pipeline_config.pbtxt`

config_path might end with "".config"".
At least,in ""Object Detection API"" doctument,the format should be consistent.

**#Problem 2:**
Some code in Tensorflow is not compatible with Chinese.

I run the following script,and get error message:
`python object_detection/train.py \`
`--logtostderr \`
`--pipeline_config_path=/home/myName/桌面/Work/Trained_Model/temp/faster_rcnn_resnet101_pets.config \`
`--train_dir=/home/myName/桌面/Work/Trained_Model/temp`


`Traceback (most recent call last):`
`File ""object_detection/train.py"", line 163, in <module>`
`tf.app.run()`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run`
`_sys.exit(main(_sys.argv[:1] + flags_passthrough))`
`File ""object_detection/train.py"", line 91, in main
    FLAGS.pipeline_config_path)`
`File ""/home/myName/桌面/resources/Tensorflow/models/research/object_detection/utils/config_util.py"", line 43, in get_configs_from_pipeline_file
    text_format.Merge(proto_str, pipeline_config)`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 533, in Merge
    descriptor_pool=descriptor_pool)`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 587, in MergeLines
    return parser.MergeLines(lines, message)`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 620, in MergeLines
    self._ParseOrMerge(lines, message)`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 635, in _ParseOrMerge
    self._MergeField(tokenizer, message)`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 735, in _MergeField
    merger(tokenizer, message, field)`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 823, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 735, in _MergeField
    merger(tokenizer, message, field)`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 874, in _MergeScalarField
    value = tokenizer.ConsumeString()`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 1237, in ConsumeString
    the_bytes = self.ConsumeByteString()`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 1252, in ConsumeByteString
    the_list = [self._ConsumeSingleByteString()]`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_format.py"", line 1277, in _ConsumeSingleByteString
    result = text_encoding.CUnescape(text[1:-1])`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_encoding.py"", line 103, in CUnescape
    result = ''.join(_cescape_highbit_to_str[ord(c)] for c in result)`
`File ""/home/myName/anaconda3/envs/tensorflow/lib/python3.6/site-packages/google/protobuf/text_encoding.py"", line 103, in <genexpr>
    result = ''.join(_cescape_highbit_to_str[ord(c)] for c in result)`
`IndexError: list index out of range`

Some code in the file `faster_rcnn_resnet101_pets.config ` is like this:
`gradient_clipping_by_norm: 10.0`
`fine_tune_checkpoint: ""/home/myName/桌面/Work/Trained_Model/temp/model.ckpt""`

As I know,in UTF-8 encoding,a English letter is represented by one byte.
So,`ord(c)` may be right,but for Chinease word,the format parser code should change a little.

**According to my judgement,I create a symbolic link(whose path is all in English) pointed to the Desktop directory which is Chinese word.
For example,`/home/myName/DesktopLink` points to `/home/myName/桌面`**
And I use the new path in the config file instead.
I run the script again.
The training phase begins correctly.

So,I've got a bug or something could be improved?
**Although the problem might be in the `protobuf` code,does it need to do some preprocess to avoid this case?**",1,,[],2018-01-26 03:42:20,open,,,['stat:awaiting tensorflower'],2018-01-26 22:26:30
1193,tensorflow/models,models,3239,zeynali,create_kitti_tf_record.py,"hi
when i run this code , convert kitti to tfrecord , i get this error : 
#################################
python3 dataset_tools/create_kitti_tf_record.py \
>         --data_dir=kitti \
>         --output_path=kitti.record
Traceback (most recent call last):
  File ""dataset_tools/create_kitti_tf_record.py"", line 65, in <module>
    tf.app.flags.DEFINE_list('classes_to_use', ['Car', 'Pedestrian' , 'DontCare'],
AttributeError: module 'tensorflow.python.platform.flags' has no attribute 'DEFINE_list'
#############################################
data_dir=kitti is my dataset , which in this folder is training  and labels . 
what is this error ? 
in folder data : kitti_label_map.pbtxt 
item {
  id: 1
  name: 'Car'
}

item {
  id: 2
  name: 'Pedestrian'
}

item {
  id: 3
  name: 'DontCare'
}
",6,,[],2018-01-24 21:19:05,open,,,[],2018-04-06 07:48:36
1194,tensorflow/models,models,3226,JesperChristensen89,Check failed: stream->parent()->GetConvolveAlgorithms,"I am trying to run [Faster R-CNN Inception V2](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2017_11_08.tar.gz) on a Jetson TX2 (Tegra GPU). 

The problem does not seem to arise from memory errors (which is the case running a larger model). The output from the resource monitor states:
```
RAM 6469/7851MB (lfb 5x4MB) cpu [51%@1981,off,off,8%@1980,56%@1980,100%@1983] EMC 2%@1600 APE 150 GR3D 0%@114
RAM 6471/7851MB (lfb 5x4MB) cpu [54%@2035,off,off,8%@2034,50%@2034,100%@2035] EMC 2%@1600 APE 150 GR3D 0%@114
RAM 6475/7851MB (lfb 5x4MB) cpu [9%@2049,off,off,10%@2034,100%@2035,100%@2033] EMC 2%@1600 APE 150 GR3D 0%@216
RAM 6475/7851MB (lfb 5x4MB) cpu [7%@2036,off,off,6%@2034,100%@2035,100%@2036] EMC 2%@1600 APE 150 GR3D 26%@114
RAM 6475/7851MB (lfb 5x4MB) cpu [5%@1982,off,off,3%@1981,100%@1982,100%@1981] EMC 2%@1600 APE 150 GR3D 0%@114
```

The output from Python states:
```
2018-01-22 08:49:31.074043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties: 
name: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005
pciBusID: 0000:00:00.0
totalMemory: 7.67GiB freeMemory: 3.20GiB
2018-01-22 08:49:31.074101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0
2018-01-22 08:49:31.756800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2705 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)
2018-01-22 08:49:55.624323: E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2018-01-22 08:49:55.624461: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
2018-01-22 08:49:55.624497: F tensorflow/core/kernels/conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)
```
I am running CUDA 8.0 with cuDNN 6.0 and TensorFlow 1.5 (also tested with 1.3).",4,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2018-01-23 15:03:40,open,,"NamedUser(login=""JesperChristensen89"")",[],2018-12-04 02:46:10
1195,tensorflow/models,models,3219,TylerBalsam,[BUG] Mobilenet SSD Object Detection Defaults Cause Image Clipping,"Hi there,

When working with the object detection library, I noticed some clashes in the code.

In the MobileNet SSD model file in the models directory, the preprocessing function is scaled from 0 - 255 to -1 to 1. In the object detection core/preprocessing.py module, contrast/hue/pixel scaling ops have this line in them:

  image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)

This clips the image unnaturally and may have side effects for people not staring at preprocessed images as they progress through the preprocessing pipeline in Tensorboard.",2,,[],2018-01-22 20:39:48,open,,,['stat:awaiting tensorflower'],2018-01-26 22:17:52
1196,tensorflow/models,models,3218,davidenitti,bugfix type mismatch tf.float32 != tf.int32 (fixes #2774),This fixes the bug described in https://github.com/tensorflow/models/issues/2774,1,,[],2018-01-22 18:26:21,open,,,['cla: yes'],2018-01-25 00:21:42
1197,tensorflow/models,models,3215,podhrmic,Ignore files generated by the protocol buffer compiler,"Since these files are autogenerated, they don't need to be show in git output. So instead of:

```
$git status
On branch master
Your branch is up-to-date with 'origin/master'.
Untracked files:
  (use ""git add <file>..."" to include in what will be committed)
	research/object_detection/protos/anchor_generator_pb2.py
	research/object_detection/protos/argmax_matcher_pb2.py
	research/object_detection/protos/bipartite_matcher_pb2.py
	research/object_detection/protos/box_coder_pb2.py
	research/object_detection/protos/box_predictor_pb2.py
	research/object_detection/protos/eval_pb2.py
	research/object_detection/protos/faster_rcnn_box_coder_pb2.py
	research/object_detection/protos/faster_rcnn_pb2.py
	research/object_detection/protos/grid_anchor_generator_pb2.py
	research/object_detection/protos/hyperparams_pb2.py
	research/object_detection/protos/image_resizer_pb2.py
	research/object_detection/protos/input_reader_pb2.py
	research/object_detection/protos/keypoint_box_coder_pb2.py
	research/object_detection/protos/losses_pb2.py
	research/object_detection/protos/matcher_pb2.py
	research/object_detection/protos/mean_stddev_box_coder_pb2.py
	research/object_detection/protos/model_pb2.py
	research/object_detection/protos/optimizer_pb2.py
	research/object_detection/protos/pipeline_pb2.py
	research/object_detection/protos/post_processing_pb2.py
	research/object_detection/protos/preprocessor_pb2.py
	research/object_detection/protos/region_similarity_calculator_pb2.py
	research/object_detection/protos/square_box_coder_pb2.py
	research/object_detection/protos/ssd_anchor_generator_pb2.py
	research/object_detection/protos/ssd_pb2.py
	research/object_detection/protos/string_int_label_map_pb2.py
	research/object_detection/protos/train_pb2.py
```

the user sees only

```
$git status
On branch master
Your branch is up-to-date with 'origin/master'.
Untracked files:
  (use ""git add <file>..."" to include in what will be committed)
```",5,,[],2018-01-22 16:38:54,open,,,['cla: yes'],2018-05-10 00:52:44
1198,tensorflow/models,models,3212,NeoChaos12,[TCN] Specify minimum tf-nightly version,"### System information
- **What is the top-level directory of the model you are using**:
https://github.com/tensorflow/models/tree/master/research/tcn

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04 LTS, Trusty Tahr

- **TensorFlow installed from (source or binary)**:
pip

- **TensorFlow version (use command below)**:
tf-nightly - various versions

- **Bazel version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
CUDA 7.0~9.0, cuDNN 6.0, 7.0

- **GPU model and memory**:
NVIDIA Titan X( PASCAL) 12GB

- **Exact command to reproduce**:
N/A

### Describe the problem
The documentation of TCN mentions that tf-nightly is required, but fails to mention a minimum tf-nightly version. This is important since tf-nightly is updated quite frequently, and the latest version of tf-nightly may not be available for use to everyone due to individual restrictions.

### Source code / logs
",2,,[],2018-01-22 09:42:18,open,,,['stat:awaiting tensorflower'],2018-11-27 07:41:13
1199,tensorflow/models,models,3196,Tsuihao,[SSD] Small object detection,"Hi all,

I have a question regarding the configuration of SSD.
An interesting task for me is to fine-tuning the SSD_mobilenet_v1_coco_2017_11_17 with [Bosch small traffic light dataset](https://hci.iwr.uni-heidelberg.de/node/6132).

However, the default setting is to resize the image into 300 x 300 **(image_resizer).**
Here is the total loss during training.
It loss maintains around 6. (Please ignore the overlapping at 5000 steps, due to some re-launch trainign process.)
![image](https://user-images.githubusercontent.com/16992390/35113749-dc9e5daa-fc82-11e7-8e99-b6d06a05c2c9.png)

I think the trend of the total loss is okay.
However, when I stop around 12k and feed with the test dataset (around 90 images for a short try). There is nothing detected. 

![image](https://user-images.githubusercontent.com/16992390/35113972-88afece4-fc83-11e7-9d3e-e411d49d9650.png)


Personally, I have some doubts about this issue:
1. Maybe the small traffic lights are too small for SSD?
2. However, why the total loss curve displayed a correct ""learning"" process?

Can I simply change the config of image size into 512 x 512 or even larger value (1000 x 1000)?
Will this work correctly as well?

Regards,
Hao
",70,,[],2018-01-18 18:16:03,open,,,['stat:community support'],2019-04-06 09:13:17
1200,tensorflow/models,models,3195,foreverYoungGitHub,Add pr_curve for object detection api for issue #3081,"Add pr_curve for object detection api for issue #3081.

Change few functions mentioned in issue #3081 without add new ones. Tested with the newest tensorboard.",26,,[],2018-01-18 16:11:35,open,,,['cla: yes'],2019-04-08 19:25:45
1201,tensorflow/models,models,3192,dslomov,Failure on ci.bazel.build,"Bazel 0.9.0: https://ci.bazel.build/blue/organizations/jenkins/tf_models_syntaxnet/detail/tf_models_syntaxnet/909/pipeline/
Nightly: https://ci.bazel.build/blue/organizations/jenkins/Global%2Ftf_models_syntaxnet/detail/tf_models_syntaxnet/362/pipeline/",1,"NamedUser(login=""jart"")","[NamedUser(login=""jart"")]",2018-01-18 10:58:09,open,,,['stat:awaiting tensorflower'],2018-01-24 07:04:07
1202,tensorflow/models,models,3184,ngc92,fixed use of wrong variable,"Fixed what was probably a type:
`show_groundtruth` now uses `ignore_groundtruth` instead of unrelated `visualization_export_dir`.",1,,[],2018-01-17 23:58:50,open,,,['cla: yes'],2018-01-17 23:58:52
1203,tensorflow/models,models,3183,gcscoding,pixel_domain_adaptation Research Folder Update,"
### System information
- **What is the top-level directory of the model you are using**: Domain_Adaptation
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mint Linux
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.4
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
Code in models/research/domain_adaptation/pixel_domain_adaptation/ does not appear to have been updated after the research folder transition was made. Is it possible to update these files to match new directory differences? I keep reaching roadblocks when trying to run through the instructions provided because of directory errors. Program unable to find the correct folders needed to run. 

",2,"NamedUser(login=""bousmalis"")","[NamedUser(login=""bousmalis"")]",2018-01-17 21:55:13,open,,,['type:build/install'],2018-04-06 07:46:50
1204,tensorflow/models,models,3177,bkowshik,Datasets have _mask in their name only when faces_only is False,"Ref: [Distributed Training on the Oxford-IIIT Pets Dataset on Google Cloud](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md)

---

The names of the datasets should be:
- `pet_train.record`
- `pet_val.record`

But, the script outputs the datasets with the names of:
- `pet_train_with_masks.record`
- `pet_val_with_masks.record`

Default value for `faces_only` is `True`.

- [research/object_detection/dataset_tools/create_pet_tf_record.py#L49](https://github.com/tensorflow/models/blob/master/research/object_detection/dataset_tools/create_pet_tf_record.py#L49)

---

*NOTE: I am not a 💯 sure about this change as I started just 2 days using this repository.*",6,,[],2018-01-17 09:30:06,open,,,['cla: yes'],2018-05-12 11:02:09
1205,tensorflow/models,models,3176,Ellie68,How to control the number of threads on Tensorflow and TF-Slim?,"Do we have any options to control the number of threads in TF-Slim both in training and evaluation processes?

Specifically, I use [this network](https://github.com/pudae/tensorflow-densenet) for my classification problem. I changed the evaluation part in a way that runs train and evaluation in parallel like [this code](https://github.com/mnuke/tf-slim-mnist). I can run it on my own CPU without any problem. But I can't execute them on a supercomputer. It seems that it is related to the very large number of threads which are being created by Tensorflow. If the number of threads exceeds the maximum number of threads pre-set in SLURM (= 28) then the job will fail. Since it's unable to create new threads it will end up with error ""resource temporarily unavailable"".

This error provided when the code tries to restore parameters from checkpoints. If there is no limitation on the number of threads (like on my pc) it works fine:

    INFO:tensorflow:Restoring parameters from ./model.ckpt-0
    INFO:tensorflow:Starting evaluation at
    I tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0]
    I tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_5[0]
    INFO:tensorflow:Evaluation [1/60]

However, when there is a limitation on the number of threads (like SLURM job submission on supercomputers) we get:

    INFO:tensorflow:Restoring parameters from ./model.ckpt-0
    terminate called after throwing an instance of 'std::system_error'
    what():  Resource temporarily unavailable

I tried to limit the number of CPU threads used by Tensorflow to 1 by creating config like:

      FLAGS.num_preprocessing_threads=1

      config = tf.ConfigProto()
      config.intra_op_parallelism_threads = FLAGS.num_preprocessing_threads
      config.inter_op_parallelism_threads = FLAGS.num_preprocessing_threads
    
        slim.evaluation.evaluation_loop(
            master=FLAGS.master,
            checkpoint_path=each_ckpt,
            logdir=FLAGS.eval_dir,
            num_evals=num_batches,
            eval_op=list(names_to_updates.values()) + print_ops,
            variables_to_restore=variables_to_restore,
            session_config=config)
But unfortunately, that didn't help. In my opinion, the main problem we are having here is the fact that we are not able to control the number of threads here. Although we set it to 1 with various TF options you can actually see that this job is creating many more threads on the node:


    slurm_script─┬─python───128*[{python}]
                 └─python───8*[{python}]

Training script is creating 128 threads and evaluation script is creating 8 (both numbers vary over time).  

P.S. I'm using Python 2.7.13 and Tensorflow 1.3.0.",4,,[],2018-01-17 07:10:49,open,,,[],2018-11-06 08:08:40
1206,tensorflow/models,models,3174,mbz,Adding Stochastic Variational Version,,7,,[],2018-01-16 23:30:01,open,,,['cla: yes'],2019-03-27 16:24:25
1207,tensorflow/models,models,3171,zerodarkzone,Syntaxnet doesn't work with GPU support,"### System information
-  OS Platform and Distribution: Linux Ubuntu 16.04
-  Syntaxnet built from source from the master branch
-  Bazel version: 0.5.4
-  CUDA/cuDNN version: 8.0 / 6
-  GPU model and memory: Nvidia GTX 1080TI / 12 GB:

### 
I have been trying to build syntaxnet with gpu support but the tests fail with CUDA_ERROR_OUT_OF_MEMORY. I've already tried all the fixes proposed in the other issues talking about the gpu support

### Source code / logs
[test.log](https://github.com/tensorflow/models/files/1632140/test.log)

",2,,[],2018-01-15 15:31:48,open,,,"['stat:awaiting tensorflower', 'type:support']",2018-12-15 19:01:39
1208,tensorflow/models,models,3163,VastoLorde95,Yolov1,"This is a partially complete work of the YOLO object detection model. Since the YOLO model does not fit neatly into the meta architecture / feature extractor format, we have done our best to follow the same - the actual model, has been implemented as yolov1_feature_extractor.py and the preprocessing, postprocessing and loss functions have been implemented as yolov1_meta_arch.py. All of our implementation is based off the origin paper by Joseph Redmon.

We have not implemented the pipeline that allows to train this model hence, we are opening this pull request so that others who have better knowledge about the same can add on to our work and complete the same.",4,,[],2018-01-14 13:03:40,open,,,['cla: no'],2018-12-29 10:09:06
1209,tensorflow/models,models,3158,siyuanzhuang,"Load a Tensorflow model into memory,It takes too long times","#Load a (frozen) Tensorflow model into memory.
  detection_graph = tf.Graph()
  with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
      serialized_graph = fid.read()
      od_graph_def.ParseFromString(serialized_graph)
      tf.import_graph_def(od_graph_def, name='')
--------------------------------------------
use ssd_mobilenet_v1_pets pb model,size:181M
It takes too long times,Is there any way to solve it?thanks!",7,"NamedUser(login=""allenlavoie"")","[NamedUser(login=""allenlavoie"")]",2018-01-13 09:15:53,open,,,['type:support'],2018-10-22 22:52:22
1210,tensorflow/models,models,3155,MeghaMaheshwari,"Performance issues : Speed is very slow, around .8 seconds per frame on pre-trained model","I am using the pre-trained model ""ssd_mobilenet_v1_coco_2017_11_17"" and the results for object detection for vehicles is pretty good. However, the performance is extremely slow and hence I am unable to use it for my purpose. I resized the image to a smaller one and even chopped the horizon from the image so that the detection is faster, but it doesnt seem to help. Is this a known issue? Any suggestions here",3,,[],2018-01-12 19:22:46,open,,,['type:bug/performance'],2018-08-02 22:14:38
1211,tensorflow/models,models,3150,koho,Update create_pet_tf_record.py,"In the `get_class_name_from_filename` function, give a readable error when the regex failed to match the filename.",2,,[],2018-01-12 09:15:57,open,,,['cla: no'],2018-01-12 09:16:00
1212,tensorflow/models,models,3147,FightForCS,Pre-trained FPN based object detection model?,"In the object detection model zoo, I have not found a pre-trained model using FPN as the feature extractor, is there someone working on this?",1,,[],2018-01-12 03:44:20,open,,,['type:feature'],2018-04-06 07:46:46
1213,tensorflow/models,models,3143,rickhg12hs,Feature Request: Freeze capability NasNet-A-Large Checkpoint on a CPU,"### System information
- **What is the top-level directory of the model you are using**:
https://storage.googleapis.com/download.tensorflow.org/models/nasnet-a_large_04_10_2017.tar.gz

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes (see below)

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
== cat /etc/issue ===============================================
Linux steelers.steelersnet 4.14.11-200.fc26.x86_64 #1 SMP Wed Jan 3 13:58:53 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""26 (Workstation Edition)""
VERSION_ID=26
REDHAT_BUGZILLA_PRODUCT_VERSION=26
REDHAT_SUPPORT_PRODUCT_VERSION=26
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
== tensorflow import ============================================
tf.VERSION = 1.4.0
tf.GIT_VERSION = v1.3.0-rc1-5312-g8a4d849691
tf.COMPILER_VERSION = v1.3.0-rc1-5312-g8a4d849691
Sanity check: array([1], dtype=int32)

$ python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
2018-01-10 22:56:18.576633: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library
/usr/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
v1.3.0-rc1-6922-ga77096897f 1.5.0-rc0

- **Bazel version (if compiling from source)**:
$ bazel version
Build label: 0.8.1- (@non-git)
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Dec 6 22:56:54 2017 (1512601014)
Build timestamp: 1512601014
Build timestamp as int: 1512601014

- **CUDA/cuDNN version**:
Not Applicable
- **GPU model and memory**:
Not Applicable
- **Exact command to reproduce**:
See below

### Describe the problem

The NasNet-A-Large trained checkpoint advertised [here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models) and downloadable from [here](https://storage.googleapis.com/download.tensorflow.org/models/nasnet-a_large_04_10_2017.tar.gz) appears to be only freezeable for a GPU.  Attempts to freeze it with a CPU generates errors.  There is an apparent incapatibility here between NHWC and NCHW data formats.

Using pretrained models on a CPU is expected and should be a usable feature.

### Source code / logs

```python
$ ipython3
Python 3.6.3 (default, Oct  9 2017, 12:11:29) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow as tf
   ...: import tensorflow.contrib.slim as slim
   ...: import numpy as np
   ...: 
   ...: import sys
   ...: sys.path.append(""/usr/local/src/tensorflow/models/research/slim"")
   ...: from nets.nasnet.nasnet import build_nasnet_large, nasnet_large_arg_scop
   ...: e
   ...: height = 299
   ...: width = 299
   ...: channels = 3
   ...: FREEZE_DIR = ""./models/build_freeze_graphs""
   ...: # Create graph
   ...: X = tf.placeholder(tf.float32, shape=[None, height, width, channels])
   ...: with slim.arg_scope(nasnet_large_arg_scope()):
   ...:     logits, end_points = build_nasnet_large(X, num_classes=1001,is_train
   ...: ing=False)
   ...: softmax = end_points[""Predictions""]
   ...: saver = tf.train.Saver()
   ...: X_test = np.ones((1,299,299,3))  # a fake image
   ...: 
   ...: # Execute graph
   ...: with tf.Session() as sess:
   ...:     saver.restore(sess, FREEZE_DIR + ""/nasnet-a_large_04_10_2017-model.c
   ...: kpt"")
   ...:     tf.train.write_graph(sess.graph_def, FREEZE_DIR, 'nasnet-a_large.pbt
   ...: xt')
   ...:     
2018-01-10 23:15:42.496988: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library
/usr/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
INFO:tensorflow:Restoring parameters from ./models/build_freeze_graphs/nasnet-a_large_04_10_2017-model.ckpt

In [2]: 

```

```bash
$ python3 /usr/local/src/tensorflow/tensorflow/tensorflow/python/tools/freeze_graph.py --input_graph ./nasnet-a_large.pbtxt --input_checkpoint ./nasnet-a_large_04_10_2017-model.ckpt --output_node_names final_layer/predictions --output_graph ./frozen_nasnet-a_large.pb
2018-01-11 03:48:11.933436: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library
/usr/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Converted 1546 variables to const ops.
```


```python

$ ipython3
Python 3.6.3 (default, Oct  9 2017, 12:11:29) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: TF_SRC_DIR = ""/usr/local/src/tensorflow""
   ...: TF_TF_SRC_DIR = TF_SRC_DIR + ""/tensorflow""
   ...: TF_MODELS_SRC_DIR = TF_SRC_DIR + ""/models""
   ...: TF_EXAMPLES_DIR = TF_TF_SRC_DIR + ""/tensorflow/examples""
   ...: 

In [2]: import importlib.util
   ...: import sys
   ...: 
   ...: # Load TF retrain module
   ...: spec = importlib.util.spec_from_file_location(""retrain"", TF_EXAMPLES_DIR
   ...:  + ""/image_retraining/retrain.py"")
   ...: retrain = importlib.util.module_from_spec(spec)
   ...: spec.loader.exec_module(retrain)
   ...: 
2018-01-11 04:11:22.152580: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library
/usr/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

In [3]: DATA_DIR = ""./data/processed""
   ...: IMAGE_DIR = DATA_DIR + ""/JPG/Train""
   ...: ARCHITECTURE = ""nasnet-a_large""
   ...: 

In [4]: class C:
   ...:     pass
   ...: 
   ...: retrain.FLAGS = C()
   ...: 
   ...: retrain.FLAGS.architecture = ARCHITECTURE
   ...: retrain.FLAGS.bottleneck_dir = DATA_DIR + '/' + ARCHITECTURE + ""/bottlen
   ...: ecks""
   ...: retrain.FLAGS.eval_step_interval = 200
   ...: retrain.FLAGS.final_tensor_name = ""final_result""
   ...: retrain.FLAGS.flip_left_right = False
   ...: retrain.FLAGS.how_many_training_steps = 20000
   ...: retrain.FLAGS.image_dir = IMAGE_DIR
   ...: retrain.FLAGS.intermediate_output_graphs_dir = DATA_DIR + '/' + ARCHITEC
   ...: TURE + ""/intermed_output_graphs""
   ...: retrain.FLAGS.intermediate_store_frequency = 0
   ...: retrain.FLAGS.learning_rate = 0.005
   ...: retrain.FLAGS.model_dir = DATA_DIR + ""/models""
   ...: retrain.FLAGS.output_graph = DATA_DIR + '/' + ARCHITECTURE + ""/output_gr
   ...: aph.pb""
   ...: retrain.FLAGS.output_labels = DATA_DIR + '/' + ARCHITECTURE + ""/output_l
   ...: abels.txt""
   ...: retrain.FLAGS.print_misclassified_test_images = False
   ...: retrain.FLAGS.random_brightness = 0
   ...: retrain.FLAGS.random_crop = 0
   ...: retrain.FLAGS.random_scale = 0
   ...: retrain.FLAGS.summaries_dir = DATA_DIR + '/' + ARCHITECTURE + ""/retrain_
   ...: logs""
   ...: retrain.FLAGS.test_batch_size = -1
   ...: retrain.FLAGS.testing_percentage = 10
   ...: retrain.FLAGS.train_batch_size = 100
   ...: retrain.FLAGS.validation_batch_size = 100
   ...: retrain.FLAGS.validation_percentage = 10
   ...: 
   ...: 

In [5]: def create_model_info(architecture):
   ...:     return {
   ...:       'data_url': ""localhost:12345/fake/path/frozen_nasnet-a_large.pb"",
   ...:       'bottleneck_tensor_name': ""final_layer/dropout/Identity:0"",
   ...:       'bottleneck_tensor_size': 4032,
   ...:       'input_width': 299,
   ...:       'input_height': 299,
   ...:       'input_depth': 3,
   ...:       'resized_input_tensor_name': ""Placeholder:0"",
   ...:       'model_file_name': ""frozen_nasnet-a_large.pb"",
   ...:       'input_mean': 128,
   ...:       'input_std': 128,
   ...:       'quantize_layer': False,
   ...:   }
   ...: 
   ...: retrain.create_model_info = create_model_info
   ...: 

In [6]: retrain.main(None)
Not extracting or downloading files, model already present in disk
Model path:  ./data/processed/models/frozen_nasnet-a_large.pb
INFO:tensorflow:Looking for images in 'Iceberg'
INFO:tensorflow:Looking for images in 'NotIceberg'
INFO:tensorflow:Creating bottleneck at ./data/processed/nasnet-a_large/bottlenecks/Iceberg/1475-e8b76fb7.jpg_nasnet-a_large.txt
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1349     try:
-> 1350       return fn(*args)
   1351     except errors.OpError as e:

~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1328                                    feed_dict, fetch_list, target_list,
-> 1329                                    status, run_metadata)
   1330 

~/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [1,75,75,42] vs. shape[2] = [1,42,75,75]
	 [[Node: cell_stem_0/cell_output/concat = _MklConcatV2[N=4, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](cell_stem_0/comb_iter_1/combine/add, cell_stem_0/comb_iter_2/combine/add, cell_stem_0/comb_iter_3/combine/add, cell_stem_0/comb_iter_4/combine/add, cell_17/split/split_dim, DMT/_121, DMT/_122, cell_stem_0/comb_iter_3/combine/add:1, DMT/_123, DMT/_124)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/usr/local/src/tensorflow/tensorflow/tensorflow/examples/image_retraining/retrain.py in create_bottleneck_file(bottleneck_path, image_lists, label_name, index, image_dir, category, sess, jpeg_data_tensor, decoded_image_tensor, resized_input_tensor, bottleneck_tensor)
    381         sess, image_data, jpeg_data_tensor, decoded_image_tensor,
--> 382         resized_input_tensor, bottleneck_tensor)
    383   except Exception as e:

/usr/local/src/tensorflow/tensorflow/tensorflow/examples/image_retraining/retrain.py in run_bottleneck_on_image(sess, image_data, image_data_tensor, decoded_image_tensor, resized_input_tensor, bottleneck_tensor)
    316   bottleneck_values = sess.run(bottleneck_tensor,
--> 317                                {resized_input_tensor: resized_input_values})
    318   bottleneck_values = np.squeeze(bottleneck_values)

~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--> 895                          run_metadata_ptr)
    896       if run_metadata:

~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1127       results = self._do_run(handle, final_targets, final_fetches,
-> 1128                              feed_dict_tensor, options, run_metadata)
   1129     else:

~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1343       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1344                            options, run_metadata)
   1345     else:

~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1362           pass
-> 1363       raise type(e)(node_def, op, message)
   1364 

InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [1,75,75,42] vs. shape[2] = [1,42,75,75]
	 [[Node: cell_stem_0/cell_output/concat = _MklConcatV2[N=4, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](cell_stem_0/comb_iter_1/combine/add, cell_stem_0/comb_iter_2/combine/add, cell_stem_0/comb_iter_3/combine/add, cell_stem_0/comb_iter_4/combine/add, cell_17/split/split_dim, DMT/_121, DMT/_122, cell_stem_0/comb_iter_3/combine/add:1, DMT/_123, DMT/_124)]]

Caused by op 'cell_stem_0/cell_output/concat', defined at:
  File ""/usr/bin/ipython3"", line 11, in <module>
    sys.exit(start_ipython())
  File ""/usr/lib/python3.6/site-packages/IPython/__init__.py"", line 125, in start_ipython
    return launch_new_instance(argv=argv, **kwargs)
  File ""/usr/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/lib/python3.6/site-packages/IPython/terminal/ipapp.py"", line 356, in start
    self.shell.mainloop()
  File ""/usr/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 480, in mainloop
    self.interact()
  File ""/usr/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 471, in interact
    self.run_cell(code, store_history=True)
  File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2856, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-6-c66df78b6d6a>"", line 1, in <module>
    retrain.main(None)
  File ""/usr/local/src/tensorflow/tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 1024, in main
    create_model_graph(model_info))
  File ""/usr/local/src/tensorflow/tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 291, in create_model_graph
    model_info['resized_input_tensor_name'],
  File ""/home/rick/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/home/rick/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 548, in import_graph_def
    op_def=op_def)
  File ""/home/rick/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/home/rick/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1617, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [1,75,75,42] vs. shape[2] = [1,42,75,75]
	 [[Node: cell_stem_0/cell_output/concat = _MklConcatV2[N=4, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](cell_stem_0/comb_iter_1/combine/add, cell_stem_0/comb_iter_2/combine/add, cell_stem_0/comb_iter_3/combine/add, cell_stem_0/comb_iter_4/combine/add, cell_17/split/split_dim, DMT/_121, DMT/_122, cell_stem_0/comb_iter_3/combine/add:1, DMT/_123, DMT/_124)]]


During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-6-c66df78b6d6a> in <module>()
----> 1 retrain.main(None)

/usr/local/src/tensorflow/tensorflow/tensorflow/examples/image_retraining/retrain.py in main(_)
   1063                         FLAGS.bottleneck_dir, jpeg_data_tensor,
   1064                         decoded_image_tensor, resized_image_tensor,
-> 1065                         bottleneck_tensor, FLAGS.architecture)
   1066 
   1067     # Add the new layer that we'll be training.

/usr/local/src/tensorflow/tensorflow/tensorflow/examples/image_retraining/retrain.py in cache_bottlenecks(sess, image_lists, image_dir, bottleneck_dir, jpeg_data_tensor, decoded_image_tensor, resized_input_tensor, bottleneck_tensor, architecture)
    486             sess, image_lists, label_name, index, image_dir, category,
    487             bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,
--> 488             resized_input_tensor, bottleneck_tensor, architecture)
    489 
    490         how_many_bottlenecks += 1

/usr/local/src/tensorflow/tensorflow/tensorflow/examples/image_retraining/retrain.py in get_or_create_bottleneck(sess, image_lists, label_name, index, image_dir, category, bottleneck_dir, jpeg_data_tensor, decoded_image_tensor, resized_input_tensor, bottleneck_tensor, architecture)
    428                            image_dir, category, sess, jpeg_data_tensor,
    429                            decoded_image_tensor, resized_input_tensor,
--> 430                            bottleneck_tensor)
    431   with open(bottleneck_path, 'r') as bottleneck_file:
    432     bottleneck_string = bottleneck_file.read()

/usr/local/src/tensorflow/tensorflow/tensorflow/examples/image_retraining/retrain.py in create_bottleneck_file(bottleneck_path, image_lists, label_name, index, image_dir, category, sess, jpeg_data_tensor, decoded_image_tensor, resized_input_tensor, bottleneck_tensor)
    383   except Exception as e:
    384     raise RuntimeError('Error during processing file %s (%s)' % (image_path,
--> 385                                                                  str(e)))
    386   bottleneck_string = ','.join(str(x) for x in bottleneck_values)
    387   with open(bottleneck_path, 'w') as bottleneck_file:

RuntimeError: Error during processing file ./data/processed/JPG/Train/Iceberg/1475-e8b76fb7.jpg (ConcatOp : Dimensions of inputs should match: shape[0] = [1,75,75,42] vs. shape[2] = [1,42,75,75]
	 [[Node: cell_stem_0/cell_output/concat = _MklConcatV2[N=4, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](cell_stem_0/comb_iter_1/combine/add, cell_stem_0/comb_iter_2/combine/add, cell_stem_0/comb_iter_3/combine/add, cell_stem_0/comb_iter_4/combine/add, cell_17/split/split_dim, DMT/_121, DMT/_122, cell_stem_0/comb_iter_3/combine/add:1, DMT/_123, DMT/_124)]]

Caused by op 'cell_stem_0/cell_output/concat', defined at:
  File ""/usr/bin/ipython3"", line 11, in <module>
    sys.exit(start_ipython())
  File ""/usr/lib/python3.6/site-packages/IPython/__init__.py"", line 125, in start_ipython
    return launch_new_instance(argv=argv, **kwargs)
  File ""/usr/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/lib/python3.6/site-packages/IPython/terminal/ipapp.py"", line 356, in start
    self.shell.mainloop()
  File ""/usr/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 480, in mainloop
    self.interact()
  File ""/usr/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 471, in interact
    self.run_cell(code, store_history=True)
  File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2856, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-6-c66df78b6d6a>"", line 1, in <module>
    retrain.main(None)
  File ""/usr/local/src/tensorflow/tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 1024, in main
    create_model_graph(model_info))
  File ""/usr/local/src/tensorflow/tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 291, in create_model_graph
    model_info['resized_input_tensor_name'],
  File ""/home/rick/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/home/rick/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 548, in import_graph_def
    op_def=op_def)
  File ""/home/rick/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/home/rick/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1617, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [1,75,75,42] vs. shape[2] = [1,42,75,75]
	 [[Node: cell_stem_0/cell_output/concat = _MklConcatV2[N=4, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](cell_stem_0/comb_iter_1/combine/add, cell_stem_0/comb_iter_2/combine/add, cell_stem_0/comb_iter_3/combine/add, cell_stem_0/comb_iter_4/combine/add, cell_17/split/split_dim, DMT/_121, DMT/_122, cell_stem_0/comb_iter_3/combine/add:1, DMT/_123, DMT/_124)]]
)

```
",2,,[],2018-01-11 03:24:24,open,,,"['stat:awaiting tensorflower', 'type:bug/performance']",2018-01-18 09:52:00
1214,tensorflow/models,models,3142,fyang26,Force the type of height and width to be int in visualization_utils.py for detection,"Since the functionality to use floating point indices was deprecated in numpy>1.11.0, a TypeError ""'numpy.float64' object cannot be interpreted as an index"" may occur when calling `add_cdf_image_summary` function. It happens when using numpy>=1.12.0, where `height` and `width` can be floating point numbers which makes `np.reshape` to fail. Forcing `height` and `width` to be integer can make the code more robust.",3,,[],2018-01-11 02:22:59,open,,,['cla: yes'],2018-01-11 03:47:19
1215,tensorflow/models,models,3136,gustavz,Low GPU and CPU Usage while Inference / realtime detection,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4 with GPU
- **Bazel version (if compiling from source)**: newest
- **CUDA/cuDNN version**: CUDA 9 / cuDNN 7
- **GPU model and memory**: 
Laptop: GeForce GTX 1050 4GB 
Jetson Tx2: Tegra 8GB
- **Exact command to reproduce**:
clone my repo https://github.com/GustavZ/realtime_object_detection
and run object_detection.py


### Describe the problem
I am using the SSD Mobilenet for realtime inference with a Webcam as Input using OpenCV and i get following Performance:
Laptop: ~25 fps at ~40% GPU and ~25% CPU Usage 
Jetson: ~5 fps at ~5-10% GPU and 10-40% CPU Usgae


Any hints why the Object Detection API is so slow on Inference. 
Training may be easy and fast ok, but inference / really using the models for realtime object detection is very slow and does not use full GPU.
(For comparison YOLO with darknet runs at 90-100% GPU Usage with 3x higher fps)

Here is a screenshot what nvidia-smi and top give me while inferencing on the laptop
![screenshot from 2018-01-10 15-40-12](https://user-images.githubusercontent.com/29252883/34778233-08f7ec52-f61d-11e7-85a0-3e94541b439a.png)
",8,,[],2018-01-10 14:24:39,open,,,['stat:awaiting tensorflower'],2019-03-25 06:31:21
1216,tensorflow/models,models,3132,kolesman,object_detection A problem (bug?) when training a detection model in the multiclass setting,"### System information
- **What is the top-level directory of the model you are using**: research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 4.9.65
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**:  8.0 / 6
- **GPU model and memory**: P100 16GB
- **Exact command to reproduce**: N/A

### Describe the problem
I use the object detection API to fine-tune the faster-rcnn detection model on
my custom dataset. The dataset often has multiple identical object boxes with
different labels. Thus, I use sigmoid score converter, sigmoid classification
loss and enable ""merge_multiple_label_boxes: true"" in the config file. Unfortunately,
I get very poor model in the end. The box labels are generally correct, but their locations
and confidences are poor.

I suspect that there is either something wrong with my configuration file or there is
a bug in how multiclass detection datasets are handled.

I can reliably reproduce the wrong behavior by enabling the option ""merge_multiple_label_boxes: true""
for multi-class datasets. In other words, the problem appears when a *k-hot label encoding* is used.

I verified that, when using the sigmoid score converter and the sigmoid
loss, a detection model works fine with 1-hot label encoding (i.e. unique labels per box).
Thus, I presume that k-hot encoding breaks the detection pipeline, but I am
not able to find the exact reason.

In the following I describe a minimal example for reproducing the error.

### Source code / logs
I've created a fork with minimal changes, which allows to reproduce the problem: https://github.com/kolesman/models

Specifically, I implement the following changes:

1. Modify the *create_pet_tf_record.py* to produce multiclass dataset.
    A box labeled by a class $c$ is populated by labels $c + 1$ and $c + 2$.

2. Modify the sample config file faster_rcnn_resnet101_pets.config by
      -> enabling sigmoid score converter
      -> enabling sigmoid loss
      -> enabling ""merge_multiple_label_boxes: true""

Otherwise, I closely follow the repository instructions, use mscoco pretrained model and
employ train.py and eval.py scripts:
```bash

# from research/
python object_detection/dataset_tools/create_pet_tf_record.py --label_map_path=object_detection/data/pet_label_map.pbtxt --data_dir=`pwd` --output_dir=`pwd`

# from research/object_detection
python train.py --logtostderr --pipeline_config_path=samples/configs/faster_rcnn_resnet101_pets.config --train_dir=train_dir

# from research/object_detection
python eval.py --logtostderr --pipeline_config_path=samples/configs/faster_rcnn_resnet101_pets.config --checkpoint_dir=train_dir/ --eval_dir=eval_dir

```

The resulting detection outputs after more then 10.000 iterations of training are attached here.
![multiclass](https://user-images.githubusercontent.com/460828/34731442-a28e50e2-f562-11e7-8838-15fb963ac735.png)

The labels are generally correct, but the boxes and their confidences are very poor.
Interestingly, quality of boxes degrades over the course of training.

",4,,[],2018-01-09 18:28:34,open,,,['stat:awaiting tensorflower'],2018-01-12 10:04:46
1217,tensorflow/models,models,3131,puma007,Error running Object Detection training in google ML engine - grpc epoll fd: 3,"I'm trying to train Object Detection model with gcloud ml-engine,reference to the official documents [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_cloud.md](url)，and set runtime-version=1.4，and reference this issue [https://github.com/tensorflow/models/issues/2739](url)  to modify the setup.py , but have the error:  

> worker-replica-3
2018-01-09 06:32:39.416080: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX

> ker-replica-3
grpc epoll fd: 3

> {
 insertId:  ""1fwigqcg5k37j2o""  
 jsonPayload: {
  created:  1515479559.41658   
  levelname:  ""ERROR""   
  lineno:  1051   
  message:  ""    grpc epoll fd: 3""   
  pathname:  ""ev_epoll1_linux.c""   
  thread:  917   
 }

The last error message is：
> The replica master 0 ran out-of-memory and exited with a non-zero status of 247.

I start the training job on Cloud ML Engine using the following command:  

> gcloud ml-engine jobs submit training object_detection_training_`date +%s` \\
    --job-dir=gs://mybucket/train \\
    --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz \\
    --module-name object_detection.train \\
    --region asia-east1 \\
    --config object_detection/samples/cloud/cloud.yml \\
    -- \\
    --train_dir=gs://mybucket/train \\
    --pipeline_config_path=gs://mybucket/data/ssd_mobilenet_v1_coco.config \\
    --runtime-version 1.4



  
  
  ",7,,[],2018-01-09 06:58:19,open,,,[],2018-04-06 07:46:37
1218,tensorflow/models,models,3120,grzegorznowak,Drop a dummy variable,Let's not create a `raw_data` variable just for sake of creating an extra variable.,4,,[],2018-01-07 21:41:36,open,,,['cla: yes'],2018-01-08 05:53:10
1219,tensorflow/models,models,3119,sangeet259,adds second_stage_batch_size to faster_rcnn_nas_coco.config,"The parameter `second_stage_batch_size` is required in configuration files but is missing.
A Value error is being raised as a result of which.
Fixes #2668
Thanks to @mattryles for the solution.",4,,[],2018-01-07 06:39:36,open,,,['cla: yes'],2018-01-07 06:42:07
1220,tensorflow/models,models,3116,oneTimePad,Object Detection API inference on Tegra GPU,"I have installed Tensorflow on the Nvidia Jetson TX2, and I am trying to run the object detection inference tutorial on it.

I have added swap space, so the total memory available is 16Gb. I read that the GPU and CPU share memory on the Jetson TX2. When running the tutorial all of my ram is used up, but the swap space isn't touched. However, I receive the following error stating that the there are are too many resources requested to launch. However, I am not running out of memory. 

I read a similar issue here ( https://github.com/tensorflow/serving/issues/627). While the same error is observed, I am not sure it is relevant to this situation.

```
2018-01-06 19:17:57.586328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:857] ARM64 does not support NUMA - returning NUMA node zero
2018-01-06 19:17:57.586447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: NVIDIA Tegra X2
major: 6 minor: 2 memoryClockRate (GHz) 1.3005
pciBusID 0000:00:00.0
Total memory: 7.67GiB
Free memory: 4.15GiB
2018-01-06 19:17:57.586494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-01-06 19:17:57.586517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-01-06 19:17:57.586543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0)
> /home/nvidia/object_detection_inference.py(77)<module>()
-> scores = sess.run([detection_scores],feed_dict={image_tensor: image_np_expanded})
(Pdb) n
2018-01-06 19:18:39.591828: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.83GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-01-06 19:18:43.355772: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
2018-01-06 19:18:43.357016: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.357145: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.357280: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.360367: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
2018-01-06 19:18:43.361503: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.361551: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.361575: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.361521: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362400: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362416: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362513: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362520: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362572: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362607: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362666: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362667: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362752: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362792: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362683: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362853: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362974: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.362991: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.363476: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.363499: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.363593: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.364079: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.364342: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.364436: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.364517: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.364874: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.364971: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.365039: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.365304: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.365384: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.366533: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.367183: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
2018-01-06 19:18:43.367239: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.367358: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.367361: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.367614: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.367927: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.368029: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.368488: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
2018-01-06 19:18:43.368615: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
tensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_9/Gather/Gather_2/_141 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3737_SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_9/Gather/Gather_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](^_cloopSecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_6)]]

Caused by op 'SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where', defined at:
  File ""object_detection_inference.py"", line 32, in <module>
    tf.import_graph_def(od_graph_def, name='')
  File ""/home/nvidia/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""/home/nvidia/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/nvidia/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InternalError (see above for traceback): WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: too many resources requested for launch
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Where = Where[_device=""/job:localhost/replica:0/task:0/gpu:0""](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_11/Greater)]]
	 [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_9/Gather/Gather_2/_141 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3737_SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_9/Gather/Gather_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](^_cloopSecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_6)]]
```
",13,,[],2018-01-06 19:32:16,open,,,[],2018-05-17 07:34:05
1221,tensorflow/models,models,3107,KomodoFactory,Created missing __init__.py,,1,,[],2018-01-04 15:45:15,open,,,['cla: no'],2018-01-04 15:45:18
1222,tensorflow/models,models,3105,zsz00,fix issue:Evaluation in Object Detection hanging #2225,fix issue:Evaluation in Object Detection hanging #2225,1,,[],2018-01-04 11:52:56,open,,,['cla: no'],2018-01-04 11:52:59
1223,tensorflow/models,models,3102,zsz02,fix issue:Evaluation in Object Detection hanging #2225,fix issue: Evaluation in Object Detection hanging #2225,1,,[],2018-01-04 10:51:00,open,,,['cla: no'],2018-01-04 10:51:04
1224,tensorflow/models,models,3097,Queequeg92,Update to New APIs,,0,,[],2018-01-03 06:16:31,open,,,['cla: yes'],2018-01-10 18:41:24
1225,tensorflow/models,models,3094,DecentGradient,savedModel builder im2txt,"@cshallue
@jhseu 
@sukritiramesh 
Can we have a savedModel builder script for the im2txt model?
It would not only directly benefit the users of this model,
it would be another example of how saved model works and there are precious few.

Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**: research
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: fedora 27
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:n/a
- **GPU model and memory**:n/a
- **Exact command to reproduce**:n/a

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""


### Describe the problem
@cshallue
@jhseu 
@sukritiramesh 
Can we have a savedModel builder script for the im2txt model?
It would not only directly benefit the users of this model,
it would be another example of how saved model works and there are precious few.

### Source code / logs
N/a
",0,,[],2018-01-02 23:32:32,open,,,"['stat:awaiting tensorflower', 'type:feature']",2018-01-08 15:43:44
1226,tensorflow/models,models,3092,Aldream,"Domain Adaptation: ""AttributeError: 'filter' object has no attribute 'op'"" in tensorflow/python/training/optimizer","### System information

- **Model**: `/research/domain_adaptation`
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow version**: v1.1.0-rc0-61-g1ec6ed5 1.1.0
- **CUDA/cuDNN version**: 8.0
- **Exact command to reproduce**: 
```
cd [your_path_to]/models/research/domain_adaptation
python pixel_domain_adaptation/pixelda_train.py -- --dataset_dir $DSN_DATA_DIR --source_dataset mnist --target_dataset mnist_m
```

### Problem
Training of the model crashes, with an error back from the Tensorflow library itself.

Note: I am not using Bazel since the building was crashing with errors similar to #2542.

### Logs

```
...
INFO:tensorflow:Trainable variables for scope: <filter object at 0x7fcb7ede20b8>
WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops in GraphKeys.UPDATE_OPS
Traceback (most recent call last):
  File ""pixel_domain_adaptation/pixelda_train.py"", line 409, in <module>
    tf.app.run()
  File ""/home/username/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""pixel_domain_adaptation/pixelda_train.py"", line 405, in main
    hparams=hparams)
  File ""pixel_domain_adaptation/pixelda_train.py"", line 332, in run_training
    summarize_gradients=FLAGS.summarize_gradients)
  File ""/home/username/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 436, in create_train_op
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/username/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/contrib/training/python/training/training.py"", line 437, in create_train_op
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/username/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 378, in compute_gradients
    processors = [_get_processor(v) for v in var_list]
  File ""/home/username/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 378, in <listcomp>
    processors = [_get_processor(v) for v in var_list]
  File ""/home/username/anaconda3/envs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 153, in _get_processor
    if v.op.type == ""VarHandleOp"":
AttributeError: 'filter' object has no attribute 'op'
```
",7,,[],2018-01-02 18:46:07,open,,,['stat:awaiting tensorflower'],2018-12-20 02:57:57
1227,tensorflow/models,models,3091,kannan60,using faster_rcnn_resnet101_coco model for training,"I used mobilenet model to train my images. It worked fine. In order to increase the accuracy I tried to replicate the same steps using a faster_rcnn_resnet101_coco model instead. All the steps I used were the same. When I initiate the training session, it got started and ran about 800 steps. The training loss at this point was around 0.5 which seems too good to be true. It stopped at this step and threw the following error:

`The replica worker 1 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""main"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 163, in tf.app.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run _sys.exit(main(_sys.argv[:1] + flags_passthrough)) File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 159, in main worker_job_name, is_chief, FLAGS.train_dir) File ""/root/.local/lib/python2.7/site-packages/object_detection/trainer.py"", line 332, in train saver=saver) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 763, in train sess, train_op, global_step, train_step_kwargs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 487, in train_step run_metadata=run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 889, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1120, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1317, in _do_run options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1336, in _do_call raise type(e)(node_def, op, message) UnavailableError: Endpoint read failed To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=341450659208&resource=ml_job%2Fjob_id%2Fobject_detection_188003&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22object_detection_188003%22
`
I tried initiating the jobs again from scratch. It unfortunately threw the same error. 

Any idea what the problem could be? Any help is much appreciated.",3,,[],2018-01-02 13:38:38,open,,,[],2018-04-06 07:48:05
1228,tensorflow/models,models,3081,blaskowitz100,Object detection: PR-Curves in Tensorboard,"Hello,
is  it planned to plot PR-Curves as separate summaries in Tensorboard? When not, this would be a nice feature :) ",21,,[],2017-12-29 14:45:08,open,,,['stat:awaiting response'],2019-04-09 19:50:14
1229,tensorflow/models,models,3079,ppwwyyxx,fix typo,,2,,[],2017-12-28 19:39:28,open,,,['cla: yes'],2018-07-13 05:28:06
1230,tensorflow/models,models,3078,luizcarlos01,import name 'label_map_util',"Traceback (most recent call last):
  File ""C:\Users\Luiz\Documents\object_detection_tutorial.py"", line 37, in <module>
    from utils import label_map_util
ImportError: cannot import name 'label_map_util'

-----------------------------------------------------------------------------------------------------------
CODE:


# coding: utf-8

# # Object Detection Demo
# Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start.

# # Imports

# In[ ]:

import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile
import cv2


from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

cap = cv2.VideoCapture(0)

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("".."")


# ## Object detection imports
# Here are the imports from the object detection module.

# In[ ]:

from utils import label_map_util

from utils import visualization_utils as vis_util


# # Model preparation 

# ## Variables
# 
# Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_CKPT` to point to a new .pb file.  
# 
# By default we use an ""SSD with Mobilenet"" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.

# In[ ]:

# What model to download.
MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'
MODEL_FILE = MODEL_NAME + '.tar.gz'
DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'

# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')

NUM_CLASSES = 90


# ## Download Model

# In[ ]:

opener = urllib.request.URLopener()
opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)
tar_file = tarfile.open(MODEL_FILE)
for file in tar_file.getmembers():
  file_name = os.path.basename(file.name)
  if 'frozen_inference_graph.pb' in file_name:
    tar_file.extract(file, os.getcwd())


# ## Load a (frozen) Tensorflow model into memory.

# In[ ]:

detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')


# ## Loading label map
# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine

# In[ ]:

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)


# ## Helper code

# In[ ]:

def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)


# # Detection

# In[ ]:

# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)


# In[ ]:

with detection_graph.as_default():
  with tf.Session(graph=detection_graph) as sess:
    # Definite input and output Tensors for detection_graph
    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
    # Each box represents a part of the image where a particular object was detected.
    detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
    # Each score represent how level of confidence for each of the objects.
    # Score is shown on the result image, together with the class label.
    detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
    detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
    num_detections = detection_graph.get_tensor_by_name('num_detections:0')
    while True:
      ret, image_np = cap.read()
      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
      image_np_expanded = np.expand_dims(image_np, axis=0)
      # Actual detection.
      (boxes, scores, classes, num) = sess.run(
          [detection_boxes, detection_scores, detection_classes, num_detections],
          feed_dict={image_tensor: image_np_expanded})
      # Visualization of the results of a detection.
      vis_util.visualize_boxes_and_labels_on_image_array(
          image_np,
          np.squeeze(boxes),
          np.squeeze(classes).astype(np.int32),
          np.squeeze(scores),
          category_index,
          use_normalized_coordinates=True,
          line_thickness=8)
      cv2.imshow('Obj detection',cv2.resize(image_np,(800,400)))
      if cv2.waitKey(25)& 0xFF == ord('q'):
        cv2.destroyAllWindows()
        cap.release()
        break


",3,"NamedUser(login=""hgadig"")","[NamedUser(login=""hgadig"")]",2017-12-28 18:17:37,open,,"NamedUser(login=""hgadig"")","['stat:awaiting response', 'type:support']",2018-11-10 02:15:14
1231,tensorflow/models,models,3074,Abhijit-2592,Object-detection: Feature request- visualize layer activations and step by step visualization for Faster RCNN models,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


**This is a feature request !**

While training Faster RCNN it is worth while to look at the RPN's class agnostic proposals to know  whether correct ROIs go into the ROI pooling layers. This visualization can be made in a rather indirect way by exploiting the first stage only = True option in config (after training with first stage only = False ) and using the export_inference_graph.py  after fixing #1916 .
It would be beneficial if it can be done in an easier way. (or an easier way is already present and am missing it?)

One more visualization that will be useful for debugging is: visualizing **layer-activations** in tensorboard. 

Thanks",3,,[],2017-12-28 08:01:15,open,,,['stat:contributions welcome'],2018-11-12 08:47:23
1232,tensorflow/models,models,3069,SaintNazaire,Syntaxnet dockerfile error: tensorflow/tools/ci_build/builds/configured CPU /usr/bin/env: 'bash\r': No such file or directory,"**System information**

- Windows 10 professional
- Docker image based on Ubuntu:16.10
- Docker 17.09.1-ce-win42
- Python version 2.7
- CUDA/cuDNN version not relevant (CPU install)
- GPU model and memory not relevant (CPU install)

**Command triggering issue**

`RUN cd $SYNTAXNETDIR/syntaxnet/tensorflow     && tensorflow/tools/ci_build/builds/configured CPU     && cd $SYNTAXNETDIR/syntaxnet     && bazel build -c opt @org_tensorflow//tensorflow:tensorflow_py`

**Top-level directory of the model you are using**

> ENV SYNTAXNETDIR=/opt/tensorflow PATH=$PATH:/root/bin
> $SYNTAXNETDIR/syntaxnet/tensorflow

**Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**

No, followed exact online dockerfile from exact online tensorflow/models clone (including tensorflow)

**Describe the problem**

Running exact dockerfile from local tensorflow/models clone, upon reaching configured cpu command crashes. Attempted to fix it from within the dockerfile, configured file does exists, tried to add .sh extension to no avail. Seems this is symlink error because file does exists and directory path is correct

**Error logs**

> Step 10/20 : RUN cd $SYNTAXNETDIR/syntaxnet/tensorflow     && tensorflow/tools/ci_build/builds/configured CPU     && cd $SYNTAXNETDIR/syntaxnet     && bazel build -c opt @org_tensorflow//tensorflow:tensorflow_py
>  ---> Running in 68c7e0ab9cd1
> /usr/bin/env: 'bash\r': No such file or directory
> The command '/bin/sh -c cd $SYNTAXNETDIR/syntaxnet/tensorflow     && tensorflow/tools/ci_build/builds/configured CPU     && cd $SYNTAXNETDIR/syntaxnet     && bazel build -c opt @org_tensorflow//tensorflow:tensorflow_py' returned a non-zero code: 127

**Conclusion**

5 minutes install for Syntaxnet - fails
Build install for Syntaxnet - fails
Dockerfile - fails",3,,[],2017-12-27 13:29:36,open,,,"['stat:awaiting tensorflower', 'type:support']",2018-01-11 14:53:05
1233,tensorflow/models,models,3068,SergeyBykov1,Object detection. No cropping in evaluation config results in wrong validation,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary for anaconda python
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 6.0
- **GPU model and memory**: gtx1060 6gb
- **Exact command to reproduce**:

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I train the ssd mobilenet v1 model on the kitti dataset.
I want the model input to be 640x360 (aspect ratio 16:9 ~ 1.7778)
```
    image_resizer {
      fixed_shape_resizer {
        height: 360
        width: 640
      }
    }
```
Kitti dataset image resolution is 1242x375 (aspect ratio 3.312)

I use a following preprocessing in train_config to use subimage with standard aspect ratio.
```
  data_augmentation_options {
    random_crop_to_aspect_ratio {
      aspect_ratio: 1.7778
      overlap_thresh: 0.8
    }
  }
```

So the training images are first cropped to 667x375 and are then resized to 640x360, resulting in normal training picture.

But there is no such preprocessing during the evaluation step, which means that validation images 1242x375 are just resized to 640x360, which results in images with squeezed road objects.

Because of that I get poor accuracy on the validation accuracy chart in tensorboard, but the model actually performs well during the manual testing, when I feed it with 16:9 aspect ratio images.

Is there a way to reproduce the same cropping during the validation step?",1,,[],2017-12-27 12:31:18,open,,,"['stat:awaiting tensorflower', 'type:bug/performance']",2017-12-30 09:23:22
1234,tensorflow/models,models,3061,plieningerweb,add log info about preprocess data augmentation,"it is easy to make mistakes in protobuf config and therefore mistakenly think, that augmentation was used but acutally was never applied to training. Logging will prevent this mistake more easily. 

Root cause:
In protobuf config, data_augmentation_options can only contain one option per item, but if more, only last one silently applied, all others are ignored.",6,,[],2017-12-26 18:37:26,open,,,['cla: yes'],2018-01-03 16:04:39
1235,tensorflow/models,models,3049,SaintNazaire,Installing Syntaxtnet (5mn version) fails - libstdc++.so.6: version `GLIBCXX_3.4.22' not found,"**System information**

- Windows 10 professional
- Docker image based on Ubuntu 16.04.3 LTS
- Docker 17.09.1-ce-win42
- Python version 2.7
- CUDA/cuDNN version not relevant (CPU install)
- GPU model and memory not relevant (CPU install)

**Command triggering issue**

`python -c 'import dragnn.python.load_dragnn_cc_impl, syntaxnet.load_parser_ops'`

**Top-level directory of the model you are using**

> /models/research/syntaxnet

**Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**

No, followed exact online instructions without triggering any error up to checking install

**Describe the problem**

Issuing the following command
`python -c 'import dragnn.python.load_dragnn_cc_impl, syntaxnet.load_parser_ops'`
Triggers libstdc++.so.6: version `GLIBCXX_3.4.22' not found error which is referenced several times online but a long time ago.

**Error logs**

> Traceback (most recent call last):
>   File ""<string>"", line 1, in <module>
>   File ""/usr/local/lib/python2.7/dist-packages/dragnn/python/load_dragnn_cc_impl.py"", line 22, in <module>
>     os.path.join(tf.resource_loader.get_data_files_path(), 'dragnn_cc_impl.so'))
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
>     lib_handle = py_tf.TF_LoadLibrary(library_filename, status)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.NotFoundError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /usr/local/lib/python2.7/dist-packages/dragnn/python/dragnn_cc_impl.so)",9,,[],2017-12-23 14:54:49,open,,,[],2018-04-06 07:46:28
1236,tensorflow/models,models,3048,Amitayus,feature request: register intermediate endpoints inside slim.repeat,"I want to use many outputs generated by `silm.repeat`, but I cannot get it directly. I can only rewrite it as for-loop now. Then the code becomes ugly.",1,,[],2017-12-23 02:03:37,open,,,['stat:awaiting tensorflower'],2018-01-03 01:30:40
1237,tensorflow/models,models,3042,euntaik,[Mobilenet-v1] Tensorflow Lite support,"Squeeze Operation is not supported yet in the TFLite.
This patch replaces the squeeze operation to a Reshape operation.",1,,[],2017-12-21 10:01:42,open,,,['cla: yes'],2017-12-26 05:34:16
1238,tensorflow/models,models,3033,vikramg1,"Object Detection Tutorial Jupyter Notebook checks for version equality, making non versions > 1.4.0 fail","### System information
- **What is the top-level directory of the model you are using**:
models/research/object_detection

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.03

- **TensorFlow installed from (source or binary)**:
binary, virtual env

- **TensorFlow version (use command below)**:
1.4.1

- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
6
- **GPU model and memory**:
Titan X
- **Exact command to reproduce**:
models/research/object_detection$jupyter notebook
choose object_detection_tutorial.ipynb in the brower
press SHIFT+ENTER in the first box

### Describe the problem
#version check below:
if tf.__version__ != '1.4.0':

#fails as version is 1.4.1
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-3-a27dab2761c6> in <module>()
     14 #if tf.__version__ != '1.4.0':
     15 if tf.__version__ != '1.4.0':
---> 16   raise ImportError('Please upgrade your tensorflow installation to v1.4.0!')

ImportError: Please upgrade your tensorflow installation to v1.4.0!

Suggested code change:
[reference: https://stackoverflow.com/questions/11887762/compare-version-strings-in-python]

from distutils.version import LooseVersion
if LooseVersion(tf.__version__) < LooseVersion('1.4.0'):
",1,,[],2017-12-20 20:41:35,open,,,['stat:awaiting tensorflower'],2017-12-28 19:02:02
1239,tensorflow/models,models,3026,mattryles,Feature Request - more than 100 detections,"Hi,

When running the inference the output is only ever 100 num_detections. 

Would it be possible to make the max configurable?

Edit - If it's not possible, as a workarounds would either of these be possible:
1.) Split my label map into 4 and run the inference 4 times with each part of the quarter?
2.) Train 4 separate models with a quarter of the object classes each?",4,,[],2017-12-19 13:17:20,open,,,[],2018-04-06 07:47:52
1240,tensorflow/models,models,3022,TartySG,Setting feature_columns vocabulary list default values is counter-intuitive,"Not Code-Breaking, but definitely a nice QoL :

While using tf.feature_column.categorical_column_with_vocabulary_list, e.g.

```
categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), default_value=0)
```

it makes little sense to set the default value as the integer of the position of the default value in the list. There should be an option that allow the user to set the value as a string instead, like this example :

```
categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), default_value='X')
```

Especially when you set up the code to extract the vocabulary list from the data itself, you often have no idea what number is going to be assigned to your default value. e.g. I know that the default should be ""black"", but since my data may change, I can't be sure where in the list ""black"" is going to be.

Please consider this upgrade.

Regards,

Simon

@chrish42",2,"NamedUser(login=""rohan100jain"")","[NamedUser(login=""rohan100jain"")]",2017-12-18 22:12:31,open,,,['stat:contributions welcome'],2018-09-28 00:50:25
1241,tensorflow/models,models,3018,frederictost,Enhance position of labels on processed image.,To answer issue #2771 ,2,,[],2017-12-17 21:13:21,open,,,['cla: yes'],2019-03-23 10:06:05
1242,tensorflow/models,models,3008,anj-s,Keras Resnet Model for the model garden (not intended to be merged),"Added a Keras Resnet model that works with tf.Dataset
",0,"NamedUser(login=""anj-s"")","[NamedUser(login=""anj-s"")]",2017-12-14 21:07:44,open,,,['cla: yes'],2017-12-15 22:35:18
1243,tensorflow/models,models,3005,scotthuang1989,Add a flag so we can choose if go with fast pre-processing or aggressive pre-processing,"1. add a flag so we can choose if go with fast preprocessing or aggressive preprocessing
2. update .gitignore. so the python file generate from proto file will be ignored by git.",1,,[],2017-12-14 11:43:33,open,,,['cla: yes'],2017-12-14 11:43:36
1244,tensorflow/models,models,3003,skyw,Object Detection - coco_metrics is not defined,"pipeline.config in faster_rcnn_resnet101_kitti_2017_11_08 defines eval metric ""coco_metric"". But it is not defined in evaluator.py, only the following 3 are defined

EVAL_METRICS_CLASS_DICT = {
    'pascal_voc_metrics':
        object_detection_evaluation.PascalDetectionEvaluator,
    'weighted_pascal_voc_metrics':
        object_detection_evaluation.WeightedPascalDetectionEvaluator,
    'open_images_metrics':
        object_detection_evaluation.OpenImagesDetectionEvaluator,
}",8,,[],2017-12-14 01:13:54,open,,,"['models: research', 'stat:awaiting response', 'type:support']",2019-04-02 14:43:26
1245,tensorflow/models,models,3000,kerolos,Global step time is fluctuating randomly between steps.,"# Describe the problem:

I used this model ""ssd_mobilenet_v1_coco_11_06_2017"" to be able to classify and localize my own ""3 objects"". My problem is that the global step takes sometimes between 0.5 sec/step to 2 sec /step and sometimes takes 400sec/step to 1200 sec/ step. the computer finished only 700 steps within 20 Hours, which is really weird.
Also, something I realized, 97% of the Ram is occupying when Recording summary step and the following global step occurred.

# System information:

OS: Ubuntu desktop 14.04. 
Ram: 16 GB.
Nvidia: GTX 1080  TI 11 GB ram.
Tensorflow: v1.4.0-rc1-11-g130a514.
batch-size: 10 .
CUDA/cuDNN version: Cuda 8.0 and cudnn 6 .

# Output of training processes:

INFO:tensorflow:Recording summary at step 664.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Recording summary at step 665.
INFO:tensorflow:global step 665: loss = 1.4688 (646.632 sec/step)
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global step 666: loss = 1.7181 (195.437 sec/step)
INFO:tensorflow:global step 667: loss = 2.0240 (2.869 sec/step)
INFO:tensorflow:global step 668: loss = 2.0866 (14.212 sec/step)
INFO:tensorflow:global step 669: loss = 2.1031 (2.734 sec/step)
INFO:tensorflow:global step 670: loss = 1.5433 (2.310 sec/step)
INFO:tensorflow:global step 671: loss = 2.0120 (2.487 sec/step)
INFO:tensorflow:global step 672: loss = 2.0841 (1.780 sec/step)
INFO:tensorflow:Recording summary at step 672.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global step 673: loss = 1.3150 (248.055 sec/step)
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Recording summary at step 674.
INFO:tensorflow:global step 674: loss = 1.4747 (245.788 sec/step)
INFO:tensorflow:Recording summary at step 674.
INFO:tensorflow:global step 675: loss = 2.3231 (244.705 sec/step)
INFO:tensorflow:global step 676: loss = 2.8910 (10.194 sec/step)
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global step 677: loss = 1.3781 (55.264 sec/step)
INFO:tensorflow:global step 678: loss = 1.7793 (34.336 sec/step)
INFO:tensorflow:global step 679: loss = 1.4643 (18.670 sec/step)
INFO:tensorflow:global step 680: loss = 1.7935 (1.683 sec/step)
INFO:tensorflow:global step 681: loss = 2.5205 (1.933 sec/step)
INFO:tensorflow:global step 682: loss = 1.9633 (2.103 sec/step)
INFO:tensorflow:global step 683: loss = 1.4078 (0.795 sec/step)
INFO:tensorflow:global step 684: loss = 1.8395 (1.746 sec/step)
INFO:tensorflow:global step 685: loss = 1.5991 (1.230 sec/step)
INFO:tensorflow:Recording summary at step 685.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Recording summary at step 686.
INFO:tensorflow:global step 686: loss = 1.6328 (1321.527 sec/step)
INFO:tensorflow:Recording summary at step 686.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Recording summary at step 686.
INFO:tensorflow:Recording summary at step 687.
INFO:tensorflow:global step 687: loss = 1.8285 (478.037 sec/step)
INFO:tensorflow:Recording summary at step 687.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global step 688: loss = 2.2072 (46.042 sec/step)
INFO:tensorflow:global step 689: loss = 1.7111 (17.323 sec/step)
INFO:tensorflow:global step 690: loss = 1.7003 (1.249 sec/step)
INFO:tensorflow:global step 691: loss = 1.7168 (2.057 sec/step)
INFO:tensorflow:global step 692: loss = 2.2428 (1.512 sec/step)
INFO:tensorflow:global step 693: loss = 2.1942 (1.331 sec/step)
INFO:tensorflow:global step 694: loss = 1.6748 (0.981 sec/step)
INFO:tensorflow:global step 695: loss = 1.8890 (1.642 sec/step)
INFO:tensorflow:global step 696: loss = 2.7845 (1.410 sec/step)
INFO:tensorflow:global step 697: loss = 1.9228 (1.421 sec/step)
INFO:tensorflow:Recording summary at step 697.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Recording summary at step 698.
INFO:tensorflow:global step 698: loss = 1.5280 (1297.641 sec/step)
INFO:tensorflowINFO:tensorflow:Recording summary at step 664.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Recording summary at step 665.
INFO:tensorflow:global step 665: loss = 1.4688 (646.632 sec/step)
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global step 666: loss = 1.7181 (195.437 sec/step)
INFO:tensorflow:global step 667: loss = 2.0240 (2.869 sec/step)
INFO:tensorflow:global step 668: loss = 2.0866 (14.212 sec/step)
INFO:tensorflow:global step 669: loss = 2.1031 (2.734 sec/step)
INFO:tensorflow:global step 670: loss = 1.5433 (2.310 sec/step)
INFO:tensorflow:global step 671: loss = 2.0120 (2.487 sec/step)
INFO:tensorflow:global step 672: loss = 2.0841 (1.780 sec/step)
INFO:tensorflow:Recording summary at step 672.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global step 673: loss = 1.3150 (248.055 sec/step)
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Recording summary at step 674.
INFO:tensorflow:global step 674: loss = 1.4747 (245.788 sec/step)
INFO:tensorflow:Recording summary at step 674.
INFO:tensorflow:global step 675: loss = 2.3231 (244.705 sec/step)
INFO:tensorflow:global step 676: loss = 2.8910 (10.194 sec/step)
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global step 677: loss = 1.3781 (55.264 sec/step)
INFO:tensorflow:global step 678: loss = 1.7793 (34.336 sec/step)
INFO:tensorflow:global step 679: loss = 1.4643 (18.670 sec/step)
INFO:tensorflow:global step 680: loss = 1.7935 (1.683 sec/step)
INFO:tensorflow:global step 681: loss = 2.5205 (1.933 sec/step)
INFO:tensorflow:global step 682: loss = 1.9633 (2.103 sec/step)
INFO:tensorflow:global step 683: loss = 1.4078 (0.795 sec/step)
INFO:tensorflow:global step 684: loss = 1.8395 (1.746 sec/step)
INFO:tensorflow:global step 685: loss = 1.5991 (1.230 sec/step)
INFO:tensorflow:Recording summary at step 685.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Recording summary at step 686.
INFO:tensorflow:global step 686: loss = 1.6328 (1321.527 sec/step)
INFO:tensorflow:Recording summary at step 686.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Recording summary at step 686.
INFO:tensorflow:Recording summary at step 687.
INFO:tensorflow:global step 687: loss = 1.8285 (478.037 sec/step)
INFO:tensorflow:Recording summary at step 687.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global step 688: loss = 2.2072 (46.042 sec/step)
INFO:tensorflow:global step 689: loss = 1.7111 (17.323 sec/step)
INFO:tensorflow:global step 690: loss = 1.7003 (1.249 sec/step)
INFO:tensorflow:global step 691: loss = 1.7168 (2.057 sec/step)
INFO:tensorflow:global step 692: loss = 2.2428 (1.512 sec/step)
INFO:tensorflow:global step 693: loss = 2.1942 (1.331 sec/step)
INFO:tensorflow:global step 694: loss = 1.6748 (0.981 sec/step)
INFO:tensorflow:global step 695: loss = 1.8890 (1.642 sec/step)
INFO:tensorflow:global step 696: loss = 2.7845 (1.410 sec/step)
INFO:tensorflow:global step 697: loss = 1.9228 (1.421 sec/step)
INFO:tensorflow:Recording summary at step 697.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:Recording summary at step 698.
INFO:tensorflow:global step 698: loss = 1.5280 (1297.641 sec/step)
INFO:tensorflow:Recording summary at step 698.
INFO:tensorflow:global step 699: loss = 1.4803 (51.523 sec/step)
INFO:tensorflow:Recording summary at step 699.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global step 700: loss = 1.9215 (416.182 sec/step)
INFO:tensorflow:Recording summary at step 700.

:Recording summary at step 698.
INFO:tensorflow:global step 699: loss = 1.4803 (51.523 sec/step)
INFO:tensorflow:Recording summary at step 699.
INFO:tensorflow:Saving checkpoint to path training/model.ckpt
INFO:tensorflow:global step 700: loss = 1.9215 (416.182 sec/step)
INFO:tensorflow:Recording summary at step 700.
",0,"NamedUser(login=""tombstone"")","[NamedUser(login=""tombstone"")]",2017-12-13 09:21:39,open,,,[],2017-12-19 20:39:37
1246,tensorflow/models,models,2997,Jesus,Uncomments code dependent on `tf.summary.text`,"We can use `tf.summary.text` since [TensorFlow 1.2.0](https://github.com/tensorflow/tensorflow/releases/tag/v1.2.0), 

I found this text very useful during training, so it may help others too.

",5,,[],2017-12-12 21:36:49,open,,,['cla: yes'],2018-08-27 07:59:05
1247,tensorflow/models,models,2990,markroxor,fix #327 nccl not found error,"I spent a lot of time looking for a fix for -
`fatal error: external/nccl_archive/src/nccl.h: No such file or directory `

Documenting the installation of `nvcc` saves a lot of set-up time for beginners.",4,,[],2017-12-11 13:57:42,open,,,['cla: yes'],2017-12-11 14:00:16
1248,tensorflow/models,models,2986,amirjamez,Feature Request: Exporting .meta file of an already pre-trained model (.ckpt) in tf.slim,"Using `TF.slim` and the pre-trained checkpoints I can evaluate the performance of a model. Also, using exporting inference graph I can freeze the model including its weights. However, I would like to find a way to export .meta file of a model as well. How can I do that using only the given the .ckpt file? Say on the inception_v3 case.
I guess something has to be changed in the  [export_inference_graph.py](https://github.com/tensorflow/models/blob/master/research/slim/export_inference_graph.py) file to have this feature as an option.",2,,[],2017-12-11 10:12:41,open,,,['stat:awaiting tensorflower'],2017-12-14 21:39:17
1249,tensorflow/models,models,2980,ghost,what is global step and loss?,"Can some body explain for me about:
1- global step in the output of object detection API:
Is global step the iteration? does it refer to the batch?
and what is the difference of global step/sec and sec/step?

2- Loss:
Whats is exactly loss?
Is loss sum of localization loss and classification loss?
![photo_2017-12-08_02-55-46](https://user-images.githubusercontent.com/34356270/33744768-2689f612-dbc8-11e7-9e14-690ef5782aa9.jpg)
",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-12-08 00:01:10,open,,,['stat:awaiting response'],2018-09-03 14:47:40
1250,tensorflow/models,models,2975,milanfeind,Update __init__.py,,2,,[],2017-12-07 16:05:53,open,,,['cla: no'],2017-12-07 16:05:57
1251,tensorflow/models,models,2974,jinmel,Use /usr/bin/env python instead of /usr/bin/python,"For python users using virtualenv or pyenv to use their own python this causes unexpected error.

Using python set by environment variable would fix ",4,,[],2017-12-07 12:25:52,open,,,['cla: yes'],2017-12-07 12:27:21
1252,tensorflow/models,models,2971,pankajvshrma,Print filenames in a batch while training through ssd Object Detection,I am having a error  'Tensor is Nan' while training ssd mobilenet. so I want to print filenames in a current batch while training a ssd mobilenet on my own dataset to know which images have incorrect bounding boxes.,2,,[],2017-12-07 06:21:37,open,,,['stat:awaiting tensorflower'],2018-04-10 15:19:11
1253,tensorflow/models,models,2970,glhfgg1024,Why not use tf.layers.batch_normalization?,"Hi Dear Sir/Madam,
In the following file, why don't you use the `tf.layers.batch_normalization`? But for other functions, many `tf.layers` API routines have been used. Does this mean the `tf.layers.batch_normalization` has anything wrong? Thanks.

https://github.com/tensorflow/models/blob/5a5d330539dff11eef79ca2e716fb477baf13cf9/tutorials/image/cifar10_estimator/model_base.py#L183

",2,"NamedUser(login=""protoget"")","[NamedUser(login=""protoget"")]",2017-12-07 04:51:27,open,,,['models: official'],2019-02-06 22:01:36
1254,tensorflow/models,models,2961,monomon,InvalidArgumentError: TypeError: 'numpy.float64' object cannot be interpreted as an integer,"### System information
- **What is the top-level directory of the model you are using**: `object_detection`
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (but using object_detection extensively)
- **OS Platform and Distribution**: Windows 10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: tensorflow-gpu 1.4.0
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: Cuda 8.0 / cudnn64_6 
- **GPU model and memory**: GeForce GTX 1070
- **Exact command to reproduce**: `python models/research/object_detection/train.py ...`

### Describe the problem

I recently updated tensorflow and numpy from pip. During training, I started getting this error:

    tensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError: 'numpy.float64' object cannot be interpreted as an integer

After the error training fails.

I found a similar-sounding issue: https://github.com/rbgirshick/py-faster-rcnn/issues/625

So perhaps the easy workaround is to cast the float to int before the call, or define the datatype explicitly if it is in a tensor. I wasn't able to find the location where it fails (see log below).

### Source code / logs

Find a complete traceback here: https://pastebin.com/97cKf5QT
",1,,[],2017-12-05 21:41:31,open,,,['stat:awaiting tensorflower'],2017-12-06 01:02:03
1255,tensorflow/models,models,2959,PaulChongPeng,add mobilenet_v1_075 mobilenet_v1_050 mobilenet_v1_025 in slim prepro…,add mobilenet_v1_075 mobilenet_v1_050 mobilenet_v1_025 in slim preprocessing_factory,4,,[],2017-12-05 08:47:51,open,,,['cla: yes'],2017-12-05 08:59:17
1256,tensorflow/models,models,2958,davidenitti,create_pet_tf_record.py is broken,"create_pet_tf_record.py does not work anymore
the flag faces_only=True does not work

There is an import error and even if I fix that, all the images are ignored:
WARNING:root:Invalid example: annotations/xmls/great_pyrenees_15.xml, ignoring.

the bug should be here:
mask_stack = np.stack(masks).astype(np.float32)
",14,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""jch1"")]",2017-12-04 18:40:30,open,,,[],2018-11-29 04:18:43
1257,tensorflow/models,models,2949,soumenms2015,ImportError: No module named 'tkinter' during testing the installation,"Hello I am using python3.4 and tensorflow 1.2.1 . I am getting the following error while testing the installation of object detection api.

Here is:
..models/research> python object_detection/builders/model_builder_test.py
Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 21, in <module>
    from object_detection.builders import model_builder
  File ""../models/research/object_detection/builders/model_builder.py"", line 29, in <module>
    from object_detection.meta_architectures import ssd_meta_arch
  File ""../models/research/object_detection/meta_architectures/ssd_meta_arch.py"", line 31, in <module>
    from object_detection.utils import visualization_utils
  File ""../models/research/object_detection/utils/visualization_utils.py"", line 24, in <module>
    import matplotlib.pyplot as plt
  File ""../lib/python3.4/site-packages/matplotlib/pyplot.py"", line 113, in <module>
    _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()
  File ""../lib/python3.4/site-packages/matplotlib/backends/__init__.py"", line 60, in pylab_setup
    [backend_name], 0)
  File ""../lib/python3.4/site-packages/matplotlib/backends/backend_tkagg.py"", line 6, in <module>
    from six.moves import tkinter as Tk
  File ""../lib/python3.4/site-packages/six.py"", line 92, in __get__
    result = self._resolve()
  File ""../lib/python3.4/site-packages/six.py"", line 115, in _resolve
    return _import_module(self.mod)
  File ""../lib/python3.4/site-packages/six.py"", line 82, in _import_module
    __import__(name)
ImportError: No module named 'tkinter'

Any help would be appreciated !!
",1,,[],2017-12-03 14:30:54,open,,,['stat:awaiting tensorflower'],2017-12-04 19:46:55
1258,tensorflow/models,models,2944,scotthuang1989,tensoflow slim inception v1 implementation don't have auxiliary classifiers,"I have posted this question at [stackoverflow](https://stackoverflow.com/questions/47605092/tensoflow-slim-inception-v1-implementation-dont-have-auxiliary-classifiers). But don't get answer so far, thought post it here might get answer quickly.

Here is the question:

I recently start to read this [paper ](https://arxiv.org/abs/1409.4842v1)about inception v1. in section 5, they show me a picture of the architecture of this network. I notice there are 2 auxiliary classifier to boost the back prop signal.

![xyzlf](https://user-images.githubusercontent.com/5325686/33515899-ed9caf5c-d7a3-11e7-98ca-7b649cd61f9d.png)


But after check the code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/inception_v1.py). these 2 auxiliary node are not implemented by tensorflow slim inception v1. 

I am a engineer, Maybe I misunderstand this paper, can someone kindly help me out?

",3,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-12-02 13:04:00,open,,,[],2017-12-06 11:17:56
1259,tensorflow/models,models,2942,ghost,add a tag to an xml file in pascal voc format,"can I add a tag to an xml file in pascal voc format? For example I want to add age . 

`<age>52</age>`",3,"NamedUser(login=""tombstone"")","[NamedUser(login=""tombstone""), NamedUser(login=""jch1"")]",2017-12-01 21:05:20,open,,,[],2017-12-12 03:18:22
1260,tensorflow/models,models,2941,nagachika,Add instance keys to Object Detection API exported model for prediction on ML Engine,"In the documentation of Cloud ML Engine, you should add an instance key in your SavedModel signature to match the output instances to the input instances of batch prediction.
https://cloud.google.com/ml-engine/docs/prediction-overview#instance_keys

[This Blog Post](https://cloud.google.com/blog/big-data/2017/09/performing-prediction-with-tensorflow-object-detection-models-on-google-cloud-machine-learning-engine) shows how to run Tensorflow object detection model batch prediction on Cloud ML Engine, but the results doesn't have instance keys.

This pull-request add the option `--instance_key_type` to `object_detection/export_inference_graph.py` to add instance key in SavedModel signatures. It's optional. The default behavior is not changed.
You can choose type of instance key from `int32`, `int64`, `string`.",4,,[],2017-12-01 19:22:33,open,,,['cla: yes'],2018-08-13 19:17:57
1261,tensorflow/models,models,2938,jagannadhasrikar,Merge pull request #1 from tensorflow/master,klj,2,,[],2017-12-01 05:54:36,open,,,['cla: no'],2017-12-01 05:54:38
1262,tensorflow/models,models,2933,justinshenk,Remove trailing code cell,Remove empty code cell in notebook,3,,[],2017-11-30 22:37:41,open,,,['cla: yes'],2017-11-30 22:40:22
1263,tensorflow/models,models,2926,ybsave,The faster rcnn module does not support batch_size > 1 for each clone,"https://github.com/tensorflow/models/blob/b63a73df70656ecfcd2d50bfc98b09b3ce06f635/research/object_detection/trainer.py#L155

The above line has a concat operation which only works for single image or images with the same width/height ratio, i.e., the same size after resize. 

This seems to be a bug; because in 
https://github.com/tensorflow/models/blob/01aa7a4a6190bf021f448dee26fc649f4b47753e/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py#L101, the comments show ""When training with a relative large batch size (e.g. 8), it could be desirable to enable batch norm update."" This indicates the authors plan to support more batch sizes. Also, I believe that this comment is misleading. Set batch norm parameters not trainable is OK for fine tuning, but NOT acceptable for training from scratch. If training from scratch, even if the batch size is 1, the batch norm still need to be set trainable.",4,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-11-29 21:19:09,open,,,"['stat:awaiting tensorflower', 'type:bug/performance']",2017-12-07 16:31:26
1264,tensorflow/models,models,2922,ppgiannak,Exporting features,"Hello,
I am using the object_detection API and specifically Faster RCNN to train an object detector on my own dataset. My question is this:
During testing, is there any way of exporting also the feature map that corresponds to each one of the bounding boxes the network gives as outputs from the inference?
Many thanks in advance.
",4,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-11-29 13:47:17,open,,,[],2018-05-19 09:15:30
1265,tensorflow/models,models,2911,strubell,syntaxnet: make data loader more robust to extra whitespace,"The data loader was failing when I tried to load files containing extra whitespace in blank lines. I added trims to avoid this in the future. Let me know if there is an implementation of trim that I should be using; I added functions from stackoverflow to util (the existing ones are for tensorflow strings rather than `std::string`).

I also made the relevant error message more informative, printing the line number and line.",1,,[],2017-11-27 20:20:36,open,,,['cla: yes'],2017-11-27 20:20:39
1266,tensorflow/models,models,2910,sheromon,Fix create pet,I had an issue running the pet tutorial having to do with the new faces_only option. I think my changes resolve the issue and give the desired behavior? Updating the tutorial to reflect the updated path the create_pet_tf_records.py file while I'm at it. ,4,,[],2017-11-27 19:29:04,open,,,['cla: no'],2017-12-06 21:08:53
1267,tensorflow/models,models,2901,shastakr,Change resnet_v2's wrong default preprocessing & default_image_size,"Related with issue #1497 

I tested with resnet_v2_152 on my local environment.
I found out that document is correct and code is wrong.
",2,,[],2017-11-27 01:37:03,open,,,['cla: yes'],2018-04-14 17:56:33
1268,tensorflow/models,models,2897,warmspringwinds,Shallow pre-activation resnet models,"Feature request:

Are you planning on releasing shallow v2 (preactivation) resnet pretrained models?:
https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models

Like resnet-18 and 34.

Thank you.",1,,[],2017-11-26 02:48:22,open,,,['stat:awaiting tensorflower'],2017-11-29 02:03:49
1269,tensorflow/models,models,2892,chrza,Object detection API: Inference and evaluation incompatible issue on Windows Systems ,"### System information
- **What is the top-level directory of the model you are using**:
research/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Win 10, Version 1709 (Build 16299.64) (sorry for that)
- **TensorFlow installed from (source or binary)**:
bin
- **TensorFlow version (use command below)**:
tried with 1.4.0 and 1.5.0.dev20171115
- **CUDA/cuDNN version**:
CUDA 8.0.61.2, cuDNN 6.0
- **GPU model and memory**:
GTX 1080ti 11 GB
- **other packages**:
python 3.6.3, protobuf 3.5.0.post1, six 1.11.0
also tried with python3-protobuf 2.5.0
- **Exact command to reproduce**:
`python object_detection/inference/infer_detections.py --input_tfrecord_paths=P:\\data\\test_nurLos.record --output_tfrecord_path=detections.tfrecord --inference_graph=P:\\Anaconda3\\envs\\tfgpu\\Lib\\site-packages\\tensorflow\\models\\research\\output_inference_graph\\frozen_inference_graph.pb --discard_image_pixels`


### Describe the problem
I trained  a model on my own dataset/record files and exported the inference graph sucessfully. Now I would like to infer the detections and evaluate the model from the same record file I used for evaluation with eval.py and tensorboard with the new nice [tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/oid_inference_and_evaluation.md). When I try to infer an UnicodeDecodeError is rising (see first prob Stacktrace).
I then changend the encoding of the function as_text in file tensorflow\python\util\compat.py from utf-8 to latin1. But then the problem ""TypeError: unsupported operand type(s) for &: 'str' and 'int'"" occur.

I read about workarounds to use python 2.7. But for win users there exists no tensorflow version for python 2.7. I the tried to run the script with another protobuf version (python3-protobuf 2.5.0). But no success

### Source code / logs
#### used conversion script for the record files
` def split(df, group):
    data = namedtuple('data', ['filename', 'object'])
    gb = df.groupby(group)
    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]


  def create_tf_example(group, path):
    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:
        encoded_jpg = fid.read()
    encoded_jpg_io = io.BytesIO(encoded_jpg)
    image = Image.open(encoded_jpg_io)
    width, height = image.size

    filename = group.filename.encode('utf8')
    image_format = b'jpg'
    xmins = []
    xmaxs = []
    ymins = []
    ymaxs = []
    classes_text = []
    classes = []

    for index, row in group.object.iterrows():
        xmins.append(row['xmin'] / width)
        xmaxs.append(row['xmax'] / width)
        ymins.append(row['ymin'] / height)
        ymaxs.append(row['ymax'] / height)
        classes_text.append(row['class'].encode('utf8'))
        classes.append(class_text_to_int(row['class']))

    tf_example = tf.train.Example(features=tf.train.Features(feature={
        'image/height': dataset_util.int64_feature(height),
        'image/width': dataset_util.int64_feature(width),
        'image/filename': dataset_util.bytes_feature(filename),
        'image/source_id': dataset_util.bytes_feature(filename),
        'image/encoded': dataset_util.bytes_feature(encoded_jpg),
        'image/format': dataset_util.bytes_feature(image_format),
        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
        'image/object/class/label': dataset_util.int64_list_feature(classes),
    }))
    return tf_example


def main(_):
    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
    path = os.path.join(os.getcwd(), 'images')
    examples = pd.read_csv(FLAGS.csv_input)
    grouped = split(examples, 'filename')
    for group in grouped:
        tf_example = create_tf_example(group, path)
        writer.write(tf_example.SerializeToString())

    writer.close()
    output_path = os.path.join(os.getcwd(), FLAGS.output_path)
    print('Successfully created the TFRecords: {}'.format(output_path))
`

#### First prob:
Traceback (most recent call last):
  File ""object_detection/inference/infer_detections.py"", line 96, in <module>
    tf.app.run()
  File ""P:\Anaconda3\envs\tfnightlygpu\lib\site-packages\tensorflow\python\platform\app.py"", line 129, in run
    _sys.exit(main(argv))
  File ""object_detection/inference/infer_detections.py"", line 74, in main
    image_tensor, FLAGS.inference_graph)
  File ""P:\Anaconda3\envs\tfnightlygpu\Lib\site-packages\tensorflow\models\research\object_detection\inference\detection_inference.py"", line 69, in build_inference_graph
    graph_content = graph_def_file.read()
  File ""P:\Anaconda3\envs\tfnightlygpu\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 126, in read
    pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))
  File ""P:\Anaconda3\envs\tfnightlygpu\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 94, in _prepare_value
    return compat.as_str_any(val)
  File ""P:\Anaconda3\envs\tfnightlygpu\lib\site-packages\tensorflow\python\util\compat.py"", line 106, in as_str_any
    return as_str(value)
  File ""P:\Anaconda3\envs\tfnightlygpu\lib\site-packages\tensorflow\python\util\compat.py"", line 84, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 41: invalid start byte

#### Second prob:
`INFO:tensorflow:Reading graph and building model...
Traceback (most recent call last):
  File ""P:\Anaconda3\envs\tfnightlygpu\lib\site-packages\google\protobuf\internal\python_message.py"", line 1083, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File ""P:\Anaconda3\envs\tfnightlygpu\lib\site-packages\google\protobuf\internal\python_message.py"", line 1105, in InternalParse
    (tag_bytes, new_pos) = local_ReadTag(buffer, pos)
  File ""P:\Anaconda3\envs\tfnightlygpu\lib\site-packages\google\protobuf\internal\decoder.py"", line 181, in ReadTag
    while six.indexbytes(buffer, pos) & 0x80:
TypeError: unsupported operand type(s) for &: 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/inference/infer_detections.py"", line 96, in <module>
    tf.app.run()
  File ""P:\Anaconda3\envs\tfnightlygpu\lib\site-packages\tensorflow\python\platform\app.py"", line 129, in run
    _sys.exit(main(argv))
  File ""object_detection/inference/infer_detections.py"", line 74, in main
    image_tensor, FLAGS.inference_graph)
  File ""P:\Anaconda3\envs\tfnightlygpu\Lib\site-packages\tensorflow\models\research\object_detection\inference\detection_inference.py"", line 71, in build_inference_graph
    graph_def.MergeFromString(graph_content)
  File ""P:\Anaconda3\envs\tfnightlygpu\lib\site-packages\google\protobuf\internal\python_message.py"", line 1089, in MergeFromString
    raise message_mod.DecodeError('Truncated message.')
google.protobuf.message.DecodeError: Truncated message.`",3,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-11-25 13:32:41,open,,,[],2018-03-28 05:17:16
1270,tensorflow/models,models,2886,look4pritam,Generic dataset is added.,"Creates TFRecord files for training and validation for any generic dataset using following command.

python download_and_convert_data.py --dataset_name=generic --dataset_dir=/dataset/tensorflow/flowers/ --source_dir=/dataset/flowers/

Here in this example
--source_dir contains flowers dataset but any other dataset can be input.

Then train for given dataset using following command.

python train_image_classifier.py --train_dir=/workspace/models/generic --dataset_name=generic --dataset_split_name=train --dataset_dir=/dataset/tensorflow/flowers --model_name=inception_v3 --save_interval_secs=600 --save_summaries_secs=120 --log_every_n_steps=50 --max_number_of_steps=200

Finally evaluate using following command.

python eval_image_classifier.py --checkpoint_path=/workspace/models/generic --eval_dir=/workspace/models/generic --dataset_name=generic --dataset_split_name=validation --dataset_dir=/dataset/tensorflow/flowers --model_name=inception_v3
",4,,[],2017-11-24 11:47:15,open,,,['cla: yes'],2018-10-03 11:03:48
1271,tensorflow/models,models,2885,CarstenIsert,Add functionality to restore models and Tensorboard support.,"As described in issue #2561 it is not immediately clear for beginners on how to restore a model, so I added this functionality so that you can also continue training. Additionally, I completed the Tensorboard support, so that you can easily see the words in the embedding space in Tensorboard.",1,"NamedUser(login=""a-dai"")","[NamedUser(login=""a-dai"")]",2017-11-24 08:55:47,open,,,['cla: yes'],2018-02-18 07:34:02
1272,tensorflow/models,models,2883,nguyeho7,Object Detection API: ssd_inception_v2_coco_2017_11_17 missing pipeline.config,The pipeline.config file seems missing from ssd_inception_v2_coco_2017_11_17 and ssd_mobilenet_v1_coco_2017_11_17 models. It was contained in the previous (2017_11_08 I think) version. Is there any particular reason for it?,8,,[],2017-11-23 15:27:27,open,,,['stat:awaiting tensorflower'],2018-12-19 16:32:06
1273,tensorflow/models,models,2882,tomahawklin,Swivel: Force bufsz to be the square of shard_size? ,"Hey, I'm new to swivel and I tried it on my custom dataset. Since my vocabulary is quite large, I increased the shard_size and left everything unchanged. Then I found this will not throw any error during running prep.py, but will cause encoding error while running swivel.py. For example:

`UnicodeDecodeError: 'utf8' codec can't decode byte 0xc1 in position 40: invalid start byte`

Then I realized that I also need to change bufsz to be square of shard_size. After fixing this, everything is back on track. So I'm wondering if it would be better to force bufsz to be the square of shard_size instead of hard-coding it to be 16M? Please correct me if I'm wrong. Thanks. ",7,,[],2017-11-23 15:21:38,open,,,['type:bug/performance'],2018-03-06 06:05:34
1274,tensorflow/models,models,2875,frankist,Oxford-IIIT Pet TFRecord file generation,"### System information
- environment capture script output:
```
== cat /etc/issue ===============================================
Linux xico-dell-linux 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux xico-dell-linux 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.4.0)
tensorflow (1.4.0)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.0
tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
tf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 106: nvidia-smi: command not found

== cuda libs  ===================================================
/usr/local/MATLAB/R2017a/bin/glnxa64/libcudart.so.8.0.44
```

- tensorflow version: ('v1.4.0-rc1-11-g130a514', '1.4.0')
- command to reproduce: from the models/research directory run:
`python object_detection/create_pet_tf_record.py --label_map_path=object_detection/data/pet_label_map.pbtxt --data_dir=`pwd`  --output_dir=`pwd``
- not using any gpu
- tensorflow was installed through pip
- I am using linux ubuntu 16.04

### Describe the problem
Hello all,

I was trying to follow the quick start tutorial (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md) when I faced what I think is a documentation bug.

The tutorial asks to run the command from the models/research directory:
python object_detection/create_pet_tf_record.py \
    --label_map_path=object_detection/data/pet_label_map.pbtxt \
    --data_dir=`pwd` \
    --output_dir=`pwd`

However, the script create_pet_tf_record.py is not in object_detection folder anymore, but in object_detection/dataset_tools. Furthermore, if I do not set the attribute ""--faces_only=False"", it will not generate the files pet_train.record and pet_val.record as suggested by the tutorial. Instead, it will create two empty files: pet_val_with_masks.record and pet_train_with_masks.record.

I inspected the code in create_pet_tf_record.py and there is no way the variable ""masks"" will arrive to the line 163 where ""mast_stack"" is set, without being empty if ""faces_only"" is not set to False. This leads to a deterministic fail to generate the record for all oxford images.

Thanks for the support. It is my first time using this tool, so I am not sure I configured something wrong.

Regards,
Francisco",5,,[],2017-11-22 11:32:57,open,,,['stat:awaiting tensorflower'],2017-11-29 01:20:35
1275,tensorflow/models,models,2864,Vardan95,Update exporter.py to use updated RewriterConfig,Resolve #2861 exporter.py issue.,1,,[],2017-11-21 22:34:04,open,,,['cla: yes'],2017-11-21 22:58:18
1276,tensorflow/models,models,2841,tyleretchart,"random_pad_image throwing a ""type int32 that does not match type float32"" error","### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.4.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 8, cuDNN 6
- **GPU model and memory**: 1080Ti 11 GB
- **Exact command to reproduce**:

### Describe the problem
When I add

```
data_augmentation_options {
    random_pad_image {
        min_image_height: 812.0
        min_image_width: 812.0
        max_image_height: 870.0
        max_image_width: 898.0
    }
}
```

in the train_config section of my pipeline config, and then run train.py, I get the following error:

```
Traceback (most recent call last):
  File ""../env/lib/python3.5/site-packages/object_detection/train.py"", line 163, in <module>
    tf.app.run()
  File ""/home/tyler/env/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""../env/lib/python3.5/site-packages/object_detection/train.py"", line 159, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/tyler/env/lib/python3.5/site-packages/object_detection/trainer.py"", line 217, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""/home/tyler/env/lib/python3.5/site-packages/object_detection/trainer.py"", line 77, in create_input_queue
    include_keypoints=include_keypoints))
  File ""/home/tyler/env/lib/python3.5/site-packages/object_detection/core/preprocessor.py"", line 2547, in preprocess
    results = func(*args, **params)
  File ""/home/tyler/env/lib/python3.5/site-packages/object_detection/core/preprocessor.py"", line 1131, in random_pad_image
    tf.stack([image_height, image_width]))
  File ""/home/tyler/env/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 2522, in maximum
    ""Maximum"", x=x, y=y, name=name)
  File ""/home/tyler/env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 546, in _apply_op_helper
    inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Maximum' Op has type int32 that does not match type float32 of argument 'x'.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Scale of 0 disables regularizer.
Traceback (most recent call last):
  File ""../env/lib/python3.5/site-packages/object_detection/eval.py"", line 130, in <module>
    tf.app.run()
  File ""/home/tyler/env/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""../env/lib/python3.5/site-packages/object_detection/eval.py"", line 126, in main
    FLAGS.checkpoint_dir, FLAGS.eval_dir)
  File ""/home/tyler/env/lib/python3.5/site-packages/object_detection/evaluator.py"", line 210, in evaluate
    save_graph_dir=(eval_dir if eval_config.save_graph else ''))
  File ""/home/tyler/env/lib/python3.5/site-packages/object_detection/eval_util.py"", line 393, in repeated_checkpoint_run
    return metrics
UnboundLocalError: local variable 'metrics' referenced before assignment
```

I think the issue is in the line:

```
TypeError: Input 'y' of 'Maximum' Op has type int32 that does not match type float32 of argument 'x'.
```

The proto forces the parameters min_image_height, min_image_width, max_image_height, and max_image_width to be floats, so I can't change those to ints.

It appears that this issue does not show up if I don't specify any parameters, such as:

```
data_augmentation_options {
    random_pad_image {
    }
 }
```

### Source code / logs
This is the config that I used:

```
model {
  faster_rcnn {
    num_classes: 1
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 812
        max_dimension: 812
      }
    }
    feature_extractor {
      type: ""faster_rcnn_resnet101""
      first_stage_features_stride: 16
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        height_stride: 4
        width_stride: 4
        scales: 0.03125
        scales: 0.0625
        scales: 0.125
        scales: 0.25
        scales: 0.5
        scales: 1.0
        scales: 2.0
        scales: 4.0
        aspect_ratios: 0.5
        aspect_ratios: 1.0
        aspect_ratios: 2.0
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.009999999776482582
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.699999988079071
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
        use_dropout: false
        dropout_keep_probability: 1.0
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6000000238418579
        max_detections_per_class: 300
        max_total_detections: 300
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}
train_config {
  batch_size: 2
  data_augmentation_options {
    normalize_image {
      original_minval: 0.0
      original_maxval: 255.0
      target_minval: 0.0
      target_maxval: 1.0
    }
  }
  data_augmentation_options {
    random_pad_image {
      min_image_height: 812.0
      min_image_width: 812.0
      max_image_height: 870.0
      max_image_width: 898.0

    }
  }
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    random_vertical_flip {
    }
  }
  data_augmentation_options {
    random_rotation90 {
    }
  }
  data_augmentation_options {
    normalize_image {
      original_minval: 0.0
      original_maxval: 1.0
      target_minval: 0.0
      target_maxval: 255.0
    }
  }
  optimizer {
    momentum_optimizer {
      learning_rate {
        constant_learning_rate {
          learning_rate: 0.0015249886782839894
        }
      }
      momentum_optimizer_value: 0.8637624382972717
    }
    use_moving_average: false
    moving_average_decay: 0.9997233748435974
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: ""../faster_rcnn_resnet101_coco_11_06_2017/model.ckpt""
  from_detection_checkpoint: true
  num_steps: 124
}
train_input_reader {
  label_map_path: ""liod_datasets/DETAILED_label_map.pbtxt""
  queue_capacity: 110
  min_after_dequeue: 100
  num_readers: 8
  tf_record_input_reader {
    input_path: ""liod_datasets/DETAILED_TRAINING_unbalanced.record""
  }
}
eval_config {
  num_visualizations: 1206
  num_examples: 1206
  max_evals: 1
}
eval_input_reader {
  label_map_path: ""liod_datasets/DETAILED_label_map.pbtxt""
  shuffle: false
  queue_capacity: 1
  min_after_dequeue: 0
  num_epochs: 1
  num_readers: 1
  tf_record_input_reader {
    input_path: ""liod_datasets/DETAILED_VALIDATION.record""
  }
}
```

And the command I used to run it:

```
../env/lib/python3.5/site-packages/object_detection/train.py --logtostderr --pipeline_config_path=./pipeline_train.config --train_dir=./pipeline_train --num_clones=2
```",5,,[],2017-11-20 19:10:22,open,,,['stat:awaiting tensorflower'],2019-01-01 15:48:42
1277,tensorflow/models,models,2838,argman,NASnet is not fully convolutional,"In line [here](https://github.com/tensorflow/models/blob/master/research/slim/nets/nasnet/nasnet_utils.py#L375), it uses static shape, but this causes the model can only be used when shape is pre-defined, I think a better way is to store the stride of each cell, not use the H or W of feature map",2,,[],2017-11-20 09:53:53,open,,,['stat:contributions welcome'],2018-02-21 05:32:48
1278,tensorflow/models,models,2836,qysnn,slim script doesn't work properly with Tensorboard when training on multiple GPUs,"### System information
- **What is the top-level directory of the model you are using**:research/slim
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow version (use command below)**:1.4
- **GPU model and memory**:GTX 1080 X4
- **Exact command to reproduce**:tensorboard --logdir=${TRAIN_DIR}


### Describe the problem
I am trying to use train_image_classifier.py to train Inception V4 model from scratch on flowers dataset. The training runs smoothly, but when I try to visualize it with TensorBoard as instructed in slim/README I find that TensorBoard will only show one step. Based on the output from TensorBoard, I think the problem should be multiple GPU instances write to the same TRAIN_DIR.

### Source code / logs

```
...
WARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.
WARNING:tensorflow:Detected out of order event.step likely caused by a TensorFlow restart. Purging expired events from Tensorboard display between the previous step: -1 (timestamp: -1) and current step: 0 (timestamp: 1511141420.05). Removing 38 scalars, 635 histograms, 635 compressed histograms, 4 images, and 0 audio.
WARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.
WARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.
WARNING:tensorflow:Detected out of order event.step likely caused by a TensorFlow restart. Purging expired events from Tensorboard display between the previous step: -1 (timestamp: -1) and current step: 0 (timestamp: 1511141530.01). Removing 1 scalars, 0 histograms, 0 compressed histograms, 0 images, and 0 audio.
```

",4,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-11-20 02:29:13,open,,,[],2018-04-26 17:56:11
1279,tensorflow/models,models,2832,headdab,lowproposals models are no faster than corresponding models with more proposals,"### System information
- **What is the top-level directory of the model you are using**:

research/object_detection

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Yes, I've written a simple script that allows me to select the model, input video, frames, etc.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Ubuntu 17.04

- **TensorFlow installed from (source or binary)**:

pip install tensorflow or tensorflow-gpu (in a virtual environment)

- **TensorFlow version (use command below)**:

1.4

- **Bazel version (if compiling from source)**:

N/A

- **CUDA/cuDNN version**:

Whatever was installed with python 3.5 and pip install tensorflow-gpu.  I get the same behavior, however, independent of whether I'm running with tensorflow w/o CUDA or tensorflow-gpu w/ CUDA support.

- **GPU model and memory**:

The results are independent of whether I run on a GPU.

- **Exact command to reproduce**:

Run the object detection models using any strategy you want, then change just the model name.
My script is essentially the same steps as that in:

  research/object_detection/object_detection_tutorial.ipynb

### Describe the problem

I pretty sure there's a bug in the *lowproposals* models.  I have a script
that iterates over all models on the object model detection model zoo and
the results for the lowproposal models are essentailly the same for both
runtime and number of regions detected.  Based on the table describing the
models they should be several times faster.

The table below summarizes the results obtained for 10 frames of 4k video. In it:
- model = an abbreviation obtained by taking the first letter of each word
  in the model name and using 50 or 101 in the corresonding models to
  resolve abbreviations that would conflict with my naming convention.  I'm
  also pretty sure that my table is in the same order as the models are
  presented in the mode zoo page.
- dt = the per frame execution time in seconds (single GPU run)
- regions = the average number of objects detected per frame


| model    |    dt | regions |
|----------+-------+---------|
| sm1c     | 0.264 |    5.50 |
| si2c     | 0.342 |    6.20 |
| fri2c    | 0.616 |   10.30 |
| frr50c   | 0.713 |   14.20 |
| frr50lc  | 0.732 |   14.20 |
| rr101c   | 0.719 |    8.50 |
| frr101c  | 0.834 |   13.40 |
| frr101lc | 0.809 |   13.40 |
| frir2ac  | 2.229 |   12.70 |
| frir2alc | 2.220 |   12.70 |
| frnc     | 1.899 |   15.50 |
| frnlc    | 1.907 |   15.50 |

Note that *lc results are effectively identical to the coresponding *c
results.  I get the same results using GPU and non-GPU execution (only the
runtime changes).

For SSD I'm using the Nov 17th models and for the others I'm using the Nov
8th models.  The only difference in my code is the selection of a different
model file.  I'm running top-of-tree master as of Nov 18th 20:32 PST.

### Source code / logs

The table below summarizes the results obtained for 10 frames of 4k video. In it:
- model = an abbreviation obtained by taking the first letter of each word
  in the model name and using 50 or 101 in the corresonding models to
  resolve abbreviations that would conflict with my naming convention.  I'm
  also pretty sure that my table is in the same order as the models are
  presented in the mode zoo page.
- dt = the per frame execution time in seconds (single GPU run)
- regions = the average number of objects detected per frame


| model    |    dt | regions |
|----------+-------+---------|
| sm1c     | 0.264 |    5.50 |
| si2c     | 0.342 |    6.20 |
| fri2c    | 0.616 |   10.30 |
| frr50c   | 0.713 |   14.20 |
| frr50lc  | 0.732 |   14.20 |
| rr101c   | 0.719 |    8.50 |
| frr101c  | 0.834 |   13.40 |
| frr101lc | 0.809 |   13.40 |
| frir2ac  | 2.229 |   12.70 |
| frir2alc | 2.220 |   12.70 |
| frnc     | 1.899 |   15.50 |
| frnlc    | 1.907 |   15.50 |

Note that *lc results are effectively identical to the coresponding *c
results.  I get the same results using GPU and non-GPU execution (only the
runtime changes).

For SSD I'm using the Nov 17th models and for the others I'm using the Nov
8th models.  The only difference in my code is the selection of a different
model file.  I'm running top-of-tree master as of Nov 18th 20:32 PST.
",8,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-11-19 04:44:35,open,,,[],2018-02-05 22:56:45
1280,tensorflow/models,models,2831,mrfortynine,Use more reasonable default argument values in visualization. (Issue 2754),"- Set default value for ""min_score_thresh"" to 0 in object detection visualization.
- Draw groundtruth bounding box by default.",1,,[],2017-11-18 15:34:05,open,,,['cla: yes'],2017-11-18 15:34:12
1281,tensorflow/models,models,2830,flipflop98,"Error from ""python object_detection/builders/model_builder_test.py"" command.","I have following error while doing the installation final step. 
I'm using tensorflow (1.4), cuda (8.0), cudnn (6.0), matplotlib (2.1.0) under anaconda virtual environment.

----------------------------------------------------------------------------------------------------------------------------
(tensorflow) chkim@chkim-PMTSB09D-Samsung-DeskTop:~/다운로드/models-master/research$ python object_detection/builders/model_builder_test.py

Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 21, in <module>
    from object_detection.builders import model_builder
  File ""/home/chkim/다운로드/models-master/research/object_detection/builders/model_builder.py"", line 29, in <module>
    from object_detection.meta_architectures import ssd_meta_arch
  File ""/home/chkim/다운로드/models-master/research/object_detection/meta_architectures/ssd_meta_arch.py"", line 31, in <module>
    from object_detection.utils import visualization_utils
  File ""/home/chkim/다운로드/models-master/research/object_detection/utils/visualization_utils.py"", line 24, in <module>
    import matplotlib.pyplot as plt
  File ""/home/chkim/anaconda2/envs/tensorflow/lib/python2.7/site-packages/matplotlib/pyplot.py"", line 69, in <module>
    from matplotlib.backends import pylab_setup
  File ""/home/chkim/anaconda2/envs/tensorflow/lib/python2.7/site-packages/matplotlib/backends/__init__.py"", line 14, in <module>
    line for line in traceback.format_stack()
  File ""/home/chkim/anaconda2/envs/tensorflow/lib/python2.7/site-packages/matplotlib/backends/__init__.py"", line 16, in <genexpr>
    if not line.startswith('  File ""<frozen importlib._bootstrap'))
UnicodeDecodeError: 'ascii' codec can't decode byte 0xeb in position 20: ordinal not in range(128)
-------------------------------------------------------------------------------------------------------------------------


",4,,[],2017-11-18 13:42:58,open,,,"['stat:awaiting tensorflower', 'type:bug/performance']",2017-12-27 02:43:42
1282,tensorflow/models,models,2822,vladmos,TF-models project is broken on Bazel CI,"Bazel CI for tensorflow/models/syntaxnet is broken by 4364390adbf16b57c093a05217897831f48da7d3: https://ci.bazel.io/job/tf_models_syntaxnet/

`Unexpected error reading .blazerc file '/home/ci/workspace/tf_models_syntaxnet-variation=,node=ubuntu_16.04-x86_64/research/syntaxnet/tensorflow/.tf_configure.bazelrc`

",15,,[],2017-11-17 13:46:35,open,,,['type:build/install'],2018-04-06 07:46:11
1283,tensorflow/models,models,2820,adezoguns,Bounding box for mobileNet SSD fixed on the screen and the size of .pb is 22Mb instead of 29Mb,"Good day guys.  I have a challenge with SSD mobileNet (ssd_mobilenet_v1_coco_11_06_2017) graph .pb. I have frozen the graph using export_inference_graph.py after training for object detection and the size of the frozen graph is 22MB instead of the usual 29MB. I checked the graph from the coco dataset that came with mobileNet and the @Raccoon detection graph, they were both 29MB. I tested the graph with openCV and google object_detection code and it was misbehaving. It always have a bounding box even without any object in front of the camera. Please any assistance is appreciated.

**My Setup** 
linux Os.
I train on a bigger machine with GPU but transfer the .ckpt files to my laptop (amd A10, 8gig ram) for conversion to .pb. I set up a virtual environment for Tensorflow CPU version 1.40 on the laptop. 
",4,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-11-17 09:30:12,open,,,[],2017-12-06 17:09:36
1284,tensorflow/models,models,2818,chrisrn,Bug using 2 gpus,"### System information
- **What is the top-level directory of the model you are using**:
tensorflow/models/research/slim
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: 1)GeForce GTX Titan, 2) GeForce GTX 1080
- **Exact command to reproduce**:
`python train_image_classifier.py \
  --train_dir=${TRAIN_DIR} \
  --dataset_name=DATASET_NAME \
  --dataset_split_name=train \
  --dataset_dir=${DATASET_DIR} \
  --clone_on_cpu=False \
  --num_clones=2 \
  --model_name=resnet_v2_101 \
  --max_number_of_steps=154526 \
  --batch_size=64 \
  --learning_rate=0.0001 \
  --learning_rate_decay_factor=0.1 \
  --end_learning_rate=0.00001 \
  --optimizer=momentum \
  --momentum=0.9 \
  --weight_decay=0.0001 \
  --train_image_size=180 \
  --save_interval_secs=60 \
  --save_summaries_secs=60 \
  --log_every_n_steps=100 \
  --checkpoint_path=${PRETRAINED_CHECKPOINT_DIR}/resnet_v2_101.ckpt \
  --checkpoint_exclude_scopes=resnet_v2_101/logits`



### Describe the problem
I am training resnet_v2_101 model on my custom data set. I composed the data set from the script for getting TFRecord shards, so the data are right and shuffled. The training process is really slow using 2 gpus and using the command **watch nvidia-smi** the gpus are not being utilized all the time. They can be at 80-100% both but suddenly they fall to 0% for a small amount of time. I think this is something that slows down the training. I spent a lot of time to check if the problem has to do with the reading process of the files but I cannot figure out the source.",6,,[],2017-11-17 08:43:02,open,,,['type:support'],2018-04-06 07:46:06
1285,tensorflow/models,models,2817,brucehzsun,merger from tensorflow/model,merger from tensorflow/model,2,,[],2017-11-17 03:52:11,open,,,['cla: no'],2017-11-27 09:21:10
1286,tensorflow/models,models,2808,kannan60,tfnightly - Unknown command line flag 'logtostderr',"### System information
OS Platform and Distribution: Windows 10 64 Bit
TensorFlow installed from (source or binary): binary
TensorFlow version :1.5.0-dev20171115'
Python version 3.5.2(v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55)
GPU: nVidia GeForce 755M 2GB
CPU: Intel x64-64 Intel Core i5-4200M CPU  @ 2.50Ghz, 8GB memory 

### Describe the problem
So, I have the same problem as described in #2653 
https://github.com/tensorflow/models/issues/2653

The solution given in that is to try and install tf nightly @AlgorithmR, @radzfoto. I did that as shown below:
![tf-nightly-gpu](https://user-images.githubusercontent.com/31975295/32871745-f016e06a-cad6-11e7-887a-ee7eb724ea56.JPG)

When I try to run the following code now: 
C:\Users\kannan\Documents\MD Learning\models\research\object_detection>train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config

I get the following error:

FATAL Flags parsing error: Unknown command line flag 'logtostderr'
Pass --helpshort or --helpfull to see help on flags.

Any idea how to solve this?",11,,[],2017-11-16 03:07:56,open,,,['stat:community support'],2017-12-05 18:00:32
1287,tensorflow/models,models,2805,frederictost,Correct Issue #2771 Location of object labels on images,Sometimes labels are overlapping each others.,7,,[],2017-11-15 18:47:02,open,,,['cla: yes'],2018-06-19 16:13:37
1288,tensorflow/models,models,2799,BrianZhu01,[street] when I restored the model trained by the street  meet some errors,"when I run the vgsl_train.py ,it have the right results. and saved the modes, but when I stop it ,and restart it ,it have some errors. what's wrong for it? the errors as follows :
terminate called recursively
terminate called after throwing an instance of 'terminate called after throwing an instance of 'terminate called after throwing an instance of 'std::system_errorstd::system_errorterminate called after throwing an instance of 'std::system_error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'std::system_error'
terminate called after throwing an instance of 'std::system_errorterminate called recursively
terminate called after throwing an instance of 'std::system_error'
terminate called recursively
terminate called recursively
 who can help me ?

besides ,when I delete the trained modes which obtained from the first run, it run correctly. who can help me ?
",5,,[],2017-11-15 08:40:47,open,,,['stat:awaiting tensorflower'],2017-11-29 02:30:02
1289,tensorflow/models,models,2791,plakal,AudioSet tfrecord files should be case-insensitive,"
### System information
- **What is the top-level directory of the model you are using**:

research/audioset

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Any recent version of MacOS with a case-insensitive file system, e.g., HFS+

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

None of these are relevant

### Description

The released AudioSet embedding data uses case-sensitive file names using the leading two characters of YouTube video IDs. E.g., we have all four combinations of [eE][mM].tfrecord. When our tarball is expanded on a case-insensitive filesystem, such as HFS+ on MacOS, then these clobber each other so we end up with a single file.

We should use case-insensitive names for maximum portability.

For more context, see https://groups.google.com/forum/#!topic/audioset-users/SgY2Hf6Hkf4

Ccing: @dpwe ",0,"NamedUser(login=""plakal"")","[NamedUser(login=""plakal"")]",2017-11-14 19:26:13,open,,,[],2017-11-14 19:26:13
1290,tensorflow/models,models,2790,mpeniak,Cannot freeze embedded_ssd_mobilenet_v1,"I cannot freeze embedded_ssd_mobilenet_v1. Previously I would get this:

https://github.com/tensorflow/models/issues/2777

but after reverting to previous version of that file I get error specified in this thread:

https://github.com/tensorflow/models/issues/2774

I have tried all possible combinations of versions of tf as well as models repo but no luck. 
Any ideas what to do and how to freeze this model?",4,"NamedUser(login=""petewarden"")","[NamedUser(login=""petewarden"")]",2017-11-14 09:46:44,open,,,['type:bug/performance'],2017-12-05 17:42:41
1291,tensorflow/models,models,2789,shaofan,Fix unrecognized arguments,Fix unrecognized arguments when run with arguments defined in cifar10_train or cifar10_eval,4,,[],2017-11-14 08:13:55,open,,,['cla: yes'],2017-11-21 14:11:51
1292,tensorflow/models,models,2778,rwightman,Odd batch_size specific behaviour with nasnet_large on ImageNet validation,"I've been trying to reproduce the ImageNet validation accuracies posted with the pretrained weights.

I cannot reproduce the results with an image size of 331 and the default batch size of 100 used in the eval_image_classifer script. With defaults I get roughly 64% top-1 and 74% top-5 using the command:

`python eval_image_classifier.py --dataset_dir ~/imagenet/ --dataset_split_name validation --model_name nasnet_large --checkpoint_path ~/nasnet_large/model.ckpt  --dataset_name imagenet --moving_average_decay=0.9999`

However, if I change the batch size down to 50, I hit the posted results dead on, 82.7% top-1 and 92.6% top-5. This seems rather odd to me. I've never experienced that significant of a performance drop by changing the batch size by a factor of two at eval. 

I noticed that with the batch size of 100, I could improve performance by dropping the eval image size down. There is an optimal value somewhere in the 280-290 range before it starts to fall off. My best result with batch size of 100 was at 288x288 with a 80.8% top-1 and 95.2% top-5, it falls off quickly as the image size increases from there. 

Is this possibly some sort of overflow in batch norm or another op in the network?

Note, I'm using latest released pip install of Tensorflow 1.4 for GPU. CUDA 8.0, cuDNN 6.",39,"NamedUser(login=""shlens"")","[NamedUser(login=""shlens"")]",2017-11-13 04:04:52,open,,,['type:bug/performance'],2018-04-06 07:46:02
1293,tensorflow/models,models,2772,rstebbing,Fix use of `filter` in `filter_variables`,The current use of `filter` is *not* Python 3 compatible.,1,,[],2017-11-11 02:32:49,open,,,['cla: yes'],2017-11-11 02:32:52
1294,tensorflow/models,models,2771,frederictost,utils: detected object labels are sometimes not visible on the image,"Issue:
Sometimes the **object labels** added over an image are out of image bounding. For example, see function **visualize_boxes_and_labels_on_image_array**

Solution:
Draw the label at the bottom of the detected object bounding box. The file to be modified is:
**visualization_utils.py**

    # draw.rectangle(
    #     [(left, text_bottom - text_height - 2 * margin), (left + text_width,
    #                                                       text_bottom)],
    #     fill=color)

    # draw.text(
    #     (left + margin, text_bottom - text_height - margin),
    #     display_str,
    #     fill='black',
    #     font=font)
    # text_bottom -= text_height - 2 * margin

    # Modified by Frederic TOST
    # Compute a height ratio, if ratio < 10% put the label at the bottom of rectangle
    if (top / im_height < 0.1):
        draw.rectangle(
            [(left, bottom + text_height + 2 * margin), (left + text_width,
                                                         bottom)],
            fill=color)
        draw.text(
            (left + margin, bottom),
            display_str,
            fill='black',
            font=font)
        text_bottom += text_height + 2 * margin
    else:
        draw.rectangle(
            [(left, text_bottom - text_height - 2 * margin), (left + text_width,
                                                              text_bottom)],
            fill=color)
        draw.text(
            (left + margin, text_bottom - text_height - margin),
            display_str,
            fill='black',
            font=font)
        text_bottom -= text_height - 2 * margin",12,,[],2017-11-11 00:52:32,open,,,"['stat:contributions welcome', 'type:feature']",2019-03-08 07:46:45
1295,tensorflow/models,models,2770,MisterMorden,Apparent incorrect behavior from resize_to_range() in object_detection/core/preprocessor.py,"From the comments for resize_to_range():

  The output size can be described by two cases:
  1. If the image can be rescaled so its minimum dimension is equal to the
     provided value without the other dimension exceeding max_dimension,
     then do so.
  2. Otherwise, resize so the largest dimension is equal to max_dimension.

This logic would yield the wrong behavior for images with an aspect ratio near 1. For instance, if we had a 1200x1200 image, and we had settings min_dimension = 600 and max_dimension = 1024, it seems that the desired behavior would be to rescale the image to 1024x1024. Instead, following the logic above, the image would be rescaled to 600x600.

Am I missing something?

@jch1 @tombstone @derekjchow @jesu9 @dreamdragon
",1,,[],2017-11-10 23:40:19,open,,,"['stat:contributions welcome', 'type:bug/performance']",2017-11-18 07:38:22
1296,tensorflow/models,models,2763,chenghuige,Why is nasnet model so slow using one gpu?,"tensorflow 1.4 rc0,  on p40
I'm tring nasnet, it performs excellent but one thing is curious, I find using one gpu is much slower then using 2gpu(tower loss) I'm using p40, say if one gpu batch size is 16, for 2 gpu, each gpu with batch size 16 (so batch size total 16 * 2) But since we have communication cost I suppose using 2gpu will not speed up * 2, However experiment result is like below

1 gpu: train_step:92700 duration:37.092 elapsed:[405.920] batch_size:[16] batches/s:[0.25] insts/s:[3.94] 1epoch:[74.00h]

2 gpu: train_step:174600 duration:47.933 elapsed:[236.972] batch_size:[16] gpus:[2] batches/s:[0.42] insts/s:[13.50] 1epoch:[21.60h]

I have also try using inceptionV4, resnet152, inceptionResnetV2, they all behave as expected, 2gpu not speed up *2.

Sometimes I prefer to use one gpu, since when finetune image model, multiple gpu tower loss might hurt final accuracy though I still know why..

Not sure if this is a bug, I have also asked quesion on stackoverlow. 
https://stackoverflow.com/questions/47215223/why-is-nasnet-model-so-slow-using-one-gpu
Please help close if it is not proper to discuss here. ",9,,[],2017-11-10 03:35:17,open,,,['stat:contributions welcome'],2018-03-26 12:17:39
1297,tensorflow/models,models,2761,starwars110,Problem with the new train and eval updates,"
### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8/6.1
- **GPU model and memory**: Titan X
- **Exact command to reproduce**:

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I used to use object detection to fine tune the coco trained model on SSD_Mobilenet with only 3 classes on my data and it worked fine until this new updates of train.py, eval.py, and other dependent files.

the training log used to be like this:
##########################################
WARNING:tensorflow:From C:\TensorflowModels_v0\object_detection\trainer.py:178: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:From C:\TensorFlowModels_v0\object_detection\builders\optimizer_builder.py:92: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.
WARNING:tensorflow:From C:\TensorFlowModels_v0\object_detection\meta_architectures\ssd_meta_arch.py:607: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
INFO:tensorflow:Restoring parameters from D:/object_detection/models/ssd_mobilenet_v1_coco_11_06_2017/model.ckpt
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path D:/object_detection/models0/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:global step 1: loss = 22.9493 (30.222 sec/step)
INFO:tensorflow:global step 2: loss = 20.2359 (3.919 sec/step)
INFO:tensorflow:global step 3: loss = 18.7402 (3.793 sec/step)
INFO:tensorflow:global step 4: loss = 17.9404 (3.978 sec/step)
INFO:tensorflow:global step 5: loss = 17.1826 (3.588 sec/step)
INFO:tensorflow:global step 6: loss = 15.8118 (3.605 sec/step)
INFO:tensorflow:global step 7: loss = 15.1282 (3.416 sec/step)
INFO:tensorflow:global step 8: loss = 14.2844 (3.338 sec/step)
INFO:tensorflow:global step 9: loss = 13.7035 (3.416 sec/step)
INFO:tensorflow:global step 10: loss = 14.0352 (3.245 sec/step)
INFO:tensorflow:global step 11: loss = 13.6083 (3.279 sec/step)
INFO:tensorflow:global step 12: loss = 11.4007 (3.323 sec/step)
INFO:tensorflow:global step 13: loss = 11.8029 (3.354 sec/step)
INFO:tensorflow:global step 14: loss = 10.7637 (3.338 sec/step)
INFO:tensorflow:global step 15: loss = 10.5538 (3.370 sec/step)
INFO:tensorflow:global step 16: loss = 9.9638 (3.323 sec/step)
INFO:tensorflow:global step 17: loss = 10.5116 (3.276 sec/step)
INFO:tensorflow:global step 18: loss = 11.0982 (3.338 sec/step)
INFO:tensorflow:global step 19: loss = 9.9132 (3.307 sec/step)
INFO:tensorflow:global step 20: loss = 9.5878 (3.387 sec/step)

#############################
however when I use the version with recent updates I get this report:

################################
WARNING:tensorflow:From research\object_detection\trainer.py:210: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
INFO:tensorflow:Restoring parameters from D:/object_detection/models/ssd_mobilenet_v1_coco_11_06_2017/model.ckpt
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path G:/object_detection/models1/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:global step 1: loss = 88.5192 (29.360 sec/step)
INFO:tensorflow:global step 2: loss = 85.8423 (3.781 sec/step)
INFO:tensorflow:global step 3: loss = 74.9890 (3.584 sec/step)
INFO:tensorflow:global step 4: loss = 65.3577 (3.572 sec/step)
INFO:tensorflow:global step 5: loss = 46.6875 (3.479 sec/step)
INFO:tensorflow:global step 6: loss = 37.5350 (3.323 sec/step)
INFO:tensorflow:global step 7: loss = 39.2690 (3.402 sec/step)
INFO:tensorflow:global step 8: loss = 50.3077 (3.310 sec/step)
INFO:tensorflow:global step 9: loss = 28.0526 (3.324 sec/step)
INFO:tensorflow:global step 10: loss = 31.1548 (3.349 sec/step)
INFO:tensorflow:global step 11: loss = 32.1967 (3.335 sec/step)
INFO:tensorflow:global step 12: loss = 34.1146 (3.309 sec/step)
INFO:tensorflow:global step 13: loss = 22.3245 (3.358 sec/step)
INFO:tensorflow:global step 14: loss = 20.3864 (3.393 sec/step)
INFO:tensorflow:global step 15: loss = 21.3323 (3.311 sec/step)
INFO:tensorflow:global step 16: loss = 20.5359 (3.361 sec/step)
INFO:tensorflow:global step 17: loss = 19.8039 (3.393 sec/step)
INFO:tensorflow:global step 18: loss = 20.1648 (3.413 sec/step)
INFO:tensorflow:global step 19: loss = 18.7674 (3.424 sec/step)
INFO:tensorflow:global step 20: loss = 13.9156 (3.356 sec/step)
#############################################
this does not lead to good results. The initial loss seems to be 4 times the loss I used to get. So I had to go back using previous version of object detection.

",24,,[],2017-11-09 23:14:35,open,,,['stat:awaiting tensorflower'],2019-03-05 10:12:09
1298,tensorflow/models,models,2754,mrfortynine,"Set default value for ""min_score_thresh"" to 0 in object detection visualization code.","I'm suggesting this for two reason:
- With current value `0.5`, usually nothing gets drawn at early stage of training. This can be confusing for new comer who just want to run some provided examples. For instance, I discovered this issue when trying to train a pet classifier using SSD config.
- There's usually a min score threshold in post processing step. One would expect the default behavior of visualization is just draw every box that survived that post process threshold.

```
diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index 6a37be7..d7bb952 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -60,7 +60,7 @@ def visualize_detection_results(result_dict,
                                 export_dir='',
                                 agnostic_mode=False,
                                 show_groundtruth=False,
-                                min_score_thresh=.5,
+                                min_score_thresh=.0,
                                 max_num_predictions=20):
   """"""Visualizes detection results and writes visualizations to image summaries.
 
diff --git a/research/object_detection/evaluator.py b/research/object_detection/evaluator.py
index 74722d0..7a6ca0c 100644
--- a/research/object_detection/evaluator.py
+++ b/research/object_detection/evaluator.py
@@ -177,7 +177,7 @@ def evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,
           categories=categories,
           summary_dir=eval_dir,
           export_dir=eval_config.visualization_export_dir,
-          show_groundtruth=eval_config.visualization_export_dir)
+          show_groundtruth=true)
     return result_dict
 
   variables_to_restore = tf.global_variables()
diff --git a/research/object_detection/utils/visualization_utils.py b/research/object_detection/utils/visualization_utils.py
index 5c16f7a..a7ec6a9 100644
--- a/research/object_detection/utils/visualization_utils.py
+++ b/research/object_detection/utils/visualization_utils.py
@@ -391,7 +391,7 @@ def visualize_boxes_and_labels_on_image_array(image,
                                               keypoints=None,
                                               use_normalized_coordinates=False,
                                               max_boxes_to_draw=20,
-                                              min_score_thresh=.5,
+                                              min_score_thresh=.0,
                                               agnostic_mode=False,
                                               line_thickness=4):
```",4,,[],2017-11-09 18:01:38,open,,"NamedUser(login=""bignamehyp"")",['stat:contributions welcome'],2017-11-18 15:35:38
1299,tensorflow/models,models,2752,chenyuZha,Object Detection: Multi-GPU Out of Range error for model faster_rcnn_inception_atrous_resnet_v2,"


### System information
- **OS Platform and Distribution**: Linux Ubuntu 16.04 
- **TensorFlow version (use command below)**: 1.2.1
- **CUDA/cuDNN version**: 7.5
- **GPU model and memory**:  GTX 1080i x2
- **Python version**: python 3.5



### Describe the problem
For multi-GPU training, I modified `batch_size=2`, `num_clones=2`, `ps_tasks=1` both in the `train.py` and model file config(`faster_rcnn_incpetion_atrous_resnet_v2.config`). Also, I added  `batch_queue_capacity:4` and `prefetch_queue_capacity:4` in the `faster_rcnn_incpetion_atrous_resnet_v2.config`. My file tfRecord is about 9G, when I trained with just one GPU, everything goes fine. Then with the case of 2 GPUs, I got out of range errors:

![capture du 2017-11-09 14-02-17](https://user-images.githubusercontent.com/29950360/32606718-a48e688c-c556-11e7-9196-70f44aa9f29d.png)


![capture du 2017-11-09 14-00-48](https://user-images.githubusercontent.com/29950360/32606657-694525f4-c556-11e7-8179-70c3431810be.png)

Could anyone help me to solve this problem? Thanks a lot!",1,,[],2017-11-09 13:07:39,open,,,['stat:awaiting tensorflower'],2017-12-07 10:49:42
1300,tensorflow/models,models,2750,nisseb,Object Detection API: NASNET Mobile detection (feature request),"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
With the addition of NASNET (thank you very much for sharing), the only pre-trained detection model supplied is NASNET large with Faster-RCNN detection. In the paper (https://arxiv.org/pdf/1707.07012.pdf), however, results are reported for NASNET Mobile with Faster-RCNN aswell. Is there a reason this pre-trained model is not shared? (I have for example managed to get the NASNET Mobile + Faster-RCNN architecture up and running but I don't have the computational resources to pre-train the model on COCO myself).

Also, Faster-RCNN slows down the lightweight NASNET Mobile quite a lot. Are there any plans of support for NASNET Mobile with SSD?",7,,[],2017-11-09 09:53:20,open,,,['stat:contributions welcome'],2018-10-17 14:53:27
1301,tensorflow/models,models,2747,scotthuang1989,fix: create_pascal_tf_record_test don't compatible with py3,create_pascal_tf_record_test.py will fail on python 3 due to comparing bytes with unicode issue.,1,,[],2017-11-09 01:46:10,open,,,['cla: yes'],2017-11-16 01:17:15
1302,tensorflow/models,models,2739,sagi44222,Error running training in google ML engine - No matplotlib.pyplot module,"Following the procedure to run a training in google ML engine I encounter a problem compiling the project. Here is a snipped from the job log:

`Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/eval.py"", line 50, in <module> from object_detection import evaluator File ""/root/.local/lib/python2.7/site-packages/object_detection/evaluator.py"", line 24, in <module> from object_detection import eval_util File ""/root/.local/lib/python2.7/site-packages/object_detection/eval_util.py"", line 29, in <module> from object_detection.utils import visualization_utils as vis_utils File ""/root/.local/lib/python2.7/site-packages/object_detection/utils/visualization_utils.py"", line 24, in <module> import matplotlib.pyplot as plt ImportError: No module named matplotlib.pyplot`

**Evaluation job code:**
`gcloud ml-engine jobs submit training object_detection_eval_`date +%s` \
    --job-dir=gs://${TRAIN_DIR} \
    --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz \
    --module-name object_detection.eval \
    --region us-central1 \
    --scale-tier BASIC_GPU \
    -- \
    --checkpoint_dir=gs://${TRAIN_DIR} \
    --eval_dir=gs://${EVAL_DIR} \
    --pipeline_config_path=gs://${PIPELINE_CONFIG_PATH}`

**Training input:**
```
{
  ""scaleTier"": ""BASIC_GPU"",
  ""packageUris"": [
    ""gs://TTT/packages/9a42ca0914663739bd0e7eef559ea1e821bdd82971d3a4d3c0a9fc427fc982f1/object_detection-0.1.tar.gz"",
    ""gs://TTT/packages/9a42ca0914663739bd0e7eef559ea1e821bdd82971d3a4d3c0a9fc427fc982f1/slim-0.1.tar.gz""
  ],
  ""pythonModule"": ""object_detection.eval"",
  ""args"": [
    ""--checkpoint_dir=gs://TTT/training"",
    ""--eval_dir=gs://TTT"",
    ""--pipeline_config_path=gs://TTT/ssd_mobilenet_v1_pets.config""
  ],
  ""region"": ""asia-east1"",
  ""jobDir"": ""gs://TTT/""
}
```
Any solution for this?",38,,[],2017-11-08 11:32:18,open,,,['type:docs'],2018-05-21 04:22:20
1303,tensorflow/models,models,2734,cancan101,Use range rather than xrange for compatibility with Python3,,1,,[],2017-11-08 06:08:49,open,,,['cla: yes'],2018-01-29 23:20:23
1304,tensorflow/models,models,2733,cancan101,Remove unused and missing google3 import,,1,,[],2017-11-08 05:31:41,open,,,['cla: yes'],2018-01-29 23:21:55
1305,tensorflow/models,models,2722,scotthuang1989,"object_detection api: retrain  model locally, can not stop gracefully","### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:v1.4.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**:6GB
- **Exact command to reproduce**:


### Describe the problem
I follow the document to train the model locally. with following command:

`python object_detection/train.py     --logtostderr     --pipeline_config_path=${PATH_TO_YOUR_PIPELINE_CONFIG}     --train_dir=${PATH_TO_TRAIN_DIR}
`

I set number_of_steps to 2 in config file. From the log printed on terminal I can see that the training do run 2 times, but the python process don't exit even if I hit ctrl+c, I can only terminate it by 
`kill -9 PID`
command. meanwhile, this process still consume a lot of cpu resoruce.
",2,,[],2017-11-07 14:49:17,open,,,['stat:contributions welcome'],2017-11-10 00:13:58
1306,tensorflow/models,models,2719,M1cR0xft,typo fixed in object_detection_tutorial.ipynb,it looks better now,1,,[],2017-11-07 08:24:29,open,,,['cla: yes'],2017-11-07 08:24:38
1307,tensorflow/models,models,2709,larryfu,ptb_word_lm.py  AttributeError: 'module' object has no attribute 'RNNParamsSaveable',"when running   ptb_word_lm.py  with  tf-nightly 1.5.0-dev20171103
I got this error 

    2017-11-04 16:48:59.759628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] Creating     TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
	2017-11-04 16:48:59.761131: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 221.62M (232390656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
	WARNING:tensorflow:From /home/larry/workspace/tensorflow/models/tutorials/rnn/ptb/ptb_word_lm.py:165: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
	Instructions for updating:
	Please switch to tf.train.get_or_create_global_step
	Traceback (most recent call last):
	  File ""/home/larry/workspace/tensorflow/models/tutorials/rnn/ptb/ptb_word_lm.py"", line 527, in <module>
	    tf.app.run()
	  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
	    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
	  File ""/home/larry/workspace/tensorflow/models/tutorials/rnn/ptb/ptb_word_lm.py"", line 502, in main
	    model.import_ops()
	  File ""/home/larry/workspace/tensorflow/models/tutorials/rnn/ptb/ptb_word_lm.py"", line 272, in import_ops
	    params_saveable = tf.contrib.cudnn_rnn.RNNParamsSaveable(
	AttributeError: 'module' object has no attribute 'RNNParamsSaveable'

It seems that there is no RNNParamsSaveable in cudnn_rnn 
Any idea how to fix it?",14,,[],2017-11-04 08:58:06,open,,,[],2018-04-06 07:45:48
1308,tensorflow/models,models,2697,shlens,Clean up README.md specifying command line.,,5,,[],2017-11-03 13:17:05,open,,,['cla: yes'],2017-11-03 17:37:21
1309,tensorflow/models,models,2694,CoderGxw,INFO:tensorflow:Summary name /clone_loss is illegal (Object Detection),"## System information：
tensorflow version:1.3.0
platform:ubuntu16.04
python version:python2.7
Dataset:Oxford-IIIT pet dataset
memory:16G

## Describe the problem:
I want use pet-train-model(object detection), and use Oxford-IIIT pet dataset, but I don't know how to slove this problem
code：
`object_detection/train.py \
--logtostderr \
--pipeline_config_path=/notebooks/models/research/20171102/faster_rcnn_resnet152_pets.config \
--train_dir=/notebooks/models/research/20171102`

This step is very slow, and I have the next error/output:
‘
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""

INFO:tensorflow:Restoring parameters from /notebooks/models/research/20171102/model.ckpt-0
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path /notebooks/models/research/20171102/model.ckpt
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:global step 1: loss = 399077933056.0000 (119.271 sec/step)’
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/cpu:0""](total_loss)]]

Caused by op u'CheckNumerics', defined at:
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
...
InvalidArgumentError (see above for traceback): LossTensor is inf or nan. : Tensor had NaN values
	 [[Node: CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan."", _device=""/job:localhost/replica:0/task:0/cpu:0""](total_loss)]]",13,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-11-03 05:56:44,open,,,[],2018-05-16 13:59:57
1310,tensorflow/models,models,2687,fenderrex,patch for issue #2652 now runs on python 3.x,,7,,[],2017-11-02 07:25:17,open,,,['cla: yes'],2017-11-12 19:21:08
1311,tensorflow/models,models,2686,scotthuang1989,fix input_shape parse error,"the code to parse a string: [-1, 100, 200, 3] to list is this:

```
   input_shape = [
        int(dim) if dim != '-1' else None
        for dim in FLAGS.input_shape.split(',')
```
but by splitting this with split, this will end up with [-1, 100, 200, 3. I use RE to parse this input.
",5,,[],2017-11-02 03:13:10,open,,,['cla: yes'],2017-11-09 08:37:16
1312,tensorflow/models,models,2667,leotam,Update README.md,"A few notes, there are breaking API changes from TF 0.10 to 1.4 namely tf.multiply replaces tf.mul etc. so I wouldn't recommend TF 0.10.",1,,[],2017-10-31 19:03:52,open,,,['cla: yes'],2017-10-31 19:04:03
1313,tensorflow/models,models,2659,Net-Mist,Correct Delf issues for python 3,Hi ! When using Delf with python3 I found some issues.,5,,[],2017-10-31 09:26:01,open,,,['cla: yes'],2017-11-03 08:54:02
1314,tensorflow/models,models,2654,StevenLOL,Feature request capsules network,"Looking for an implementation of Hinton's ""capsules""  in ""[Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf) ""

",4,,[],2017-10-31 03:26:21,open,,,['stat:contributions welcome'],2018-04-19 14:00:15
1315,tensorflow/models,models,2607,jlewi,Allow data_dir to be a remote location like GCS,"* Download the data to a local temporary directory so that data_dir
  can be a remote path like GCS.",3,,[],2017-10-27 04:13:12,open,,,['cla: yes'],2017-11-30 04:44:26
1316,tensorflow/models,models,2605,charlesreid1,Ambiguous instructions and stale information for differential_privacy examples,"### System information
- **What is the top-level directory of the model you are using**: `research/differential_privacy/`
- **Have I written custom code**: No, stock copy of git repo
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.0.0-65-g4763edf-dirty', '1.0.1')
- **Bazel version (if compiling from source)**: 0.5.4-homebrew

### Describe the problem

I am attempting to build the networks in the `differential_privacy/` folder, but the bazel build instructions in the README are ambiguous and do not work. The instructions are also out-of-date and inconsistent with the current repository organization and contents. I am happy to submit a PR with updated instructions if I can get some help building and running these examples.

I'll start with `research/differential_privacy/dp_sgd/`. The [dp_sgd README](https://github.com/tensorflow/models/tree/master/research/differential_privacy/dp_sgd) includes instructions on obtaining a slim NIST dataset, then building the examples with bezel. There is some out-of-date TODO info in the README, but the following worked for me:

```
$ cd; git clone https://github.com/tensorflow/models.git

$ cd models/research/slim/

$ DATA_DIR=/path/to/models/research/data

$ python download_and_convert_data.py --dataset_name=mnist --dataset_dir=""${DATA_DIR}""

$ ls ${DATA_DIR}
labels.txt
mnist_test.tfrecord
mnist_train.tfrecord
```

The README instructions never explicitly state where the commands are being run from, and therefore where the `data/` directory actually goes, so I'm just inferring that `research/data/` is the correct location. (Note, it also explicitly lists the contents of the `differential_privacy/` directory from a version of the repository from [back in December](https://github.com/panyx0718/models)).

Next, the `bazel build` command that's given in the README is:

```
bazel build -c opt differential_privacy/...
```

Again, the README is ambiguous about locations, so I tried running this from the `research/` directory: 

```
$ bazel build -c opt differential_privacy/...
ERROR: /Users/charles/models/research/differential_privacy/multiple_teachers/BUILD:85:1: no such package 'differential_privacy/multiple_teachers': BUILD file not found on package path and referenced by '//research/differential_privacy/multiple_teachers:analysis'.
ERROR: Analysis of target '//research/differential_privacy/multiple_teachers:analysis' failed; build aborted.
INFO: Elapsed time: 2.731s
```

If I try skipping the `multiple_teachers` directory by specifying `differential_privacy/dp_sgd/...` in the bazel build command, I still see problems with BUILD files not being found:

```
$ bazel build -c opt differential_privacy/dp_sgd/...
ERROR: /Users/charles/models/research/differential_privacy/dp_sgd/dp_optimizer/BUILD:32:1: no such package 'differential_privacy/dp_sgd/per_example_gradients': BUILD file not found on package path and referenced by '//research/differential_privacy/dp_sgd/dp_optimizer:dp_optimizer'.
ERROR: Analysis of target '//research/differential_privacy/dp_sgd/dp_optimizer:dp_optimizer' failed; build aborted.
INFO: Elapsed time: 0.086s
```

Likewise if I attempt to build only the `multiple_teachers/` target:

```
$ bazel build -c opt differential_privacy/multiple_teachers/...
ERROR: /Users/charles/models/research/differential_privacy/multiple_teachers/BUILD:85:1: no such package 'differential_privacy/multiple_teachers': BUILD file not found on package path and referenced by '//research/differential_privacy/multiple_teachers:analysis'.
ERROR: Analysis of target '//research/differential_privacy/multiple_teachers:analysis' failed; build aborted.
INFO: Elapsed time: 0.150s
```

These targets must be built to use either network, as there are imports of `differential_privacy.dp_sgd` or `differential_privacy.multiple_teachers` in each Python file, so there is no way around this. Again, I am happy to submit a PR that brings the instructions up to date and makes the bazel build instructions more clear, but I do need some help getting the package built. 

(Note: this bazel build issue may have originated with commit f87a58cd96d45de73c9a8330a06b2ab56749a7fa from @nealwu , which moved all of the research models into the `research/` directory, but the ambiguity in the README is the underlying problem.)",2,,[],2017-10-27 02:25:28,open,,,['models: research'],2019-02-01 22:10:46
1317,tensorflow/models,models,2604,Moondra,Model Request: FractalNet,"I can't seem to find a python tf implementation of FractalNet.
I'm still learning tensorflow, so I can't implement it on my own.

Thank you. ",1,,[],2017-10-27 02:02:39,open,,,"['stat:contributions welcome', 'type:feature']",2017-10-27 18:45:25
1318,tensorflow/models,models,2599,M1cR0xft,no need for django and flask in .gitignore,"you have copied and pasted from [Python .gitignore](https://github.com/github/gitignore/blob/master/Python.gitignore) without filtering the useless lines

there is no need for flask and django .gitignore code

this is not a django or flask project",1,,[],2017-10-26 19:06:18,open,,,['cla: yes'],2017-10-28 09:30:03
1319,tensorflow/models,models,2595,OverFlow7,Object detection : CUDA_ERROR_ILLEGAL_ADDRESS on one gpu but not the other,"Hello. 
I've been using object detection for a while now, i have two gpus (both gtx 1060 with 6gb RAM), I used to do the training on gpu:0 and the evalutation on gpu:1

suddenly yesterday the training stopped with error 

>  E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS
>  F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1
> Aborted (core dumped)
> 
 And now the training won't even start, i get the same error right away on gpu:0 but it works fine on gpu:1

When i try to do the evalutation on gpu:0 i get :  

> E tensorflow/stream_executor/cuda/cuda_driver.cc:1098] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available
> F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed
> 

I'am on ubuntu 16.04, tensorflow 1.3, and the gpu drivers are up to date (384.90)
",2,"NamedUser(login=""zheng-xq"")","[NamedUser(login=""zheng-xq"")]",2017-10-26 10:20:33,open,,,[],2018-01-15 11:28:27
1320,tensorflow/models,models,2580,saranyagopal1,Error when I change the data format to 'NCHW' for InceptionResnetV2 algorithm,"



### System information
- **What is the top-level directory of the model you are using**: slim
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (to change data format to NCHW)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: from source with MKL config (latest code synced on 23rd October, 2017)
- **TensorFlow version (use command below)**: 1.4.0rc0
- **Bazel version (if compiling from source)**: bazel was not compiled from source
- **CUDA/cuDNN version**: NA
- **GPU model and memory**:NA
- **Exact command to reproduce**: 

### Describe the problem
When I change the data format to NCHW for optimizing the model to run on CPU, I hit a run-time error:
InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [32,35,35,96] vs. shape[1] = [32,35,35,64]

This is what I did to change the data_format:
1. I did this after getting the images variable in both training and eval script:
images = tf.transpose(images, [0, 3, 1, 2])
2. I added data_format='NCHW' in slim.arg_scope in inception_resnet_v2.py:
with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                        stride=1, padding='SAME', data_format='NCHW'):
3. In all layers of inception, I did concat with 1 instead of 3.
[0001-Change-data_format-to-NCHW-for-optimized-run-on-CPU.txt](https://github.com/tensorflow/models/files/1409534/0001-Change-data_format-to-NCHW-for-optimized-run-on-CPU.txt)
 
Then I hit the above error when I run the training script:
 python train_image_classifier.py --train_dir=train1 --dataset_dir=data --dataset_name=flowers --dataset_split_name=train --model_name=inception_resnet_v2 --use-cpu

### Source code / logs
Attached both patch and logs.
[error_inc_res_v2.txt](https://github.com/tensorflow/models/files/1409487/error_inc_res_v2.txt)
[0001-Change-data_format-to-NCHW-for-optimized-run-on-CPU.txt](https://github.com/tensorflow/models/files/1409535/0001-Change-data_format-to-NCHW-for-optimized-run-on-CPU.txt)



",2,,[],2017-10-24 04:34:50,open,,,['stat:awaiting tensorflower'],2018-07-03 12:59:07
1321,tensorflow/models,models,2567,svebk,Fix flags issue in cifar10 example,"The way flags were handled actually did not allow to pass the `--train_dir` parameter to the script `cifar10_train.py` because the flags were parsed in `cifar10.py` before this argument was added.

I use explicitly the `tf.flags._global_parser` as parser and I am able to pass both the `--train_dir` and the `--data_dir` parameters to the script `cifar10_train.py`.",2,,[],2017-10-20 23:28:28,open,,,[],2017-12-27 10:08:43
1322,tensorflow/models,models,2564,jbenjos,Add the ability to automatically generate protos from setup.py,Just a step in making the install a little more automated.,2,,[],2017-10-20 16:20:42,open,,,['cla: no'],2017-10-20 16:20:46
1323,tensorflow/models,models,2562,hamsungho,Fix GRU cell bug,fix candidate activation expression,3,,[],2017-10-20 08:46:38,open,,,['cla: yes'],2017-10-23 13:42:59
1324,tensorflow/models,models,2560,offbye,Fix issue #1286,change streaming_recall_at_k to streaming_sparse_recall_at_k,4,,[],2017-10-20 02:35:08,open,,,['cla: yes'],2017-10-23 12:48:36
1325,tensorflow/models,models,2558,soma11soma11,Issue with building the preprocessing script.,"`bazel build //im2txt:download_and_preprocess_mscoco
`
```
ERROR: /private/var/tmp/_bazel_soma/4f69e99974bca4e5a152d9acea98bdb1/external/local_config_cc/BUILD:49:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-darwin_x86_64: Xcode version must be specified to use an Apple CROSSTOOL.
ERROR: Analysis of target '//im2txt:download_and_preprocess_mscoco' failed; build aborted.
```",1,,[],2017-10-19 12:41:51,open,,,"['stat:awaiting tensorflower', 'type:build/install']",2017-10-27 18:34:14
1326,tensorflow/models,models,2555,JoshVarty,Correct 3x3 convolutions to 5x5 convolutions,"Fixes #2545 

This PR corrects the 3x3 convolutions to 5x5 convolutions in Inception v1.

I have run the tests in `inception_v1_test.py` and ensured that they pass following these changes.",6,,[],2017-10-19 04:43:36,open,,,['cla: yes'],2017-11-02 05:11:41
1327,tensorflow/models,models,2547,topgetgithub,Manual build syntaxnet always failed on Ubuntu & Mac,"I changed several machine and always got the same errors (22 failed as below):

//dragnn/python:bulk_component_test                                      FAILED in 1.1s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/bulk_component_test/test.log
//dragnn/python:composite_optimizer_test                                 FAILED in 0.7s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/composite_optimizer_test/test.log
//dragnn/python:digraph_ops_test                                         FAILED in 0.8s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/digraph_ops_test/test.log
//dragnn/python:evaluation_test                                          FAILED in 0.8s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/evaluation_test/test.log
//dragnn/python:graph_builder_test                                       FAILED in 0.8s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/graph_builder_test/test.log
//dragnn/python:lexicon_test                                             FAILED in 0.8s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/lexicon_test/test.log
//dragnn/python:network_units_test                                       FAILED in 1.1s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/network_units_test/test.log
//dragnn/python:render_parse_tree_graphviz_test                          FAILED in 1.0s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/render_parse_tree_graphviz_test/test.log
//dragnn/python:render_spec_with_graphviz_test                           FAILED in 0.9s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/render_spec_with_graphviz_test/test.log
//dragnn/python:sentence_io_test                                         FAILED in 0.7s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/sentence_io_test/test.log
//dragnn/python:spec_builder_test                                        FAILED in 0.8s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/spec_builder_test/test.log
//dragnn/python:visualization_test                                       FAILED in 0.8s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/python/visualization_test/test.log
//dragnn/tools:model_trainer_test                                        FAILED in 1.4s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/dragnn/tools/model_trainer_test/test.log
//examples/dragnn:test_run_all_tutorials                                 FAILED in 0.8s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/examples/dragnn/test_run_all_tutorials/test.log
//syntaxnet:beam_reader_ops_test                                         FAILED in 0.8s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/syntaxnet/beam_reader_ops_test/test.log
//syntaxnet:graph_builder_test                                           FAILED in 1.1s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/syntaxnet/graph_builder_test/test.log
//syntaxnet:lexicon_builder_test                                         FAILED in 0.9s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/syntaxnet/lexicon_builder_test/test.log
//syntaxnet:parser_trainer_test                                          FAILED in 0.9s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/syntaxnet/parser_trainer_test/test.log
//syntaxnet:reader_ops_test                                              FAILED in 0.9s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/syntaxnet/reader_ops_test/test.log
//syntaxnet:text_formats_test                                            FAILED in 1.5s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/syntaxnet/text_formats_test/test.log
//syntaxnet/util:check_test                                              FAILED in 0.8s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/syntaxnet/util/check_test/test.log
//syntaxnet/util:registry_test                                           FAILED in 0.9s
  /root/.cache/bazel/_bazel_root/d78cc2bbfe399a9a2f97a3fcc389326e/execroot/__main__/bazel-out/local-opt/testlogs/syntaxnet/util/registry_test/test.log

Executed 52 out of 52 tests: 30 tests pass and 22 fail locally.",17,"NamedUser(login=""calberti"")","[NamedUser(login=""calberti"")]",2017-10-18 01:55:09,open,,,['type:build/install'],2018-10-30 03:11:22
1328,tensorflow/models,models,2545,JoshVarty,Inception v1 missing 5x5 Convolutions,"According to [Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842v1.pdf) the original inception modules had the following structure: 
![image](https://user-images.githubusercontent.com/1249087/31683804-ea24827c-b34b-11e7-9934-eaf4fc80234a.png)

However, while reviewing `models/research/slim/nets/inception_v1.py` I noticed that the inception modules were missing 5x5 convolutions and instead had an extra 3x3 convolution in `Branch_2`:
https://github.com/tensorflow/models/blob/6263692bf175b0e9160faa5614d5bdb1b237c878/research/slim/nets/inception_v1.py#L84-L97

Would you like me to submit a PR to correct this? If so, how would you like to handle the pre-trained networks that you have available for download?

/cc @nealwu @sguada",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-10-17 19:07:08,open,,,[],2017-10-21 18:46:22
1329,tensorflow/models,models,2541,indra215,How to extract moving mean and moving variance from the tensorflow-slim batch norm layer?,"I have been trying to extract moving_mean and moving_variance values from the UPDATE_OPS collection for the batchnorm layer in mobilenet. 

```
Tensor(""mobilenetv1/mobilenetv1/conv2d_1 depthwise/batchnorm/assignmovingavg:0"", shape=(32,), dtype=float32_ref, device=/device:cpu:0)
Tensor(""mobilenetv1/mobilenetv1/conv2d_1_depthwise/batchnorm/assignmovingavg_1:0"", shape=(32,), dtype=float32_ref, device=/device:cpu:0)
```


However, whenever I try to run the following code to extract the above tensors:


```
variables = tf.get_collection(tf.graphkeys.update_ops)
for var in variables: 
    print(sess.run(var)) 

```
The process hangs and no output can be obtained.
Can anyone please let me know of a solution/alternate method to extract these values?
Thank you!
",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-10-16 06:11:23,open,,,[],2017-10-21 18:45:55
1330,tensorflow/models,models,2540,chiayuhsu,"Problem when using eval.py, Request to allocate 0 bytes, tried to allocate 0 bytes, tried to deallocate nullptr","### System information
- **What is the top-level directory of the model you are using**: faster_rcnn_inception_resnet_v2
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.4.0-rc0-4-g933d601', '1.4.0-rc0')
- **Bazel version (if compiling from source)**: 0.6.1
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.0
- **GPU model and memory**: K80 12GB
- **Exact command to reproduce**: 
```
python object_detection/eval.py --logtostderr \
--pipeline_config_path=/home/ubuntu/tensorflow_inception/models/faster_rcnn_inception_resnet_v2.config \
--checkpoint_dir=/home/ubuntu/tensorflow_inception/models/train/ \
--eval_dir=/home/ubuntu/tensorflow_inception/models/eval/
```


### Describe the problem
I don't have any problem with training, and the tensorboard can show training information correctly. But when I use eval.py, it shows the following message. The tensorboard shows nothing about the validation. I have 8 GPUs, and I use CUDA_VISIBLE_DEVICES to specify gpu for training and validation.

### Source code / logs
```
GPU information when training and validation.
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:17.0 Off |                    0 |
| N/A   78C    P0    63W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 00000000:00:18.0 Off |                    0 |
| N/A   76C    P0    83W / 149W |  10982MiB / 11439MiB |      3%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K80           Off  | 00000000:00:19.0 Off |                    0 |
| N/A   52C    P0    59W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla K80           Off  | 00000000:00:1A.0 Off |                    0 |
| N/A   44C    P0    72W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla K80           Off  | 00000000:00:1B.0 Off |                    0 |
| N/A   53C    P0    57W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla K80           Off  | 00000000:00:1C.0 Off |                    0 |
| N/A   42C    P0    72W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla K80           Off  | 00000000:00:1D.0 Off |                    0 |
| N/A   59C    P0    60W / 149W |      0MiB / 11439MiB |     84%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla K80           Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   67C    P0   145W / 149W |  10980MiB / 11439MiB |     90%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    1     65814      C   python                                     10967MiB |
|    7     79907      C   python                                     10967MiB |
+-----------------------------------------------------------------------------+

validation log:
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
WARNING:tensorflow:From /home/ubuntu/tensorflow/models/research/object_detection/evaluator.py:184: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
2017-10-15 15:33:13.495886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-10-15 15:33:13.496256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2017-10-15 15:33:13.496278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from /home/ubuntu/tensorflow_inception/models/train/model.ckpt-0
INFO:tensorflow:Restoring parameters from /home/ubuntu/tensorflow_inception/models/train/model.ckpt-0
2017-10-15 15:33:28.878541: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-15 15:33:28.878585: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-10-15 15:33:28.878592: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-15 15:33:28.878596: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-10-15 15:33:28.878667: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-10-15 15:33:28.878683: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
WARNING:tensorflow:From /home/ubuntu/tensorflow/models/research/object_detection/evaluator.py:166: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_global_step
WARNING:tensorflow:From /home/ubuntu/tensorflow/models/research/object_detection/evaluator.py:166: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_global_step
2017-10-15 15:33:31.265735: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-15 15:33:31.265788: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-10-15 15:33:31.265799: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-15 15:33:31.265803: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-10-15 15:33:31.265833: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-10-15 15:33:31.265841: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-10-15 15:33:32.904984: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-15 15:33:32.905022: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-10-15 15:33:32.905030: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-15 15:33:32.905034: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-10-15 15:33:32.905070: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-10-15 15:33:32.905088: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-10-15 15:33:34.423554: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-15 15:33:34.423591: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-10-15 15:33:34.423599: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-15 15:33:34.423603: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-10-15 15:33:34.423629: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-10-15 15:33:34.423638: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-10-15 15:33:36.569055: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-15 15:33:36.569088: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-10-15 15:33:36.569097: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-15 15:33:36.569101: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-10-15 15:33:36.569128: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-10-15 15:33:36.569148: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
```
",12,,[],2017-10-15 16:16:03,open,,,['stat:awaiting tensorflower'],2019-01-30 15:06:09
1331,tensorflow/models,models,2524,Utumno,Ptb fixups and using argparse,"
@nealwu, @protoget

- https://github.com/Utumno/models/commit/34e9041f196b412a1b0930ee74580786a6de8dae addresses https://github.com/tensorflow/models/pull/2276#issuecomment-329885088
 - https://github.com/Utumno/models/commit/cdd82c7bf7de8a9069a8cc1f9607936ff70132bd addresses this: https://github.com/tensorflow/models/issues/2505#issuecomment-334936514 and also adds argparse",1,,[],2017-10-11 23:02:27,open,,,['cla: yes'],2017-10-13 11:22:06
1332,tensorflow/models,models,2517,voganrc,Syntaxnet Installation Failure OSX,"### System information
- **What is the top-level directory of the model you are using**: syntaxnet
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX El Capitan version 10.11.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Bazel version (if compiling from source)**: 0.6.1-homebrew
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**:

```
 git clone --recursive https://github.com/tensorflow/models.git
 cd models/research/syntaxnet/tensorflow
 ./configure
 cd ..
 bazel test --linkopt=-headerpad_max_install_names \
    dragnn/... syntaxnet/... util/utf8/...
```


### Describe the problem
//dragnn/python:graph_builder_test fails

### Source code / logs
```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/org_tensorflow/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:10:42.149270: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA
2017-10-10 22:10:42.314622: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: simple-parser
2017-10-10 22:10:42.314749: I syntaxnet/embedding_feature_extractor.cc:35] Features: input(-1).word input(-2).word input(-3).word input.word input(1).word input(2).word input(3).word
2017-10-10 22:10:42.314767: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words
2017-10-10 22:10:42.314773: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 64
2017-10-10 22:10:42.316716: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:10:42.317860: I syntaxnet/term_frequency_map.cc:101] Loaded 46 terms from dragnn/core/testdata/syntaxnet_tagger.label-map.
2017-10-10 22:10:42.317888: I syntaxnet/embedding_feature_extractor.cc:35] Features: stack.focus stack(1).focus
2017-10-10 22:10:42.317896: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: rnn
2017-10-10 22:10:42.317901: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
WARNING:tensorflow:*******************************************************
WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.
WARNING:tensorflow:Consider switching to the more efficient V2 format:
WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`
WARNING:tensorflow:now on by default.
WARNING:tensorflow:*******************************************************
2017-10-10 22:10:43.336889: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:10:44.957363: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: simple-tagger
2017-10-10 22:10:44.957433: I syntaxnet/embedding_feature_extractor.cc:35] Features: input(-1).word input(-2).word input(-3).word input.word input(1).word input(2).word input(3).word
2017-10-10 22:10:44.957450: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words
2017-10-10 22:10:44.957455: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 64
2017-10-10 22:10:44.958223: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:10:44.960177: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
2017-10-10 22:10:44.960208: I syntaxnet/embedding_feature_extractor.cc:35] Features: stack.focus
2017-10-10 22:10:44.960212: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: rnn
2017-10-10 22:10:44.960215: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
WARNING:tensorflow:*******************************************************
WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.
WARNING:tensorflow:Consider switching to the more efficient V2 format:
WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`
WARNING:tensorflow:now on by default.
WARNING:tensorflow:*******************************************************
2017-10-10 22:10:45.665411: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:10:48.036776: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: simple-tagger-lstm
2017-10-10 22:10:48.036837: I syntaxnet/embedding_feature_extractor.cc:35] Features: input(-1).word input(-2).word input(-3).word input.word input(1).word input(2).word input(3).word
2017-10-10 22:10:48.036852: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words
2017-10-10 22:10:48.036855: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 64
2017-10-10 22:10:48.037051: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:10:48.037915: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
WARNING:tensorflow:*******************************************************
WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.
WARNING:tensorflow:Consider switching to the more efficient V2 format:
WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`
WARNING:tensorflow:now on by default.
WARNING:tensorflow:*******************************************************
2017-10-10 22:10:49.315948: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:10:51.724104: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: simple-tagger
2017-10-10 22:10:51.724201: I syntaxnet/embedding_feature_extractor.cc:35] Features: input(-1).word input(-2).word input(-3).word input.word input(1).word input(2).word input(3).word
2017-10-10 22:10:51.724220: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words
2017-10-10 22:10:51.724223: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 64
2017-10-10 22:10:51.724431: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:10:51.725270: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
2017-10-10 22:10:51.725291: I syntaxnet/embedding_feature_extractor.cc:35] Features: stack.focus
2017-10-10 22:10:51.725295: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: rnn
2017-10-10 22:10:51.725298: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
WARNING:tensorflow:*******************************************************
WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.
WARNING:tensorflow:Consider switching to the more efficient V2 format:
WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`
WARNING:tensorflow:now on by default.
WARNING:tensorflow:*******************************************************
2017-10-10 22:10:52.682701: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:11:04.423220: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: simple-tagger-wrapped-lstm
2017-10-10 22:11:04.423391: I syntaxnet/embedding_feature_extractor.cc:35] Features: input(-1).word input(-2).word input(-3).word input.word input(1).word input(2).word input(3).word
2017-10-10 22:11:04.423409: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words
2017-10-10 22:11:04.423446: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 64
2017-10-10 22:11:04.424130: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:11:04.424966: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
WARNING:tensorflow:*******************************************************
WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.
WARNING:tensorflow:Consider switching to the more efficient V2 format:
WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`
WARNING:tensorflow:now on by default.
WARNING:tensorflow:*******************************************************
2017-10-10 22:11:07.829753: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:11:09.299394: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: split-tagger
2017-10-10 22:11:09.299472: I syntaxnet/embedding_feature_extractor.cc:35] Features: input(-1).word input(-2).word input(-3).word input.word input(1).word input(2).word input(3).word
2017-10-10 22:11:09.299483: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words
2017-10-10 22:11:09.299486: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 64
2017-10-10 22:11:09.300146: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:11:09.300904: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
2017-10-10 22:11:09.300933: I syntaxnet/embedding_feature_extractor.cc:35] Features: stack.focus
2017-10-10 22:11:09.300936: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: rnn
2017-10-10 22:11:09.300939: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
2017-10-10 22:11:09.300983: I syntaxnet/embedding_feature_extractor.cc:35] Features: 
2017-10-10 22:11:09.300987: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: 
2017-10-10 22:11:09.300989: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 
2017-10-10 22:11:09.301008: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.focus
2017-10-10 22:11:09.301011: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: features
2017-10-10 22:11:09.301013: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: -1
WARNING:tensorflow:*******************************************************
WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.
WARNING:tensorflow:Consider switching to the more efficient V2 format:
WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`
WARNING:tensorflow:now on by default.
WARNING:tensorflow:*******************************************************
2017-10-10 22:11:09.984176: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.EWARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:11:13.353680: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: tagger-parser
2017-10-10 22:11:13.353786: I syntaxnet/embedding_feature_extractor.cc:35] Features: input(-1).word input(-2).word input(-3).word input.word input(1).word input(2).word input(3).word
2017-10-10 22:11:13.353792: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words
2017-10-10 22:11:13.353805: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 64
2017-10-10 22:11:13.353988: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:11:13.354071: I syntaxnet/embedding_feature_extractor.cc:35] Features: 
2017-10-10 22:11:13.354076: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: 
2017-10-10 22:11:13.354079: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 
2017-10-10 22:11:13.354782: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
2017-10-10 22:11:13.354807: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.focus;stack.focus
2017-10-10 22:11:13.354811: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words;rnn
2017-10-10 22:11:13.354814: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: -1;32
2017-10-10 22:11:13.354862: I syntaxnet/embedding_feature_extractor.cc:35] Features: last-action
2017-10-10 22:11:13.354866: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: action
2017-10-10 22:11:13.354878: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
2017-10-10 22:11:13.354886: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.focus;stack.focus;stack.focus
2017-10-10 22:11:13.354889: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words;tagger;rnn
2017-10-10 22:11:13.354892: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: -1;32;32
WARNING:tensorflow:*******************************************************
WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.
WARNING:tensorflow:Consider switching to the more efficient V2 format:
WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`
WARNING:tensorflow:now on by default.
WARNING:tensorflow:*******************************************************
2017-10-10 22:11:15.208527: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:11:19.016844: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: tagger-parser
2017-10-10 22:11:19.016967: I syntaxnet/embedding_feature_extractor.cc:35] Features: input(-1).word input(-2).word input(-3).word input.word input(1).word input(2).word input(3).word
2017-10-10 22:11:19.016981: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words
2017-10-10 22:11:19.016984: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 64
2017-10-10 22:11:19.017173: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:11:19.017247: I syntaxnet/embedding_feature_extractor.cc:35] Features: 
2017-10-10 22:11:19.017253: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: 
2017-10-10 22:11:19.017255: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 
2017-10-10 22:11:19.017978: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
2017-10-10 22:11:19.017995: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.focus;stack.focus
2017-10-10 22:11:19.017998: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words;rnn
2017-10-10 22:11:19.018001: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: -1;32
2017-10-10 22:11:19.018044: I syntaxnet/embedding_feature_extractor.cc:35] Features: last-action
2017-10-10 22:11:19.018048: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: action
2017-10-10 22:11:19.018050: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
2017-10-10 22:11:19.018060: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.focus;stack.focus;stack.focus
2017-10-10 22:11:19.018063: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words;tagger;rnn
2017-10-10 22:11:19.018066: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: -1;32;32
WARNING:tensorflow:*******************************************************
WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.
WARNING:tensorflow:Consider switching to the more efficient V2 format:
WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`
WARNING:tensorflow:now on by default.
WARNING:tensorflow:*******************************************************
2017-10-10 22:11:20.852886: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:11:24.679413: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: tagger-parser
2017-10-10 22:11:24.679511: I syntaxnet/embedding_feature_extractor.cc:35] Features: input(-1).word input(-2).word input(-3).word input.word input(1).word input(2).word input(3).word
2017-10-10 22:11:24.679526: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words
2017-10-10 22:11:24.679529: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 64
2017-10-10 22:11:24.679731: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:11:24.679818: I syntaxnet/embedding_feature_extractor.cc:35] Features: 
2017-10-10 22:11:24.679823: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: 
2017-10-10 22:11:24.679825: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 
2017-10-10 22:11:24.680574: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
2017-10-10 22:11:24.680599: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.focus;stack.focus
2017-10-10 22:11:24.680603: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words;rnn
2017-10-10 22:11:24.680605: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: -1;32
2017-10-10 22:11:24.680655: I syntaxnet/embedding_feature_extractor.cc:35] Features: last-action
2017-10-10 22:11:24.680659: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: action
2017-10-10 22:11:24.680672: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
2017-10-10 22:11:24.680680: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.focus;stack.focus;stack.focus
2017-10-10 22:11:24.680683: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: words;tagger;rnn
2017-10-10 22:11:24.680685: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: -1;32;32
2017-10-10 22:11:24.718259: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
2017-10-10 22:11:24.718287: W dragnn/core/compute_session_pool.cc:58] Destroying pool: number of unreturned sessions = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:11:25.545308: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: shared
2017-10-10 22:11:25.545395: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.tag stack.tag stack(1).tag;input.word
2017-10-10 22:11:25.545401: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: tags;words
2017-10-10 22:11:25.545404: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32;64
2017-10-10 22:11:25.546317: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
2017-10-10 22:11:25.546473: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:11:25.546505: I syntaxnet/embedding_feature_extractor.cc:35] Features: stack.focus
2017-10-10 22:11:25.546509: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: rnn
2017-10-10 22:11:25.546512: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
2017-10-10 22:11:25.566936: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:11:27.029869: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: shared
2017-10-10 22:11:27.030164: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.tag stack.tag stack(1).tag;input.word
2017-10-10 22:11:27.030246: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: tags;words
2017-10-10 22:11:27.030261: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32;64
2017-10-10 22:11:27.033976: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
2017-10-10 22:11:27.034741: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:11:27.034832: I syntaxnet/embedding_feature_extractor.cc:35] Features: stack.focus
2017-10-10 22:11:27.034848: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: rnn
2017-10-10 22:11:27.034852: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
2017-10-10 22:11:27.067988: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:11:28.123592: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: shared
2017-10-10 22:11:28.123709: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.tag stack.tag stack(1).tag;input.word
2017-10-10 22:11:28.123722: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: tags;words
2017-10-10 22:11:28.123725: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32;64
2017-10-10 22:11:28.124472: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
2017-10-10 22:11:28.124630: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:11:28.124669: I syntaxnet/embedding_feature_extractor.cc:35] Features: stack.focus
2017-10-10 22:11:28.124673: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: rnn
2017-10-10 22:11:28.124676: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
2017-10-10 22:11:28.148515: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
.WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
2017-10-10 22:11:29.047052: I dragnn/core/ops/dragnn_op_kernels.cc:79] Creating new ComputeSessionPool in container handle: shared
2017-10-10 22:11:29.047146: I syntaxnet/embedding_feature_extractor.cc:35] Features: input.tag stack.tag stack(1).tag;input.word
2017-10-10 22:11:29.047154: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: tags;words
2017-10-10 22:11:29.047159: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32;64
2017-10-10 22:11:29.047986: I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from dragnn/core/testdata/syntaxnet_tagger.tag-map.
2017-10-10 22:11:29.048115: I syntaxnet/term_frequency_map.cc:101] Loaded 3 terms from dragnn/core/testdata/syntaxnet_tagger.word-map.
2017-10-10 22:11:29.048138: I syntaxnet/embedding_feature_extractor.cc:35] Features: stack.focus
2017-10-10 22:11:29.048142: I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: rnn
2017-10-10 22:11:29.048153: I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: 32
2017-10-10 22:11:29.068348: I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
..
======================================================================
ERROR: testStructuredTrainingNotImplementedDeath (__main__.GraphBuilderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/__main__/dragnn/python/graph_builder_test.py"", line 550, in testStructuredTrainingNotImplementedDeath
    expected=_LABELED_PARSER_EXPECTED_SENTENCES)
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/__main__/dragnn/python/graph_builder_test.py"", line 356, in RunFullTrainingAndInference
    train = builder.add_training_from_config(target)
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/__main__/dragnn/python/graph_builder.py"", line 540, in add_training_from_config
    **kwargs)
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/__main__/dragnn/python/graph_builder.py"", line 353, in build_training
    lambda: comp.build_greedy_training(*args)))
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py"", line 296, in new_func
    return func(*args, **kwargs)
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py"", line 1840, in cond
    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py"", line 1706, in BuildCondBranch
    original_result = fn()
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/__main__/dragnn/python/graph_builder.py"", line 353, in <lambda>
    lambda: comp.build_greedy_training(*args)))
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/__main__/dragnn/python/component.py"", line 393, in build_greedy_training
    with tf.control_dependencies([tf.assert_equal(self.training_beam_size, 1)]):
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/org_tensorflow/tensorflow/python/ops/check_ops.py"", line 318, in assert_equal
    _assert_static(condition_static, data)
  File ""/private/var/tmp/_bazel_voganrc/84f257278b69232e5174bb7ca894d15f/execroot/__main__/bazel-out/darwin_x86_64-opt/bin/dragnn/python/graph_builder_test.runfiles/org_tensorflow/tensorflow/python/ops/check_ops.py"", line 101, in _assert_static
    raise ValueError('\n'.join(data_static))
ValueError: 
Condition x == y did not hold element-wise:
x (parser/TrainingBeamSize:0) = 
8
y (train-testFullInference-train-simple-parser/cond/assert_equal/y:0) = 
1

----------------------------------------------------------------------
Ran 19 tests in 51.583s

FAILED (errors=1)
```
",4,,[],2017-10-10 22:22:20,open,,,['stat:awaiting tensorflower'],2017-10-22 21:17:57
1333,tensorflow/models,models,2514,wensheng,update research/street to be compatible with tf 1.x,These changes make vgsl_model_test.py pass for current version of TF,1,,[],2017-10-10 09:41:39,open,,,[],2017-10-10 09:41:40
1334,tensorflow/models,models,2511,ankur6ue,An example of building an external ops on Windows,"**The Problem:**

Using TensorFlow version 1.3.0 on Windows 10. 

Thanks for providing a windows port for Tensorflow. Is there an example that shows how to compile external C/C++ code into a so/dll that can be loaded by tensorflow as an external op (without building entire tensorflow source) on Windows? The word2vec code that builds word2vec_ops.so/dll would be a great example.. 

This thread seems to suggest that this is possible now:
https://github.com/tensorflow/models/issues/1103

Incidently, I also tried building TensorFlow from source on windows. It almost works, but I get the following when I run: MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj, as suggested on the build from source on windows using cmake readme (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md)



       ""C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_bu
       ild_pip_package.vcxproj"" (default target) (1) ->
       (PostBuildEvent target) ->
         EXEC : error : could not create 'build\bdist.win-amd64\wheel\tensorflow-1.3.0.data\purelib\tensorflow\contrib\
       tensor_forest\hybrid\python\models\stochastic_hard_decisions_to_data_then_nn.py': No such file or directory [C:\
       Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_
       pip_package.vcxproj]
         C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: The
       command ""setlocal\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmak
       e\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorfl
       ow/tensorflow/tools/pip_package/setup.py C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorflow/ten
       sorflow/contrib/cmake/build/tf_python/\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\te
       nsorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorfl
       ow/tensorflow/contrib/cmake/build/Release/pywrap_tensorflow_internal.dll C:/Telesens/dev/apps/src/MachineLearnin
       g/TensorFlow/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/python/_pywrap_tensorflow_inter
       nal.pyd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorfl
       ow/tensorflow/contrib/cmake/build/Release/pywrap_tensorflow_internal.lib C:/Telesens/dev/apps/src/MachineLearnin
       g/TensorFlow/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/python/\r [C:\Telesens\dev\apps
       \src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxpr
       oj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorfl
       ow/tensorflow/tools/pip_package/README C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorflow/tenso
       rflow/contrib/cmake/build/tf_python/\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tens
       orflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorfl
       ow/tensorflow/tools/pip_package/MANIFEST.in C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorflow/
       tensorflow/contrib/cmake/build/tf_python/\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow
       \tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorfl
       ow/tensorflow/contrib/learn/python/learn/datasets/data/boston_house_prices.csv C:/Telesens/dev/apps/src/MachineL
       earning/TensorFlow/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/contrib/learn/python/lear
       n/datasets/data/\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake
       \build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorfl
       ow/tensorflow/contrib/learn/python/learn/datasets/data/iris.csv C:/Telesens/dev/apps/src/MachineLearning/TensorF
       low/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/contrib/learn/python/learn/datasets/data
       /\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorfl
       ow/tensorflow/contrib/learn/python/learn/datasets/data/text_test.csv C:/Telesens/dev/apps/src/MachineLearning/Te
       nsorFlow/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/contrib/learn/python/learn/datasets
       /data/\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_
       python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorfl
       ow/tensorflow/contrib/learn/python/learn/datasets/data/text_train.csv C:/Telesens/dev/apps/src/MachineLearning/T
       ensorFlow/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/contrib/learn/python/learn/dataset
       s/data/\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy_directory C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/tem
       p/tensorflow/tensorflow/core C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorflow/tensorflow/cont
       rib/cmake/build/tf_python/tensorflow/include/tensorflow/core\r [C:\Telesens\dev\apps\src\MachineLearning\TensorF
       low\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy_directory C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/tem
       p/tensorflow/tensorflow/contrib/cmake/build/tensorflow/core C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/
       temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/include/tensorflow/core\r [C:\Telesens\dev\a
       pps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vc
       xproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy_directory C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/tem
       p/tensorflow/tensorflow/stream_executor C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorflow/tens
       orflow/contrib/cmake/build/tf_python/tensorflow/include/tensorflow/stream_executor\r [C:\Telesens\dev\apps\src\M
       achineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy_directory C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/tem
       p/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src/google C:/Telesens/dev/apps/src/MachineLea
       rning/TensorFlow/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/include/google\r [C:\Telese
       ns\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_pa
       ckage.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy_directory C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/tem
       p/tensorflow/tensorflow/contrib/cmake/build/eigen/src/eigen/Eigen C:/Telesens/dev/apps/src/MachineLearning/Tenso
       rFlow/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/include/Eigen\r [C:\Telesens\dev\apps\
       src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxpro
       j]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy_directory C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/tem
       p/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive C:/Telesens/dev/apps/src/MachineLearning/Tens
       orFlow/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/include/external/eigen_archive\r [C:\
       Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_
       pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy_directory C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/tem
       p/tensorflow/third_party/eigen3 C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorflow/tensorflow/c
       ontrib/cmake/build/tf_python/tensorflow/include/third_party/eigen3\r [C:\Telesens\dev\apps\src\MachineLearning\T
       ensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: ""C:\Pr
       ogram Files (x86)\cmake\bin\cmake.exe"" -E copy_directory C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/tem
       p/tensorflow/tensorflow/contrib/cmake/build/eigen/src/eigen/unsupported/Eigen C:/Telesens/dev/apps/src/MachineLe
       arning/TensorFlow/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/include/unsupported/Eigen\
       r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_
       build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: setloc
       al\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
       on_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: cd C:\
       Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python\r [C:\
       Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_
       pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: C:\r [
       C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python_bui
       ld_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: C:\Pyt
       hon36_x64\python.exe C:/Telesens/dev/apps/src/MachineLearning/TensorFlow/temp/tensorflow/tensorflow/contrib/cmak
       e/build/tf_python/setup.py bdist_wheel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\te
       nsorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :cmEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd
       \r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_python
       _build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: endloc
       al & call :cmErrorLevel %errorlevel% & goto :cmDone\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\
       tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmErr
       orLevel\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf
       _python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: exit /
       b %1\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_py
       thon_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmDon
       e\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cmake\build\tf_pytho
       n_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: if %er
       rorlevel% neq 0 goto :VCEnd\r [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\co
       ntrib\cmake\build\tf_python_build_pip_package.vcxproj]
       C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(133,5): error MSB3073: :VCEnd
       "" exited with code 1. [C:\Telesens\dev\apps\src\MachineLearning\TensorFlow\temp\tensorflow\tensorflow\contrib\cm
       ake\build\tf_python_build_pip_package.vcxproj]
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",5,,[],2017-10-09 18:05:27,open,,,[],2018-08-08 12:03:12
1335,tensorflow/models,models,2509,HaoLiuHust,Towards accurate multi-person pose estimation in the wild,"Is this paper implemented in models?
",8,,[],2017-10-09 06:30:33,open,,,[],2018-12-27 09:31:33
1336,tensorflow/models,models,2505,Utumno,Clarifications/suggestions for models/tutorials/rnn/ptb,"I have been trying to adapt the models/tutorials/rnn/ptb to my needs. Along the way I run across some questions and suggestions for improvement - so this classifies as feature request :)

- why are we [exporting then importing the metagraph](https://github.com/tensorflow/models/blob/1464a721fef7da67e87b0f1f1a710b1e904e8764/tutorials/rnn/ptb/ptb_word_lm.py#L486-L496) ? This needs comments as it adds complexity to already complex code (hey it's a tutorial)
- since Supervisor is on its way to deprecation (wise, we have countless APIs for managed sessions, it only adds to learning curve) shouldn't we change [the session](https://github.com/tensorflow/models/blob/1464a721fef7da67e87b0f1f1a710b1e904e8764/tutorials/rnn/ptb/ptb_word_lm.py#L502-L504) to a MonitoredTrainingSession ?
- the [comment on using a static rnn](https://github.com/tensorflow/models/blob/1464a721fef7da67e87b0f1f1a710b1e904e8764/tutorials/rnn/ptb/ptb_word_lm.py#L226-L234) is badly outdated as it refers to the `tensorflow_models/tutorials/rnn/rnn.py's rnn()`. How should this be rephrased as of now ?
- some variables need renaming (max_max_epoch input_ etc) to more descriptive names

I can take this up as pull request but I need some feedback on the points above I can then incorporate into the pull.

Thanks

",25,,[],2017-10-06 12:48:29,open,,,[],2019-01-04 12:49:43
1337,tensorflow/models,models,2485,claudefalbriard,"Failure at TF Inception Sample using S390x, module classify_image.py ","**System information**

. OS Platform and Distribution: s390x Ubuntu 16.4
. TensorFlow installed from (source or binary): source
. TensorFlow version (use command below): 1.3.1
. Python version: 2.7.12

**Error Description**

. Attempt to execute the first sample from tutorial for image recognition with Python API and Tensorflow. The Error shows up at the begin of the execution of the classify_image.py code under the default folder: tutorials/image/imagenet 


I've tried to execute the basic sample from the TF models library (classify_image.py) and received the following error message: 

```
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 659, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Cannot reshape a tensor with 1041082757314414592 elements to shape [16777216,524288] (8796093022208 elements) for 'pool_3/_reshape' (op: 'Reshape') with input shapes: [1,22546423,22546423,2048], [2] and with input tensors computed as partial shapes: input[1] = [16777216,524288].
```

The error message was displayed by the following code section of the TF module at framework/common_shapes.py: 

```
 try:
    with errors.raise_exception_on_not_ok_status() as status:
      output = pywrap_tensorflow.RunCppShapeInference(
          graph_def_version, node_def_str, input_shapes, input_tensors,
          input_tensors_as_shapes, status)
  except errors.InvalidArgumentError as err:
    if err.message.startswith(""No shape inference function exists for op""):
      missing_shape_fn = True
    else:
      raise ValueError(err.message)
```

I'm using latest Github distribution of Tensorflow 1.3.1 built from source under a S390x CPU (LinuxOne) 
 
```
>>> import tensorflow as tf
>>> print tf.VERSION
1.3.1
```",9,,[],2017-10-02 11:45:04,open,,,['stat:community support'],2017-10-18 04:54:15
1338,tensorflow/models,models,2479,youthHan,Object Detection: Error('TrainConfig' object has no attribute 'ignore_groundtruth') when 'eval.py' the model with flag --eval_training_data set to True,"### System information
- **/research/object_detection**:
- **prepare my own data and use the default ssd_inceptionv2_coco model**:
- **Linux Ubuntu 16.04**:
- **TensorFlow installed from source**:
- **TensorFlow version 1.2.0**:

### Describe the problem
Error occurs as below when I evaluate the model with the flag --eval_training_data=True. The model is the one which I trained with the default ssd_inceptionv2_coco config, using my own prepared data.

It seems that 
`model_config = pipeline_config.model
 if FLAGS.eval_training_data:
    eval_config = pipeline_config.train_config` in `eval.py`, but the `train.proto` doesn't provide the 'ignore_groundtruth' field.

### Source code / logs
```
Traceback (most recent call last):
  File ""eval.py"", line 161, in <module>
    tf.app.run()
  File ""/home/ai-i-usr/tools/lpy3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""eval.py"", line 157, in main
    FLAGS.checkpoint_dir, FLAGS.eval_dir)
  File ""/home/ai-i-hanmingfei/proj/models/research/object_detection/evaluator.py"", line 125, in evaluate
    if eval_config.ignore_groundtruth and not eval_config.export_path:
AttributeError: 'TrainConfig' object has no attribute 'ignore_groundtruth'

```
and when I add the `ignore_groungtruth=True ` in the `train_config` of `ssd_inceptionv2_coco.config`. Error occurs:
```
Traceback (most recent call last):
  File ""eval.py"", line 161, in <module>
    tf.app.run()
  File ""/home/ai-i-usr/tools/lpy3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""eval.py"", line 138, in main
    model_config, eval_config, input_config = get_configs_from_pipeline_file()
  File ""eval.py"", line 94, in get_configs_from_pipeline_file
    text_format.Merge(f.read(), pipeline_config)
  File ""/home/ai-i-usr/tools/lpy3/lib/python3.5/site-packages/google/protobuf/text_format.py"", line 481, in Merge
    descriptor_pool=descriptor_pool)
  File ""/home/ai-i-usr/tools/lpy3/lib/python3.5/site-packages/google/protobuf/text_format.py"", line 535, in MergeLines
    return parser.MergeLines(lines, message)
  File ""/home/ai-i-usr/tools/lpy3/lib/python3.5/site-packages/google/protobuf/text_format.py"", line 568, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""/home/ai-i-usr/tools/lpy3/lib/python3.5/site-packages/google/protobuf/text_format.py"", line 583, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""/home/ai-i-usr/tools/lpy3/lib/python3.5/site-packages/google/protobuf/text_format.py"", line 684, in _MergeField
    merger(tokenizer, message, field)
  File ""/home/ai-i-usr/tools/lpy3/lib/python3.5/site-packages/google/protobuf/text_format.py"", line 773, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/home/ai-i-usr/tools/lpy3/lib/python3.5/site-packages/google/protobuf/text_format.py"", line 652, in _MergeField
    (message_descriptor.full_name, name))
google.protobuf.text_format.ParseError: 167:3 : Message type ""object_detection.protos.TrainConfig"" has no field named ""ignore_groundtruth"".
```
",2,,[],2017-09-29 02:05:25,open,,"NamedUser(login=""hgadig"")",['stat:awaiting response'],2018-11-08 19:22:50
1339,tensorflow/models,models,2476,pankajkgupta,fix #2470 : Create imagenet sub-dir only if does not exist,"The imagenet download and preprocess script failed due to some bug in bazel file path parsing system. If I re-run the imagenet download and preprocess script, it throws an error saying-

OSError: [Errno 17] File exists:
because some of the dataset was previously extracted.

Ideally the script should only try to create a directory if it does not exist.

My commit fixes just this problem and has just one line of code added.",1,,[],2017-09-28 15:47:09,open,,,['cla: yes'],2017-09-28 18:35:18
1340,tensorflow/models,models,2474,chihyuwang,Object Detection ssd mobilenet Training not reading in batch norm parameters,"I was following the directions to train a ssd_mobilenet model that trains the detection network from scratch after initializing the backbone with the weights of a model trained on Imagenet. I downloaded the pre-trained mobilenet checkpoint for this experiment. 
MobileNet_v1_0.25_128
mobilenet_v1_0.25_128_2017_06_14.tar.gz
https://github.com/tensorflow/models/tree/master/research/slim

However, I get warning messages complaining that the batch norm parameters could not be loaded (see below for the messages). Has anyone else seen this issue or any pointers on how I can try to debug it?

Below I show my train_config to set up the experiment from the stock one given in the detection library and the warning messages I see about not reading in

train_config: {
  batch_size: 24
  optimizer {
    rms_prop_optimizer: {
      learning_rate: {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.004
          decay_steps: 800720
          decay_factor: 0.95
        }
      }
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""object_detection/mobilenet_v1_025/mobilenet_v1_0.25_128.ckpt""
  from_detection_checkpoint: false
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
}

INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_128/BatchNorm/beta] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_128/BatchNorm/gamma] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_128/BatchNorm/moving_mean] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_128/BatchNorm/moving_variance] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_128/weights] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_64/BatchNorm/beta] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_64/BatchNorm/gamma] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_64/BatchNorm/moving_mean] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_64/BatchNorm/moving_variance] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_64/weights] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_64/BatchNorm/beta] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_64/BatchNorm/gamma] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_64/BatchNorm/moving_mean] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_64/BatchNorm/moving_variance] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_64/weights] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_32/BatchNorm/beta] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_32/BatchNorm/gamma] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_32/BatchNorm/moving_mean] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_32/BatchNorm/moving_variance] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_32/weights] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_256/BatchNorm/beta] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_256/BatchNorm/gamma] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_256/BatchNorm/moving_mean] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_256/BatchNorm/moving_variance] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_256/weights] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_128/BatchNorm/beta] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_128/BatchNorm/gamma] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_128/BatchNorm/moving_mean] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_128/BatchNorm/moving_variance] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_128/weights] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_128/BatchNorm/beta] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_128/BatchNorm/gamma] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_128/BatchNorm/moving_mean] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_128/BatchNorm/moving_variance] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_128/weights] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_64/BatchNorm/beta] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_64/BatchNorm/gamma] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_64/BatchNorm/moving_mean] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_64/BatchNorm/moving_variance] not available in checkpoint
WARNING:root:Variable [MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_64/weights] not available in checkpoint
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
2017-09-28 12:54:07.684949: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-28 12:54:07.684970: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-28 12:54:07.684974: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-28 12:54:07.684977: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-28 12:54:07.684980: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-09-28 12:54:07.783602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-28 12:54:07.783851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.759
pciBusID 0000:02:00.0
Total memory: 7.92GiB
Free memory: 7.81GiB
2017-09-28 12:54:07.783862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-09-28 12:54:07.783866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-09-28 12:54:07.783871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0)
2017-09-28 12:54:08.774752: I tensorflow/core/common_runtime/simple_placer.cc:697] Ignoring device specification /device:GPU:0 for node 'prefetch_queue_Dequeue' because the input edge from 'prefetch_queue' is a reference connection and already has a device field set to /device:CPU:0
INFO:tensorflow:Restoring parameters from object_detection/mobilenet_v1_025/mobilenet_v1_0.25_128.ckpt
INFO:tensorflow:Restoring parameters from object_detection/mobilenet_v1_025/mobilenet_v1_0.25_128.ckpt
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Assign requires shapes of both tensors to match. lhs shape= [16] rhs shape= [8]
         [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma, save/RestoreV2_1)]]

Caused by op u'save/Assign_1', defined at:
  File ""object_detection/train.py"", line 200, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""object_detection/train.py"", line 196, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/chiuyu/code2/tensorflow/models/research/object_detection/trainer.py"", line 219, in train
    init_saver = tf.train.Saver(available_var_map)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1140, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1172, in build
    filename=self._filename)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 688, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 419, in _AddRestoreOps
    assign_ops.append(saveable.restore(tensors, shapes))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 155, in restore
    self.op.get_shape().is_fully_defined())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py"", line 274, in assign
    validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 43, in assign
    use_locking=use_locking, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [16] rhs shape= [8]
         [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma, save/RestoreV2_1)]]

INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Assign requires shapes of both tensors to match. lhs shape= [16] rhs shape= [8]
         [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma, save/RestoreV2_1)]]

Caused by op u'save/Assign_1', defined at:
  File ""object_detection/train.py"", line 200, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""object_detection/train.py"", line 196, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/chiuyu/code2/tensorflow/models/research/object_detection/trainer.py"", line 219, in train
    init_saver = tf.train.Saver(available_var_map)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1140, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1172, in build
    filename=self._filename)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 688, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 419, in _AddRestoreOps
    assign_ops.append(saveable.restore(tensors, shapes))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 155, in restore
    self.op.get_shape().is_fully_defined())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py"", line 274, in assign
    validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 43, in assign
    use_locking=use_locking, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [16] rhs shape= [8]
         [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma, save/RestoreV2_1)]]

Traceback (most recent call last):
  File ""object_detection/train.py"", line 200, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""object_detection/train.py"", line 196, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/chiuyu/code2/tensorflow/models/research/object_detection/trainer.py"", line 296, in train
    saver=saver)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 738, in train
    master, start_standard_services=False, config=session_config) as sess:
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 964, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 792, in stop
    stop_grace_period_secs=self._stop_grace_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 953, in managed_session
    start_standard_services=start_standard_services)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 708, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session
    init_fn(sess)
  File ""/home/chiuyu/code2/tensorflow/models/research/object_detection/trainer.py"", line 221, in initializer_fn
    init_saver.restore(sess, train_config.fine_tune_checkpoint)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1560, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [16] rhs shape= [8]
         [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma, save/RestoreV2_1)]]

Caused by op u'save/Assign_1', defined at:
  File ""object_detection/train.py"", line 200, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""object_detection/train.py"", line 196, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/chiuyu/code2/tensorflow/models/research/object_detection/trainer.py"", line 219, in train
    init_saver = tf.train.Saver(available_var_map)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1140, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1172, in build
    filename=self._filename)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 688, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 419, in _AddRestoreOps
    assign_ops.append(saveable.restore(tensors, shapes))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 155, in restore
    self.op.get_shape().is_fully_defined())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py"", line 274, in assign
    validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 43, in assign
    use_locking=use_locking, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [16] rhs shape= [8]
         [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[""loc:@FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma, save/RestoreV2_1)]]

ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'init_ops/report_uninitialized_variables/boolean_mask/Gather:0' shape=(?,) dtype=string>
If you want to mark it as used call its ""mark_used()"" method.
It was originally created here:
['File ""object_detection/train.py"", line 200, in <module>\n    tf.app.run()', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))', 'File ""object_detection/train.py"", line 196, in main\n    worker_job_name, is_chief, FLAGS.train_dir)', 'File ""/home/chiuyu/code2/tensorflow/models/research/object_detection/trainer.py"", line 296, in train\n    saver=saver)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 663, in train\n    ready_op = tf_variables.report_uninitialized_variables()', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 175, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 144, in _add_should_use_warning\n    wrapped = TFShouldUseWarningWrapper(x)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 101, in __init__\n    stack = [s.strip() for s in traceback.format_stack()]']
==================================
ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'init_ops/report_uninitialized_variables/boolean_mask/Gather:0' shape=(?,) dtype=string>
If you want to mark it as used call its ""mark_used()"" method.
It was originally created here:
['File ""object_detection/train.py"", line 200, in <module>\n    tf.app.run()', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))', 'File ""object_detection/train.py"", line 196, in main\n    worker_job_name, is_chief, FLAGS.train_dir)', 'File ""/home/chiuyu/code2/tensorflow/models/research/object_detection/trainer.py"", line 296, in train\n    saver=saver)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 663, in train\n    ready_op = tf_variables.report_uninitialized_variables()', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 175, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 144, in _add_should_use_warning\n    wrapped = TFShouldUseWarningWrapper(x)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 101, in __init__\n    stack = [s.strip() for s in traceback.format_stack()]']
==================================








System information

What is the top-level directory of the model you are using: object_detection
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below):
>>> tf.__version__
'1.3.0'

Bazel version (if compiling from source): 0.5.1
CUDA/cuDNN version:  CUDA 8.0, CUDNN 6.0
nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Tue_Jan_10_13:22:03_CST_2017
Cuda compilation tools, release 8.0, V8.0.61

cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
#define CUDNN_MAJOR      6
#define CUDNN_MINOR      0
#define CUDNN_PATCHLEVEL 21
--



GPU model and memory: GeForce GTX 1080, with Total memory: 7.92GiB , Free memory: 7.81GiB
Exact command to reproduce: I am using the vanilla train.py command from the object detection README: 
python2 object_detection/train.py \
    --logtostderr \
    --train_dir='object_detection/data_mobile025/train' \
 --pipeline_config_path='object_detection/data_mobile025/ssd_mobilenet_v1_025.config'


",8,,[],2017-09-28 05:12:34,open,,,['stat:awaiting tensorflower'],2018-11-07 14:38:28
1341,tensorflow/models,models,2469,lucasmoura,Add GloVe model  and word embedding base files,"This MR requests has the idea of creating a common structure for any word embedding model that should be added to the project.

In order to do that, I have created a base word embedding class in python to provide common method for  any word embedding model, such as a method to evaluate the model or to train models concurrently. The  second file is a C++ class used to provide base methods to parse a corpus and creates the vocabulary,  word frequency count and word to id dictionary. 

Both of these classes were created based on the implementation of the [word2vec model](https://github.com/tensorflow/models/tree/master/tutorials/embedding). They were also the base for how the GloVe model should be implemented.

The GloVe model created is an example of how using both of these base class can simplify the creation of word embedding models. In the Glove model it was necessary only to implement the forward pass and the loss function related to GloVe. And to create the examples batches, it was necessary only to implement the Co-Ocurrence matrix in C++ and create the batches in a similar way to the word2vec model.",2,,[],2017-09-27 14:56:03,open,,,['cla: yes'],2017-10-23 19:43:30
1342,tensorflow/models,models,2461,gzoller,Feature Request - Compound Proper Nouns,"I'm using the syntaxnet (English) parser.
I've written no custom code.
Running in a Docker (Ubuntu I think) on MacOS.
Don't know component versions -- pulled latest from github master (running inside the docker I couldn't get the 2 env scripts listed below to work)

### Describe the problem

I noticed that the parser doesn't seem to recognize compound proper nouns. If you gave it ""New York"", it recognizes ""York"" with modifier ""New"", not ""New York"". Could the algorithm recognize that if there are consecutive capitalized words then they may be a proper noun and group all words in one string as the noun? Note that wouldn't work in every case: ""United States of America"" would only recognize ""United States"" and the ""of America"" part would be some kind of modifier, but it's still a great start!

There's also the challenge of first word in sentence.  ""My New York vacation was great"" would (if this algorithm worked) see ""My New York"" as a proper noun.  Could the ML learn to differentiate 
""My"" from ""New York""?  ""New York is a big city"" would still find ""New York"" even though ""New"" was a first word.

### Source code / logs
n/a",0,"NamedUser(login=""andorardo"")","[NamedUser(login=""andorardo"")]",2017-09-26 14:11:45,open,,,['type:support'],2017-09-27 17:45:52
1343,tensorflow/models,models,2451,scotthuang1989,street model can not start run,"### System information
- **What is the top-level directory of the model you are using**:models/research/street/python
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:no gpu support
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I flow the [guide](https://github.com/tensorflow/models/tree/master/research/street) and try to run this command:
```
cd python
train_dir=/tmp/fsns
rm -rf $train_dir
python vgsl_train.py --max_steps=10000 --num_preprocess_threads=1 \
  --train_data=../data/testdata/fsns-00000-of-00001 \
  --initial_learning_rate=0.0001 --final_learning_rate=0.0001 \
  --train_dir=$train_dir
```
but I get following error: 

>   File ""/home/scott/github/models/research/street/python/nn_ops.py"", line 22, in <module>
>     rnn = tf.load_op_library(""../cc/rnn_ops.so"")
>   File ""/home/scott/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 64, in load_op_library
>     None, None, error_msg, error_code)
> 
> tensorflow.python.framework.errors_impl.NotFoundError: ../cc/rnn_ops.so: undefined symbol: _ZN10tensorflow7strings8internal9CatPiecesB5cxx11ESt16initialize
> r_listINS_11StringPieceEE
> 
the `rnn_ops.so`  is in the right folder (cc subfolder), I build rnn_ops.so with this command:
```
cd cc
TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')
g++ -std=c++11 -shared rnn_ops.cc -o rnn_ops.so -fPIC -I $TF_INC -O3 -mavx
```

### Source code / logs

> Traceback (most recent call last):
>   File ""vgsl_train.py"", line 19, in <module>
>     import vgsl_model
>   File ""/home/scott/github/models/research/street/python/vgsl_model.py"", line 25, in <module>
>     import vgslspecs
>   File ""/home/scott/github/models/research/street/python/vgslspecs.py"", line 24, in <module>
>     import nn_ops
>   File ""/home/scott/github/models/research/street/python/nn_ops.py"", line 22, in <module>
>     rnn = tf.load_op_library(""../cc/rnn_ops.so"")
>   File ""/home/scott/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 64, in load_op_library
>     None, None, error_msg, error_code)
> tensorflow.python.framework.errors_impl.NotFoundError: ../cc/rnn_ops.so: undefined symbol: _ZN10tensorflow7strings8internal9CatPiecesB5cxx11ESt16initialize
> r_listINS_11StringPieceEE
> ",2,,[],2017-09-25 02:56:10,open,,,['stat:contributions welcome'],2017-09-28 16:06:26
1344,tensorflow/models,models,2448,tengshaofeng,are you considered add the densenet or dual path net in slim models?,"hi, i am so appreciated with your great job.
and i pay a close attention on this website all the time.
look forward your update. thanks.",1,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-09-23 08:31:23,open,,,"['stat:awaiting tensorflower', 'type:feature']",2017-09-25 21:10:12
1345,tensorflow/models,models,2442,frankkloster,Export Inference Model Error,"After training the pet model in according to the directions [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md) and [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md), specifically training on a local machine, I tried to export my model. This is where I seem to be running into issues. After running the command
```
python export_inference_graph.py \
    --input_type image_tensor \
    --pipeline_config_path ssd_mobilenet_v1_pets.config
    --trained_checkout_prefix data/model.ckpt
    --output_directory output_inference_graph.pb
```
I run into the follow
```
2017-09-22 08:59:26.159018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.2405
pciBusID 0000:03:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
2017-09-22 08:59:26.159073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-09-22 08:59:26.159083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-09-22 08:59:26.159096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)
2017-09-22 08:59:27.246947: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.250254: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.250456: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.254837: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.255132: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.255752: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.257124: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.257963: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.259200: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.260032: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.260870: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.261117: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.264178: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.264817: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.264883: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.265083: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.265378: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.265646: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.265724: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.266107: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.266684: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
2017-09-22 08:59:27.267486: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
Traceback (most recent call last):
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1306, in _run_fn
    status, run_metadata)
  File ""/home/frank/anaconda3/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""export_inference_graph.py"", line 106, in <module>
    tf.app.run()
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""export_inference_graph.py"", line 102, in main
    FLAGS.output_directory)
  File ""/home/frank/tensorflow/models/research/object_detection/exporter.py"", line 376, in export_inference_graph
    optimize_graph, output_collection_name)
  File ""/home/frank/tensorflow/models/research/object_detection/exporter.py"", line 336, in _export_inference_graph
    trained_checkpoint_prefix=trained_checkpoint_prefix)
  File ""/home/frank/tensorflow/models/research/object_detection/exporter.py"", line 295, in _write_graph_and_checkpoint
    saver.restore(sess, trained_checkpoint_prefix)
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1560, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]

Caused by op 'save/Assign_10', defined at:
  File ""export_inference_graph.py"", line 106, in <module>
    tf.app.run()
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""export_inference_graph.py"", line 102, in main
    FLAGS.output_directory)
  File ""/home/frank/tensorflow/models/research/object_detection/exporter.py"", line 376, in export_inference_graph
    optimize_graph, output_collection_name)
  File ""/home/frank/tensorflow/models/research/object_detection/exporter.py"", line 336, in _export_inference_graph
    trained_checkpoint_prefix=trained_checkpoint_prefix)
  File ""/home/frank/tensorflow/models/research/object_detection/exporter.py"", line 291, in _write_graph_and_checkpoint
    tf.import_graph_def(inference_graph_def, name='')
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/frank/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [228] rhs shape= [546]
	 [[Node: save/Assign_10 = Assign[T=DT_FLOAT, _class=[""loc:@BoxPredictor_2/ClassPredictor/biases""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](BoxPredictor_2/ClassPredictor/biases, save/RestoreV2_10/_25)]]
```
Please let me know if you need anything else!

Thanks!",2,,[],2017-09-22 16:18:57,open,,,['stat:awaiting tensorflower'],2017-10-11 06:41:55
1346,tensorflow/models,models,2441,MatthewShotton,Refactor object detection tutorial to use recommended PIL to numpy conversion,"This PR simplifies the object detection tutorial a little by removing the helper function for converting PIL/Pillow images to numpy arrays by using the numpy array interface on the PIL.Image object (introduced PIL 1.1.6). Which seems to be the preferred method for converting between the two [1] [2].

[1] [https://stackoverflow.com/a/384926](https://stackoverflow.com/a/384926)
[2] [http://effbot.org/zone/pil-changes-116.htm](http://effbot.org/zone/pil-changes-116.htm)",0,,[],2017-09-22 15:20:31,open,,,['cla: yes'],2017-09-28 02:43:24
1347,tensorflow/models,models,2424,SourcePowered,Run Object delection train.py ERROR,"```
Traceback (most recent call last):
  File ""train.py"", line 198, in <module>
    tf.app.run()
  File ""E:\Development_Software\Python\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""train.py"", line 145, in main
    model_config, train_config, input_config = get_configs_from_multiple_files()
  File ""train.py"", line 127, in get_configs_from_multiple_files
    text_format.Merge(f.read(), train_config)
  File ""E:\Development_Software\Python\Python36\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 118, in read
    self._preread_check()
  File ""E:\Development_Software\Python\Python36\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 78, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""E:\Development_Software\Python\Python36\lib\contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""E:\Development_Software\Python\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open:  : ϵͳ\udcd5Ҳ\udcbb\udcb5\udcbdָ\udcb6\udca8\udcb5\udcc4·\udcbe\udcb6\udca1\udca3
```

On Windows 10, Python 3.6.2, Tensorflow 1.3.

I have spent 11 days trying to study it, but I can't solve it at all. I checked all kinds of coding problems and couldn't find any clue.

Please help me, I'm hopeless now. Thank you",6,,[],2017-09-21 03:39:08,open,,,['stat:awaiting response'],2018-12-14 13:05:34
1348,tensorflow/models,models,2423,buaaliyi,[Attention OCR] demo_inference needs the same image preprocess as train phase,"Found a bug maybe... I think the inference phase should pre-process the images first, as same as the training phase",0,,[],2017-09-21 03:36:21,open,,,['cla: yes'],2017-09-28 02:43:24
1349,tensorflow/models,models,2409,jlim262,Add default parameters for visualize_detection_results,"To configure **min_score_thresh** and **max_num_predictions** from **eval_config** of config file, it needs to add those variables into **eval.proto** and add default params into **eval_util.visualize_detection_results()** from **evaluator.py**. 
Those default values are .5(for **min_score_thresh**) and 20(for  **max_num_predictions**) which are for default values of **eval_util.visualize_detection_results()** as below. 

```
def visualize_detection_results(result_dict,
                                tag,
                                global_step,
                                categories,
                                summary_dir='',
                                export_dir='',
                                agnostic_mode=False,
                                show_groundtruth=False,
                                min_score_thresh=.5,
                                max_num_predictions=20):
```",0,,[],2017-09-19 05:27:47,open,,,['cla: yes'],2017-09-28 02:43:23
1350,tensorflow/models,models,2408,buaaliyi,Fix 'momentum' gflag type in attention_ocr,"Fix the typo, FLAGS.momentum should be defined as float type.",0,,[],2017-09-19 03:12:43,open,,,['cla: yes'],2017-09-28 02:43:23
1351,tensorflow/models,models,2402,ronrest,TF-Slim model Inputs - (range and dtype),"## Issue Description
The pretrained TF-Slim models (at least the mobilenet models) do not contain information about what range of values or datatype they expect as inputs. 

For instance, the source code for the mobilenet architecture ([here](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py#L154), and [here](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py#L283)), only contains information about the dimensions of the inputs, stating: 

    inputs: a tensor of shape [batch_size, height, width, channels].

But it does not contain the following important information:

- Range of the input values. Should they be: 
    - 0 to 255 ints?
    - -1 to 1 floats?
    - 0 to 1 floats?

- Datatypes. Should they be:
    - Unsigned 8 bit integers?
    - int32
    - float 32

## Proposed Solution

Including this information in either (or, preferably both): 

- the docstrings of the source code
- the README.md file 
    - eg, on the table that lists all the models if each model expects something different. 
    - or in a single spot in the README.md file if all the models expect the same type of input values. 

If someone is willing to inform me what they expect as inputs, then I would be willing to make some of the proposed solutions to the documentation. 
 
",2,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-09-17 10:18:41,open,,,"['stat:awaiting tensorflower', 'type:docs']",2018-03-28 07:38:45
1352,tensorflow/models,models,2401,ronrest,Update README.md,"Fix issue #2400
- Add column for links to graph def files for pre-trained model
- Included links for mobilenet models and Inception v3
- Addresses this feature request https://github.com/tensorflow/models/issues/",0,,[],2017-09-17 10:00:03,open,,,[],2017-09-28 02:43:23
1353,tensorflow/models,models,2400,ronrest,link to graph_def file in TF Slim models readme.md,"## Feature Request Description
The readme.md file for the TF Slim pre-trained models contains several columns, with information and links.

However, it is missing a column with links to the graph_def file. 

## Proposed solution
I propose something like this (only a subset of the table shown below): 


Model | Code | Graph File | Checkpoint | Top-1 Accuracy| Top-5 Accuracy |
:----:|:------------:|:----------:|:----------:|:-------------:|:--------------:|
..|..|..|..|..|..|
[MobileNet_v1_1.0_224](https://arxiv.org/pdf/1704.04861.pdf)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py)|[graph](http://download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz)|[mobilenet_v1_1.0_224_2017_06_14.tar.gz](http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz)|70.7|89.5|
[MobileNet_v1_0.50_160](https://arxiv.org/pdf/1704.04861.pdf)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py)|[graph](http://download.tensorflow.org/models/mobilenet_v1_0.50_160_frozen.tgz)|[mobilenet_v1_0.50_160_2017_06_14.tar.gz](http://download.tensorflow.org/models/mobilenet_v1_0.50_160_2017_06_14.tar.gz)|59.9|82.5|
[MobileNet_v1_0.25_128](https://arxiv.org/pdf/1704.04861.pdf)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py)|[graph](http://download.tensorflow.org/models/mobilenet_v1_0.25_128_frozen.tgz)|[mobilenet_v1_0.25_128_2017_06_14.tar.gz](http://download.tensorflow.org/models/mobilenet_v1_0.25_128_2017_06_14.tar.gz)|41.3|66.2|


I am willing to create a Pull Request with this proposed change. 
",0,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-09-17 09:42:50,open,,,"['stat:awaiting tensorflower', 'type:docs']",2019-03-15 23:28:21
1354,tensorflow/models,models,2395,xzy256, Why does ‘Mixed_5b’  contains 5x5 cov  in inception v3 net,"How do explain remain `5x5` conv layer? 
When  i see the v3 source code[models/slim/nets/inception_v3.py], i find it remains `5x5` convolution layer and lack of only one ` 1x1 - 3x3` cov layer branch. According  v3 paper `Rethinking the Inception Architecture for Computer Vision`, `5x5` convolution  is replaced by` two 3x3`. 
<pre>
with tf.variable_scope('Branch_0'):
          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
with tf.variable_scope('Branch_1'):
          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope='Conv2d_0a_1x1')
          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5], scope='Conv2d_0b_5x5')
 with tf.variable_scope('Branch_2'):
          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],  scope='Conv2d_0b_3x3')
          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],  scope='Conv2d_0c_3x3')
 with tf.variable_scope('Branch_3'):
          branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],  scope='Conv2d_0b_1x1')

</pre>",1,,[],2017-09-15 09:45:01,open,,,['stat:awaiting tensorflower'],2017-09-15 19:42:31
1355,tensorflow/models,models,2388,balajiravichandiran,Error on validating model config in model_builder.py ,"I tried to train object detection with my own data, can run locally without any issue, i followed tutorial to deploy on Google Cloud, I always get same. All my buckets are same regional.
[ssd_mobilenet_cocov2.txt](https://github.com/tensorflow/models/files/1302973/ssd_mobilenet_cocov2.txt)



Training input	
```
{
  ""scaleTier"": ""CUSTOM"",
  ""masterType"": ""standard_gpu"",
  ""workerType"": ""standard_gpu"",
  ""parameterServerType"": ""standard"",
  ""workerCount"": ""5"",
  ""parameterServerCount"": ""3"",
  ""packageUris"": [
    ""gs://mv_regional/train_checkpoint/packages/524cd800eacdfbca89a487dc1bb1501482b9d14481e814be2c0fbcc9a0fd031a/object_detection-0.1.tar.gz"",
    ""gs://mv_regional/train_checkpoint/packages/524cd800eacdfbca89a487dc1bb1501482b9d14481e814be2c0fbcc9a0fd031a/slim-0.1.tar.gz""
  ],
  ""pythonModule"": ""object_detection.train"",
  ""args"": [
    ""--train_dir=gs://mv_regional/train_checkpoint/train"",
    ""--pipeline_config_path=gs://mv_regional/ssd_mobilenet_cocov2.config""
  ],
  ""region"": ""us-central1"",
  ""runtimeVersion"": ""1.0"",
  ""jobDir"": ""gs://mv_regional/train_checkpoint/""
}
```



Error message
The replica master 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 198, in <module> tf.app.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run _sys.exit(main(_sys.argv[:1] + flags_passthrough)) File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 194, in main worker_job_name, is_chief, FLAGS.train_dir) File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/trainer.py"", line 159, in train detection_model = create_model_fn() File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/builders/model_builder.py"", line 70, in build raise ValueError('model_config not of type model_pb2.DetectionModel.') ValueError: model_config not of type model_pb2.DetectionModel. The replica worker 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 198, in <module> tf.app.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run _sys.exit(main(_sys.argv[:1] + flags_passthrough)) File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 194, in main worker_job_name, is_chief, FLAGS.train_dir) File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/trainer.py"", line 159, in train detection_model = create_model_fn() File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/builders/model_builder.py"", line 70, in build raise ValueError('model_config not of type model_pb2.DetectionModel.') ValueError: model_config not of type model_pb2.DetectionModel. The replica worker 1 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 198, in <module> tf.app.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run _sys.exit(main(_sys.argv[:1] + flags_passthrough)) File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 194, in main worker_job_name, is_chief, FLAGS.train_dir) File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/trainer.py"", line 159, in train detection_model = create_model_fn() File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/builders/model_builder.py"", line 70, in build raise ValueError('model_config not of type model_pb2.DetectionModel.') ValueError: model_config not of type model_pb2.DetectionModel. The replica worker 2 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 198, in <module> tf.app.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run _sys.exit(main(_sys.argv[:1] + flags_passthrough)) File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 194, in main worker_job_name, is_chief, FLAGS.train_dir) File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/trainer.py"", line 159, in train detection_model = create_model_fn() File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/builders/model_builder.py"", line 70, in build raise ValueError('model_config not of type model_pb2.DetectionModel.') ValueError: model_config not of type model_pb2.DetectionModel. The replica worker 3 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 198, in <module> tf.app.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run _sys.exit(main(_sys.argv[:1] + flags_passthrough)) File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 194, in main worker_job_name, is_chief, FLAGS.train_dir) File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/trainer.py"", line 159, in train detection_model = create_model_fn() File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/builders/model_builder.py"", line 70, in build raise ValueError('model_config not of type model_pb2.DetectionModel.') ValueError: model_config not of type model_pb2.DetectionModel. The replica worker 4 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 162, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 198, in <module> tf.app.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run _sys.exit(main(_sys.argv[:1] + flags_passthrough)) File ""/root/.local/lib/python2.7/site-packages/object_detection/train.py"", line 194, in main worker_job_name, is_chief, FLAGS.train_dir) File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/trainer.py"", line 159, in train detection_model = create_model_fn() File ""/root/.local/lib/python2.7/site-packages/object_detection/object_detection/builders/model_builder.py"", line 70, in build raise ValueError('model_config not of type model_pb2.DetectionModel.') ValueError: model_config not of type model_pb2.DetectionModel. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=381957343869&resource=ml_job%2Fjob_id%2Fobject_detection_1505389225&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22object_detection_1505389225%22
",10,,[],2017-09-14 12:45:55,open,,,['stat:awaiting tensorflower'],2017-10-11 07:21:13
1356,tensorflow/models,models,2387,shamanez,Object Detection : Cannot run eval.py locally . It gives warnings for TF 1.3 ,"I am using tensorflow 1.3 . 
My training script run well locally . But evaluation is not showing the results or printing the results . 
Here's the tensorboard Visualization 
![image](https://user-images.githubusercontent.com/16892570/30423380-3b8b5474-9975-11e7-92bc-5db7e90ef2c1.png)

Then this gives this directory 
![image](https://user-images.githubusercontent.com/16892570/30423468-76e47bc2-9975-11e7-8b58-ba38a03bbc31.png)
",7,,[],2017-09-14 09:52:31,open,,,['stat:awaiting tensorflower'],2018-02-23 09:50:04
1357,tensorflow/models,models,2386,Prasad9,Explanation of keys present in TFRecord,"I have done lot of Google search but I am not able to find any answer to my questions. In generation of TFRecords (ex: [create_pet_tf_record.py](https://github.com/tensorflow/models/blob/master/object_detection/create_pet_tf_record.py)), there are few keys for which I am not able to find any documentation associated with it.
1) There is `image/object/difficult` and `image/object/truncated` which I am not able to understand. Regarding the `image/object/truncated` key, I believe, if the image of object to be detected is in a cropped manner, we set the value to 1 and 0 otherwise. Is that correct?

2) Please can somebody tell me all the values present in `image/object/view`. Right now I am just aware of value of `Frontal`.

3) In many of the blogs which I was looking into before, they have created a key for `occlusion` as well, which I am not able to find in any of the official examples of generating TFRecord. Is there any key for occlusion or not?

4) Please can somebody give me the link where all the keys present in TFRecord are explained in detail?
",7,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-09-14 08:59:21,open,,,"['stat:contributions welcome', 'type:docs']",2018-04-15 01:51:10
1358,tensorflow/models,models,2382,AyushiAggarwal,[ParseyMcParseface][Ambiguity] Parse score for sentence with more than one interpretation OR Multiple parse trees for a single input sentence,"Using ParseyMcParseface to analyse ambiguous sentences such as the infamous example below: 

_I saw a man on the hill with a telescope._

1.  Is there a way to evaluate the different parse trees for this sentence? Such as - some sort of a confidence score/grammaticality score generated by the parser for each sentence? If yes, where can I view it?
2. Alternatively, is there a way to obtain multiple parse trees for a single input sentence? 
",1,,[],2017-09-13 22:53:35,open,,,[],2018-04-06 07:45:26
1359,tensorflow/models,models,2377,phongnhhn92,Using GPU for object detection during testing time.,"Hello guys, I successfully trained my object detector using my custom dataset.Using the file object_detection_tutorial.ipynb provided from the original object detection module, I observed the fact that during the inference time, the CPU has to work a lot (almost 100%) while the GPU usage doesnt change much. 
`(boxes, scores, classes, num_detections) = sess.run([boxes, scores, classes, num_detections],              feed_dict={image_tensor: image_np_expanded})`
I suspect that 'feed_dict={image_tensor: image_np_expanded})` will use CPU for detection so I am asking you guys that is there any ways to utilize GPU for object detection using Tensorflow ? 
In my case, it took 0.3 s for SSD_MobileNets but FasterRCNN_resnet 101 took almost 8 second to make prediction for one image.
![8x](https://user-images.githubusercontent.com/11288381/30363071-cb610ae4-9899-11e7-9def-a33e34746938.png)
![highcpu](https://user-images.githubusercontent.com/11288381/30363077-d089cbd2-9899-11e7-9780-cecb9bc81135.png)



",6,,[],2017-09-13 06:40:18,open,,,['stat:awaiting tensorflower'],2018-06-14 22:10:08
1360,tensorflow/models,models,2376,jingjingwang,Release other pre-trained object detection models?,"### Describe the problem
Could you provide other pre-trained models that are evaluated in your object detection [paper](https://arxiv.org/pdf/1611.10012.pdf)? For example, those based on VGG. Right now the [model zoo](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md) only has 5 models. It would be very helpful for people who want to evaluate/test more model architectures on more data. I hope it's not much work since these models have already been evaluated.
",2,,[],2017-09-13 06:31:18,open,,,"['stat:contributions welcome', 'type:feature']",2017-11-18 07:56:08
1361,tensorflow/models,models,2374,EpochalEngineer,Frozen pretrained Faster RCNN/RFCN networks from model zoo yielding different outputs on different GPUs and runs,"### System information
- **What is the top-level directory of the model you are using**:
Using unmodified pretrained coco models: faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017, faster_rcnn_resnet101_coco_11_06_2017, rfcn_resnet101_coco_11_06_2017

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
UPDATE: tested on two machines now, both reproduce it:
Machine 1: Linux Ubuntu 14.04.4 LTS
Machine 2: Linux Ubuntu 16.04.2 LTS

- **TensorFlow installed from (source or binary)**:
official docker container, with last commit 58fb6d7e257f28cd7934316d6ae7a81ec42a533a
docker version from 2017-08-24T02:37:57.51182742Z

- **TensorFlow version (use command below)**:
('v1.2.0-5-g435cdfc', '1.2.1')

- **Bazel version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
From official docker: CUDA 8., cuDNN 5.1.10

- **GPU model and memory**:
Machine 1: Three nVIDIA GeForce GTX 1080, 12 GB
Machine 2: Two nVIDIA GeForce GTX 1080, 12 GB

- **Exact command to reproduce**:
Running object_detection_tutorial.ipynb with different GPUs, either with export CUDA_VISIBLE_DEVICES=, or by setting it in the session config.  Version that runs through 3 GPUs several times and compares output is included.

### Describe the problem
Running on different GPUs yields different results, and GPUs 1 and 2 are not deterministic.  This is accomplished by making devices 1,2 invisible, and tensorflow runs on 0, and so forth.  This is using frozen pretrained networks from this repository's linked [model zoo](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md) and the supplied object_detection_tutorial.ipynb with no modifications other than setting the cuda visible_device_list.  The SSD frozen models, however, give identical outputs on the 3 GPUs from what I have seen.

I have also run [cuda_memtest](https://sourceforge.net/projects/cudagpumemtest/) on all 3 GPUs, logs attached

UPDATE: I just tested on a second machine with 2 GPUs, and reproduced the issue.  GPU 0 is deterministic, GPU 1 is not (and often produces bad results).

### Source code / logs
I've attached the diff of the modified object_detection_tutorial.ipynb which loops over 3 GPUs 3 times and prints out the top box scores, which change depending on the run.  Also attached is a PDF of that ipynb with detections drawn on it.  Text output:

> Evaluating image 0
> 
> 	Running on GPU 0
> 		Top 4 box scores: 
> 		Iter 1: [ 0.99978215  0.99857557  0.95300484  0.91580492]
> 		Iter 2: [ 0.99978215  0.99857557  0.95300484  0.91580492]
> 		Iter 3: [ 0.99978215  0.99857557  0.95300484  0.91580492]
> 
> 	Running on GPU 1
> 		Top 4 box scores: 
> 		Iter 1: [ 0.68702352  0.16781448  0.13143283  0.12993629]
> 		Iter 2: [ 0.18502565  0.16854601  0.08074528  0.07859289]
> 		Iter 3: [ 0.18502565  0.16854601  0.05546702  0.05111229]
> 
> 	Running on GPU 2
> 		Top 4 box scores: 
> 		Iter 1: [ 0.68702352  0.16781448  0.13143283  0.12993629]
> 		Iter 2: [ 0.18941374  0.18502565  0.16854601  0.16230994]
> 		Iter 3: [ 0.18502565  0.16854601  0.05546702  0.05482833]
> 
> 
> Evaluating image 1
> 
> 	Running on GPU 0
> 		Top 4 box scores: 
> 		Iter 1: [ 0.99755412  0.99750346  0.99380219  0.99067008]
> 		Iter 2: [ 0.99755412  0.99750346  0.99380219  0.99067008]
> 		Iter 3: [ 0.99755412  0.99750346  0.99380219  0.99067008]
> 
> 	Running on GPU 1
> 		Top 4 box scores: 
> 		Iter 1: [ 0.96881998  0.96441168  0.96164131  0.96006596]
> 		Iter 2: [ 0.9377929   0.91686022  0.80374646  0.79758978]
> 		Iter 3: [ 0.90396696  0.89217037  0.85456908  0.85334581]
> 
> 	Running on GPU 2
> 		Top 4 box scores: 
> 		Iter 1: [ 0.9377929   0.91686022  0.80374646  0.79758978]
> 		Iter 2: [ 0.9377929   0.91686022  0.80374646  0.79758978]
> 		Iter 3: [ 0.9377929   0.91686022  0.80374646  0.79758978]

[object_detection_tutorial.diff.txt](https://github.com/tensorflow/models/files/1313430/object_detection_tutorial.diff.txt)

[gpu_output_differences.pdf](https://github.com/tensorflow/models/files/1313428/gpu_output_differences.pdf)

Updated with longer run:
[cuda_memtest.log.txt](https://github.com/tensorflow/models/files/1315285/cuda_memtest.log.txt)


",5,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""jch1"")]",2017-09-13 01:39:20,open,,,['type:bug/performance'],2018-04-06 07:45:22
1362,tensorflow/models,models,2372,gauss-clb,A bug in object detection?,"In [argmax_matcher.py](https://github.com/tensorflow/models/blob/master/object_detection/matchers/argmax_matcher.py#L158), the algorithm of `_force_match_for_each_row` is wrong?

It seems that use the maximum of row to force_match_for_each_row, but many rows can have  maximum  with the same column.

For example,  to use similarity
```
similarity = np.array([[0,1,2],
                        [1,2,3],
                        [2,3,4]], dtype=np.int32)
```
instead of similarity in [test_return_correct_matches_unmatched_row_while_using_force_match](https://github.com/tensorflow/models/blob/master/object_detection/matchers/argmax_matcher_test.py#L167). You will get matched_rows with length of 2.

Another question is [assertEmpty](https://github.com/tensorflow/models/blob/master/object_detection/matchers/argmax_matcher_test.py#L47), it will raise a error, because I can't find it is a member function of `tf.test.TestCase`.

",2,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-09-12 11:40:00,open,,,"['stat:awaiting tensorflower', 'type:bug/performance']",2018-05-21 10:20:13
1363,tensorflow/models,models,2368,yzy123,Update eval_on_adversarial.py,,1,,[],2017-09-11 22:08:52,open,,,['cla: no'],2017-09-28 02:43:22
1364,tensorflow/models,models,2366,ALEXKIRNAS,Object detection | Keypoints to object detection model,"Hi,
@jch1 , @tombstone , @derekjchow , I would appreciate your help.

**ADDING FUNCTIONALITY** 
I recently start to use SSD with MobileNet feature extractor for solving face detection problem . In term of this task I also need to to detect landmarks (keypoints). I saw that repo contain some files that connected with keypoints ( [link](https://github.com/tensorflow/models/blob/master/object_detection/core/keypoint_ops.py)  ), but  I don`t see how to add them to model.  So can you describe how to do that?

With the best regards,
Alexander",3,,[],2017-09-11 15:45:22,open,,,"['stat:awaiting tensorflower', 'type:feature']",2018-06-01 18:25:27
1365,tensorflow/models,models,2365,leacoleaco,"fixed ""learning rate is illegal"" warning",when I training the data with tensorflow 1.3， the console print the warning text ”learning rate is illegal“ .  so i find and changed this word,3,,[],2017-09-11 13:26:26,open,,,['cla: yes'],2017-09-28 02:43:22
1366,tensorflow/models,models,2361,w19787,latest version is using deprecated api,"
### System information
- **What is the top-level directory of the model you are using**: slim/InceptionV4
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Bazel version (if compiling from source)**: no
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no Gpu, 8G memory
- **Exact command to reproduce**:


### How to reproduce
fine turning flower dataset based on pre-trained inception V4 chkpt.
python train_image_classifier.py    --train_dir=${TRAIN_DIR}    --dataset_name=flowers    --dataset_split_name=train    --dataset_dir=${DATASET_DIR}    --model_name=inception_v4    --checkpoint_path=${CHECKPOINT_PATH}    --checkpoint_exclude_scopes=InceptionV4/Logits,InceptionV4/AuxLogits    --trainable_scopes=InceptionV4/Logits,InceptionV4/AuxLogits    --max_number_of_steps=200    --batch_size=32    --learning_rate=0.01    --learning_rate_decay_type=fixed    --save_interval_secs=60    --save_summaries_secs=60    --log_every_n_steps=100    --optimizer=rmsprop    --weight_decay=0.00004    --clone_on_cpu=True


### issue
WARNING:tensorflow:From train_image_classifier.py:466: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.
WARNING:tensorflow:From /home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.compute_weighted_loss instead.
WARNING:tensorflow:From /home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.add_loss instead.
WARNING:tensorflow:From train_image_classifier.py:468: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.
WARNING:tensorflow:From /home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.compute_weighted_loss instead.
WARNING:tensorflow:From /home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.add_loss instead.
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.


### finding 

on Aug 31, 2007, the slim update version bring the issue in. I don't know why the old api is using in latest version.
git diff 3d6dc1dd0a93be848a742a1f26ffb51ff96a9d52 42f507f51bd612a9d5dda01672d9460a68b4914f",1,,[],2017-09-11 03:05:49,open,,,['stat:awaiting tensorflower'],2017-09-11 18:40:57
1367,tensorflow/models,models,2360,ecooler,add script that create graph file for inception model,"I add a script file that can create graph file for inception model which
will help you export trained inception deep learning models.",1,,[],2017-09-10 12:11:07,open,,,['cla: no'],2017-09-28 02:43:22
1368,tensorflow/models,models,2359,UndeadBlow,Stuck training with increased input size,"### System information
- **What is the top-level directory of the model you are using**: models/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0-rc2
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: 375.82
- **GPU model and memory**: GeForce GTX 770 4GB
- **Exact command to reproduce**:
as default:
```
 python3 object_detection/train.py \
    --logtostderr \
    --pipeline_config_path=ssd_mobilenet_v1_signs.config \
    --train_dir=object_detection/test

```

### Problem
I'm trying to fine tune (to be more precise to overfit some little dataset) ssd_mobilenet_v1 from coco checkpoint. With default parameters all is ok, but when I try to change inpurt size from 300x300 to 512x512 or 600x600 or any other, training stucks. I've tried all possible combinations of hyperparameters, tried to train from scratch, tried to train on Pascal VOC, but every time training stucks. For example, with default parameters from ssd_mobilenet_v1_signs.config and default PascalVOC2012, with the only change:
```
image_resizer {
      fixed_shape_resizer {
        height: 512
        width: 512
      }
    }
```
Training stucks on error with value ~10-13. On some little subsets (when I'm try to overfit for test) it gives little error (like ~0.2) but then it finds no objects on train frames (all scores very small like network didn't learn something at all).

Does the network recalculated when resizing? Is it supposed that a change in the input size should generally work?

### Source code / logs
Source code unchanged, except described. 
Logs is like endless:
```
....
INFO:tensorflow:global step 20408: loss = 12.8249 (1.114 sec/step)
INFO:tensorflow:global step 20409: loss = 11.7554 (0.877 sec/step)
INFO:tensorflow:global step 20410: loss = 12.4740 (0.863 sec/step)
....
```
",1,,[],2017-09-08 15:29:39,open,,,['stat:awaiting tensorflower'],2017-09-11 18:42:02
1369,tensorflow/models,models,2341,ArjanSchouten,Add option to configure max GPU memory fraction for training and evaluation in object_detection,"**Subfolder**: object_detection
**Use Case**:
I want to follow the accuracy over time during training. I run train.py and eval.py at the same time. train.py is directly allocating all GPU VRAM so eval.py can't restore the graph anymore and results in OOM (since I use one GPU).

**Changed method definition**:
An additional parameter is added in eval_util.py (method repeated_checkpoint_run and run_checkpoint_once). If this is problematic then I would suggest leaving the config option for eval and only add the option to train.proto.

(Original [PR](https://github.com/tensorflow/models/pull/2318))",3,,[],2017-09-05 13:55:09,open,,,['cla: yes'],2017-09-28 02:43:21
1370,tensorflow/models,models,2336,ArjanSchouten,Feature request random rotating object_detection API,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0
- **Bazel version (if compiling from source)**: 0.5.2

**Situation:**
I'm using the object detection API to detect features on images. The problem I work on is not focused on photograph but on more structural images. My training set now contains around 500 images. The accuracy has increased a lot since I added some data augmentation options.

The results I see so far are promising. For the same boxes with a horizontal feature it is working perfect. For the same box vertical it is failing. My training set contains them both. 

**Feature request:**
I was wondering if it can be improved by randomly rotating the input images to fix some of the not detected vertical aligned boxes. Currently this preprocessing option does not exist in object_detection. Do you think adding an preprocessor feature for random rotation is helpful (is a PR useful)?",7,,[],2017-09-04 12:15:46,open,,,"['stat:awaiting tensorflower', 'type:feature']",2019-03-21 13:05:33
1371,tensorflow/models,models,2332,isharaux,ImportError: No module named 'nets',"I am trying to convert trained_checkpoint  to final frozen model from the export_inference_graph.py script provided in tensorflow/models,but the following error results.
And yes,I have already setup $PYTHONPATH to ""models/slim"" but still I get this error,can someone help me out?

```
$ echo $PYTHONPATH
:/home/ishara/tensorflow_models/models:/home/ishara/tensorflow_models/models/slim
```

*********************************************************************************************************************
```
$sudo python3 object_detection/export_inference_graph.py  --input_type image_tensor  --pipeline_config_path = ""ssd_inception_v2_pets.config""  --trained_checkpoint_prefix=""output/model.ckpt-78543""  --output_directory=""birds_inference_graph.pb""

Traceback (most recent call last):
  File ""object_detection/export_inference_graph.py"", line 74, in <module>
    from object_detection import exporter
  File ""/usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/exporter.py"", line 28, in <module>
    
  File ""/usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/builders/model_builder.py"", line 30, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py"", line 28, in <module>
ImportError: No module named 'nets'
```
********************************************************************************************************************
I have been struggling with this for days now,tried many solutions nothing work
I am using Ubuntu 16.04 with tensorflow gpu version.",10,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-09-04 04:46:27,open,,,"['stat:awaiting tensorflower', 'type:bug/performance']",2019-01-15 04:34:14
1372,tensorflow/models,models,2331,bjmajic,maybe a bug in models/slim/deployment/model_deploy.py,"def _sum_clones_gradients(clone_grads):
  """"""Calculate the sum gradient for each shared variable across all clones.
  This function assumes that the clone_grads has been scaled appropriately by
  1 / num_clones.

howerver,  in ""def _optimize_clone(optimizer, clone, num_clones, regularization_losses, **kwargs):""
there is :
if sum_loss is not None:
    with tf.device(clone.device):
      clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)

i think ' clone_grad ' is not scaled appropriately by 1 / num_clones.

",0,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-09-04 03:14:14,open,,,[],2017-09-04 03:19:58
1373,tensorflow/models,models,2328,bwuzhang,tfdbg on slim not compatible with Object Detection API,"### System information
- **What is the top-level directory of the model you are using**: 
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.3 with latest checkout of tensorflow/tensorflow/contrib/slim/python/slim/learning.py
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 8.0, 5.1
- **GPU model and memory**: 1080 Ti 12GB
- **Exact command to reproduce**:

### Describe the problem
I want to debug a NaN error with tfdbg on the Object Detection API rfcn network. I checkout latest version of 'tensorflow/tensorflow/contrib/slim/python/slim/learning.py' to include the tfdbg support. After hitting 'run' twice in tfdbg, I encountered the following error. This can be reproduced with the pets example by adding 'session_wrapper=tf_debug.LocalCLIDebugWrapperSession' to the 'slim.learning.train' in 'models/object_detection/trainer.py'. 

### Source code / logs
2017-09-03 13:04:11.473343: I tensorflow/core/debug/debug_graph_utils.cc:229] For debugging, tfdbg is changing the parallel_iterations attribute of the Enter/RefEnter node ""gradients/map/while/TensorArrayReadV3/Enter_1_grad/b_acc_1"" on device ""/job:localhost/replica:0/task:0/gpu:0"" from 16 to 1. (This does not affect subsequent non-debug runs.)
INFO:tensorflow:Error reported to Coordinator: <type 'exceptions.ValueError'>, Node name 'parallel_read/filenames/Assert/Assert/data_0' is not found in partition graphs of device /job:localhost/replica:0/task:0/cpu:0.
Traceback (most recent call last):
  File ""object_detection/train.py"", line 202, in <module>
    tf.app.run()
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""object_detection/train.py"", line 198, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/local/mnt/workspace/chris/projects/models/object_detection/trainer.py"", line 310, in train
    session_wrapper=tf_debug.LocalCLIDebugWrapperSession)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 777, in train
    sv.stop(threads, close_summary_writer=True)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/contextlib.py"", line 35, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 964, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 792, in stop
    stop_grace_period_secs=self._stop_grace_secs)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 238, in _run
    enqueue_callable()
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 570, in wrapped_runner
    callable_runner_args=runner_args)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 532, in run
    run_end_resp = self.on_run_end(run_end_req)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 319, in on_run_end
    self._dump_root, partition_graphs=partition_graphs)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 690, in __init__
    self._load_all_device_dumps(partition_graphs, validate)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 712, in _load_all_device_dumps
    self._load_partition_graphs(partition_graphs, validate)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 1009, in _load_partition_graphs
    self._validate_dump_with_graphs(device_name)
  File ""/local/mnt/workspace/chris/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 1208, in _validate_dump_with_graphs
    ""device %s."" % (datum.node_name, device_name))
ValueError: Node name 'parallel_read/filenames/Assert/Assert/data_0' is not found in partition graphs of device /job:localhost/replica:0/task:0/cpu:0.

",8,,[],2017-09-03 17:05:46,open,,,['stat:awaiting tensorflower'],2018-05-29 09:55:35
1374,tensorflow/models,models,2322,jayshah19949596,Error importing string_int_label_map_pb2,"When I run the object_detection_tutorial.ipynb i get the following error 
ImportError Traceback (most recent call last)
in ()
----> 1 from utils import label_map_util
2
3 from utils import visualization_utils as vis_util
D:\Personal\learn\ML\DL\Tensorflow\Official-Tutorial\models-master\object_detection\utils\label_map_util.py in ()
20 import tensorflow as tf
21 from google.protobuf import text_format
---> 22 from object_detection.protos import string_int_label_map_pb2
23
24
ImportError: cannot import name 'string_int_label_map_pb2'

I did run the following command : ""C:\Program Files\protoc\bin\protoc"" object_detection/protos/*.proto --python_out=.

but still error in importing string_int_label_map_pb2",13,,[],2017-09-01 08:50:36,open,,,['stat:awaiting response'],2019-03-31 08:34:05
1375,tensorflow/models,models,2309,zhosteven,Modify the configuration directory.,,0,,[],2017-08-30 10:14:38,open,,,['cla: yes'],2017-09-28 02:43:21
1376,tensorflow/models,models,2308,thalitadru,DenseNet and FC-DenseNet,"I implemented slim models for DenseNet and FC-DenseNet, based on the original lasagne/theano code made available by the authors of FC-DenseNets. Hope it will be useful. 
- [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)
- [The One Hundred Layers Tiramisu: Fully Convolutional
  DenseNets for Semantic Segmentation](https://arxiv.org/pdf/1611.09326.pdf) - [code](https://github.com/SimJeg/FC-DenseNet/blob/master/FC-DenseNet.py)



",4,,[],2017-08-30 10:01:36,open,,,['cla: yes'],2018-02-28 11:01:05
1377,tensorflow/models,models,2303,scottclowe,DOC: Change link for slim's imagenet instructions,"We now link to the instructions to download and convert imagenet to
TFrecords contained in the slim README instead of the inception
README.

The build script in the inception folder does not create a
labels.txt file and so it is no longer compatible with the slim
train script.",0,,[],2017-08-29 09:47:10,open,,,['cla: yes'],2017-09-28 02:43:21
1378,tensorflow/models,models,2302,scottclowe,DOC: Disambiguate folder to build bazel for slim,"Add a `cd` command in the instructions for downloading and converting imagenet. This lets users unfamiliar with bazel (which is most of them) know they shouldn't be in the slim folder when running the command. The directory is different for these instructions compared with the [instructions in the inception folder](inception#getting-started), which the slim README also [links to and tells you to follow](slim#downloading-and-converting-to-tfrecord-format).

I have written the instruction in the same manner as the [instructions for installation in the inception folder](inception#getting-started).",0,,[],2017-08-29 09:38:40,open,,,['cla: yes'],2017-09-28 02:43:21
1379,tensorflow/models,models,2300,deepjedi,Fix the call to build_imagenet_data.py,The Python extension is missing in the BUILD_SCRIPT string.,3,,[],2017-08-28 12:52:45,open,,,['cla: yes'],2017-09-28 02:43:20
1380,tensorflow/models,models,2298,scottclowe,DOC: Fix links in slim readme,"Fixes two links in [`slim/README.md`](slim/README.md).

1. Point to correct shell script for downloading ImageNet to TFrecords (was incorrectly [`download_and_preprocess_imagenet.sh`](slim/datasets/download_and_preprocess_imagenet.sh), now correctly [`download_and_convert_imagenet.sh`](slim/datasets/download_and_convert_imagenet.sh)).
2. Fix broken markdown for a link to ImageNet (text and URL were split by a line-break). Subsequent text in the paragraph is line-wrapped appropriately following this change.",0,,[],2017-08-28 12:11:05,open,,,['cla: yes'],2017-09-28 02:43:20
1381,tensorflow/models,models,2297,yatendragoel,Change exporting_models.md to reflect the updated command in export_inference_graph.py,"Since the export_inference_graph.py was changed in [this pull request](https://github.com/tensorflow/models/pull/2053/files), the corresponding [markdown file](https://github.com/yatendragoel/models/blob/master/object_detection/g3doc/exporting_models.md) also had to be changed",1,,[],2017-08-28 09:18:10,open,,,['cla: yes'],2017-09-28 02:43:20
1382,tensorflow/models,models,2293,shamanez,Object Detection Feature : Is there are way to freeze the weights of feature extractor and only train the score layers ? ,Can you add a flag to configuration file ? ,3,"NamedUser(login=""tombstone"")","[NamedUser(login=""tombstone"")]",2017-08-28 03:25:33,open,,,['type:feature'],2017-09-13 04:38:37
1383,tensorflow/models,models,2287,LEAAN,Thesis,,1,,[],2017-08-25 09:43:43,open,,,['cla: no'],2017-09-28 02:43:20
1384,tensorflow/models,models,2275,huapingchen,warning message when run the object_detection evaluation Job:,"There is a warning information when run the Object_detection Evaluation Job, indicate ""Please switch to tf.train.get_global_step"", want to know how to handle this issue? 

#Env: Centos

#the eval job:
_**python3 object_detection/eval.py --logtostderr --pipeline_config_path=/home/tf_training/obj_work/models/model/faster_rcnn_resnet101_pets.config --checkpoint_dir=/home/tf_training/obj_work/models/model/train --eval_dir=/home/tf_training/obj_work/models/model/eval**_

#the warning message when run
_**WARNING:tensorflow:From /home/tf/models/object_detection/evaluator.py:166: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_global_step**_

#copy the  part of the code object_detection/evaluator.py:166 as below:
#line 166
   _**global_step = tf.train.global_step(sess, slim.get_global_step())**_
#line 31
_**slim = tf.contrib.slim**_

",1,"NamedUser(login=""tombstone"")","[NamedUser(login=""tombstone"")]",2017-08-23 18:25:07,open,,,['stat:awaiting tensorflower'],2017-09-26 00:26:51
1385,tensorflow/models,models,2274,esox,Installation guide forgets to mention protobuf when mentioning alternative route,"Hello.

In the installation guide, while the directions following the sentence:

> The remaining libraries can be installed on Ubuntu 16.04 using via apt-get:

Are correct, the pip ""way"" is incomplete. It mentions:

`sudo pip install pillow
sudo pip install lxml 
sudo pip install jupyter 
sudo pip install matplotlib`

Without mentioning that protobuf still needs to be installed (either using apt-get, homebrew or others).  I believe that breaking out the installation of protobuf would be beneficial for the sake of clarity.

I would therefore propose to add the following sentence after the pip way:

Install protobuf using your favorite package management tool (e.g., homebrew of MacOSX)
",0,"NamedUser(login=""mhyttsten"")","[NamedUser(login=""mhyttsten"")]",2017-08-23 18:05:03,open,,,[],2017-08-28 17:55:46
1386,tensorflow/models,models,2268,datitran,Make the record scripts tests python 3 compatible and add further test.,"I added one more test where you have two classes on one image so that it is more explicit that the bounding boxes are a list of xmins, ymins, xmaxs, ymaxs etc and also made it python 3 compatible.",0,,[],2017-08-22 13:13:49,open,,,['cla: yes'],2017-09-28 02:43:19
1387,tensorflow/models,models,2267,VastoLorde95,Object Detection API - Feature Request: YOLO and YOLOv2,"With reference to a previous [thread](https://github.com/tensorflow/models/issues/1880), I am volunteering to implement the YOLO and YOLOv2 models for the Object Detection API.",52,,[],2017-08-22 09:51:00,open,,,"['stat:contributions welcome', 'type:feature']",2019-02-13 20:12:28
1388,tensorflow/models,models,2265,cx343,Make the file changes  effective,,1,,[],2017-08-22 03:43:03,open,,,['cla: no'],2017-09-28 02:43:19
1389,tensorflow/models,models,2260,jaewchoi,Add saving label map proto ,,0,,[],2017-08-19 12:52:37,open,,,['cla: yes'],2017-09-28 02:43:18
1390,tensorflow/models,models,2258,zakizhou,"rpn_box_encodings 3D not 4D, fix it",,3,,[],2017-08-19 10:39:47,open,,,['cla: yes'],2017-09-28 02:43:18
1391,tensorflow/models,models,2255,mrezak,Huge RAM usage with LFADS model,"We observed that when we tried to run LFADS on continuous data (hence using gaussian cost function of LFADS) with 300 trials where the data dimensionality was 40 and the length of each trial was 900 time samples ( 300x40x900 matrix), LFADS takes a huge amount of RAM (CPU RAM) about 30+GB.
The RAM usage gradually ramps up to 30+GB, and most of the RAM growth occurs during the initial part of the training (or even before the training starts) and it becomes steady after that. The GPU RAM usage would be around 1GB.

We also observed that the length of the trials (time samples - in our case 900) is the critical variable that sets the RAM usage, the number of trials and data dimensionality do not lead to this huge memory usage. If we decrease the time samples for e.g. to 300, the RAM usage significantly decreases (to e.g. 2-4GB range). 

We are using LFADS model:
https://github.com/tensorflow/models/tree/master/lfads
Tensorflow ver: 1.2
Cuda Ver: 8.0
OS: Linux CentOS

This issue limits us from being able to use LFADS on data with a hundreds of time sample long due to this memory issue. 
",1,,[],2017-08-19 05:19:38,open,,,"['stat:awaiting tensorflower', 'type:bug/performance']",2017-09-13 19:08:21
1392,tensorflow/models,models,2248,shamanez,Object Detection API : Feature request for have weighted softmax in order to work with poorly perform classes . ,In object detection sometimes there are few classes with low validation accuracy .  If we can use weighted softmax or score function I believe we can give more attention to poorly perform classes lost when doing the gradient upgrade . ,1,,[],2017-08-18 06:31:46,open,,,['stat:awaiting tensorflower'],2017-08-18 17:27:32
1393,tensorflow/models,models,2243,haoyangz,Cost calculation in Variational Autoencoder model is wrong,"### System information
- **What is the top-level directory of the model you are using**: autoencoder
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: not relevant
- **TensorFlow version (use command below)**: not relevant
- **Bazel version (if compiling from source)**:not relevant
- **CUDA/cuDNN version**: not relevant
- **GPU model and memory**: not relevant
- **Exact command to reproduce**: not relevant

### Describe the problem
The [reconstruction error](https://github.com/tensorflow/models/blob/master/autoencoder/autoencoder_models/VariationalAutoencoder.py#L24) in variational autoencoder is calculated without specifying the ""axis"" argument, resulting in calculating the sum of reconstruction error across the whole batch. However, the [latent error](https://github.com/tensorflow/models/blob/master/autoencoder/autoencoder_models/VariationalAutoencoder.py#L25) is calculated with axis=1, resulting in a 1D vector representing the latent error for each sample in the batch. When we add them up and take the average to get the [cost](https://github.com/tensorflow/models/blob/master/autoencoder/autoencoder_models/VariationalAutoencoder.py#L26), the 0-d reconstruction error will get broadcast to each element of the latent error, resulting in a larger cost than it should be. The right way should be either mean + mean, or mean(vector + vector).

### Source code / logs
N/A
",7,,[],2017-08-17 23:37:57,open,,,"['stat:contributions welcome', 'type:bug/performance']",2017-09-20 18:40:37
1394,tensorflow/models,models,2242,ankitvgupta,Fix batch norm documentation for is_training,Fixes the documentation to reflect the deprecation of is_training as an argument to resnet_arg_scope and moves is_training to be passed into the network directly.',3,,[],2017-08-17 21:26:12,open,,,['cla: yes'],2017-09-28 02:43:18
1395,tensorflow/models,models,2241,haamoon,[Slim] Scripts for Training ResNet From Scratch,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.0.0
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: 6xK80 
- **Exact command to reproduce**: See below

### Describe the problem
Although it is stated in the slim model that train_image_classifier.py can be used to train models from scratch, I found it really hard in practice without any working example. In my case, I am trying to train ResNet from scratch on a local machine with 6xK80s. After spending much time figuring out the difference between replica and clone arguments and how effective batch size is computed, I came up with this script:
```
DATASET_DIR=/nv/hmart1/ashaban6/scratch/data/imagenet_RF_record
TRAIN_DIR=/nv/hmart1/ashaban6/scratch/train_dir
DEPTH=50
NUM_CLONES=8

CUDA_VISIBLE_DEVICES=""0,1,2,3,4,5,6,7,8"" python train_image_classifier.py --train_dir=${TRAIN_DIR} --dataset_name=imagenet --model_name=resnet_v1_${DEPTH} --max_number_of_steps=100000000 --batch_size=32 --learning_rate=0.1 --learning_rate_decay_type=exponential --dataset_split_name=train --dataset_dir=${DATASET_DIR} --optimizer=momentum --momentum=0.9 --learning_rate_decay_factor=0.1 --num_epochs_per_decay=30 --weight_decay=0.0001 --num_readers=12 --num_clones=$NUM_CLONES
``` 
I followed the same settings as is suggested in the paper. I am using 8 GPUs on a local machine with batch_size 32 so the effective batch size is 32x8=256. Learning rate is initially set to 0.1 and will be decayed by 10 every 30 epochs. After 70K steps (70000x256/1.2e6 ~ 15 epochs), the top-1 performance on the validation set is as low as ~14% while it should be around 50% after that many iterations. I used this command to get the top-1 performance:
```
DATASET_DIR=/nv/hmart1/ashaban6/scratch/data/imagenet_RF_record
CHECKPOINT_FILE=/nv/hmart1/ashaban6/scratch/train_dir/
DEPTH=50

CUDA_VISIBLE_DEVICES=""10"" python eval_image_classifier.py --alsologtostderr --checkpoint_path=${CHECKPOINT_FILE} --dataset_dir=${DATASET_DIR} --dataset_name=imagenet --dataset_split_name=validation --model_name=resnet_v1_${DEPTH}
```
With the lack of working examples it is hard to say if there is a bug in the slim training code or a problem in my script. Providing a working script would be really helpful.


",13,,[],2017-08-17 19:54:27,open,,,[],2018-05-14 07:33:37
1396,tensorflow/models,models,2237,wmouchere,Object Detection API: Batch Normalization not applied on BoxPredictor (SSD),"It seems batch normalization and custom activation function are not supported for the classification and regression layers for the SSD architecture. I found this line : https://github.com/tensorflow/models/blob/b14765c3a8b119ab8dae3d706d623397f99d1011/object_detection/core/box_predictor.py#L515 and successfully added batch norm by replacing it with `with slim.arg_scope(self._conv_hyperparams), slim.arg_scope([slim.conv2d], activation_fn=None):`.
Not sure if this is intended but the MobileNet Pets configuration has batch normalisation in the box predictor config, which is misleading !
(See  https://github.com/tensorflow/models/blob/b14765c3a8b119ab8dae3d706d623397f99d1011/object_detection/samples/configs/ssd_mobilenet_v1_pets.config#L72)",2,,[],2017-08-17 15:18:07,open,,,['stat:awaiting tensorflower'],2017-08-17 22:49:21
1397,tensorflow/models,models,2233,ryo2000,object_detection.utils.ObjectDetectionEvaluation clear_detections  issue,"detection_keys are initialized as set() 

`self.detection_keys = set()` 

but clear_detections  reset  detection_keys as dictionary.

`self.detection_keys = {}` 

",1,,[],2017-08-17 08:38:46,open,,,['stat:awaiting tensorflower'],2017-08-17 19:43:08
1398,tensorflow/models,models,2228,shamanez,"Cannot use exponential linear units (elu) for ""conv_hyperparams"" in the configuration file. Only RELU_6 ",I think API do not support to use elu . ,6,,[],2017-08-16 21:37:15,open,,,[],2018-04-06 07:45:09
1399,tensorflow/models,models,2226,YixuanLi,fix exporter api,"Removing the extra argument `save_relative_paths` in the exporter API function call. 

------
```Traceback (most recent call last):
  File ""object_detection/export_inference_graph.py"", line 106, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""object_detection/export_inference_graph.py"", line 102, in main
    FLAGS.output_directory)
  File ""/home/ubuntu/yixuan-li/object_detection/exporter.py"", line 376, in export_inference_graph
    optimize_graph, output_collection_name)
  File ""/home/ubuntu/yixuan-li/object_detection/exporter.py"", line 336, in _export_inference_graph
    trained_checkpoint_prefix=trained_checkpoint_prefix)
  File ""/home/ubuntu/yixuan-li/object_detection/exporter.py"", line 294, in _write_graph_and_checkpoint
    save_relative_paths=True)
TypeError: __init__() got an unexpected keyword argument 'save_relative_paths'
```",4,,[],2017-08-16 17:42:14,open,,,['cla: yes'],2017-09-28 02:43:18
1400,tensorflow/models,models,2225,RobinBaumann,Evaluation in Object Detection hanging,"### System information
- **What is the top-level directory of the model you are using**: tensorflow/models/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (well, i actually just adjusted the pipeline config to fit my dataset)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64bit
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0 /5.1
- **GPU model and memory**: GeForce GTX1060 6GB
- **Exact command to reproduce**: `python object_detection\eval.py --logtostderr --pipeline_config_path=C:\Users\robin\PycharmProjects\test\my_net\models\faster_r-cnn_resnet101\ pipeline.config --checkpoint_dir=C:\Users\robin\PycharmProjects\test\my_net\models\faster_r-cnn_resnet101\train\ --eval_dir=C:\Users\robin\PycharmProjects\test\my_net\models\faster_r-cnn_resnet101\eval\`

### Describe the problem
I am able to train with the object detecion API on my own dataset, which I created using the `create_pascal_tf_record.py` script (I adjusted it a bit, but mainly the paths). I also checked the generated TFRecord files with the Tensorflow Testing module and verified, that the reconstructed images are similar to the original ones. 

I use the existing `faster_r-cnn_resnet101_voc07.config` file and only adjusted the paths and num_classes. The training runs like a charm, but when I start the eval.py script, it hangs with the message ""*INFO:tensorflow:Restoring parameters from C:\Users\robin\PycharmProjects\test\my-net\models\faster_r-cnn_resnet101\train\model.ckpt-123805*"" (see full log below).

After this I have to `CTRL+C`

However, I can see some output in Tensorboard, but only one value after each `CTRL+C` for mAP but nothing else in the other diagrams etc. 

As mentioned by others having the same issue, running the evaluation and training parallel doesn't work for me, and I can't even imagine that it should be done this way. When I try it, my cuda crashed because the GPU runs out of memory.

Btw I also tried the whole evaluation process on the Oxford-IIIT Pet Dataset and am facing the same issue.

### Source code / logs
The whole log after I hit `CTRL+C` (the part where it hangs is bold):

> C:\Users\robin\models>python object_detection\eval.py --logtostderr --pipeline_config_path=C:\Users\robin\PycharmProjects\test\my_net\models\faster_r-cnn_resnet101\pipeline.config --checkpoint_dir=C:\Users\robin\PycharmProjects\test\my_net\models\faster_r-cnn_resnet101\train\ --eval_dir=C:\Users\robin\PycharmProjects\test\my_net\models\faster_r-cnn_resnet101\eval\
> INFO:tensorflow:Scale of 0 disables regularizer.
> INFO:tensorflow:Scale of 0 disables regularizer.
> INFO:tensorflow:Scale of 0 disables regularizer.
> INFO:tensorflow:Scale of 0 disables regularizer.
> 2017-08-16 07:40:03.000943: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
> 2017-08-16 07:40:03.001072: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
> 2017-08-16 07:40:03.001933: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
> 2017-08-16 07:40:03.002044: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
> 2017-08-16 07:40:03.002153: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
> 2017-08-16 07:40:03.002263: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
> 2017-08-16 07:40:03.002357: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
> 2017-08-16 07:40:03.002451: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
> 2017-08-16 07:40:03.313527: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:940] Found device 0 with properties:
> name: GeForce GTX 1060 6GB
> major: 6 minor: 1 memoryClockRate (GHz) 1.7085
> pciBusID 0000:01:00.0
> Total memory: 6.00GiB
> Free memory: 5.01GiB
> 2017-08-16 07:40:03.313690: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:961] DMA: 0
> 2017-08-16 07:40:03.314894: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   Y
> 2017-08-16 07:40:03.314995: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)
> **INFO:tensorflow:Restoring parameters from C:\Users\robin\PycharmProjects\test\my_net\models\faster_r-cnn_resnet101\train\model.ckpt-123805
> INFO:tensorflow:Restoring parameters from C:\Users\robin\PycharmProjects\test\my_net\models\faster_r-cnn_resnet101\train\model.ckpt-123805**
> Traceback (most recent call last):
>   File ""object_detection\eval.py"", line 161, in <module>
>     tf.app.run()
>   File ""C:\Users\robin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))
>   File ""object_detection\eval.py"", line 157, in main
>     FLAGS.checkpoint_dir, FLAGS.eval_dir)
>   File ""C:\Users\robin\models\object_detection\evaluator.py"", line 211, in evaluate
>     save_graph_dir=(eval_dir if eval_config.save_graph else ''))
>   File ""C:\Users\robin\models\object_detection\eval_util.py"", line 524, in repeated_checkpoint_run
>     time.sleep(time_to_next_eval)
> KeyboardInterrupt
",42,,[],2017-08-16 06:13:02,open,,,['stat:awaiting tensorflower'],2018-09-29 02:52:10
1401,tensorflow/models,models,2224,CasiaFan,add random vertical flip,Add random vertical flip during image preprocessing,1,,[],2017-08-16 05:10:14,open,,,['cla: yes'],2017-09-28 02:43:18
1402,tensorflow/models,models,2222,civilman628,code change make object detection model export fail,"https://github.com/tensorflow/models/blob/master/object_detection/exporter.py#L294

has a extra line which make export ckpt to .pb fail:

```
Traceback (most recent call last):
  File ""object_detection/export_inference_graph.py"", line 106, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""object_detection/export_inference_graph.py"", line 102, in main
    FLAGS.output_directory)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/models/object_detection/exporter.py"", line 376, in export_inference_graph
    optimize_graph, output_collection_name)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/models/object_detection/exporter.py"", line 336, in _export_inference_graph
    trained_checkpoint_prefix=trained_checkpoint_prefix)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/models/object_detection/exporter.py"", line 294, in _write_graph_and_checkpoint
    save_relative_paths=True)
TypeError: __init__() got an unexpected keyword argument 'save_relative_paths'
```

you need move out **save_relative_paths=True**  in order to make export run pass.

------------ BTW----------

the read me here are wrong:

 https://github.com/tensorflow/models/blob/master/object_detection/g3doc/exporting_models.md

```
python object_detection/export_inference_graph \
    --input_type image_tensor \
    --pipeline_config_path ${PIPELINE_CONFIG_PATH} \
    --checkpoint_path model.ckpt-${CHECKPOINT_NUMBER} \
    --inference_graph_path output_inference_graph.pb
```

the parameters are not right, you need to change as the following code:

```
python export_inference_graph \
    --input_type image_tensor \
    --pipeline_config_path path/to/ssd_inception_v2.config \
    --trained_checkpoint_prefix path/to/model.ckpt \
    --output_directory path/to/exported_model_directory
```",8,,[],2017-08-15 21:30:49,open,,,[],2018-04-06 07:45:04
1403,tensorflow/models,models,2216,irmowan,Object Detection API doesn't have L2 Normalization as paper does,"In Liu. 's SSD paper, and Google ''Speed/accuracy trade-off' paper, SSD with VggNet need to have a L2 normalization after conv4_3 layer(feature layer), but I haven't found any configuration or core code about this. 
It seems different from the paper, how can I add this L2-norm layer as the paper does?",2,,[],2017-08-15 09:30:35,open,,,['stat:awaiting tensorflower'],2017-08-16 03:12:08
1404,tensorflow/models,models,2213,shamanez,Object Detection API : SSD classification loss in Object Detection API is different from the original paper .,"In the paper they have used a softmax cross entropy loss. But here in the model is using **sigmoid cross entropy loss** . 

Why is has changed ? What is the effect ? 

Check this configuration file . [This says it is weighted sigmoid](https://github.com/tensorflow/models/blob/master/object_detection/samples/configs/ssd_mobilenet_v1_pets.config#L110)",5,,[],2017-08-15 05:22:28,open,,,['stat:awaiting tensorflower'],2018-12-11 09:15:02
1405,tensorflow/models,models,2211,mees,Perspective Transformer Multi-GPU Training broken,"### System information
- **What is the top-level directory of the model you are using**: Ptn directory
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Source 
- **TensorFlow version (use command below)**: ('v1.3.0-rc1-27-g2784b1c', '1.3.0-rc2')
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: 4x Titan X
- **Exact command to reproduce**: bazel-bin/train_ptn   '--checkpoint_dir=/home/meeso/models/ptn/my_models/no_finetune' '--inp_dir=/home/meeso/Datasets/shapenet_tf/' '--ps_tasks=1' '--worker_replicas=3'

### Describe the problem
The ability to train the perspective transformer on more than one gpu seems broken. I have 4 GPUs in one machine, so I would like to speedup the training times. It is not clear if I am running it correctly, I also tried running an instance with just '--ps_tasks=1'  on one terminal and then in another just the '--worker_replicas=3', but it also failed.

### Source code / logs
`InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'init_ops/init_all_tables': Operation was explicitly assigned to /job:worker but available devices are [ /job:localhost/replica:0/task:0/cpu:0, /job:localhost/replica:0/task:0/gpu:0, /job:localhost/replica:0/task:0/gpu:1, /job:localhost/replica:0/task:0/gpu:2, /job:localhost/replica:0/task:0/gpu:3 ]. Make sure the device specification refers to a valid device.
	 [[Node: init_ops/init_all_tables = NoOp[_device=""/job:worker""]()]]
`
I attach the output with the complete error.
[error.txt](https://github.com/tensorflow/models/files/1223382/error.txt)
",10,,[],2017-08-14 20:37:59,open,,,['stat:awaiting tensorflower'],2018-05-18 04:53:08
1406,tensorflow/models,models,2208,Chaoscendence,python 3.x compatibility,"On default, GFile.read() returns a python string which would depend on the python version. Set GFile to bytes mode and the function will work on both python 2.x and 3.x.",3,,[],2017-08-14 12:12:36,open,,,['cla: yes'],2017-09-28 02:43:18
1407,tensorflow/models,models,2205,quietcricket,Remove hard coded _NUM_CLASSES,_NUM_CLASSES can be derived from labels_to_names dictionary. ,3,,[],2017-08-14 04:31:12,open,,,['cla: yes'],2017-09-28 02:43:17
1408,tensorflow/models,models,2203,shamanez,How transfer learning happening in tensorfow object detection API ? Can we keep feature extractors weight unchanged while only changing the predictor weights . ,"There are few types of transfer learning . When using TF Object Detection API do we fine tune weights of the feature extractor when using already  [trained models on COCO Data Set]( https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md) .

**Because sometimes we can just keep the weight set of feature extractor fix while training only the predictor layers .** 




  [1]: https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md",30,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-08-13 19:35:49,open,,"NamedUser(login=""skye"")",['type:docs'],2019-04-01 04:37:42
1409,tensorflow/models,models,2202,IanPhilips,"Line in object_detection installation procedure makes no sense:"" If you wish to avoid running this manually, you can add it as a new line to the end of your ~/.bashrc file.""","This is from the [Installation docs](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/installation.md#add-libraries-to-pythonpath): 
"" # From tensorflow/models/
export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
Note: This command needs to run from every new terminal you start. If you wish to avoid running this manually, you can add it as a new line to the end of your ~/.bashrc file."" 

This advice assumes the user always is in tensorflow/models/ when they initiate the terminal. In other situations, for example docker, you do not default to the tensorflow/models directory so adding the above line to your bashrc would actually fudge your PYTHONPATH variable every time you attach to the terminal, i.e. your PYTHONPATH would be set to ':/root:/root/slim' every time you start up a container with this line in it. 

",2,,[],2017-08-13 16:22:28,open,,,['stat:awaiting tensorflower'],2017-10-14 17:00:43
1410,tensorflow/models,models,2196,dzautner,Do not skip download if image file already exists,"As the script uses the '-c' flag with wget, it allows wget to continue downloading from where the previous download stopped. By skipping the download all-together when the file already exists it renders this flag meaningless, and would render the script broken if a previous download has failed before finishing.",3,,[],2017-08-11 12:17:57,open,,,['cla: yes'],2017-09-28 02:43:17
1411,tensorflow/models,models,2194,RobinBaumann,Corrupt JPEG data: 245 extraneous bytes before marker 0xd9,"### System information
- **What is the top-level directory of the model you are using**:  tensorflow/models
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0 / 5.1
- **GPU model and memory**: GeForce GTX 1060 6GB
- **Exact command to reproduce**:  python object_detection\eval.py --logtostderr --checkpoint_dir=C:\Users\TVS\tensorflow\models\out\ --eval_dir=C:\Users\TVS\tensorflow\models\eval\ --pipeline_config_path=C:\Users\TVS\tensorflow\models\object_detection\samples\configs\faster_rcnn_resnet101_pets.config

### Describe the problem
When training the Oxford-IIIT Pet Dataset from Scratch (without any finetune checkpoint from COCO set), I get the following error message while running the eval.py script:

> Corrupt JPEG data: 245 extraneous bytes before marker 0xd9

I have build the TFRecord files with the script provided in the object detection api, but get the same for an PASCAL VOC like own dataset.  I saw some guys having similar problems related to libjpeg, could it be that this error is related to libjpeg? 

I am sure that my TFRecords are valid, because I can reconstruct valid Image and Annotation data from it. 

Since I did not write any own code I suspect this to be a Bug.

",30,,[],2017-08-11 11:20:40,open,,,['stat:awaiting tensorflower'],2018-12-31 04:44:06
1412,tensorflow/models,models,2192,rolai,Assert height or width of preprocessed_inputs is not less than 33 in …,"In `FasFasterRCNN`, the input image spatial size (height or width) should be not less than 33. It is checked in `FasterRCNNResnetV1FeatureExtractor._extract_proposal_features`, but missed in `FasterRCNNInceptionResnetV2FeatureExtractor`.
Furthermore, the comments of `FasterRCNNInceptionResnetV2FeatureExtractor._extract_proposal_features` contains this assertion, but the code missed.",0,,[],2017-08-11 08:21:43,open,,,['cla: yes'],2017-09-28 02:43:17
1413,tensorflow/models,models,2182,sakshamgupta006,Update bipartite_matcher.py,"To resolve the issue (https://github.com/tensorflow/models/issues/1698).

Due to this, object detection tests gives import errors.",0,,[],2017-08-10 13:44:27,open,,,['cla: yes'],2017-09-28 02:43:17
1414,tensorflow/models,models,2171,seizeTheDayMin,fix deprecated  word2vec load function in skip_thougths,"The previous word2vec load function is depreacted(not warning, occurring error) in the latest gensim.

```
# deprecate 
gensim.models.Word2Vec.load_word2vec_format
# allowed function in the latest gensim
gensim.models.keyedvectors.KeyedVectors.load_word2vec_format

ix deprecated word2vec load function in skip_thougths",2,,[],2017-08-09 09:02:50,open,,,['cla: yes'],2017-11-02 15:37:38
1415,tensorflow/models,models,2162,mees,[Feature_request] Add training and test set loss in PTN for tensorboard visualization,"The perspective transformer net shows only the validation error in tensorboard during training, it would be very helpful to also show the training and the test set loss during training.
",4,,[],2017-08-08 16:22:44,open,,,['stat:awaiting tensorflower'],2017-08-18 11:40:12
1416,tensorflow/models,models,2159,mees,"fixes bug #2157, loads the pretrained encoder to finetune the decoder", bug #2157 didn't load the pretrained decoder when training  the volumetric decoder. This patch loads the latest checkpoint model from the given folder.,1,,[],2017-08-08 13:27:02,open,,,['cla: yes'],2017-09-28 02:43:16
1417,tensorflow/models,models,2155,arkanath,Fixing some bugs and adding ptn directory info in root README,,3,,[],2017-08-08 10:00:34,open,,,['cla: yes'],2017-09-28 02:43:16
1418,tensorflow/models,models,2153,cancan101,mobilenet_v1 missing data_format,"### System information
- **What is the top-level directory of the model you are using**:
`models/slim/nets/mobilenet_v1.py`

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.

```
== cat /etc/issue ===============================================
Linux node014-jupyter-20170708-132328 4.4.0-1022-aws #31-Ubuntu SMP Tue Jun 27 11:27:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux node014-jupyter-20170708-132328 4.4.0-1022-aws #31-Ubuntu SMP Tue Jun 27 11:27:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.0)
protobuf (3.2.0)
tensorflow-gpu (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Aug  1 17:13:34 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |
| N/A   59C    P0    67W / 149W |      0MiB / 11439MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
```

### Describe the problem
in `models/slim/nets/mobilenet_v1.py`, `mobilenet_v1` does not accept a `data_format` argument meaning the model can only be trained in a channels last format which is not optimal.",2,,[],2017-08-08 04:32:37,open,,,['stat:awaiting tensorflower'],2017-08-30 06:02:07
1419,tensorflow/models,models,2150,YuxinxinChen,imagenet_distributed_train using inception v3 stuck on saving check points forever.,"### System information
```bash
== cat /etc/issue ===============================================
Linux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.0)
protobuf (3.3.0)
tensorflow-gpu (1.2.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.0
tf.GIT_VERSION = v1.2.0-rc2-21-g12f033d
tf.COMPILER_VERSION = v1.2.0-rc2-21-g12f033d
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Mon Aug  7 21:00:40 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 0000:00:1E.0     Off |                    0 |
| N/A   51C    P0    72W / 149W |  10944MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      2886    C   /bin/python                                  10938MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
```

I am running the imagenet_distributed_train.py of inception: https://github.com/tensorflow/models/tree/master/inception, with 16 AWS p2x2 machines. I didn't change any code of inception and follow the guidance to run imagenet_distributed_train using parallel-ssh.

The script I use to run parallel-ssh:
```python
from pssh.pssh_client import ParallelSSHClient
import datetime
from pprint import pprint
from pssh.utils import load_private_key

output_ps = []
output_worker = []
some host ip here
ps = [host1,host2,host3]
worker = [host0,host1,host2,host3,host4,host5,host6,host7,host8,host9,host10,host11,host12,host13,host14,host15]
client_ps = ParallelSSHClient(ps, user='centos')
client_worker = ParallelSSHClient(worker, user='centos')

output_ps = client_ps.run_command('%s', host_args=(
    ('/imagenet/run_ps.sh --job_name ps --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_ps.sh --job_name ps --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_ps.sh --job_name ps --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ))

output_worker = client_worker.run_command( '%s', host_args=(
    ('/imagenet/run_worker.sh --job_name worker --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 3 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 4 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 5 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 6 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 7 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 8 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 9 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 10 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 11 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 12 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 13 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 14 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
    ('/imagenet/run_worker.sh --job_name worker --task_id 15 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),
       ))

client_ps.join(output_ps)
#client_worker.join(output_worker)
pprint(output_ps.values()[0].exit_code)
#pprint(output_worker.values()[0].exit_code)

for host, host_output in output_ps.items():
    for line in host_output.stdout:
        print(""Host [%s] - %s"" % (host, line))
```
I think this script worked fine because I logged in every machine and checked with ps command and ensured the program was running with correct parameters. Then the program just worked fine but to some point, it started to save checkpoints forever(here is the output of worker0):

```bash
INFO:tensorflow:Worker 0: 2017-08-04 06:46:08.510727: step 2340, loss = 11.22(2.0 examples/sec; 15.788  sec/batch)
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Worker 0: 2017-08-04 06:53:55.553703: step 2370, loss = 10.30(2.1 examples/sec; 15.573  sec/batch)
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Worker 0: 2017-08-04 07:01:44.226068: step 2400, loss = 10.84(2.1 examples/sec; 15.421  sec/batch)
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Running Summary operation on the chief.
INFO:tensorflow:Finished running Summary operation.
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt
(same saving checkpoint output forever)
```
I ran nvidia-smi and found the GPU wasn't working and same with other nodes. The output of worker 1-15 just stucked on step 2400 and didn't do any progress.  I tried this several time on new set of 16 machines but it all stucked on saving checkpoint forever problem at some time.  I guess it might be a bug in tensorflow? Or does this caused network failure? but it didn't retrun any network failure error.
",1,,[],2017-08-07 22:28:36,open,,,['stat:awaiting tensorflower'],2017-08-14 23:55:31
1420,tensorflow/models,models,2147,mari-linhares,Fixing typos and style and changing default arguments,,1,,[],2017-08-07 14:37:49,open,,,['cla: yes'],2017-09-28 02:43:16
1421,tensorflow/models,models,2145,irmowan,Bug: Change protobuf to list in Object Detection API,"
there is a 'subtract_channel_mean' preprocess op in Object Detection API.
https://github.com/tensorflow/models/blob/master/object_detection/core/preprocessor.py#L1462

Problem 1:
When I add this preprocessing op in the pipeline.config, and begin to train, it raises error:

    TypeError: Expected float32, got <google.protobuf.pyext._message.RepeatedScalarContainer object at 0x7f17df4fde30> of type 'RepeatedScalarContainer' instead.

Add 'means = list(means)' before Line 1474, and the problem solved(but this way is not elegant).

You need to fix it.

Problem 2:
This config is in the model config as a preprocessing config. However, it seems that if this operation be done in the training procedure, it also need to be done in the eval & test. But it seems not.
",2,,[],2017-08-07 09:59:53,open,,,['stat:awaiting tensorflower'],2017-11-19 02:14:21
1422,tensorflow/models,models,2139,vertix,Broken bazel build of //object_detection:all,"Please go to Stack Overflow for help and support:

### System information
- **What is the top-level directory of the model you are using**:
None
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Nope
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS 10.12.6
- **TensorFlow installed from (source or binary)**:
source (but should not matter)
- **TensorFlow version (use command below)**:
v1.2.0-5-g435cdfc 1.2.1
- **Bazel version (if compiling from source)**:
0.5.2-homebrew
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
None
- **Exact command to reproduce**:
```bash
git clone https://github.com/tensorflow/models.git
cd models
bazel build //object_detection:train
```

### Describe the problem
 //object_detection targets don't build. 

I want to use tensorflow/models as an external dependency of my project (I use bazel as a build system). On the other hand one also expects the BUILD files to be correct in this repository.

For my use case (external dependency) you can reproduce it using: 
```bash
mkdir my_workspace
cd my_workspace
echo 'http_archive(
    name = ""tf_models"",
    sha256 = ""96f6bdc2a9044f0ce8daa0f20f8e227d3c47409a1df12293ea9eaa6816cde328"",
    urls = [""https://github.com/tensorflow/models/archive/3d792f9.tar.gz""],
    strip_prefix =""models-3d792f935d652b2c7793b95aa3351a5551dc2401"",
)' > WORKSPACE
bazel build --verbose_failures @tf_models//object_detection:train
```

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
For  `bazel build //object_detection:train`:
```
ERROR: /Users/vertix/Documents/models/object_detection/BUILD:11:1: no such package 'tensorflow': BUILD file not found on package path and referenced by '//object_detection:train'.
ERROR: Analysis of target '//object_detection:train' failed; build aborted.
INFO: Elapsed time: 11,901s
```

For  `bazel build @tf_models//object_detection:train`:
```
ERROR: /private/var/tmp/_bazel_vertix/0bd4347ec154918ae178683310d45d71/external/tf_models/object_detection/BUILD:28:1: no such package '@tf_models//tensorflow_models/object_detection/core': BUILD file not found on package path and referenced by '@tf_models//object_detection:trainer'.
ERROR: /private/var/tmp/_bazel_vertix/0bd4347ec154918ae178683310d45d71/external/tf_models/object_detection/BUILD:28:1: no such package '@tf_models//tensorflow_models/object_detection/core': BUILD file not found on package path and referenced by '@tf_models//object_detection:trainer'.
ERROR: Analysis of target '@tf_models//object_detection:train' failed; build aborted.
INFO: Elapsed time: 100,886s
```",4,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""jch1"")]",2017-08-06 14:01:45,open,,,[],2018-04-06 07:44:55
1423,tensorflow/models,models,2138,Akelio-zhang,Evaluation should use validation datasets,Evaluation should use 'validation' (not 'train' datasets) datasets in the last part (Apply fine tuned model to some images.) of this walkthrough file.,7,,[],2017-08-06 12:32:13,open,,,['cla: yes'],2017-09-28 02:43:16
1424,tensorflow/models,models,2130,EmGarr,slim resnet architecture different from the original paper,"Hi guys,
@sguada and @nathansilberman 
Why does your [resnet implementation](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v1.py) use stride 2 in the conv2  (block1) instead of stride 1 and for conv5 (block4) stride 1 instead of stride 2?

```python
 """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""
  blocks = [
      resnet_v1_block('block1', base_depth=64, num_units=3, stride=2),
      resnet_v1_block('block2', base_depth=128, num_units=4, stride=2),
      resnet_v1_block('block3', base_depth=256, num_units=6, stride=2),
      resnet_v1_block('block4', base_depth=512, num_units=3, stride=1),
]
```
Instead of:
```python
 """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""
  blocks = [
      resnet_v1_block('block1', base_depth=64, num_units=3, stride=1),
      resnet_v1_block('block2', base_depth=128, num_units=4, stride=2),
      resnet_v1_block('block3', base_depth=256, num_units=6, stride=2),
      resnet_v1_block('block4', base_depth=512, num_units=3, stride=2),
]
```
Thanks.",1,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-08-04 13:33:30,open,,,['stat:awaiting tensorflower'],2017-08-07 21:35:16
1425,tensorflow/models,models,2129,adrinjalali,Syntaxnet outside bazel gives segfault,"Top level directory: syntaxnet
Custom code: no
OS: Ubuntu 17.04
bazel release 0.5.3
CUDA 8.0/cuDNN 6.0
GPU: Tesla M60

What I'm trying to do is to have `syntaxnet` as a package, which I can `import` and use its modules.
I've followed the build steps as explained [here](https://github.com/tensorflow/models/tree/master/syntaxnet), and installed the `syntaxnet-xxxxx.whl` package.

I can also run the syntaxnet's `parse_eval` using the `bazel` environment, using:

    echo ""parse my sentence please!"" | syntaxnet/models/parsey_universal/parse.sh downloads/English

Now, having installed the syntaxnet's package, I get a segfault doing this:
```
$ python -c ""import syntaxnet.load_parser_ops""
*** Error in `python': free(): invalid pointer: 0x0000560d65890598 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x7908b)[0x7f45d325508b]
/lib/x86_64-linux-gnu/libc.so.6(+0x82c3a)[0x7f45d325ec3a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f45d3262d2c]
/home/adrin/Projects/tf-models-python2/syntaxnet/.venv/local/lib/python2.7/site-packages/syntaxnet/parser_ops.so(+0x2ea73a)[0x7f45baf1a73a]
...
```
the rest of the log is found in [core-dump.txt](https://github.com/tensorflow/models/files/1200279/core-dump.txt).

I suspect the origin of the problem is that the tensorflow which syntaxnet is compiled against is very different from what's installed as a package on my system. Basically, when we run syntaxnet using `bazel`'s platform, we have these values inside the `PYTHONPATH`:

```
/path/to/models/syntaxnet/bazel-bin/syntaxnet/parser_eval.runfiles
/path/to/models/syntaxnet/bazel-bin/syntaxnet/parser_eval.runfiles/protobuf/python
/path/to/models/syntaxnet/bazel-bin/syntaxnet/parser_eval.runfiles/six_archive
/path/to/models/syntaxnet/bazel-bin/syntaxnet/parser_eval.runfiles/org_tensorflow
/path/to/models/syntaxnet/bazel-bin/syntaxnet/parser_eval.runfiles/__main__
/path/to/models/syntaxnet/bazel-bin/syntaxnet/parser_eval.runfiles/protobuf
```
which is why when I do `print(tf.GIT_VERSION, tf.VERSION)` inside `parser_eval.py` in `bazel` environment, I get: `unknown 1.0.1`, but running the following command gives me a different result:

```
$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.2.0-5-g435cdfc', '1.2.1')
```

Another reason I believe this is the reason, is that I can run the following command w/o any errors:

```
PYTHONPATH=""/path/to/models/syntaxnet/bazel-bin/syntaxnet/parser_eval.runfiles/org_tensorflow"" python -c ""import syntaxnet.load_parser_ops""
```
",4,"NamedUser(login=""calberti"")","[NamedUser(login=""calberti"")]",2017-08-04 12:01:01,open,,,['stat:awaiting tensorflower'],2017-08-16 09:31:15
1426,tensorflow/models,models,2122,nikhileshsharma91,eval_image_classifier.py Number of images evaluated wrong,"I followed the tutorial to finetune the inception model for the Flowers dataset.

The flowers dataset has 350 validation images specified in the flowers.py file.

But when I ran eval_image_classifier.py and modified it to print the number of TP,FP,TN,FN

The results:
```
 tensorflow/core/kernels/logging_ops.cc:79] eval/TrueNegatives[64]
I tensorflow/core/kernels/logging_ops.cc:79] eval/TruePositives[286]
I tensorflow/core/kernels/logging_ops.cc:79] eval/FalsePositives[2]
I tensorflow/core/kernels/logging_ops.cc:79] eval/FalseNegatives[48]
```
If you add the them up, its a total of 400. But the number of validation images is 350.

I did fine tuning for my custom dataset, where the validation images were 150 with only two classes.

The results were:
```
I tensorflow/core/kernels/logging_ops.cc:79] eval/TruePositives[11]
I tensorflow/core/kernels/logging_ops.cc:79] eval/TrueNegatives[155]
I tensorflow/core/kernels/logging_ops.cc:79] eval/FalsePositives[4]
I tensorflow/core/kernels/logging_ops.cc:79] eval/FalseNegatives[30]
I tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0.83]
I tensorflow/core/kernels/logging_ops.cc:79] eval/AreaUnderCurve[0.62156773]
```

If you add them up its comes out to be 200.

Why is this happening? Where are the additional 50 images coming from?

Is there a way to modify eval_image_classifier.py to print the name of the validation images with its predictions and labels?",5,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-08-04 03:23:26,open,,,[],2018-12-08 06:46:37
1427,tensorflow/models,models,2111,Yangel-hide,Update exporting_models.md,,1,,[],2017-08-03 14:59:36,open,,,['cla: no'],2017-09-28 02:43:14
1428,tensorflow/models,models,2110,Yangel-hide,Update exporting_models.md,,1,,[],2017-08-03 14:42:38,open,,,['cla: no'],2017-09-28 02:43:14
1429,tensorflow/models,models,2102,jaesik817,PathNet: Evolution Channels Gradient Descent in Super Neural Networks,"I implemented PathNet with Tensorflow, which is to learn a huge neural networks with sparse activation and evolution channel selection, https://arxiv.org/abs/1701.08734.
",3,,[],2017-08-03 07:53:05,open,,,['cla: yes'],2017-09-28 02:43:14
1430,tensorflow/models,models,2098,gauss-clb,"Why in cifar10_multi_gpu_train.py, there is no tf.train.Coordinator()?","View [here](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L237), the api doc says we should use `tf.train.Coordinator()` to stop threads, but why `cifar10_multi_gpu_train.py` don't use  `tf.train.Coordinator()`?",1,,[],2017-08-02 14:55:00,open,,,"['stat:awaiting tensorflower', 'type:support']",2017-08-02 19:20:20
1431,tensorflow/models,models,2097,DragosBobolea,Doc: Update export_inference_graph example,Parameter names are different now,4,,[],2017-08-02 13:05:39,open,,,['cla: yes'],2017-09-28 02:43:14
1432,tensorflow/models,models,2095,freedomtan,add a script that retrains MobilenetV1 for flowers,"This script performs the following operations:
1. Downloads the Flowers dataset
2. Fine-tunes a MobilenetV1 model on the Flowers training set.
3. Evaluates the model on the Flowers validation set.

Usage:
cd slim
./scripts/finetune_mobilenet_v1_on_flowers.sh
    default: 1.0 224
./scripts/finetune_mobilenet_v1_on_flowers.sh {1.0,0.75,0.50,0.25}
    {1.0,0.75,0.50,0.25} 224
./scripts/finetune_mobilenet_v1_on_flowers.sh {1.0,0.75,0.50,0.25} {224,192,160,128}",0,,[],2017-08-02 08:45:32,open,,,['cla: yes'],2017-09-28 02:43:14
1433,tensorflow/models,models,2094,kristofbc,Don't add ReLU automatically,"TensorFlow-Slim use ReLU activations by default for convolution, deconvolution and fully-connected transformations.

After speaking with @cbfinn, the ReLU were unintentional. They were introduced after open-sourcing the code and using TensorFlow-Slim.",4,,[],2017-08-02 08:18:15,open,,,['cla: yes'],2017-09-28 02:43:13
1434,tensorflow/models,models,2087,civilman628,create more than one tf.record files for object dection training,"Now the object detection API provides the sample code to create tf.record file for PASCAL VOC in **create_pascal_tf_record.py**

But it only generates one file training and one file for validation. This will cause the single file is very large and hard to load into memory, especially on our own dataset. furthermore, the model config file can only assign one file for training and validation as well in 'input_path'.

We need a more flexible to let training process to read more than one files one by one, just like in classification model, **train-001.record, train-002.record**...


 ",10,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-08-01 17:21:31,open,,,['type:feature'],2018-03-15 16:43:06
1435,tensorflow/models,models,2082,s-gupta,Fix for loading meshes.,Fix for #2079.,1,,[],2017-08-01 07:58:15,open,,,['cla: yes'],2017-09-28 02:43:13
1436,tensorflow/models,models,2069,datitran,Fix minor mistakes so that it won't be confusing.,Filename and classes_text need to be of type byte in the cat example and the input of the encoded image is wrong. This is confusing for readers.,0,,[],2017-07-29 21:02:06,open,,,['cla: yes'],2017-09-28 02:43:13
1437,tensorflow/models,models,2061,dheera,High CPU usage even when running on GPU,"### System information
- **What is the top-level directory of the model you are using**: ssd_inception_v2_coco_11_06_2017
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')
- **CUDA/cuDNN version**: 5.1
- **GPU model and memory**: GTX1070
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When running continuous image detection, the Tensorflow object detection models consume excessively high CPU usage even with GPU support enabled. On i5 with GTX1070 the inferences are running on GPU but TensorFlow also consumes 300% CPU.

One can verify that OpenCV is not consuming high CPU by commenting the sess.run() line and the box-printing for loop below it.

### Source code / logs

```
#!/usr/bin/env python3

import numpy as np
import tensorflow as tf
import os
import sys
import time
import cv2

from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as vis_util

from matplotlib import pyplot as plt

MODEL_NAME = 'ssd_inception_v2_coco_11_06_2017'
PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')
NUM_CLASSES = 90
PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'
DEVICE = 0
WIDTH = 640
HEIGHT = 480
GPU_MEMORY = 0.1

# gpu settings

gpu_options = tf.GPUOptions(
    per_process_gpu_memory_fraction = GPU_MEMORY,
    allow_growth = True
)

config = tf.ConfigProto(
    device_count = { 'GPU': 1 },
    gpu_options = gpu_options
)

# open camera

cap = cv2.VideoCapture(DEVICE)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, WIDTH)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, HEIGHT)

# load graph

detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')

# load labels

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)
print(category_index)
i = 0

with detection_graph.as_default():
    with tf.Session(config=config, graph=detection_graph) as sess:
        while True:
            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
            scores = detection_graph.get_tensor_by_name('detection_scores:0')
            classes = detection_graph.get_tensor_by_name('detection_classes:0')
            num_detections = detection_graph.get_tensor_by_name('num_detections:0')
            ret, image_np = cap.read()
            image_np_expanded = np.expand_dims(image_np, axis=0)
            (boxes, scores, classes, num_detections) = sess.run(
              [boxes, scores, classes, num_detections],
              feed_dict={image_tensor: image_np_expanded})
            for box, score, _class in zip(np.squeeze(boxes), np.squeeze(scores), np.squeeze(classes)):
                if score > 0.5:
                    label = category_index[_class]['name']
                    print(label, score, box)
            print(i)
            i += 1

```",9,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-07-28 08:05:15,open,,,['stat:contributions welcome'],2018-08-20 00:27:34
1438,tensorflow/models,models,2038,MaxBareiss,[object detection] Dequeue placement issue,"### System information
- **What is the top-level directory of the model you are using**:
object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows Server 2012 R2
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.3.0rc0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0 / 5.1.10
- **GPU model and memory**:
GTX 1080 Ti 11GB
- **Exact command to reproduce**:
```
python C:\Users\name\git-repos\models\object_detection\train.py --logtostderr --pipe
line_config_path=ssd_inception_v2.config --train_dir=.
```
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The output of the object_detection `train.py` file includes this line:

```
2017-07-25 12:39:37.289202: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\simple_placer.cc:697] Ignoring device specification /device:GPU:0 for node 'prefetch_queue_Dequeue' because the input edge from 'prefetch_queue' is a reference connection and already has a device field set to /device:CPU:0
```

This is related to issue #1390. Is this a performance problem? This problem existed in 1.2.x as well.
",1,,[],2017-07-25 16:54:26,open,,,"['models: research', 'stat:awaiting response', 'type:bug/performance']",2019-02-06 21:50:40
1439,tensorflow/models,models,2037,coolmatt1024,Update DenoisingAutoencoder.py,"For the denoising autoencoder, I think it should be 'self.scale' not 'scale' in the graph. If not ,I think it unnecessary to fefine placeholder of scale because it is not connected wtih any node.",3,,[],2017-07-25 14:52:30,open,,,['cla: yes'],2017-09-28 02:43:13
1440,tensorflow/models,models,2036,AnranQi,problem when loading MobileNet pretrain model to replace the inception v3 model,"### System information

- **OS Platform and Distribution :Linux Ubuntu 14.04
- **TensorFlow version : 1.2


### Describe the problem
I use MobileNet to replace Inception v3 in im2txt. And download the MobileNet pretrain model from [https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md](url)
mobilenet_v1_1.0_224.ckpt.data-00000-of-00001
mobilenet_v1_1.0_224.ckpt.index
mobilenet_v1_1.0_224.ckpt.meta

### Source code / logs
the code I use to load the pretrain model is 

 ```python
def setup_inception_initializer(self):


    if self.mode != ""inference"":
      saver = tf.train.Saver(self.inception_variables)

      def restore_fn(sess):
        tf.logging.info(""Restoring Inception variables from checkpoint file %s"",
                        self.config.inception_checkpoint_file)
        saver.restore(sess, self.config.inception_checkpoint_file)

      self.init_fn = restore_fn
```

the error I got is
```
DataLossError (see above for traceback): Unable to open table file /home/turinggpu/yuhao/im2txt_anran/im2txt/new_data/mobilenet_v1_1.0_128.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
 [[Node: save/RestoreV2_41 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_41/tensor_names, save/RestoreV2_41/shape_and_slices)]]
[[Node: save/RestoreV2_42/_197 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_412_save/RestoreV2_42"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

ERROR:tensorflow:
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'init_ops/report_uninitialized_variables/boolean_mask/Gather:0' shape=(?,) dtype=string>
If you want to mark it as used call its ""mark_used()"" method.
It was originally created here:
['File ""train.py"", line 114, in <module>\n    tf.app.run()', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))', 'File ""train.py"", line 110, in main\n    saver=saver)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 655, in train\n    ready_op = tf_variables.report_uninitialized_variables()', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 170, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 139, in _add_should_use_warning\n    wrapped = TFShouldUseWarningWrapper(x)', 'File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 96, in __init__\n    stack = [s.strip() for s in traceback.format_stack()]']

```

Any suggestions? 
Thanks for help.",5,"NamedUser(login=""petewarden"")","[NamedUser(login=""petewarden""), NamedUser(login=""aselle"")]",2017-07-25 07:23:53,open,,,"['stat:awaiting tensorflower', 'type:support']",2018-09-12 16:17:40
1441,tensorflow/models,models,2035,wm901115nwpu,shufflenet implement in tensorflow ,Could you implement in tensorflow? Maybe this approach is higher than the mobilenet in object detection.,2,,[],2017-07-25 03:19:49,open,,,['stat:contributions welcome'],2018-06-12 18:13:37
1442,tensorflow/models,models,2034,zhangyilalala,OMM during training in Object detection with batch size>12,"I am trying to train the ssd_inception_v2

the training break with the error 
the result error:
ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1917,1]       



when start,I get the message bellow:

Total memory: 11.90GiB
Free memory: 11.75GiB


**Ignoring device specification /device:GPU:0 for node 'prefetch_queue_Dequeue' because the input edge from 'prefetch_queue' is a reference connection and already has a device field set to /device:CPU:0**

**I wonder what the message mean,I use nvidia-smi to see the gpu GPU-Util is not full,sometimes 0%.and if the batch size>12,run out with OOM.if the batch size<12,then it's fine** 
**Which thing  should I fix to get out of the error?**

This is related to issue #1390 #2038


",13,,[],2017-07-25 02:22:27,open,,,['type:bug/performance'],2019-01-11 09:22:10
1443,tensorflow/models,models,2030,yufengg,removed tf.contrib.slim,it is not used,0,,[],2017-07-25 00:00:22,open,,,['cla: yes'],2017-09-28 02:43:13
1444,tensorflow/models,models,2025,AdamSanderson93,Object_Detection unable to finetune from non-detection checkpoint,"### System information
- **What is the top-level directory of the model you are using**: Object_Detection
- **Have I written custom code**: Modified Config file
- **OS Platform and Distribution**: Linux Ubuntu 14.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.2.1
- **CUDA/cuDNN version**: CUDA 8 / cuDNN 5.1
- **GPU model and memory**: gtx 980 4gb
- **Exact command to reproduce**:

python models/object_detection/train.py --train_dir=Test --pipeline_config_path=faster_rcnn_resnet101_Button_Cell.config 

### Describe the problem
When trying to fine-tune from a non-detection checkpoint (from_detection_checkpoint: false in config) the model will not properly load the variables. Prints out a large amount of warnings in this form...

WARNING:root:Variable [resnet_v1_101/conv1/BatchNorm/moving_mean] not available in checkpoint
WARNING:root:Variable [resnet_v1_101/conv1/BatchNorm/moving_variance] not available in checkpoint
WARNING:root:Variable [resnet_v1_101/conv1/weights] not available in checkpoint

Followed by:

ValueError: No variables to save

If I turn from_detection_checkpoint to true everything loads and runs properly. I am using an unedited version of the api and a slightly modified version of the cocosample config. Just filled in where the checkpoints are. I have attempted this using the checkpoints provided on the model detection zoo. Specifically the rfcn_resnet101_coco, and faster_rcnn_resnet101_coco models.

I apologize if the formatting is improper I am rather new to github.
Please let me know if any other information is required.",17,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-07-24 17:44:32,open,,,[],2018-05-02 10:34:14
1445,tensorflow/models,models,2017,chungyeh,Add mobilenet V1 1.0 224 fine tune script ,I signed it!,6,,[],2017-07-24 07:03:36,open,,,['cla: no'],2018-03-02 03:58:40
1446,tensorflow/models,models,2016,hughbzhang,Spatial Transformer Networks cuts off last row and column off input,"Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

Also, please understand that many of the models included in this repository are experimental and research-style code. If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **What is the top-level directory of the model you are using**:
models/transformer
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Only the script attached below to reproduce the issue
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Tested on both MacOS Sierra and Linux Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.2.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
n/a
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:

```python
import tensorflow as tf
import numpy as np
from spatial_transformer import transformer

def identity():

    original_theta = np.array([[1., 0., 0.],
                               [0., 1., 0.]])

    theta = tf.Variable(initial_value=np.expand_dims(original_theta.flatten(), 0))
    
    original_U = np.reshape(np.arange(25, dtype=""float32""), [5,5])
    reshaped_U = np.expand_dims(np.expand_dims(original_U, 2), 0)
    U = tf.Variable(initial_value=reshaped_U)

    assert(U.shape == (1,5,5,1))
    assert(theta.shape == (1,6))

    raw_output = transformer(U, theta, (5,5))
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    output = np.array(sess.run(raw_output))
   
    assert(reshaped_U.shape == (1,5,5,1))
    assert(output.shape == (1,5,5,1))

    print(output)
    print(reshaped_U)

    assert((output == reshaped_U).all())
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I cloned the models/transformer directory to generalize 2D spatial transformers to 3D. In the process, I found a few bugs that caused spatial transformers to not output the exact transformation specified. After investigating, I found that there are some indexing errors which cause the problem shown above. I'd be happy to make a PR for any or all of the following if it would be helpful:
1) The fixed 2D spatial transformer
2) Newly implemented 3D spatial transformer
3) Basic test cases for both of the above

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",15,,[],2017-07-24 03:18:20,open,,,"['stat:community support', 'type:bug/performance']",2017-08-07 17:54:43
1447,tensorflow/models,models,2015,satchamo,Create simple API for running syntaxnet without rebuilding models,"<a href=""https://stackoverflow.com/questions/40736066/syntaxnet-how-does-parser-eval-py-receive-stdin"">There</a> <a href=""https://github.com/dmansfield/models/tree/documents-from-tensor"">have</a> <a href=""https://github.com/JoshData/models/tree/online-parsing"">been</a> <a href=""https://github.com/ljm625/syntaxnet-rest-api"">numerous</a> <a href=""https://github.com/IINemo/syntaxnet_api_server/blob/e80c53100622b400fc4c639cb80db1f429e7b2fc/src/syntaxnet_api_server/syntaxnet_rus_api.py"">attempts</a> to solve the problem of ""streaming"" sentences to syntaxnet. None of them are ideal.

It would be great if there was a blessed way to:

1) initialize the models (which take several seconds)
2) feed sentences to them
3) capture the output immediately

In code, this would be something like:
```python
models = initialize_syntaxnet()
while True:
    sentence = raw_input(""Enter your sentence"")
    output = tag_and_parse(sentence, models)
    print(output)
```

As it stands now, demo.sh waits until EOF is read, and then outputs everything (and then exits). To run it on another sentence, you have to run the program again (which takes several seconds to bootstrap).",7,,[],2017-07-23 19:40:19,open,,,"['stat:contributions welcome', 'type:feature']",2018-12-13 15:51:16
1448,tensorflow/models,models,2013,mengqhui,Merge pull request #1 from tensorflow/master,2017-06-23,1,,[],2017-07-22 10:43:52,open,,,['cla: no'],2018-05-05 04:36:07
1449,tensorflow/models,models,2010,peterminhk,Fix warning 'Summary name /clone_loss is illegal',"When clone.scope is empty a warning(info) like below occurs.
```
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
```

Since '/' is contained in clone.scope, removed it from '/clone_loss' string.",0,,[],2017-07-21 17:04:12,open,,,['cla: yes'],2017-09-28 02:43:12
1450,tensorflow/models,models,2009,Pibborn,"inception: Fix issue #1976, merge mac script","This commit fixes the bug described in issue #1976; it also makes the download_and_preprocess_flowers_mac.sh script redundant by checking whether shuf or gshuf are available. 

Would 2 different PRs be preferred? I could do that, too.",3,,[],2017-07-21 16:32:59,open,,,['cla: yes'],2017-09-28 02:43:12
1451,tensorflow/models,models,2007,korrawat,Fix label_id_offset inconsistencies (obj detection),"The [""using your own dataset"" tutorial](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/using_your_own_dataset.md) says ""Label maps should always start from id 1."" However, the current code still has some inconsistencies.

Right now, the default value for `label_id_offset` in `eval_util.py`'s [`evaluate_detection_results_pascal_voc` method](https://github.com/tensorflow/models/blob/master/object_detection/eval_util.py#L111-L125) is 0 (instead of 1). When I run an evaluation using a pre-trained model on the PASCAL VOC, for example, `num_classes` will be 21 instead of 20, giving me the following warning.

```bash
WARNING:root:The following classes have no ground truth examples: 0
```
Changing the default value of `label_id_offset` to 1 will fix it.

### Questions
Should the comment in L111 (""Pascal VOC evaluator assumes foreground index starts from zero."") also change from ""zero"" to ""one""?

By the way, what is allowed as the `categories`?
- At least, the ids should start from 1. The `label_id_offset` can allow for other cases.
- Maybe they should also be in an increasing order and consecutive? I'm not sure whether anything will break if this condition is not satisfied (e.g. the label map is 1=cat1, 5=cat5, 3=cat3). It seems like some codes are making these assumptions?
If these should be satisfied, maybe some checks can be added in `label_map_util.py`'s [`_validate_label_map` method](https://github.com/tensorflow/models/blob/master/object_detection/utils/label_map_util.py#L25).

Possibly related issues: #1856, #1936 

@derekjchow Could you please take a look? Thank you!",4,"NamedUser(login=""tombstone"")","[NamedUser(login=""tombstone"")]",2017-07-21 13:34:43,open,,,['cla: yes'],2017-11-06 14:41:16
1452,tensorflow/models,models,2000,liuzjmike,More efficient per-example gradient for conv2d in differential_privacy,Perform more efficient per-example gradient calculation with respect to filters of conv2d operations,9,,[],2017-07-21 01:30:21,open,,,['cla: yes'],2017-09-28 02:43:12
1453,tensorflow/models,models,1990,tcexeexe,cannot import name label_map_util,"l follow the Installation step to install in python2.7,debian 8 and pass the ""model_builder_test.py""
but when l run the Object Detection Demo,the error happend:

root@australia_debian1:~/PycharmProjects/objdetection# ipython example.py
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
/root/PycharmProjects/objdetection/example.py in <module>()
     17 # This is needed since the notebook is stored in the object_detection folder.
     18 sys.path.append("".."")
---> 19 from utils import label_map_util
     20 
     21 from utils import visualization_utils as vis_util

ImportError: cannot import name label_map_util






dose anyone know how to solve this problem?


",18,,[],2017-07-20 02:56:02,open,,,['stat:awaiting response'],2019-03-15 13:18:13
1454,tensorflow/models,models,1979,igor0,Fix up download_and_preprocess_imagenet.sh,"As of 444310f2, download_imagenet.sh expects an absolute path the synsets file. But,
download_and_preprocess_imagenet.sh specifies the synsets file as a relative path.

Also, there were a couple of fixes to download_image.sh that are also needed in
download_imagenet.sh (dd983ea6, 9997b250).",2,,[],2017-07-18 16:11:45,open,,,['cla: no'],2017-09-28 02:43:11
1455,tensorflow/models,models,1976,Pibborn,download_and_preprocess_flowers fails due to a script error,"I am trying to run the inception model using the flowers dataset. I am using Mac OS X El Capitan, using tensorflow version:

```bash
$  python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" 
v1.2.0-5-g435cdfc 1.2.1
```

### Steps to reproduce
```bash
git clone https://github.com/tensorflow/models
cd models/inception
bazel build //inception:download_and_preprocess_flowers
mkdir flowers-data
bazel-bin/inception/download_and_preprocess_flowers flowers-data
 [download info...]
 >mv: rename flower_photos to flowers-data/raw-data/train: No such file or directory
```

I was able to fix this on my machine by editing the ``models/inception/inception/data/download_and_preprocess_flowers.sh`` script [from line 67 onwards](https://github.com/tensorflow/models/blob/master/inception/inception/data/download_and_preprocess_flowers.sh#L67):

```bash
cd ..
mv ""${DATA_DIR}/flower_photos"" ""${TRAIN_DIRECTORY}""
```

which substitutes 

```bash
mv flowers_photos ""$(TRAIN_DIRECTORY}""
```

This makes sense (at least to me :-)) because if you have the script do ``echo $(pwd)`` between line 68 and 69, you get ``/path/to/repo/models/inception/${DATA-DIR}``. Therefore, if you don't ``cd ..``, the script will then try to move ``/path/to/repo/models/inception/${DATA-DIR}/flower_photos/`` to ``/path/to/repo/models/inception/${DATA-DIR}/${DATA-DIR}/flower_photos/raw-data/train`` on line 69, which will fail.

Another thing to note is that line 68 silently fails - you can verify that by removing the ``-f`` option. Is that the intended behaviour?

Would a PR be appreciated, or am I mistaken about this being an error in the script? This was my first experience with anything bazel.",4,,[],2017-07-18 13:03:01,open,,,['stat:contributions welcome'],2017-07-25 15:43:50
1456,tensorflow/models,models,1972,chakpongchung,[object detection feature request]: use multiple gpu for training,"

### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:  from pip
- **TensorFlow version (use command below)**: 

('v1.2.0-5-g435cdfc', '1.2.1')

- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: 1080TI *2
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""



I am trying to use two gpu to speed up the training with data parallelism, does the code have such features? Would Model parallelism be faster?",41,,[],2017-07-17 14:01:25,open,,,['type:feature'],2019-03-19 03:18:34
1457,tensorflow/models,models,1968,xiaoerlaigeid,Update label_map_util.py,,4,,[],2017-07-17 02:41:01,open,,,['cla: yes'],2017-09-28 02:43:11
1458,tensorflow/models,models,1961,aayushmudgal,resolving when there are no detections,Remove_invalid_boxes breaks when the detected_boxes is empty. The fix checks if there is any detected_boxes as an input.,3,,[],2017-07-14 20:48:08,open,,,['cla: yes'],2017-09-28 02:43:11
1459,tensorflow/models,models,1952,starcrest,configs for object detection model zoo,"Could you please release the configuration files (pipeline_pb2.TrainEvalPipelineConfig or train_pb2.TrainConfig) used to train the models in the object_detection [model zoo](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md)?  I'd like to see the optimization parameters, etc, to learn best practices -- and ideally be able to reproduce from scratch.

Sorry if I missed it.  Thanks!",5,,[],2017-07-14 04:37:44,open,,,[],2018-11-14 15:22:51
1460,tensorflow/models,models,1950,YanLiang0813,Did MaskRCNNBoxPredictor use FPN ?,"
Recently, I use the Object Detection API, and I find in faster_rcnn_resnet101_voc07.config it used mask_rcnn_box_predictor, I want to know whether it use FPN when it predict anchors? are there some one give me some help? Thanks!!!",3,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-07-14 02:09:53,open,,,[],2017-07-17 16:41:18
1461,tensorflow/models,models,1946,dfridman1,Convert RepeatedScalarContainer to a list of floats,"Convert `means` parameter of `subtract_channel_mean` function from type `RepeatedScalarContainer` to a list of floats.

Fix `TypeError: Expected float32, got <google.protobuf.pyext._message.RepeatedScalarContainer object at 0x7fb7e6d65618> of type 'RepeatedScalarContainer' instead.`",3,,[],2017-07-13 16:51:58,open,,,"['awaiting review', 'cla: yes']",2017-09-28 02:43:10
1462,tensorflow/models,models,1942,rniebecker,Object detection using GPU on Windows is about 5 times slower than on Ubuntu,"### System information
- **What is the top-level directory of the model you are using**:
ssd_mobilenet_v1_coco_11_06_2017
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04 
Windows 10 build 15063.413
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.2.0-rc2-21-g12f033d
- **Bazel version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
8.0/5.1
- **GPU model and memory**:
Ubuntu GTX970 4GB
Windows GTX1080 12GB
- **Exact command to reproduce**:
Using Object Detection Demo with the 2 images provided

### Describe the problem
Installed Nvidia Cuda and cuDNN on both systems.
Installed tensorflow on Ubuntu and Windows with GPU support both times using Python 3.5.2 and the native pip installation.
Installed and setup models as required.
 
My Windows is running on a machine with a GTX1080, my Ubuntu box with GTX970.
Comparing the results between the 2 setups shows that on Windows the object detection is about 5 times slower than on Ubuntu.
Measuring the GPU load on Windows using GPU-Z shows that the GPU is barely being used when the object detection is running, only jumping up to about 16% from time to time.
I did the same test with tensorflow without GPU support on the same machine and the result is about 20% slower than compared to with GPU.

Interestingly when using the mnist_deep.py example from site-packages/tensorflow/examples/tutorials/mnist the GPU load on my Windows machine goes up to about 62%.

It looks like object_detection is for some reason not using the full capabilities of the GPU on Windows, the question is why? 

Are there any additional configurations I have to perform to speed it up? 
Are there any Nvidia windows graphic driver settings I can/should use?
Anyone else having the same issue?

Cheers,
Ralf
",21,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-07-13 09:01:26,open,,,"['models: official', 'stat:awaiting tensorflower', 'type:bug/performance']",2019-02-01 22:10:28
1463,tensorflow/models,models,1936,chakpongchung,eval.py stuck: The following classes have no ground truth examples,"I got the follow result after running:

# modify the config file for the right path
PATH_TO_YOUR_PIPELINE_CONFIG=""object_detection/samples/configs/faster_rcnn_resnet101_voc07.config""
PATH_TO_TRAIN_DIR=""out""
PATH_TO_EVAL_DIR=""out""

# From the tensorflow/models/ directory
GPU_ID=1
CUDA_VISIBLE_DEVICES=${GPU_ID} python object_detection/eval.py --debug\
    --logtostderr \
    --pipeline_config_path=${PATH_TO_YOUR_PIPELINE_CONFIG} \
    --checkpoint_dir=${PATH_TO_TRAIN_DIR} \
    --eval_dir=${PATH_TO_EVAL_DIR}




**WARNING:root:The following classes have no ground truth examples: [ 0  2  3  8  9 11 13 14 16 17 18]**
/local/home/cpchung/software/models/object_detection/utils/metrics.py:145: RuntimeWarning: invalid value encountered in true_divide
  num_images_correctly_detected_per_class / num_gt_imgs_per_class)



Where can I find the result of this evalution?
",17,,[],2017-07-12 20:54:04,open,,,[],2018-03-30 20:43:56
1464,tensorflow/models,models,1932,anne1994,Freeze graph errors,"When i try to run the freeze graph on the finetuned checkpoint i am getting the following errors.
Traceback (most recent call last):
  File ""freeze_graph.py"", line 255, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""freeze_graph.py"", line 187, in main
    FLAGS.variable_names_blacklist)
  File ""freeze_graph.py"", line 179, in freeze_graph
    variable_names_blacklist)
  File ""freeze_graph.py"", line 116, in freeze_graph_with_def_protos
    variable_names_blacklist=variable_names_blacklist)
TypeError: convert_variables_to_constants() got an unexpected keyword argument 'variable_names_blacklist'

I tried commenting variable_names_blacklist and then given the inputs 
 --input_graph=/data/tmp/p-models-1/inception_v3/graph.pbtxt --input_checkpoint=/data/tmp/models-1/inception_v3/model.ckpt-1000 --output_graph=/data/tmp/frozen_graph.pb --output_node_names=InceptionV3/ AuxLogits/Logits/Predictions 

i get 
Traceback (most recent call last):
  File ""freeze_graph.py"", line 255, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""freeze_graph.py"", line 187, in main
    FLAGS.variable_names_blacklist)
  File ""freeze_graph.py"", line 179, in freeze_graph
    variable_names_blacklist)
  File ""freeze_graph.py"", line 115, in freeze_graph_with_def_protos
    output_node_names.split("",""))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py"", line 234, in convert_variables_to_constants
    inference_graph = extract_sub_graph(input_graph_def, output_node_names)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py"", line 158, in extract_sub_graph
    assert d in name_to_node_map, ""%s is not in graph"" % d
AssertionError: softmax is not in graph

Any solutions?


",3,"NamedUser(login=""petewarden"")","[NamedUser(login=""petewarden"")]",2017-07-11 23:52:25,open,,,[],2017-07-17 12:11:21
1465,tensorflow/models,models,1930,micahprice,object_detection aspect ratio width/height or height/width?,"Digging through the source code, it isn't obvious to me whether the aspect ratio is defined as height/width or width/height. I've asked this question on StackOverflow, and I know that at least two people have dug through the documentation and papers and not found it explained explicitly.  My assumption would be that it's width/height (so 10 would have w:h of 10:1).
Here's the code from the config files that I'm talking about:

   anchor_generator {
      ssd_anchor_generator {
        num_layers: 4
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: .5
        aspect_ratios: 2.
        aspect_ratios: 10.0
        reduce_boxes_in_lowest_layer: true
      }",2,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-07-11 21:49:39,open,,,[],2017-08-10 18:33:38
1466,tensorflow/models,models,1928,sguada,detection_model_zoo need updated model names to include the date,https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md,1,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow""), NamedUser(login=""jch1"")]",2017-07-11 13:15:51,open,,,[],2017-07-13 18:03:22
1467,tensorflow/models,models,1919,YuxianMeng,Update exporter.py,This pull request is according to [#issue1916](https://github.com/tensorflow/models/issues/1916),3,,[],2017-07-11 03:53:43,open,,,"['awaiting review', 'cla: yes']",2017-09-28 02:43:10
1468,tensorflow/models,models,1915,StevenLOL,adversarial_text tensorflow: Loss is nan,"This error occurs randomly when run the adversarial code for dbpedia dataset.
```
INFO:tensorflow:step 15400, loss = 0.09 (294.3 examples/sec; 0.217 sec/batch)
INFO:tensorflow:step 15410, loss = 0.05 (312.1 examples/sec; 0.205 sec/batch)
INFO:tensorflow:step 15420, loss = 0.11 (284.1 examples/sec; 0.225 sec/batch)
INFO:tensorflow:step 15430, loss = 0.10 (300.9 examples/sec; 0.213 sec/batch)
INFO:tensorflow:Error reported to Coordinator: <type 'exceptions.OverflowError'>, Loss is nan
Traceback (most recent call last):
  File ""/home/steven/.cache/bazel/_bazel_steven/c97db0dd409c586d669fa291622dae1b/execroot/models/bazel-out/local-fastbuild/bin/adversarial_text/train_classifier.runfiles/__main__/adversarial_text/train_classifier.py"", line 64, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/steven/.cache/bazel/_bazel_steven/c97db0dd409c586d669fa291622dae1b/execroot/models/bazel-out/local-fastbuild/bin/adversarial_text/train_classifier.runfiles/__main__/adversarial_text/train_classifier.py"", line 60, in main
    pretrained_model_dir=FLAGS.pretrained_model_dir)
  File ""/data/gits/models/adversarial_text/train_utils.py"", line 98, in run_training
    global_step_val = train_step(sess, train_op, loss, global_step)
  File ""/data/gits/models/adversarial_text/train_utils.py"", line 138, in train_step
    raise OverflowError('Loss is nan')
OverflowError: Loss is nan
ERROR: Non-zero return code '1' from command: Process exited with status 1.

```

### System information
- **What is the top-level directory of the model you are using**:
/data/gits/models/adversarial_text/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes, this is for dbpedia dataset
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.2.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: GTX1080 8GB
- **Exact command to reproduce**:
```
	bazel run :train_classifier -- \
	    --train_dir=$TRAIN_DIR \
	    --data_dir=$dataset_DATA_DIR \
	    --vocab_size=${vocab_size}  \
	    --embedding_dims=${embedding_dims} \
	    --rnn_cell_size=1024 \
	    --cl_num_layers=1 \
	    --cl_hidden_size=128 \
	    --learning_rate=0.0005 \
	    --learning_rate_decay_factor=0.9998 \
	    --max_steps=150000 \
	    --max_grad_norm=1.0 \
	    --num_timesteps=${num_timesteps} \
	    --keep_prob_emb=0.5 \
	    --normalize_embeddings \
	    --adv_training_method=vat \
	    --num_classes=${num_classes} \
	    --perturb_norm_length=5.0 \
```
",4,"NamedUser(login=""rsepassi"")","[NamedUser(login=""rsepassi"")]",2017-07-11 03:06:03,open,,,[],2017-08-29 18:15:29
1469,tensorflow/models,models,1911,rogerguess,attention_ocr: Python 3 compatibility,,4,,[],2017-07-10 16:27:46,open,,,"['cla: yes', 'stat:awaiting response']",2017-09-28 02:43:10
1470,tensorflow/models,models,1902,AruniRC,Python 3 issue: does not support implicit comparisons in sorted(dictionary),"### System information
- **What is the top-level directory of the model you are using**: models/object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 
    Cuda compilation tools, release 8.0, V8.0.44
    NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.39
- **GPU model and memory**: Titan X (Pascal)
- **Exact command to reproduce**:
The object detector API [ training locally example: ](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_locally.md)
```
# From the tensorflow/models/ directory
python object_detection/train.py \
    --logtostderr \
    --pipeline_config_path=${PATH_TO_YOUR_PIPELINE_CONFIG} \
    --train_dir=${PATH_TO_TRAIN_DIR}
```

### Describe the problem

Python 3 no longer allows sorting operations on a list of {tuples of strings} and strings.

This situation arises when trying to train a Faster R-CNN network on PASCAL locally, following the steps in the [example](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_locally.md), in `tensorflow/python/training/input.py`, line 385, in `_as_tensor_list()`

>     return [tensors[k] for k in **sorted(tensors)**]
> TypeError: unorderable types: str() < tuple()

The dictionary `tensors` has keys that are a mix of tuples and strings:

> 
> tensors.keys()
> dict_keys(['groundtruth_difficult', 'filename', 'groundtruth_instance_masks', ('filename', 'runtime_shapes'), ('groundtruth_is_crowd', 'runtime_shapes'), 'image', ('groundtruth_difficult', 'runtime_shapes'), ('groundtruth_classes', 'runtime_shapes'), 'key', 'groundtruth_instance_classes', 'groundtruth_classes', ('source_id', 'runtime_shapes'), 'source_id', ('groundtruth_instance_classes', 'runtime_shapes'), ('groundtruth_area', 'runtime_shapes'), 'groundtruth_area', ('groundtruth_instance_masks', 'runtime_shapes'), ('image', 'runtime_shapes'), 'groundtruth_is_crowd', ('key', 'runtime_shapes'), ('groundtruth_boxes', 'runtime_shapes'), 'groundtruth_boxes'])

Calling `sorted()` on this would work in Python 2.7, but not in Python 3, because the latter no longer supports the implicit call to `cmp()` used to sort the dict keys in the default setting. Now we need to explicitly provide a comparison function to `sorted()`.

If the ordering is important, perhaps `sorted(tensors, key=lambda s:s[0])` might do the trick.


### Source code / logs

This problem can be reproduced by the example for training Faster R-CNN locally on the PASCAL dataset using resnet-101 for config.


PS:

It seems that the original code was developed on and for Python 2.7. Could you perhaps mention this in the README, so that users can avoid spending time attempting to make this work on Python 3?

thanks,
Aruni




",3,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-07-09 16:45:35,open,,,[],2017-08-17 03:15:44
1471,tensorflow/models,models,1901,jematos92,Merge Inception_v4 model from the models repositories to tensorflow,"Hi all, 
I was wondering if there are any plans to merge the inception_v4 model locate at
https://github.com/tensorflow/models/tree/master/slim/nets

to here:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim/python/slim/nets
Also, How are you planning to maintain in sync this two repositories for the future. 

Thanks, 
",1,,[],2017-07-09 16:41:55,open,,,[],2018-04-06 07:44:33
1472,tensorflow/models,models,1894,CatalinVoss,Fix slim walkthrough to use training version of preprocessing when training,The method load_batch has an argument is_training which is passed on to the inception pre-processing code (which does two different versions of preprocessing for training vs. eval). This should set to true in example where we're training something.,3,,[],2017-07-07 22:20:33,open,,,"['awaiting review', 'cla: yes']",2017-09-28 02:43:10
1473,tensorflow/models,models,1890,chenyuZha,Can't find output node 'detection_masks' when export inference graph,"
### System information
- **What is the top-level directory of the model you are using**: Object Detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow version (use command below)**: 1.1
- **GPU model and memory**: GeForce GTX 1050 Ti/PCIe/SSE2
## Describe problem
When I tried to export my inference graph by using **export_inference_graph.py**, I found that in the file **exporter.py** there is an optional output node : **detection_masks**, and I would like to activate this option so that I can get the height and width of my boxes...How can I activate this option? I checked the proto files but I didn't find it..
",6,"NamedUser(login=""tombstone"")","[NamedUser(login=""tombstone"")]",2017-07-07 15:48:26,open,,"NamedUser(login=""chenyuZha"")",[],2017-07-31 07:40:13
1474,tensorflow/models,models,1885,antoniosimunovic,Evaluation during training,"Hi,

is there a way to run model evaluation during training? It seems to me that I need to stop training process and run evaluation on checkpoints in train directory to get some metrics. It would be more elegant to run the evaluation process after X number of training steps.

",9,,[],2017-07-07 11:41:39,open,,,"['stat:contributions welcome', 'type:feature']",2018-11-01 03:40:43
1475,tensorflow/models,models,1880,xhzcyc,Will Object_detection models add yolo/yolo2 ?,The yolo/yolo2 models also have good performance. Will you add them into the object_detection models? ,2,,[],2017-07-07 01:16:03,open,,,"['stat:contributions welcome', 'type:feature']",2017-08-20 12:39:35
1476,tensorflow/models,models,1876,chenyuZha,API ObjectDetection size of  input images issues ,"**GPU  : GeForce GTX 1080 Ti/PCIe/SSE2 (11 GB)**
**Tensorflow version: 1.1.0**
**Python version: 2.7.12**
**Model checkpoint : ssd_mobilenet_v1_coco_11_06_2017**

**Context** :After read the tutorial of ObjectDetection, I converted my own image data sets to tfRecord files by create the *.xml file to each image. As the sizes of images of my own data sets are very large : about **width=4000pixels** and **height=2000pixels** for each , the train.tfRecord is about **55G**.

**Issues**: When I began to train with **train.py** by using the lightest model  **ssd_mobilenet_v1_coco_11_06_2017**, after 4-5 steps, it crashed by error OOM.
The error message is below:
![125](https://user-images.githubusercontent.com/29950360/27914310-08056dd6-6263-11e7-83ca-8249a3ff0c9a.png)
Il seems like that the OOM error happened when allocating tensor with shape[1,2969,3546,3],
As the capacity of my GPU is 11GB, I didn't understand why it causes this problem..





",37,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-07-06 14:06:43,open,,"NamedUser(login=""chenyuZha"")","['stat:awaiting tensorflower', 'type:bug/performance']",2018-11-30 18:20:00
1477,tensorflow/models,models,1873,hjp709394,Fix preprocessing bugs,1. tf.constant could not accept a proto field correctly. 2. height/width should be int32 instead of float,0,,[],2017-07-06 09:23:41,open,,,"['awaiting review', 'cla: yes']",2017-09-28 02:43:10
1478,tensorflow/models,models,1868,briantse100,about 10% difference mAP result between object detection API and tf-faster-rcnn when train faster rcnn with res101 on voc2007,"## Systeme Information

###OS Platform and Distribution: Linux Ubuntu 14.04
###TensorFlow installed from: Anaconda TensorFlow
###TensorFlow version: ('v1.0.0-65-g4763edf-dirty', '1.0.1')
###CUDA/cuDNN version: Cuda 8.0, Cudnn 5.1
###GPU model and memory: 2 Tesla K-40, Memory 12G / 2 GF GTX-980, Memory 4G

## Describe the Problem
The result of mAP is 74.5% using the framework of [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn) with res101 and voc 2007, iter times 70000. But object detection API is 64.8%, iter times 170000. The [pre-trained model](http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz) is the same. Anyone has the same  issue?",4,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-07-06 03:16:37,open,,,[],2018-05-24 14:34:59
1479,tensorflow/models,models,1866,Sanghoon94,Using pretrained embedding in Seq2seq with attention model,"Hello, 

I am trying to implement seq2seq model, by converting the sample code in 'models/tutorials/rnn/translate'.
It would be great if anyone can tell me how to use a pretrained embedding, such as google's word2vec.

It seems that the seq2seq model the sample code is using is ""embedding_attention_seq2seq"", which is from 'tf.contrib.legacy_seq2seq'.
Encoder is already attached with the embedding matrix using 'EmbeddingWrapper', and the decoder is ""embedding_attention_decoder"".

I don't want to train the embedding matrix while learning the seq2seq model, and is there any way to do so, while not building the seq2seq model on my own?

Thank you for your time.",13,,[],2017-07-06 01:25:51,open,,,[],2018-04-06 07:44:24
1480,tensorflow/models,models,1856,ckalas,eval.py stops evaluating after 3-5 images,"On both the pet dataset and my own custom dataset, the evaluation script seemingly stops evaluating images after a few samples. In both cases there is a warning about no ground truth samples for some of the classes, but I altered the line to remove the warning occuring and the evaluation still stops. Using python 3.5

```
WARNING:root:The following classes have no ground truth examples: [11 15 16 17 18 19]
/home/chris/tensorflow/models/object_detection/utils/metrics.py:144: RuntimeWarning: invalid value encountered in true_divide
  num_images_correctly_detected_per_class / num_gt_imgs_per_class)

```
There is no error, only the warning, but after that nothing new appears in tensorboar.
On a side note, does the number of examples need to be set in the config file? seems strange to be hard coding...

`eval_config: {
  num_examples: 158
}
`",16,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-07-05 06:39:19,open,,,[],2017-09-15 08:51:29
1481,tensorflow/models,models,1854,aloerch,GPU memory error with train.py and eval.py running together,"### System information
- **What is the top-level directory of the model you are using**: object_detection
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: not yet...
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18
- **TensorFlow installed from (source or binary)**: pip tensorflow-gpu
- **TensorFlow version (use command below)**: ('v1.2.0-rc2-21-g12f033d', '1.2.0')
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.44
- **GPU model and memory**: eVGA GTX 1080Ti, 11Gb
- **Exact command to reproduce**:
Run 1st in one terminal:

```
python object_detection/train.py \
    --logtostderr \
    --pipeline_config_path=/home/meh/.virtualenvs/Project/lib/python2.7/site-packages/tensorflow/models/object_detection/samples/configs/faster_rcnn_resnet101_pets_learn.config \
    --train_dir=/home/meh/.virtualenvs/Project/models/model/train
```
That runs fine, training works... then run in 2nd terminal:

```
python object_detection/eval.py \
    --logtostderr \
    --pipeline_config_path=/home/meh/.virtualenvs/Project/lib/python2.7/site-packages/tensorflow/models/object_detection/samples/configs/faster_rcnn_resnet101_pets_learn.config \
    --checkpoint_dir=/home/meh/.virtualenvs/Project/models/model/train \
    --eval_dir=/home/meh/.virtualenvs/Project/models/model/eval
```
Evaluation fails with an error about the GPU being out of memory (training continues though in the other terminal window with no problem). Here is the traceback:

```
tensorflow/models $ python object_detection/eval.py \
>     --logtostderr \
>     --pipeline_config_path=/home/meh/.virtualenvs/Project/lib/python2.7/site-packages/tensorflow/models/object_detection/samples/configs/faster_rcnn_resnet101_pets_learn.config \
>     --checkpoint_dir=/home/meh/.virtualenvs/Project//models/model/train \
>     --eval_dir=/home/meh/.virtualenvs/Project//models/model/eval
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
INFO:tensorflow:Scale of 0 disables regularizer.
2017-07-04 13:05:20.813917: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-04 13:05:20.813976: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-04 13:05:20.813987: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-07-04 13:05:20.813995: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-04 13:05:20.814005: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-07-04 13:05:21.414540: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11713708032
Traceback (most recent call last):
  File ""object_detection/eval.py"", line 161, in <module>
    tf.app.run()
  File ""/home/meh/.virtualenvs/Project/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""object_detection/eval.py"", line 157, in main
    FLAGS.checkpoint_dir, FLAGS.eval_dir)
  File ""/home/meh/.virtualenvs/Project/lib/python2.7/site-packages/tensorflow/models/object_detection/evaluator.py"", line 211, in evaluate
    save_graph_dir=(eval_dir if eval_config.save_graph else ''))
  File ""/home/meh/.virtualenvs/Project/lib/python2.7/site-packages/tensorflow/models/object_detection/eval_util.py"", line 515, in repeated_checkpoint_run
    keys_to_exclude_from_results)
  File ""/home/meh/.virtualenvs/Project/lib/python2.7/site-packages/tensorflow/models/object_detection/eval_util.py"", line 359, in run_checkpoint_once
    sess = tf.Session(master, graph=tf.get_default_graph())
  File ""/home/meh/.virtualenvs/Project/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1292, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/meh/.virtualenvs/Project/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 562, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/home/meh/.virtualenvs/Project/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
```


Based on: 

> Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11713708032

 it looks like eval.py appears to be trying to run the evaluation on the same GPU that is actively doing the training. I have a 2nd GPU, an eVGA GTX 1080 FTW with 8Gb of RAM that I would be happy to run the eval.py on, and generally speaking, I know how to write my own tensorflow graph using tf.device('/gpu:1') but... I cannot figure out where to insert this in the object_detection code.

I would recommend adding the ability to select the GPU to use for both training and evaluation, possibly as a flag. In the meantime, any help you can offer regarding where I can insert that in the eval.py tree would be much appreciated.
",34,,[],2017-07-04 21:14:46,open,,,['stat:awaiting tensorflower'],2018-07-31 11:20:13
1482,tensorflow/models,models,1849,rogerguess,Tensorflow: Windows or NO Windows,"The models here seem to assume you are not trying to use Windows. Also, some models do not specify which version of python is required.

",20,,[],2017-07-04 06:54:56,open,,,"['stat:contributions welcome', 'type:build/install']",2017-09-08 09:33:52
1483,tensorflow/models,models,1845,anikethjr,adversarial_text doesn't seem to work for rcv1 like dataset,"I have a dataset whose format is the same as that of the rcv1 dataset. When I follow the instructions given in the README file, I get an error while training : 

INFO:tensorflow:step 10, loss = 2.95 (235.2 examples/sec; 0.136 sec/batch)
INFO:tensorflow:Error reported to Coordinator: <type 'exceptions.OverflowError'>, Loss is nan
Traceback (most recent call last):
  File ""/home/aniketh/.cache/bazel/_bazel_aniketh/8c5282dbcffe18d6c89921f5f1ad06fa/execroot/__main__/bazel-out/local-fastbuild/bin/train_classifier.runfiles/__main__/train_classifier.py"", line 63, in <module>
    tf.app.run()
  File ""/home/aniketh/adversarial/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/aniketh/.cache/bazel/_bazel_aniketh/8c5282dbcffe18d6c89921f5f1ad06fa/execroot/__main__/bazel-out/local-fastbuild/bin/train_classifier.runfiles/__main__/train_classifier.py"", line 59, in main
    pretrained_model_dir=FLAGS.pretrained_model_dir)
  File ""/data/aniketh/utopia/adversarial_text/train_utils.py"", line 92, in run_training
    global_step_val = train_step(sess, train_op, loss, global_step)
  File ""/data/aniketh/utopia/adversarial_text/train_utils.py"", line 132, in train_step
    raise OverflowError('Loss is nan')
OverflowError: Loss is nan
ERROR: Non-zero return code '1' from command: Process exited with status 1.


The csv files are in the required format - three files test.csv, train.csv and unlab.csv. Each file has two columns - the first with the classification (four classes - 1,2,3,4) , the unlabelled data has 0 in the classification column. The second column has the text data.

Exact commands which I ran:

bazel run :gen_vocab -- --output_dir=/data/aniketh/utopia/data --dataset=rcv1 --rcv1_input_dir=/data/aniketh/utopia/csv --lowercase=False

bazel run :gen_data -- --output_dir=/data/aniketh/utopia/data --dataset=rcv1 --rcv1_input_dir=/data/aniketh/utopia/csv --lowercase=False --label_gain=False --num_classes=4

bazel run :pretrain -- \
    --train_dir=/data/aniketh/utopia/pretrain \
    --data_dir=/data/aniketh/utopia/data \
    --num_classes=4 \
    --vocab_size=75606 \
    --embedding_dims=50 \
    --rnn_cell_size=64 \
    --num_candidate_samples=100 \
    --batch_size=32 \
    --learning_rate=0.001 \
    --learning_rate_decay_factor=0.9999 \
    --max_steps=100 \
    --max_grad_norm=1.0 \
    --num_timesteps=50 \
    --keep_prob_emb=0.5 \
    --adv_training_method=at \
    --use_seq2seq_autoencoder=True \
    --normalize_embeddings
    

bazel run :train_classifier -- \
    --train_dir=/data/aniketh/utopia/train \
    --pretrained_model_dir=/data/aniketh/utopia/pretrain \
    --data_dir=/data/aniketh/utopia/data \
    --num_classes=4 \
    --vocab_size=75606 \
    --embedding_dims=50 \
    --rnn_cell_size=64 \
    --cl_num_layers=1 \
    --cl_hidden_size=30 \
    --batch_size=32 \
    --learning_rate=0.0005 \
    --learning_rate_decay_factor=0.9998 \
    --max_steps=1000 \
    --max_grad_norm=1.0 \
    --num_timesteps=50 \
    --keep_prob_emb=0.5 \
    --normalize_embeddings \
    --adv_training_method=at \
    --use_seq2seq_autoencoder=True \
    --perturb_norm_length=5.0 


bazel run :evaluate -- \
    --eval_dir=/data/aniketh/utopia/eval \
    --checkpoint_dir=/data/aniketh/utopia/train \
    --eval_data=train \
    --run_once \
    --num_examples=1307 \
    --data_dir=/data/aniketh/utopia/data \
    --vocab_size=75606 \
    --embedding_dims=50 \
    --rnn_cell_size=64 \
    --batch_size=32 \
    --num_timesteps=50 \
    --use_seq2seq_autoencoder=True \
    --normalize_embeddings",11,"NamedUser(login=""rsepassi"")","[NamedUser(login=""rsepassi"")]",2017-07-03 14:38:25,open,,,[],2017-08-31 10:31:20
1484,tensorflow/models,models,1844,dimsava,Counter-intuitive `num_examples` in detector configuration file,"I experiment with the tutorial on Oxford-IIIT-Pet dataset to retrain a detector on a new dataset and subsequently test on it (different training/testing sets):

    python object_detection/train.py    (arguments omitted for brevity)
    python object_detection/eval.py

In the `ssd_inception_v2_pets.config` configuration file there is the field `num_examples` in section`eval_config` that has the default value 2000. A different (bigger) value for this results in a different mAP, which is a bit counter-intuitive. The number of examples in the testing .tfrecords for the Pet dataset is 1104, so all provided values for `num_examples` are bigger. 

Am I doing something wrong or is something unclear?",1,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-07-03 11:01:03,open,,,"['stat:awaiting tensorflower', 'type:docs']",2017-07-07 19:04:38
1485,tensorflow/models,models,1843,FPerezHernandez92,Running Localy don't work,"I download the models: https://github.com/tensorflow/models
I installed all and I do in directory: /models-master/:
$ protoc object_detection/protos/*.proto --python_out=.
$ export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
$ python object_detection/builders/model_builder_test.py

All fine, but, If I try do:
https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_locally.md
First I download annotations and images in directory: /object_detection/ , and I do in directory: /models-master/object_detection/
$ tar -xvf annotations.tar.gz
$ tar -xvf images.tar.gz
$ python create_pet_tf_record.py --data_dir=pwd --output_dir=pwd

In the pipeline I use the ""faster_rcnn_resnet101_pets.config"" and I changed the next:
fine_tune_checkpoint: ""/home/fperez/Descargas/models-master/object_detection/model.ckpt""
I don't know, where the model.ckpt is. The other change is:
""""""
train_input_reader: {
tf_record_input_reader {
input_path: ""/home/fperez/Descargas/models-master/object_detection/pet_train.record""
}
label_map_path: ""/home/fperez/Descargas/models-master/object_detection/data/pet_label_map.pbtxt""
}
eval_config: {
num_examples: 2000
}
eval_input_reader: {
tf_record_input_reader {
input_path: ""/home/fperez/Descargas/models-master/object_detection/pet_val.record""
}
label_map_path: ""/home/fperez/Descargas/models-master/object_detection/data/pet_label_map.pbtxt""
}
""""""

I do in directory: /models-master/
$ python object_detection/train.py --logtostderr --pipeline_config_path=object_detection/ --train_dir=object_detection/
and I had the next error:
""""""
Traceback (most recent call last):
File ""object_detection/train.py"", line 198, in
tf.app.run()
File ""/home/fperez/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""object_detection/train.py"", line 143, in main
model_config, train_config, input_config = get_configs_from_pipeline_file()
File ""object_detection/train.py"", line 103, in get_configs_from_pipeline_file
text_format.Merge(f.read(), pipeline_config)
File ""/home/fperez/.local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 125, in read
pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))
File ""/usr/lib/python2.7/contextlib.py"", line 24, in exit
self.gen.next()
File ""/home/fperez/.local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.FailedPreconditionError: object_detection
""""""

But, when I do ""import object_detection"" in python terminal, It works fine. What is it my problem? Sorry for the inconvenience",9,,[],2017-07-03 10:49:32,open,,,"['stat:contributions welcome', 'type:build/install']",2017-09-22 00:33:06
1486,tensorflow/models,models,1840,Heidisnaps,I want to train my own image dataset,"Hi.

I want to train my own image dataset.
can I train with api code?

If I can train my own image dataset, how can I do.",28,,[],2017-07-03 07:09:03,open,,,"['stat:community support', 'type:docs']",2019-03-02 05:57:46
1487,tensorflow/models,models,1836,groakat,SSD MobileNet: Cannot finetune from checkpoint,"Hi,

I am trying to finetune SDD MobileNet and I am failing because somehow the variables are not found in the checkpoint even though they are present. Finetunig FasterRCNN on the same dataset works fine btw.

```
/ExponentialMovingAverage not found in checkpoint
2017-07-02 22:59:48.794763: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_1/ClassPredictor/biases not fo
und in checkpoint
2017-07-02 22:59:48.794821: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_1/BoxEncodingPredictor/biases/
RMSProp_1 not found in checkpoint
2017-07-02 22:59:48.810102: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/BoxEncodingPredictor/biases/
ExponentialMovingAverage not found in checkpoint
2017-07-02 22:59:48.813253: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key FeatureExtractor/MobilenetV1/Conv2d_9_point
wise/BatchNorm/moving_variance not found in checkpoint
2017-07-02 22:59:48.813408: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/BoxEncodingPredictor/biases/
RMSProp not found in checkpoint
2017-07-02 22:59:48.813459: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_1/ClassPredictor/biases/RMSPro
p_1 not found in checkpoint
2017-07-02 22:59:48.813511: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_1/ClassPredictor/weights/Expon
entialMovingAverage not found in checkpoint
2017-07-02 22:59:48.813561: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/BoxEncodingPredictor/biases
not found in checkpoint
2017-07-02 22:59:48.813613: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_1/ClassPredictor/weights/RMSPr
op not found in checkpoint
2017-07-02 22:59:48.813666: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_1/ClassPredictor/weights/RMSPr
op_1 not found in checkpoint
2017-07-02 22:59:48.813721: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_1/ClassPredictor/weights not f
ound in checkpoint
2017-07-02 22:59:48.818346: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/BoxEncodingPredictor/weights
 not found in checkpoint
2017-07-02 22:59:48.829594: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/BoxEncodingPredictor/weights
/RMSProp_1 not found in checkpoint
2017-07-02 22:59:48.829707: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/ClassPredictor/biases/Expone
ntialMovingAverage not found in checkpoint
2017-07-02 22:59:48.832935: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/BoxEncodingPredictor/biases/
RMSProp_1 not found in checkpoint
2017-07-02 22:59:48.833103: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/ClassPredictor/biases not fo
und in checkpoint
2017-07-02 22:59:48.833143: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/BoxEncodingPredictor/weights
/ExponentialMovingAverage not found in checkpoint
2017-07-02 22:59:48.833195: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key FeatureExtractor/MobilenetV1/Conv2d_9_point
wise/BatchNorm/moving_mean not found in checkpoint
2017-07-02 22:59:48.833244: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/BoxEncodingPredictor/weights
/RMSProp not found in checkpoint
2017-07-02 22:59:48.833294: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/ClassPredictor/biases/RMSPro
p not found in checkpoint
2017-07-02 22:59:48.835462: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/ClassPredictor/biases/RMSPro
p_1 not found in checkpoint
2017-07-02 22:59:48.835508: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/ClassPredictor/weights not f
ound in checkpoint
2017-07-02 22:59:48.885331: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/ClassPredictor/weights/RMSPr
op not found in checkpoint
2017-07-02 22:59:48.887146: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/BoxEncodingPredictor/biases/
ExponentialMovingAverage not found in checkpoint
2017-07-02 22:59:48.887275: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/BoxEncodingPredictor/biases/
RMSProp_1 not found in checkpoint
2017-07-02 22:59:48.887440: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/ClassPredictor/weights/Expon
entialMovingAverage not found in checkpoint
2017-07-02 22:59:48.887446: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/BoxEncodingPredictor/biases
not found in checkpoint
2017-07-02 22:59:48.887461: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/BoxEncodingPredictor/biases/
RMSProp not found in checkpoint
2017-07-02 22:59:48.887467: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_2/ClassPredictor/weights/RMSPr
op_1 not found in checkpoint
2017-07-02 22:59:48.887513: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key FeatureExtractor/MobilenetV1/Conv2d_9_point
wise/BatchNorm/gamma/RMSProp_1 not found in checkpoint
2017-07-02 22:59:48.890507: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/BoxEncodingPredictor/weights
 not found in checkpoint
2017-07-02 22:59:48.891118: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/BoxEncodingPredictor/weights
/ExponentialMovingAverage not found in checkpoint
2017-07-02 22:59:48.894821: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/ClassPredictor/biases/Expone
ntialMovingAverage not found in checkpoint
2017-07-02 22:59:48.899697: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/BoxEncodingPredictor/weights
/RMSProp not found in checkpoint
2017-07-02 22:59:48.902223: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/ClassPredictor/weights not f
ound in checkpoint
2017-07-02 22:59:48.902278: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/BoxEncodingPredictor/weights
/RMSProp_1 not found in checkpoint
2017-07-02 22:59:48.903933: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key FeatureExtractor/MobilenetV1/Conv2d_9_point
wise/BatchNorm/gamma/RMSProp not found in checkpoint
2017-07-02 22:59:48.904057: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/ClassPredictor/weights/Expon
entialMovingAverage not found in checkpoint
2017-07-02 22:59:48.904106: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/ClassPredictor/biases not fo
und in checkpoint
2017-07-02 22:59:48.904158: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/ClassPredictor/biases/RMSPro
p_1 not found in checkpoint
2017-07-02 22:59:48.904214: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/ClassPredictor/biases/RMSPro
p not found in checkpoint
2017-07-02 22:59:48.904274: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/ClassPredictor/weights/RMSPr
op not found in checkpoint
2017-07-02 22:59:48.907282: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_4/BoxEncodingPredictor/biases
not found in checkpoint
2017-07-02 22:59:48.907388: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_3/ClassPredictor/weights/RMSPr
op_1 not found in checkpoint
2017-07-02 22:59:48.955458: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key FeatureExtractor/MobilenetV1/Conv2d_9_point
wise/BatchNorm/gamma/ExponentialMovingAverage not found in checkpoint
2017-07-02 22:59:48.957467: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_4/BoxEncodingPredictor/biases/
RMSProp not found in checkpoint
2017-07-02 22:59:48.957677: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_4/BoxEncodingPredictor/weights
/ExponentialMovingAverage not found in checkpoint
2017-07-02 22:59:48.957730: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_4/BoxEncodingPredictor/biases/
ExponentialMovingAverage not found in checkpoint
2017-07-02 22:59:48.957773: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_4/BoxEncodingPredictor/weights
/RMSProp_1 not found in checkpoint
2017-07-02 22:59:48.957865: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_4/BoxEncodingPredictor/weights
 not found in checkpoint
2017-07-02 22:59:48.957902: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_4/BoxEncodingPredictor/weights
/RMSProp not found in checkpoint
2017-07-02 22:59:48.957823: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_4/BoxEncodingPredictor/biases/
RMSProp_1 not found in checkpoint
2017-07-02 22:59:48.961594: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key FeatureExtractor/MobilenetV1/Conv2d_9_point
wise/BatchNorm/gamma not found in checkpoint
2017-07-02 22:59:48.966140: W tensorflow/core/framework/op_kernel.cc:1165] Not found: Key BoxPredictor_4/ClassPredictor/weights/Expon
entialMovingAverage not found in checkpoint
```
...
```

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1285, in _do_call
    return fn(*args)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1264, in _run_fn
    status, run_metadata)
  File ""/opt/conda/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_st
atus
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Key BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage not fo
und in checkpoint
         [[Node: save_1/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save_1/Const
_0_0, save_1/RestoreV2_1/tensor_names, save_1/RestoreV2_1/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmp/models/object_detection/train.py"", line 198, in <module>
    tf.app.run()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/tmp/models/object_detection/train.py"", line 194, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/tmp/models/object_detection/trainer.py"", line 290, in train
    saver=saver)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 738, in train
    master, start_standard_services=False, config=session_config) as sess:
  File ""/opt/conda/lib/python3.6/contextlib.py"", line 82, in __enter__
    return next(self.gen)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 964, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 792, in stop
    stop_grace_period_secs=self._stop_grace_secs)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/opt/conda/lib/python3.6/site-packages/six.py"", line 686, in reraise
    raise value
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 953, in managed_session
    start_standard_services=start_standard_services)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 708, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 273, in prepare_session
    config=config)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 205, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1555, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 896, in run
    run_metadata_ptr)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1279, in _do_run
    options, run_metadata)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1298, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage not fo
und in checkpoint
         [[Node: save_1/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save_1/Const
_0_0, save_1/RestoreV2_1/tensor_names, save_1/RestoreV2_1/shape_and_slices)]]

Caused by op 'save_1/RestoreV2_1', defined at:
  File ""/tmp/models/object_detection/train.py"", line 198, in <module>
    tf.app.run()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/tmp/models/object_detection/train.py"", line 194, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/tmp/models/object_detection/trainer.py"", line 275, in train
    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1140, in __init__
    self.build()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1172, in build
    filename=self._filename)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 688, in build
    restore_sequentially, reshape)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 661, in restore_v2
    dtypes=dtypes, name=name)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2534, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1203, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage not found in checkpo
int
         [[Node: save_1/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save_1/Const
_0_0, save_1/RestoreV2_1/tensor_names, save_1/RestoreV2_1/shape_and_slices)]]

ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'init_ops/report_uninitialized_variables/boolean_mask/Gather:0' shape=(?,) dtype=string>
If you want to mark it as used call its ""mark_used()"" method.
It was originally created here:
['File ""/tmp/models/object_detection/train.py"", line 198, in <module>\n    tf.app.run()', 'File ""/opt/conda/lib/python3.6/site-packag
es/tensorflow/python/platform/app.py"", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))', 'File ""/tmp/models/o
bject_detection/train.py"", line 194, in main\n    worker_job_name, is_chief, FLAGS.train_dir)', 'File ""/tmp/models/object_detection/t
rainer.py"", line 290, in train\n    saver=saver)', 'File ""/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/
learning.py"", line 663, in train\n    ready_op = tf_variables.report_uninitialized_variables()', 'File ""/opt/conda/lib/python3.6/site
-packages/tensorflow/python/util/tf_should_use.py"", line 170, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))', '
File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 139, in _add_should_use_warning\n    wrap
ped = TFShouldUseWarningWrapper(x)', 'File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 96,
 in __init__\n    stack = [s.strip() for s in traceback.format_stack()]']
```

If I inspect the checkpoint, all these variables seem to exist:

```
FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma (DT_FLOAT) [256]
FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/ExponentialMovingAverage (DT_FLOAT) [256]
FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/RMSProp (DT_FLOAT) [256]
FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/RMSProp_1 (DT_FLOAT) [256]
FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/moving_mean (DT_FLOAT) [256]
FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/moving_variance (DT_FLOAT) [256]
FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights (DT_FLOAT) [3,3,256,1]
FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/ExponentialMovingAverage (DT_FLOAT) [3,3,256,1]
FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/RMSProp (DT_FLOAT) [3,3,256,1]
FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/RMSProp_1 (DT_FLOAT) [3,3,256,1]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/moving_mean (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/moving_variance (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights (DT_FLOAT) [1,1,256,512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/ExponentialMovingAverage (DT_FLOAT) [1,1,256,512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/RMSProp (DT_FLOAT) [1,1,256,512]
FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/RMSProp_1 (DT_FLOAT) [1,1,256,512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/moving_mean (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/moving_variance (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/ExponentialMovingAverage (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/RMSProp (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/RMSProp_1 (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/moving_mean (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/moving_variance (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/ExponentialMovingAverage (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/RMSProp (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/RMSProp_1 (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/moving_mean (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/moving_variance (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/ExponentialMovingAverage (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/RMSProp (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/RMSProp_1 (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/moving_mean (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/moving_variance (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/ExponentialMovingAverage (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/RMSProp (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/RMSProp_1 (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/moving_mean (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/moving_variance (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/ExponentialMovingAverage (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/RMSProp (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/RMSProp_1 (DT_FLOAT) [3,3,512,1]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/ExponentialMovingAverage (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/RMSProp (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/RMSProp_1 (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/moving_mean (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/moving_variance (DT_FLOAT) [512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/ExponentialMovingAverage (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/RMSProp (DT_FLOAT) [1,1,512,512]
FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/RMSProp_1 (DT_FLOAT) [1,1,512,512]
global_step (DT_INT64) []
```

I am using the checkpoint `ssd_mobilenet_v1_coco_11_06_2017.tar.gz` from the model zoo, tensorflow 1.2 and the master branch of tensorflow/models",37,,[],2017-07-02 23:14:43,open,,,"['stat:awaiting response', 'type:support']",2019-03-13 07:12:45
1488,tensorflow/models,models,1833,yiyangjiang,VGG16 pre-trained model's accuracy is a little Lower than expected,"I evaluated the vgg_16 model on imagenet validation set with eval_image_classifier.py.

```
python eval_image_classifier.py --alsologtostderr \
    --checkpoint_path=/home/jyy/ckpt/vgg_16.ckpt \
    --dataset_dir=/media/jyy/F/tf_imagenet \
    --dataset_name=imagenet \
    --dataset_split_name=validation \
    --model_name=vgg_16 \
    --labels_offset=1
```

The val data was generated using script [https://github.com/tensorflow/models/blob/master/inception/inception/data/download_and_preprocess_imagenet.sh](url)

The ckpt file is downloaded from [http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz](url)

The final top 1 accuracy is 70.93% which is a little lower than 71.5% in the table.
While the top 5 accuracy is 89.82% which is right.
VGG 16|Code|vgg_16_2016_08_28.tar.gz|71.5|89.8

I wish to know if there is anything wrong.

Thanks!

UPDATE:
I also evaluated ResNet_V1_152 and the accuracy is right.
ResNet V1 152|Code|resnet_v1_152_2016_08_28.tar.gz|76.8|93.2
```
2017-07-02 01:19:57.291579: I tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0.76798]
2017-07-02 01:19:57.292239: I tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_5[0.9318]
```",3,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-07-01 17:44:24,open,,,['type:support'],2018-01-06 09:13:35
1489,tensorflow/models,models,1829,jhelie,Fix some lgtm alerts,"This PR fixes issues flagged by lgtm.com code queries. Some are minors and some should fix errors or unintentional code behaviour.

I think the suggested fixes make sense but someone more familiar with the code base and what was the initial intention may suggest an alternative way. In particular there seem to be many `if` `elif` blocks that look like they could throw errors due to non initialised variables.

The standard lgtm engineering analytics highlights dependencies, vulnerabilities and currently flag [184 alerts](https://lgtm.com/projects/g/tensorflow/models/alerts/) but you can investigate further by writing [your own custom queries](https://lgtm.com/docs/ql/about-ql) to explore your code base.

I personally find lgtm alerts most useful when using [PR integration](https://lgtm.com/docs/lgtm/config/pr-integration) as it allows to check and deal with potential issues before they find their way into master - for instance several of the alerts fixed by this PR were introduced in recent commits (notably 5b9d9097cc255becef4b5460c4b951c143d7a380 and 3a65897989c4711c2b0d8544a1a8a455ac654cec).

Full disclosure: I work on lgtm.com (and play with tensorflow), hence my interest!

Hope that helps, any question please ask.",14,,[],2017-06-30 23:31:55,open,,,['cla: yes'],2017-09-28 02:43:10
1490,tensorflow/models,models,1809,humayun,How to retrain existing SSD_Mobilenet_MSCOCO model for a new class label,"Dear all,

I run the trained model (ssd_mobilenet_v1_coco_11_06_2017) to detect bounding box on my dataset, but this trained model is not performing well because the main object that I am detecting is not in trained model.
Here is example that I am following:
https://github.com/tensorflow/models/blob/master/object_detection/object_detection_tutorial.ipynb

First Question: How can I retrain the existing ssd_mobilenet_v1_coco_11_06_2017 model with one addition class. For the new class, I generate boundingbox annotation in xml format.
 
Second Question: How can I train a new model only with my own dataset which has only two classes. If possible share any demo or code or tutorial.

Thank you.

-Humayun
",19,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-06-29 08:15:30,open,,,['type:feature'],2018-08-22 19:24:49
1491,tensorflow/models,models,1798,pdcoded,Trouble exporting  ptb_word_lm.py ,"Can someone write a export script for ptb_word_lm model similar to Object detection exporting script ?

Thanks

",1,,[],2017-06-28 21:05:34,open,,,"['stat:contributions welcome', 'type:feature']",2017-07-06 22:30:56
1492,tensorflow/models,models,1796,KirillDyagilev,Feature request: release the fast version of the Faster RCNN Inception ResNet V2 detector,"- **What is the top-level directory of the model you are using**:
Object detection

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04 

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
1.2

- **Bazel version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
CUDA 8.0, cuDNN 5.1

- **GPU model and memory**:
GTX 1080i, 13 Gb

- **Exact command to reproduce**:
N/A

### Describe the problem
**Feature request** Could you please release additional configurations of the Faster RCNN Inception ResNet V2 detector?

Currently released is the most accurate (and the slowest) atrous version, e.g., it uses Stride 8 and 300 proposals. 

1. Is it possible to release the fast version of this detector that was described in the original paper, i.e., Stride 16 and 50 proposals?

2. Is it possible to play with these configuration parameters without re-training?

Thank you!

",12,,[],2017-06-28 18:42:05,open,,,"['stat:contributions welcome', 'type:feature']",2019-03-09 18:26:36
1493,tensorflow/models,models,1770,kumasento,Make the WORK_DIR in download script absolute,"If `WORK_DIR` is not absolute, the `download_imagenet.sh` shell might not be able to locate synsets.txt",0,,[],2017-06-26 23:17:41,open,,,['cla: yes'],2017-09-28 02:43:09
1494,tensorflow/models,models,1766,Garima-Mishra,ResourceExhaustedError during retraining in Object detection API,"I am trying to train the mobilenet v1 with pets data set.
from tensorflow.python.client import device_lib
device_lib.list_local_devices()
gives following output 
name: GeForce GTX 750 Ti
major: 5 minor: 0 memoryClockRate (GHz) 1.15
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.93GiB

But when I run training. I get the error 
tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: OOM when allocating tensor with shape[24,128,75,75]
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.ResourceExhaustedError'>, OOM when allocating tensor with shape[24,128,75,75]

Even tried with pascal dataset and other pre-trained models also. I was able to run call models for running the object detection tutorial.
I followed the instruction given for running_locally.md
One thing I observed that all of the GPU memory get exhausted before throwing the error.
What is the minimum GPU memory required for training?",12,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-06-26 14:00:45,open,,,['stat:awaiting tensorflower'],2019-04-04 11:34:49
1495,tensorflow/models,models,1765,wxp0329,"Caused by op u'Loss/BoxClassifierLoss/Loss/sub', defined at: ","Caused by op u'Loss/BoxClassifierLoss/Loss/sub', defined at:
  File ""object_detection/train.py"", line 198, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""object_detection/train.py"", line 194, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""/home/wangxiaopeng/models-master/object_detection/trainer.py"", line 192, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""/home/wangxiaopeng/models-master/slim/deployment/model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""/home/wangxiaopeng/models-master/object_detection/trainer.py"", line 133, in _create_losses
    losses_dict = detection_model.loss(prediction_dict)
  File ""/home/wangxiaopeng/models-master/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1173, in loss
    groundtruth_classes_with_background_list))
  File ""/home/wangxiaopeng/models-master/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 1329, in _loss_box_classifier
    batch_reg_targets, weights=batch_reg_weights) / normalizer
  File ""/home/wangxiaopeng/models-master/object_detection/core/losses.py"", line 71, in __call__
    return self._compute_loss(prediction_tensor, target_tensor, **params)
  File ""/home/wangxiaopeng/models-master/object_detection/core/losses.py"", line 157, in _compute_loss
    diff = prediction_tensor - target_tensor
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 846, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2582, in _sub
    result = _op_def_lib.apply_op(""Sub"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2528, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1203, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Incompatible shapes: [1,63,4] vs. [1,64,4]
         [[Node: Loss/BoxClassifierLoss/Loss/sub = Sub[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](Loss/BoxClassifierLoss/Reshape_9, Loss/BoxClassifierLoss/stack_4)]]
         [[Node: clone_loss/_1631 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_6487_clone_loss"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]",7,"NamedUser(login=""tombstone"")","[NamedUser(login=""tombstone""), NamedUser(login=""jch1"")]",2017-06-26 12:04:38,open,,,['stat:awaiting tensorflower'],2017-08-10 07:37:12
1496,tensorflow/models,models,1760,agshift,Problem in retraining ssd_mobilenet on my own dataset,"Hi,
I am trying to retrain 'ssd_mobilenet' on my own dataset. I am using 5 classes. I converted my dataset with annotated bounding boxes to the desired TFRecord format without errors and changed the .config file as well. I then followed 'Running Locally'  section and executed:
python object_detection/train.py --logtostderr --pipeline_confi
g_path=${PATH_TO_YOUR_PIPELINE_CONFIG} --train_dir=${PATH_TO_TRAIN_DIR}

What I observe is that the loss stays constant and it gets killed after 5 steps.

INFO:tensorflow:Recording summary at step 0.
INFO:tensorflow:global step 1: loss = 0.3352 (11.099 sec/step)
INFO:tensorflow:global step 2: loss = 0.3352 (4.418 sec/step)
INFO:tensorflow:global step 3: loss = 0.3352 (5.504 sec/step)
INFO:tensorflow:global step 4: loss = 0.3352 (7.470 sec/step)
INFO:tensorflow:global step 5: loss = 0.3352 (5.705 sec/step)
Killed

Could you please give me some pointers on what I could do in solving this?
Thanks,
Amit
",17,"NamedUser(login=""jch1"")","[NamedUser(login=""jch1"")]",2017-06-25 16:25:42,open,,,[],2018-01-20 12:08:35
1497,tensorflow/models,models,1743,sleepfin,Feature request: PVAnet,"It will be great if PVAnet is added to nets factory for image classification and object detection.
[https://arxiv.org/pdf/1608.08021.pdf](https://arxiv.org/pdf/1608.08021.pdf)
I've forked the project and I'm working on PVAnet for some days.
I can submit a pull request if you need this network.",1,,[],2017-06-23 05:07:44,open,,,['stat:contributions welcome'],2017-06-23 20:31:55
1498,tensorflow/models,models,1727,korrawat,Fix inconsistent layer names in Inception v3,See #1725 for explanation,0,,[],2017-06-22 03:48:46,open,,,['cla: yes'],2017-09-28 02:43:09
1499,tensorflow/models,models,1725,korrawat,Inception v3 scope name typos?,"I was looking at TensorBoard's Graphs of Inception v3, and found that the scope name of two layers do not match their actual filter sizes. This discrepancy made me confused when tracing sizes of the tensors that were passed along the net. This results from its [slim implementation](https://github.com/tensorflow/models/blob/master/slim/nets/inception_v3.py#L217) as shown below.

Lines 216-217
```python
branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,
                       padding='VALID', scope='Conv2d_1a_1x1')
```

Lines 222-223
```python
branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,
                       padding='VALID', scope='Conv2d_1a_1x1')
```

The filter size for both layers is 3x3, but the scope name says 1x1. **Are these typos?** However, note that `Conv2d_1a_3x3` is already used as the scope for the very first layer of the whole net, if that matters. 

Trying to find how other layers in other branches are named, I believe that in each branch, the layer is called `{type}_0{char}_{filter_size}` where `char` is `a` in the first layer and `b, c, ...` in the next ones. If this is the actual naming convention, the first layer (Lines 216-217) should be named `Conv2d_0a_3x3` and the second one (Lines 222-223) `Conv2d_0c_3x3`. However, there are some other layers that do not match this assumed naming pattern.

Lines | Current name | Proposed name
:---: | :---: | :---:
171 | `Conv2d_0b_1x1` | `Conv2d_0a_1x1`
172-173 | `Conv_1_0c_5x5` | `Conv2d_0b_5x5`
**216-217** | **`Conv2d_1a_1x1`** | **`Conv2d_0a_3x3`**
**222-223** | **`Conv2d_1a_1x1`** | **`Conv2d_0c_3x3`**
225-226 | `MaxPool_1a_3x3` | `MaxPool_0a_3x3`
351-352 | `Conv2d_1a_3x3` | `Conv2d_0b_3x3`
359-360 | `Conv2d_1a_3x3` | `Conv2d_0d_3x3`
362-363 | `MaxPool_1a_3x3` | `MaxPool_0a_3x3`
376 | `Conv2d_0b_3x1` | `Conv2d_0c_3x1`

If there are other reasons for the current naming that I am not aware of, please let me know. If these are edited, the [implementation in tensorflow repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/inception_v3.py) should be edited as well.

",3,,[],2017-06-22 03:04:35,open,,,[],2018-04-06 07:44:11
1500,tensorflow/models,models,1721,conner-starsky,Error After Optimizing Model for Eval: tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value,"I have been trying to optimize a model for deployment using Tensorflow tools graph_transforms using the following configuration:
```
../../tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
	--in_graph=""/home/conner/models/object_detection/inference_graph_v1.pb"" \
	--out_graph=""/home/conner/models/object_detection/optimized_inference_graph_v1.pb"" \
	--inputs='image_tensor' \
	--outputs='detection_boxes,detection_scores,detection_classes,num_detections' \
	--transforms='
	  strip_unused_nodes(type=float, shape=""1,-1,-1,3"")
	  remove_nodes(op=Identity, op=CheckNumerics)
	  fold_constants(ignore_errors=true)
	  fold_batch_norms
	  fold_old_batch_norms'
```
This completes without error, however when I try to eval the model I get the following error:
```
Traceback (most recent call last):
  File ""benchmark_prediction_speed.py"", line 92, in <module>
    feed_dict={image_tensor: image_np_expanded})
  File ""/home/conner/.pyenv/versions/2.7.7/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/home/conner/.pyenv/versions/2.7.7/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/conner/.pyenv/versions/2.7.7/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""/home/conner/.pyenv/versions/2.7.7/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value
```
I have tried look for a operation to add to the list to ignore to stop the program from breaking, but I cannot find anything that seems to fix it. 
",7,,[],2017-06-21 19:06:29,open,,,[],2018-04-06 07:44:06
1501,tensorflow/models,models,1712,higepon,Adds train and validation loss log for tensorboard,"When running train.py, it was a bit hard to see if the loss is decreasing.
By adding the log, we will be able to see the loss for each buckets on tensorboard.",0,,[],2017-06-21 03:27:56,open,,,['cla: yes'],2017-09-28 02:43:09
1502,tensorflow/models,models,1699,callofdutyops,fix slim evaluation in slim_walkthrough.ipynb,"1. Since the `slim.evaluation.evaluation()` has been removed from `evaluation.py` (see commits fafc8b7 in tensorflow), if we call slim.evaluation.evaluation() then will get: `TypeError: 'module' object is not callable`, so this should be modified to `slim.evaluation.evaluate_once()` and accordingly, some parameters modification.
2. The return value of `evaluate_once()` is a dict not a list, so the `for loop` which is to show the `metric_values` results should be modified.
3. Minors comments modification, such as explaining cannot get exact result of `1` suming the probabilities across all classes when run on GPU.",3,,[],2017-06-20 15:07:34,open,,,"['awaiting review', 'cla: yes']",2017-12-05 13:45:58
1503,tensorflow/models,models,1597,bryancresswell,Object_detection example error,"I am in the midst of doing the example on object_detection at `https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_pets.md`, and when i run the script for create_pet_tf_record.py, i get this error.

```
python3 create_pet_tf_record.py     --label_map_path=data/pet_label_map.pbtxt     --data_dir=`pwd`     --output_dir=`pwd`
/home/cressy/Desktop/Code/models/object_detection/utils/dataset_util.py:75: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.
  if not xml:
Traceback (most recent call last):
  File ""create_pet_tf_record.py"", line 211, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""create_pet_tf_record.py"", line 206, in main
    image_dir, train_examples)
  File ""create_pet_tf_record.py"", line 175, in create_tf_record
    tf_example = dict_to_tf_example(data, label_map_dict, image_dir)
  File ""create_pet_tf_record.py"", line 90, in dict_to_tf_example
    encoded_jpg = fid.read()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 125, in read
    pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 93, in _prepare_value
    return compat.as_str_any(val)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py"", line 106, in as_str_any
    return as_str(value)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py"", line 84, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte
```",11,"NamedUser(login=""derekjchow"")","[NamedUser(login=""derekjchow"")]",2017-06-17 21:48:34,open,,,['stat:awaiting owner'],2018-11-01 15:13:10
1504,tensorflow/models,models,1594,justanhduc,Fix compatibility for python 3,'long' is no longer in py3,0,,[],2017-06-17 13:29:17,open,,,['cla: yes'],2017-09-28 02:43:08
1505,tensorflow/models,models,1592,ramananbalakrishnan,Move setup script for object_detection to the relevant sub-directory,,5,,[],2017-06-17 05:38:54,open,,,['cla: yes'],2017-09-28 02:43:08
1506,tensorflow/models,models,1553,RusAlmighty,Inception-resnet v2 implementation differ from the paper,"the implementation of [inception-resnet v2](https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py) doesn't follow the details in the [original paper ](https://arxiv.org/pdf/1602.07261.pdf). 

- The number of repetition (Inception-resnet-A,B,C) is twice than what is described in figure 15.
- the Stem block doesn't contain a concat after 3X3 MaxPool as described in Figure 
- the last ""Conv2d_7b_1x1"" isn't mentioned in the document at all

This seems to be a different net from the one published in the paper, it should bear a different name",5,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-06-14 09:55:37,open,,,[],2019-02-28 10:58:25
1507,tensorflow/models,models,1547,vineet-gupta,Compositional Kernels,,0,,[],2017-06-12 19:00:14,open,,,['cla: yes'],2017-09-28 02:43:08
1508,tensorflow/models,models,1544,axionl,Import the 'cpu_count',The ’cpu_count‘ is imported to replace the number '16'.,3,,[],2017-06-10 15:07:21,open,,,['cla: yes'],2017-09-28 02:43:08
1509,tensorflow/models,models,1527,astorfi,add accuracy for each training batch,,11,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-06-03 00:07:32,open,,,['cla: yes'],2018-04-04 18:42:00
1510,tensorflow/models,models,1509,happynoom,A robot trader using LSTM helping buying and selling stock in market,"We design a solution, named DeepTrade, including history data representation, neural network construction and trading optimization methods, which could maximizing our profit based on passed experience.",24,,[],2017-05-26 16:12:42,open,,,['cla: yes'],2018-06-12 10:56:53
1511,tensorflow/models,models,1504,ethirajsrinivasan,Update classify_image.py,Update the model to inception_v3,1,,[],2017-05-25 05:51:56,open,,,"['cla: yes', 'stat:awaiting tensorflower']",2017-09-28 02:43:06
1512,tensorflow/models,models,1497,asanakoy,Resnet_v2 wrong default preprocessing and default_image_size,"In the readme file it is specified that 

> ^ ResNet V2 models use Inception pre-processing and input image size of 299 (use --preprocessing_name inception --eval_image_size 299 when using eval_image_classifier.py).

But in the source code it is different:
1. ResNet V2 preprocessing is set to `vgg_preprocessing`  [preprocessing_factory.py#L61](https://github.com/tensorflow/models/blob/master/slim/preprocessing/preprocessing_factory.py#L61)
2. Default image size is set to 224 [https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v2.py#L219](resnet_v2.py#L219)

I don't know which source to believe. One of this is wrong.

",2,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-05-22 01:13:24,open,,,['stat:awaiting owner'],2018-04-14 17:56:24
1513,tensorflow/models,models,1493,KranthiGV,[im2txt][feature request] Finetuning the trained im2txt model on an other dataset,"### System information
- **What is the top-level directory of the model you are using**:
tensorflow/models/im2txt
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No. Not yet.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.1.0

Since the vocabulary file that is generated doesn't uniquely assign an integer to a word. In the different pass (when you load another dataset to continue training you already trained im2txt model), the output is limited to the words found in the 2nd dataset.

When I use the previous vocabulary file, it is not capturing the words in the current vocabulary. It would be great if you add a feature to augment the previous vocabulary by adding the newer vocabulary.

Thank you!  ",2,,[],2017-05-21 02:16:28,open,,,['type:feature'],2018-04-06 07:44:02
1514,tensorflow/models,models,1490,loretoparisi,Request: pre-trained text summarization model,"As in many cases in ML/DL when doing text summarization, sequence to sequence for NMT, to train LSTM it is needed the [Annotated English Gigaword](https://catalog.ldc.upenn.edu/LDC2012T21) dataset in the [LDC](https://catalog.ldc.upenn.edu/byproject) corpora.
Due to licensing issues (the Dataset license costs 6,000$), `tensorflow` cannot provide the dataset itself:

```
We used the Gigaword dataset described in Rush et al. A Neural Attention Model for Sentence Summarization.

We cannot provide the dataset due to the license. See ExampleGen in data.py about the data format. data/data contains a toy example. Also see data/vocab for example vocabulary format. In How To Run below, users can use toy data and vocab provided in the data/ directory to run the training by replacing the data directory flag.
```

But what about to provide pre-trained models to run the graph? Facebook Research fairseq [provides](https://github.com/facebookresearch/fairseq) several pre-trained models for seq2seq tasks, etc. So is it possibile to have basic pre-trained models (not necessary state-of-the-art, thought that would be the best case)? If not, that is due to licensing issues that involves data transform from one domain (text) to another (float tensors)?

Thank you",8,,[],2017-05-19 07:35:24,open,,,['type:feature'],2018-11-26 04:36:45
1515,tensorflow/models,models,1462,rylanchiu,Using of bucket in textsum,"I'd like to tackle the issue of the using of bucket in models/textsum. There is a `batch_reader` which supports bucketing. But in the `seq2seq_attention`, the length of sequence is directly defined by `hps.enc_timesteps` and `hps.dec_timesteps`.  I wonder why there is such a setting. And why don't you use dynamic_rnn instead. Do you think about making the sequence length more flexible?",2,"NamedUser(login=""panyx0718"")","[NamedUser(login=""panyx0718"")]",2017-05-11 20:13:23,open,,,['stat:awaiting tensorflower'],2017-08-24 13:42:16
1516,tensorflow/models,models,1457,ultrons,modification to resnet_model.py and resnet_main.py,"resnet_model.py:
- Replaced _batch_norm function with tf.layers.batch_normalization
It was also one of TODOs as code comment.

- resnet_model.py, resnet_main.py
Added hyperparameter options for dropout on identity and conv branch going to addition block with residual cell. As also noted in Wide Resnet Paper (https://arxiv.org/pdf/1605.07146.pdf) that dropout was found to be counter-productive on the identity branch. Authors recommend to implement it between conv layers instead. in my experiment however dropout on conv branch just before the addition actually improves test accuracy (92.98 in ~70.0k steps, keep_prob=0.9). I thought it could useful for other explorers.",2,,[],2017-05-10 06:20:22,open,,,"['cla: yes', 'stat:awaiting tensorflower']",2017-09-28 02:43:06
1517,tensorflow/models,models,1452,dubchek,Non-trainable layers are updated during fine-tuning,"### System information
- **What is the top-level directory of the model you are using**: models-master\slim
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I haven't written my code. I've only applied  https://github.com/tensorflow/models/pull/1422 and https://github.com/tensorflow/models/pull/1346 fixes which are related to reading of the datasets
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows Server 2016 (Google Cloud Platform)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: I didn't use bazel
- **CUDA/cuDNN version**: 8.0.61/5.1
- **GPU model and memory**: 1 x NVIDIA Tesla K80, 15GB RAM
- **Exact command to reproduce**: 

Checkpoint is downloaded and unzipped manually.

**set work_dir=C:\Dev\work_dir
set train_dir=%work_dir%\train_dir
set datasets=%work_dir%\datasets
set checkoints=%work_dir%\checkpoints
set curr_model=inception_resnet_v2
set curr_ckpt_date=2016_08_30
python train_image_classifier.py ^
--train_dir=%train_dir%\%curr_model% ^
--dataset_dir=%datasets%\%curr_dataset% ^
--dataset_name=%curr_dataset% ^
--dataset_split_name=train ^
--model_name=%curr_model% ^
--checkpoint_path=%checkpoints%\%curr_model%\%curr_model%_%curr_ckpt_date%.ckpt ^
--checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits ^
--trainable_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits**

So the resolved command looks like:

**python train_image_classifier.py ^
--train_dir=C:\Dev\work_dir\train_dir\inception_resnet_v2 ^
--dataset_dir=C:\Dev\work_dir\datasets\flowers ^
--dataset_name=flowers ^
--dataset_split_name=train ^
--model_name=inception_resnet_v2 ^
--checkpoint_path=C:\Dev\work_dir\checkpoints\inception_resnet_v2\inception_resnet_v2_2016_08_30.ckpt ^
--checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits ^
--trainable_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits**

### Describe the problem
I fine-tuned inception-resent-v2 model on flowers dataset. I applied appropriate flags:
--checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits ^
--trainable_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits
I expected that only Logits and AuxLogits blocks will be trained and all other layers will not be changed during the fine-tuning. However after training I have found that Tensorboard displays changes for BatchNormalization and Convolutional weights (histograms ->InceptionResnetV2 section) for such blocks as Repeat, Repeat2, Block8 etc. From my point of view it is a bug because those layers should be untouched during the fine-tuning but they are.
Could you please review this issue?

### Source code / logs
Sources:
https://drive.google.com/open?id=0B71CmqrSAIWWNGhVUG43Ulc5UFk

Original checkpoint:
https://drive.google.com/open?id=0B71CmqrSAIWWNGhVUG43Ulc5UFk

Training directory:
https://drive.google.com/open?id=0B71CmqrSAIWWSndmSnVzaUptcGM

Screenshot of changed non-trainable layers weights:
![screen shot 2017-05-07 at 1 56 39 pm](https://cloud.githubusercontent.com/assets/17811772/25780326/a7e5503c-332e-11e7-8681-684fb310a3ae.jpg)",4,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-05-07 11:14:07,open,,,['stat:awaiting tensorflower'],2018-03-02 04:03:11
1518,tensorflow/models,models,1451,guitaowufeng,[domain adaptation] NotFoundError: .../domain_adaptation/domain_separation/datasets/mnist_train.tfrecord,"## System information

### -What is the top-level directory of the model:
`~/guitao_test/models-master$ls`
```
...
 adversarial_text                domain_adaptation                 resnet
attention_ocr                   im2txt                            skip_thoughts
AUTHORS                         inception                         slim
 ...
```
I'm facing this issue-
When I follow the the instructions: 
[https://github.com/tensorflow/models/tree/master/domain_adaptation](url)
 and run 
`./bazel-bin/domain_adaptation/domain_separation/dsn_train  \
      --similarity_loss=dann_loss  \
      --basic_tower=dann_mnist  \
      --source_dataset=mnist  \
      --target_dataset=mnist_m  \
      --learning_rate=0.0117249  \
      --gamma_weight=0.251175  \
      --weight_decay=1e-6  \
      --layers_to_regularize=fc3  \
      --nouse_separation  \
      --master=""""  \
      --dataset_dir=${/home/guitao/tf_code/domain_adaptation/domain_separation/datasets}  \
      -v --use_logging`

I get this error:
`NotFoundError (see above for traceback): /home/guitao/tf_code/domain_adaptation/domain_separation/datasets/mnist_train.tfrecord`
`[[Node: parallel_read/ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](parallel_read/TFRecordReaderV2, parallel_read/filenames)]]
`
Do I need to download the mnist and mnist_m datasets to implement the experiment? If so, how can I tackle these dataset to fit the experiment?
Can you please help me how to resolve it?",3,"NamedUser(login=""bousmalis"")","[NamedUser(login=""bousmalis"")]",2017-05-07 09:07:27,open,,,[],2017-09-08 09:34:20
1519,tensorflow/models,models,1450,udnaan,Alternative instructions to enable gpu support,,1,,[],2017-05-07 04:42:47,open,,,"['awaiting review', 'cla: yes']",2017-09-28 02:43:06
1520,tensorflow/models,models,1440,sgrvinod,"""Segmentation fault (core dumped)"" while trying to run Syntaxnet on GPU","Hi, I'm new to tensorflow, and I've been trying to get syntaxnet to work on GPU.

I've tried the suggestions in #762 and #248, but these 6 tests still fail during compilation:
```
INFO: Elapsed time: 1005.136s, Critical Path: 720.82s
//syntaxnet:arc_standard_transitions_test                                PASSED in 0.0s
//syntaxnet:binary_segment_state_test                                    PASSED in 0.0s
//syntaxnet:binary_segment_transitions_test                              PASSED in 0.1s
//syntaxnet:char_ngram_string_extractor_test                             PASSED in 0.0s
//syntaxnet:char_properties_test                                         PASSED in 0.1s
//syntaxnet:char_shift_transitions_test                                  PASSED in 0.1s
//syntaxnet:head_transitions_test                                        PASSED in 0.1s
//syntaxnet:label_transitions_test                                       PASSED in 0.1s
//syntaxnet:morphology_label_set_test                                    PASSED in 0.0s
//syntaxnet:once_transitions_test                                        PASSED in 0.1s
//syntaxnet:parser_features_test                                         PASSED in 0.0s
//syntaxnet:segmenter_utils_test                                         PASSED in 0.0s
//syntaxnet:sentence_features_test                                       PASSED in 0.1s
//syntaxnet:shared_store_test                                            PASSED in 0.1s
//syntaxnet:tagger_transitions_test                                      PASSED in 0.0s
//syntaxnet/util:check_test                                              PASSED in 1.9s
//syntaxnet/util:registry_test                                           PASSED in 1.7s
//syntaxnet:whole_sentence_features_test                                 PASSED in 0.0s
//util/utf8:unicodetext_unittest                                         PASSED in 0.0s
//syntaxnet:beam_reader_ops_test                                         FAILED in 1 out of 2 in 19.3s
  /home/sgrvinod/.cache/bazel/_bazel_root/6a2ae9a751b7d16f58fb35c0b19b00c3/execroot/syntaxnet/bazel-out/local_linux-opt/testlogs/syntaxnet/beam_reader_ops_test/test.log
//syntaxnet:graph_builder_test                                           FAILED in 1 out of 2 in 57.3s
  /home/sgrvinod/.cache/bazel/_bazel_root/6a2ae9a751b7d16f58fb35c0b19b00c3/execroot/syntaxnet/bazel-out/local_linux-opt/testlogs/syntaxnet/graph_builder_test/test.log
//syntaxnet:lexicon_builder_test                                         FAILED in 1 out of 2 in 8.2s
  /home/sgrvinod/.cache/bazel/_bazel_root/6a2ae9a751b7d16f58fb35c0b19b00c3/execroot/syntaxnet/bazel-out/local_linux-opt/testlogs/syntaxnet/lexicon_builder_test/test.log
//syntaxnet:parser_trainer_test                                          FAILED in 1 out of 2 in 54.4s
  /home/sgrvinod/.cache/bazel/_bazel_root/6a2ae9a751b7d16f58fb35c0b19b00c3/execroot/syntaxnet/bazel-out/local_linux-opt/testlogs/syntaxnet/parser_trainer_test/test.log
//syntaxnet:reader_ops_test                                              FAILED in 1 out of 2 in 9.8s
  /home/sgrvinod/.cache/bazel/_bazel_root/6a2ae9a751b7d16f58fb35c0b19b00c3/execroot/syntaxnet/bazel-out/local_linux-opt/testlogs/syntaxnet/reader_ops_test/test.log
//syntaxnet:text_formats_test                                            FAILED in 1 out of 2 in 8.2s
  /home/sgrvinod/.cache/bazel/_bazel_root/6a2ae9a751b7d16f58fb35c0b19b00c3/execroot/syntaxnet/bazel-out/local_linux-opt/testlogs/syntaxnet/text_formats_test/test.log

Executed 25 out of 25 tests: 19 tests pass and 6 fail locally.
There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.

```
The log files don't seem to have any errors, except for `Segmentation fault      (core dumped)` at the end of each file. An example:
```
----------------------------------------------------------------------
Ran 6 tests in 17.048s

OK (skipped=1) # this line can vary based on the number ""skipped""
external/bazel_tools/tools/test/test-setup.sh: line 159:  1161 Segmentation fault      (core dumped) ""${TEST_PATH}"" ""$@"" 
```
(I can paste any of the complete log files here, if it helps.)

If I try to run `echo 'Bob brought the pizza to Alice.' | syntaxnet/demo.sh`, it does parse it, but I also get the same error:

```
Input: Bob brought the pizza to Alice .
Parse:
brought VBD ROOT
 +-- Bob NNP nsubj
 +-- pizza NN dobj
 |   +-- the DT det
 +-- to IN prep
 |   +-- Alice NNP pobj
 +-- . . punct
syntaxnet/demo.sh: line 56:  4346 Segmentation fault      (core dumped) $PARSER_EVAL --input=$INPUT_FORMAT --output=stdout-conll --hidden_layer_sizes=64 --arg_prefix=brain_tagger --graph_builder=structured --task_context=$MODEL_DIR/context.pbtxt --model_path=$MODEL_DIR/tagger-params --slim_model --batch_size=1024 --alsologtostderr
      4347                       (core dumped) | $PARSER_EVAL --input=stdin-conll --output=stdout-conll --hidden_layer_sizes=512,512 --arg_prefix=brain_parser --graph_builder=structured --task_context=$MODEL_DIR/context.pbtxt --model_path=$MODEL_DIR/parser-params --slim_model --batch_size=1024 --alsologtostderr
      4348                       (core dumped) | bazel-bin/syntaxnet/conll2tree --task_context=$MODEL_DIR/context.pbtxt --alsologtostderr
```

System details:
```
Ubuntu 14.04
tensorflow/tensorflow-gpu 1.1.0
bazel 0.4.5
protobuf 3.3.0
gcc 4.9.4
cuda 8.0
cudnn 5

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    Off  | 0000:02:00.0     Off |                  N/A |
| 23%   39C    P8     9W / 250W |      1MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN X (Pascal)    Off  | 0000:03:00.0     Off |                  N/A |
| 23%   37C    P8    10W / 250W |      1MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  TITAN X (Pascal)    Off  | 0000:04:00.0     Off |                  N/A |
| 23%   37C    P8     8W / 250W |      1MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  TITAN X (Pascal)    Off  | 0000:05:00.0      On |                  N/A |
| 26%   45C    P8    15W / 250W |    380MiB / 12188MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    3      1204    G   /usr/lib/xorg/Xorg                             199MiB |
|    3      2753    G   compiz                                         115MiB |
|    3      2923    G   ...el-token=F3B47FAA7A984F7C6D4069D4A1AD5C54    62MiB |
+-----------------------------------------------------------------------------+
```
Everything works fine with just the CPU.

Any help would be appreciated, thanks.

",5,"NamedUser(login=""markomernick"")","[NamedUser(login=""markomernick"")]",2017-05-04 03:16:02,open,,,[],2018-03-22 07:22:05
1521,tensorflow/models,models,1430,ivikash,Fixed abstract in seq2seq_attention,Fixes #1361 ,2,,[],2017-05-01 14:06:24,open,,,"['cla: yes', 'stat:awaiting owner']",2018-02-24 18:35:36
1522,tensorflow/models,models,1426,tobiajo,Slim: Add tf.device(/job:ps/task:0) block to global_step,"To solve [issue #1419](https://github.com/tensorflow/models/issues/1419) there we get the exception:
`ValueError: When using replicas, all Variables must have their device set: name: ""global_step""`
For when using multiple worker replicas, i.e. running distributed TensorFlow.

Solved with the accepted answer from: http://stackoverflow.com/questions/38793718/distributed-tensorflow-valueerror-when-when-using-replicas-all-variables-mus",0,,[],2017-04-28 21:31:25,open,,,"['cla: yes', 'stat:awaiting tensorflower']",2017-09-28 02:43:06
1523,tensorflow/models,models,1419,tobiajo,"Slim throws all Variables must have their device set: name: ""global_step"" after upgrade tensorflow to v1.0.0","When running [Slim](https://github.com/tensorflow/models/tree/master/slim) with multiple worker replicas I get the following exception:
```
Traceback (most recent call last):
  File ""train_image_classifier.py"", line 594, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""train_image_classifier.py"", line 396, in main
    train()
  File ""train_image_classifier.py"", line 590, in train
    sync_optimizer=optimizer if FLAGS.sync_replicas else None)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 715, in train
    init_fn=init_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 336, in __init__
    self._verify_setup()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 881, in _verify_setup
    ""their device set: %s"" % op)
ValueError: When using replicas, all Variables must have their device set: name: ""global_step""
op: ""VariableV2""
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@global_step""
    }
  }
}
attr {
  key: ""container""
  value {
    s: """"
  }
}
attr {
  key: ""dtype""
  value {
    type: DT_INT64
  }
}
attr {
  key: ""shape""
  value {
    shape {
    }
  }
}
attr {
  key: ""shared_name""
  value {
    s: """"
  }
}
```

The line numbers does not match because of that I have added [this](https://github.com/tobiajo/yarntf/commit/23fdddc8fcfb4a55a90ed8f692b0f44100c759df) to `train_image_classifier.py`.

@nathansilberman @sguada ",11,"NamedUser(login=""tfboyd"")","[NamedUser(login=""tfboyd"")]",2017-04-27 06:47:26,open,,,['type:support'],2017-09-12 02:05:00
1524,tensorflow/models,models,1418,buaa1406LiuZheng,Implementation Question about the slim Resnet Model Downsampling Position,"I found that in the slim.nets.resnet_v1 and slim.nets.resnet_v2, the implementations of Resnet have a different Downsampling Position compared with the original paper[1] and its official caffe implementation[2].

Take the resnet_v1_50 for example, it downsamples at the first three blocks i.e. block1, block2 and block3, and in each block, it downsamples at the last unit i.e. the last bottleneck, and in each bottleneck, it downsamples at the middle conv layer i.e. the 3*3 conv layer.
But in the original implementation, it downsamples at the last three blocks i.e. block2, block3 and block4, and in each block, it downsamples at the first unit, and in each bottleneck, it downsamples at the first conv layer.

I'm not sure whether it's indeed a variant implementation or I got something wrong.

[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
    Deep Residual Learning for Image Recognition. arXiv:1512.03385
[2]https://github.com/KaimingHe/deep-residual-networks",10,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-04-27 01:57:08,open,,,['stat:awaiting owner'],2018-09-19 11:58:08
1525,tensorflow/models,models,1412,mdda,InceptionV1 has inconsistent naming in Mixed_5b,"This could be made into just a comment instead, because as it stands it could **break loading of the existing checkpoint** (if done by name).

I noticed this because I was creating an equivalent Keras version, which generated the 'inception blocks' via a subroutine, making the naming completely uniform.


",3,,[],2017-04-26 13:17:12,open,,,"['cla: yes', 'stat:awaiting owner']",2018-02-24 21:26:25
1526,tensorflow/models,models,1396,JJTasi,[domain adaptation] no such package 'domain_adaptation/datasets',"### System information
-**What is the top-level directory of the model**: 
`GPU-Server1:~/domain_adaptaion$ ls `
`datasets  domain_separation  __init__.py  README.md  WORKSPACE
`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:1.0
- **Bazel version**:0.4.5
- **CUDA/cuDNN version**:8.0
- **GPU model and memory**: GeForce GTX TITAN 6GB

I'm facing this issue-
When running
`bazel build -c opt domain_separation/...`


I get this error:
`ERROR: /home/jaytest/domain_adaptaion/domain_separation/BUILD:103:1: no such package 'domain_adaptation/datasets': BUILD file not found on package path and referenced by '//domain_separation:models_test'.
ERROR: Analysis of target '//domain_separation:models_test' failed; build aborted.
INFO: Elapsed time: 0.983s`

And I run again
`bazel build -c opt domain_separation/...`

I get another error:
`ERROR: /home/jaytest/domain_adaptaion/domain_separation/BUILD:79:1: no such package 'domain_adaptation/datasets': BUILD file not found on package path and referenced by '//domain_separation:dsn_train'.
ERROR: Analysis of target '//domain_separation:dsn_train' failed; build aborted.
INFO: Elapsed time: 0.096s
`
I already checked these files are under the ""domain_separation"" folder 

Can you please help me how to resolve it?",8,"NamedUser(login=""bousmalis"")","[NamedUser(login=""bousmalis"")]",2017-04-25 05:54:07,open,,,"['models: official', 'stat:awaiting owner', 'type:build/install']",2019-02-01 21:58:54
1527,tensorflow/models,models,1376,haoran8899,wrapped_units.LayerNormBasicLSTMNetwork,"No1.
http://stackoverflow.com/questions/43509103/typeerror-init-got-an-unexpected-keyword-argument-reuse

for No1,i modify the file dragnn/python/wrapped_units.py by del del the "", reuse=True"" in 
""return tf.contrib.rnn.LayerNormBasicLSTMCell(
          num_units, layer_norm=self._attrs['layer_norm'], reuse=True)""  to slove the problem.

No2. using in dragnn model as blow:
 tagger.set_network_unit(name='wrapped_units.LayerNormBasicLSTMNetwork', hidden_layer_sizes='256)
when i run 
""text = '他們 是 親朋 好友 .'
tokens = [sentence_pb2.Token(word=word, start=-1, end=-1) for word in text.split()]
sentence = sentence_pb2.Sentence()
sentence.token.extend(tokens)
with tf.Session(graph=graph) as sess:
    # Restore the model we just trained.
    builder.saver.restore(sess, CHECKPOINT_FILENAME
)
    annotations, traces = sess.run([annotator['annotations'], annotator['traces']],
                      feed_dict={annotator['input_batch']: [sentence.SerializeToString()]})
HTML(visualization.trace_html(traces[0]))""

jupyter notebook show log as blow
[W 11:51:42.335 NotebookApp] Saving untrusted notebook trainer_tutorial.ipynb
I dragnn/core/compute_session_pool.cc:55] Destroying pool: total number of sessions created = 1
*** Error in `/usr/bin/python2.7': double free or corruption (!prev): 0x000000000109e160 ***
[I 11:52:25.327 NotebookApp] KernelRestarter: restarting kernel (1/5)
WARNING:root:kernel ad3ac582-fb41-4ffe-9a4f-e7b41ca1eabf restarted
 
source:
IN[1]: 
import os.path
import time
import random
import tensorflow as tf
from IPython.display import HTML
from tensorflow.python.platform import gfile
from tensorflow.python.platform import tf_logging as logging
from google.protobuf import text_format
from syntaxnet.ops import gen_parser_ops
from syntaxnet import load_parser_ops
from syntaxnet import task_spec_pb2
from syntaxnet import sentence_pb2
from dragnn.protos import spec_pb2 
from dragnn.python.sentence_io import ConllSentenceReader
from dragnn.python import evaluation
from dragnn.python import graph_builder
from dragnn.python import lexicon
from dragnn.python import load_dragnn_cc_impl
from dragnn.python import render_parse_tree_graphviz
from dragnn.python import render_spec_with_graphviz
from dragnn.python import spec_builder
from dragnn.python import trainer_lib
from dragnn.python import visualization

DATA_DIR = '/usr/cep/nlp/np/data/zht'
TENSORBOARD_DIR = '/usr/local/models/syntaxnet/tensorflow/tensorflow/tensorboard'
CHECKPOINT_FILENAME = '{}/zht.checkpoint'.format(DATA_DIR)
TRAINING_CORPUS_PATH = '{}/zh-ud-train.conllu'.format(DATA_DIR)
DEV_CORPUS_PATH = '{}/zh-ud-dev.conllu'.format(DATA_DIR)

assert os.path.isfile(TRAINING_CORPUS_PATH), '训练语料库不存在'

logging.set_verbosity(logging.WARN)

lexicon.build_lexicon(DATA_DIR, TRAINING_CORPUS_PATH)

lookahead = spec_builder.ComponentSpecBuilder('lookahead')
lookahead.set_network_unit(
    name='FeedForwardNetwork', hidden_layer_sizes='256')
lookahead.set_transition_system(name='shift-only', left_to_right='true')
lookahead.add_fixed_feature(name='words', fml='input.word', embedding_dim=32)
lookahead.add_rnn_link(embedding_dim=-1)
lookahead.fill_from_resources(DATA_DIR)

tagger = spec_builder.ComponentSpecBuilder('tagger')
tagger.set_network_unit(name='wrapped_units.LayerNormBasicLSTMNetwork', hidden_layer_sizes='256')
tagger.set_transition_system(name='tagger')
tagger.add_rnn_link(embedding_dim=-1)
tagger.add_token_link(source=lookahead, fml='input.focus', embedding_dim=32)
tagger.fill_from_resources(DATA_DIR)

parser = spec_builder.ComponentSpecBuilder('parser')
parser.set_network_unit(name='FeedForwardNetwork', hidden_layer_sizes='256',
                        layer_norm_hidden='true')
parser.set_transition_system(name='arc-standard')
parser.add_token_link(source=lookahead, fml='input.focus', embedding_dim=32)
parser.add_token_link(
    source=tagger, fml='input.focus stack.focus stack(1).focus',
    embedding_dim=32)

parser.add_fixed_feature(name='labels', embedding_dim=16,
                         fml=' '.join([
                             'stack.child(1).label',
                             'stack.child(1).sibling(-1).label',
                             'stack.child(-1).label',
                             'stack.child(-1).sibling(1).label',
                             'stack(1).child(1).label',
                             'stack(1).child(1).sibling(-1).label',
                             'stack(1).child(-1).label',
                             'stack(1).child(-1).sibling(1).label',
                             'stack.child(2).label',
                             'stack.child(-2).label',
                             'stack(1).child(2).label',
                             'stack(1).child(-2).label']))

parser.add_link(
        source=parser,  # recurrent connection
        name='rnn-stack',  # unique identifier
        fml='stack.focus stack(1).focus',  # look for both stack tokens
        source_translator='shift-reduce-step',  # maps token indices -> step
        embedding_dim=32)  # project down to 64 dims

parser.fill_from_resources(DATA_DIR)

master_spec = spec_pb2.MasterSpec()
master_spec.component.extend([lookahead.spec, tagger.spec, parser.spec])
HTML(render_spec_with_graphviz.master_spec_graph(master_spec))

IN[2]:
graph = tf.Graph()
with graph.as_default():
    hyperparam_config = spec_pb2.GridPoint(
        learning_method='adam',
        learning_rate=0.0005, 
        adam_beta1=0.9, adam_beta2=0.9, adam_eps=0.001,
        decay_steps=128000,
        dropout_rate=0.8, gradient_clip_norm=1,
        use_moving_average=True,
        seed=1)    
    builder = graph_builder.MasterBuilder(master_spec, hyperparam_config)
    target = spec_pb2.TrainTarget(
        name='all',
        unroll_using_oracle=[False, True, True], # train tagger & parser on gold unrolling, skip lookahead
        component_weights=[0, 0.5, 0.5]) # tagger and parser losses have equal weights
    trainer = builder.add_training_from_config(target)
    annotator = builder.add_annotation(enable_tracing=True)
    builder.add_saver()

N_STEPS = 20
BATCH_SIZE = 64
with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    training_corpus = ConllSentenceReader(
        TRAINING_CORPUS_PATH, projectivize=True).corpus()
    dev_corpus = ConllSentenceReader(DEV_CORPUS_PATH).corpus()[:200]
    for step in xrange(N_STEPS):
        trainer_lib.run_training_step(sess, trainer, training_corpus, batch_size=BATCH_SIZE)
        tf.logging.warning('Step %d/%d', step + 1, N_STEPS)
    parsed_dev_corpus = trainer_lib.annotate_dataset(sess, annotator, dev_corpus)
    pos, uas, las = evaluation.calculate_parse_metrics(dev_corpus, parsed_dev_corpus)
    tf.logging.warning('POS %.2f UAS %.2f LAS %.2f', pos, uas, las)
    builder.saver.save(sess, CHECKPOINT_FILENAME)

IN[3]:
text = '他們 是 親朋 好友 .'
tokens = [sentence_pb2.Token(word=word, start=-1, end=-1) for word in text.split()]
sentence = sentence_pb2.Sentence()
sentence.token.extend(tokens)

with tf.Session(graph=graph) as sess:
    builder.saver.restore(sess, CHECKPOINT_FILENAME)
    annotations, traces = sess.run([annotator['annotations'], annotator['traces']],
                      feed_dict={annotator['input_batch']: [sentence.SerializeToString()]})
HTML(visualization.trace_html(traces[0]))

",2,,[],2017-04-21 05:42:35,open,,,"['stat:awaiting tensorflower', 'type:bug/performance']",2018-02-26 19:15:22
1528,tensorflow/models,models,1368,cesarboucas,changed the tune corpus variable name from dev to tune,"The variable named as `tune_corpus` indicate that the treebank will be used to tune the params. While the original name `dev_corpus` suggests to use the development corpus that [should not be used for training proper](http://universaldependencies.org/conll17/data.html). One suggestion is to take 5% of the train corpus to make a tune corpus, that can be downloaded [here](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1990).",8,,[],2017-04-18 22:14:43,open,,,"['cla: yes', 'stat:awaiting response']",2018-02-24 21:25:27
1529,tensorflow/models,models,1361,ChengjinLi,"[textsum] textsum.seq2seq_attention.py line 40,the ‘headline’ value should be 'abstract'","When i run the textsum tutorials，i encounter a problem like this ""File ""~/models/textsum/batch_reader.py"", line 264, in _GetExFeatureText
    return ex.features.feature[key].bytes_list.value[0] ...  IndexError: list index out of range"" .
I found a value error in  textsum.seq2seq_attention.py line 40,  the ‘headline’ value should be replace to 'abstract'.",1,,[],2017-04-18 06:26:09,open,,,['stat:awaiting owner'],2018-02-24 18:35:53
1530,tensorflow/models,models,1346,Steven-N-Hart,Converted cPickle to _pickle for python3 and solved a jpg reading issue,"I have been using python 3.5 and had some issues I sorted out.  THought I would add them back so others wouldn't have the same Issues I have.

For example< I kept getting the `TypeError: 'jpg' has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)` error.  All I had to do was to prefix the jpg in  `slim/datasets/download_and_convert_flowers.py`.

The other change was to change `slim/datasets/download_and_convert_cifar10.py` where it imports cPickle as this was bombing in python 3.5.

The rest are just spacing changes for PEP8 compliance. ",8,,[],2017-04-14 03:33:19,open,,,"['cla: no', 'stat:awaiting owner']",2018-04-06 07:47:34
1531,tensorflow/models,models,1342,Kongsea,Use batch_norm in contrib/layers/python/layers/layers.py in Resnet model,Fix a TODO using batch_norm in contrib/layers/python/layers/layers.py in Resnet model.,5,,[],2017-04-13 01:41:28,open,,,"['cla: yes', 'stat:awaiting owner']",2018-02-24 21:23:33
1532,tensorflow/models,models,1320,xcyan,[spatial_transformer bug] what happens if the coordinates fall out of the sampling region,"# models/transformer/spatial_transformer.py
Seems that it simply takes the boundary value for sampling rather than dropping it.
However, in the torch implementation, there is such sanity check (L76-L80).
Reference: https://github.com/qassemoquab/stnbhwd/blob/master/generic/BilinearSamplerBHWD.c
",3,,[],2017-04-09 08:54:21,open,,,"['stat:community support', 'type:support']",2017-04-10 23:36:05
1533,tensorflow/models,models,1286,lixiangchun,How print the prediction probabilities in `eval_image_classifier.py`,"Currently, I followed tutorial in [https://github.com/tensorflow/models/tree/master/slim](https://github.com/tensorflow/models/tree/master/slim) to fine-tune the inception_v3 model on pathological images. Image data was converted and stored as TFRecord.

When finished fine-tuning, I run `eval_image_classifier.py` on validation set with the new model. The  `eval_image_classifier.py` just print accuracy. 

However, what I want is to print all probabilities for every class. Can anyone help me address it?  

Thanks in advance for any advise.
 ",15,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-04-01 10:02:21,open,,,"['stat:awaiting owner', 'type:feature']",2018-09-26 12:24:00
1534,tensorflow/models,models,1257,KranthiGV,[im2txt] Unable to preprocess mscoco dataset for show and tell,"I'm trying to preprocess mscoco dataset for use in im2txt model. 
I'm using tensorflow 1.0 for GPU on a GTX 1070 with 16 GB RAM.
Python version: 3.5.3
```
(tensorflow) timberners@galileo:/media/timberners/magicae/models/im2txt$ bazel-bin/im2txt/download_and_preprocess_mscoco ""${MSCOCO_DIR}""
/media/timberners/magicae/models/im2txt
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Loaded caption metadata for 82783 images from /media/timberners/magicae/models/im2txt/data/mscoco/raw-data/annotations/captions_train2014.json
Processing captions.
Finished processing 414113 captions for 82783 images in /media/timberners/magicae/models/im2txt/data/mscoco/raw-data/annotations/captions_train2014.json
Loaded caption metadata for 40504 images from /media/timberners/magicae/models/im2txt/data/mscoco/raw-data/annotations/captions_val2014.json
Processing captions.
Finished processing 202654 captions for 40504 images in /media/timberners/magicae/models/im2txt/data/mscoco/raw-data/annotations/captions_val2014.json
Creating vocabulary.
Total words: 29415
Words in vocabulary: 11519
Wrote vocabulary file: /media/timberners/magicae/models/im2txt/data/mscoco/word_counts.txt
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.7465
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.47GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
Launching 8 threads for spacings: [[0, 73296], [73296, 146592], [146592, 219888], [219888, 293184], [293184, 366480], [366480, 439776], [439776, 513072], [513072, 586368]]
Exception in thread Thread-3:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\xb4\\x00\\xb4\\x00\\x00\\xff\\xe2\\ has type str, but expected one of: bytes

Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe2\\x0cXICC_ has type str, but expected one of: bytes

Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xdb\\x00C\\x0 has type str, but expected one of: bytes

Exception in thread Thread-7:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xdb\\x00C\\x0 has type str, but expected one of: bytes
Exception in thread Thread-6:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe2\\x0cXICC_ has type str, but expected one of: bytes
Exception in thread Thread-4:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe2\\x0cTICC_ has type str, but expected one of: bytes


Exception in thread Thread-5:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe1\\x0b\\x0e has type str, but expected one of: bytes


Exception in thread Thread-8:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01\\xce\\x01\\xce\\x00\\x00\\xff\\xe2\\ has type str, but expected one of: bytes

2017-03-26 01:57:03.430551: Finished processing all 586368 image-caption pairs in data set 'train'.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
Launching 4 threads for spacings: [[0, 2533], [2533, 5066], [5066, 7599], [7599, 10132]]
Exception in thread Thread-10:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xdb\\x00C\\x0 has type str, but expected one of: bytes

Exception in thread Thread-11:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xdb\\x00C\\x0 has type str, but expected one of: bytes

Exception in thread Thread-9:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe2\\x0cXICC_ has type str, but expected one of: bytes

Exception in thread Thread-12:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01,\\x01,\\x00\\x00\\xff\\xdb\\x00C\\x0 has type str, but expected one of: bytes

2017-03-26 01:57:04.812453: Finished processing all 10132 image-caption pairs in data set 'val'.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
Launching 8 threads for spacings: [[0, 2533], [2533, 5066], [5066, 7600], [7600, 10133], [10133, 12666], [12666, 15200], [15200, 17733], [17733, 20267]]
Exception in thread Thread-16:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xfe\\x00\\x0c has type str, but expected one of: bytes

Exception in thread Thread-19:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xdb\\x00C\\x0 has type str, but expected one of: bytes

Exception in thread Thread-13:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xdb\\x00C\\x0 has type str, but expected one of: bytes

Exception in thread Thread-15:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00`\\x00`\\x00\\x00\\xff\\xdb\\x00C\\x0 has type str, but expected one of: bytes

Exception in thread Thread-20:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe1\\n\\x00XM has type str, but expected one of: bytes
Exception in thread Thread-18:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe1\\x01\\xc7 has type str, but expected one of: bytes

Exception in thread Thread-14:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01,\\x01,\\x00\\x00\\xff\\xdb\\x00C\\x0 has type str, but expected one of: bytes

Exception in thread Thread-17:
Traceback (most recent call last):
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/home/timberners/anaconda3/envs/tensorflow/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 281, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 227, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/media/timberners/magicae/models/im2txt/bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py"", line 192, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe2\\x0cXICC_ has type str, but expected one of: bytes


2017-03-26 01:57:05.852854: Finished processing all 20267 image-caption pairs in data set 'test'.

```",8,,[],2017-03-25 20:52:28,open,,,"['stat:contributions welcome', 'type:bug/performance']",2018-09-10 10:22:26
1535,tensorflow/models,models,1251,arsenious,[Neural GPU] Running in Python 3.5,"

Thank you for the code
I tried running the code in Python 3.5 (after converting the code using 2to3) , Tensorflow 1.01 but it ran into a couple of issues. Some fixes:

1) In wmt_utils.py, change line 44 to ensure everything inside re.compile is of type byte
_WORD_SPLIT = re.compile( b""(["" + _PUNCTUATION.encode() + b""])"" )

2) In data_utils.py , change line 113 to keep k as an int
k = (l-1)//2

I've just tested the bmul task and it seems to be running properly. I'll update as and when I find more issues.",10,"NamedUser(login=""lukaszkaiser"")","[NamedUser(login=""lukaszkaiser"")]",2017-03-24 01:44:07,open,,,['stat:contributions welcome'],2019-02-06 22:57:38
1536,tensorflow/models,models,1196,beeva-enriqueotero,[slim] Problem with train_lenet_on_mnist when requiring parameter server,"Hello,

I get an error when trying `train_lenet_on_mnist.sh` with `--worker_replicas=2` and `--num_ps_tasks=1`

```
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Cannot assign a device to node 'save/RestoreV2_7': Could not satisfy explicit device specification '/job:worker/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0, /job:localhost/replica:0/task:0/gpu:0, /job:localhost/replica:0/task:0/gpu:1, /job:localhost/replica:0/task:0/gpu:2, /job:localhost/replica:0/task:0/gpu:3, /job:localhost/replica:0/task:0/gpu:4, /job:localhost/replica:0/task:0/gpu:5, /job:localhost/replica:0/task:0/gpu:6, /job:localhost/replica:0/task:0/gpu:7
	 [[Node: save/RestoreV2_7 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:worker/device:CPU:0""](save/Const, save/RestoreV2_7/tensor_names, save/RestoreV2_7/shape_and_slices)]]
```
Tested on aws p2.8x instance with tensorflow_gpu-1.0.1-cp27-cp27mu-manylinux1_x86_64.whl and also compiled from source.

Thanks",20,"NamedUser(login=""sguada"")","[NamedUser(login=""sguada"")]",2017-03-17 14:07:51,open,,"NamedUser(login=""concretevitamin"")","['stat:awaiting owner', 'type:support']",2018-07-02 19:21:55
1537,tensorflow/models,models,917,yunjey,Can i install tensorflow/models package with pip install?,"tensorflow/models

I installed tensorflow through anaconda, and here I want to include the tensorflow/model.
Can i install tensorflow/models package with pip install?

I want to import the code as shown below.

import nets.inception_resnet_v2",19,"NamedUser(login=""lintian06"")","[NamedUser(login=""lintian06"")]",2017-01-18 06:59:38,open,,"NamedUser(login=""asimshankar"")","['stat:contributions welcome', 'type:build/install', 'type:feature']",2019-03-02 06:46:05
1538,tensorflow/models,models,830,inferrna,Place model formula into its archive,"my problem is very similar to https://github.com/tensorflow/models/issues/538 - I can't produce model with morphology-map and char-ngram-map files present. But the fact that pretrained models (https://github.com/tensorflow/models/blob/master/syntaxnet/universal.md) contains this files points to existence of the right formula. Can you, please, place it (train.sh, context.pbtx, etc) into each model's zip archive?",10,"NamedUser(login=""calberti"")","[NamedUser(login=""calberti"")]",2016-12-29 14:31:48,open,,,"['stat:awaiting owner', 'type:feature']",2018-02-22 19:27:22
1539,tensorflow/models,models,827,PowerAndSpeed,_bytes_feature(encoded_image) will throw an exception,"## Please let us know which model this issue is about (specify the top-level directory)
When I using ptyhon3.5.2 to run script build_mscoco_data,there will throw an exception in line
```

 context = tf.train.Features(feature={
      ""image/image_id"": _int64_feature(image.image_id),
      ""image/data"": _bytes_feature(encoded_image),
  })
```

the debug info before this has no warning,error and exception 

```
Loaded caption metadata for 200 images from /Users/apple1/Desktop/im2txt_new/im2txt/im2txt/data/raw-data/annotations/captions_train2014.json
Proccessing captions.
Finished processing 1000 captions for 200 images in /Users/apple1/Desktop/im2txt_new/im2txt/im2txt/data/raw-data/annotations/captions_train2014.json
Loaded caption metadata for 50 images from /Users/apple1/Desktop/im2txt_new/im2txt/im2txt/data/raw-data/annotations/captions_val2014.json
Proccessing captions.
Finished processing 250 captions for 50 images in /Users/apple1/Desktop/im2txt_new/im2txt/im2txt/data/raw-data/annotations/captions_val2014.json
Creating vocabulary.
Total words: 1601
Words in vocabulary: 463
Wrote vocabulary file: /tmp/word_counts.txt
Launching 8 threads for spacings: [[0, 151], [151, 302], [302, 453], [453, 605], [605, 756], [756, 907], [907, 1058], [1058, 1210]]
```
I only use 200 train images and 50 val images to test, I have processed two json files, captions and images all paired.

when step on 

```
def _bytes_feature(value):
  """"""Wrapper for inserting a bytes Feature into a SequenceExample proto.""""""
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
```
there will throw an exception

```
Exception in thread Thread-6:
Traceback (most recent call last):
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/Users/apple1/Desktop/im2txt_new/im2txt/im2txt/data/build_mscoco_data.py"", line 278, in _process_image_files
    sequence_example = _to_sequence_example(image, decoder, vocab)
  File ""/Users/apple1/Desktop/im2txt_new/im2txt/im2txt/data/build_mscoco_data.py"", line 224, in _to_sequence_example
    ""image/data"": _bytes_feature(encoded_image),
  File ""/Users/apple1/Desktop/im2txt_new/im2txt/im2txt/data/build_mscoco_data.py"", line 189, in _bytes_feature
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/internal/python_message.py"", line 517, in init
    copy.extend(field_value)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/internal/containers.py"", line 275, in extend
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/internal/containers.py"", line 275, in <listcomp>
    new_values = [self._type_checker.CheckValue(elem) for elem in elem_seq_iter]
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/internal/type_checkers.py"", line 108, in CheckValue
    raise TypeError(message)
TypeError: 'b\'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00d\\x00d\\x00\\x00\\xff\\xe2\\x0cXICC_PROFILE\\x00\\x01\\x01\\x00\\x00\\x0cHLino\\x02\\x10\\x00\\x00mntrRGB XYZ \\x07\\xce\\x00\\x02\\x00\\t\\x00\\x06\\x001\\x00\\x00acspMSFT\\x00\\x00\\x00\\x00IEC sRGB\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf6\\xd6\\x00\\x01\\x00\\x00\\x00\\x00\\xd3-HP  \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x11cprt\\x00\\x00\\x01P\\x00\\x00\\x003desc\\x00\\x00\\x01\\x84\\x00\\x00\\x00lwtpt\\x00\\x00\\x01\\xf0\\x00\\x00\\x00\\x14bkpt\\x00\\x00\\x02\\x04\\x00\\x00\\x00\\x14rXYZ\\x00\\x00\\x02\\x18\\x00\\x00\\x00\\x14gXYZ\\x00\\x00\\x02,\\x00\\x00\\x00\\x14bXYZ\\x00\\x00\\x02@\\x00\\x00\\x00\\x14dmnd\\x00\\x00\\x02T\\x00\\x00\\x00pdmdd\\x00\\x00\\x02\\xc4\\x00\\x00\\x00\\x88vued\\x00\\x00\\x03L\\x00\\x00\\ has type <class 'str'>, but expected one of: ((<class 'bytes'>,),)
```
",5,"NamedUser(login=""cshallue"")","[NamedUser(login=""cshallue""), NamedUser(login=""jvishnuvardhan"")]",2016-12-29 07:39:24,open,,,"['stat:awaiting owner', 'type:bug/performance']",2019-02-06 22:02:50
1540,tensorflow/models,models,673,petterhh,[SyntaxNet] Settings for training SyntaxNet models on UD data,"At [Parsey's Cousins](https://github.com/tensorflow/models/blob/master/syntaxnet/universal.md), you provide evaluation of tagging and parsing with the various UD datasets. However, there is no documentation on how these models were trained (in terms of arguments, etc.). I want to train SyntaxNet on the Norwegian data used (The Norwegian Dependency Treebank) with the original tag set in the treebank, but have gotten very poor results with the default settings provided in the tutorial. Do you have documentation on how these models were trained (specifically Norwegian), preferably the code itself?",3,"NamedUser(login=""calberti"")","[NamedUser(login=""calberti"")]",2016-11-21 11:56:50,open,,,['stat:awaiting owner'],2018-05-18 10:02:04
1541,tensorflow/models,models,250,dmansfield,syntaxnet: Enable building Parsey McParseface in one model to be served in TF serving,"At a high level, this PR enables the entire Parsey McParseface model to be built in a single model, exported, and served with TF serving.

This required two main changes:
1. The ability to use tensor input instead of stdin/stdout/files for feeding ""text"" input. This is really two changes - the first is to be able to pass serialized sentence protos into BeamParseReader (instead of having it read and parse text).  This is necessary to be able to connect the ""output"" of brain-tagger to the ""input"" of brain-parser.  The second part is to be able to feed ""text"" input to DocumentSource.  This is necessary if the sentences to be parsed are coming in via grpc as in TF serving, for example.
2. An actual script to build and export the Parsey model all in one session.  To accomplish this a new script is created, parsey_mcparseface.py.  This script ""replaces"" demo.sh by building both halves of the model in a single session using name_scope/variable_scope to separate the tagger and parser.  It exports the model (3 different ways) so it can be imported in a session bundle by TF serving (as well as viewed by tensorboard, and thirdly as a ""pbtxt"" file for manual inspection).

This PR has the following issues/limitations:
1. The interface to BeamParseReader and DocumentSource has changed in an incompatible way.  The input tensor (which may be used to pass text/sentence protos) is required, even when not used.  The decision to use is controlled by an attribute.
2. The model, once loaded in TF serving, does not seem to be threadsafe.
3. The script, parsey_mcparseface.py, currently has an option of where to export to, but does not function in a meaningful way if the option is not given (it just parses a hard-coded sentence in the source code in this case).
",21,,[],2016-06-29 17:54:58,open,,,"['cla: yes', 'stat:awaiting owner']",2018-02-24 21:20:07
